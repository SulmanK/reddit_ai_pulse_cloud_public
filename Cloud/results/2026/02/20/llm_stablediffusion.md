---
title: "Stable Diffusion Subreddit"
date: "2026-02-20"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["StableDiffusion", "AI Art", "Image Generation"]
---

# Overall Ranking and Top Discussions
*   1. [WAN VACE Example Extended to 1 Min Short](https://v.redd.it/y2pp6gj0ookg1) (Score: 47)
    *   This thread discusses a 1-minute WAN VACE example video, with users praising its originality and humor compared to other AI content. Comments include "Excellent" and "Loved it :D". Some users inquired about how combat animations were achieved, as this is typically difficult with Wan models.
*   2. [Using AI chat to refine Stable Diffusion prompt structure](https://www.reddit.com/r/StableDiffusion/comments/1ra102a/using_ai_chat_to_refine_stable_diffusion_prompt/) (Score: 34)
    *   This thread explores the use of AI chat (LLMs) to improve Stable Diffusion prompts. Users shared their experiences, noting that piping prompts into LLMs can be hit-or-miss and resource-intensive, while others regularly use ChatGPT for generating long, detailed prompts.
*   3. [LTX-2 - Avoid Degradation](https://v.redd.it/aqr2zweouokg1) (Score: 14)
    *   This discussion centers on a video demonstrating LTX-2 and methods to prevent degradation in AI-generated video. Users praised the video and discussed technical aspects like avoiding the last frame and improving coherence, acknowledging current limitations in character consistency and lip-syncing for longer videos.
*   4. [Multi-Image References using LTX2 in ComfyUI](https://www.reddit.com/r/StableDiffusion/comments/1ra31ft/multiimage_references_using_ltx2_in_comfyui/) (Score: 6)
    *   Users in this thread explored methods for incorporating multiple image references with LTX2 in ComfyUI. Discussions included leveraging Flux2 Klein 9B for initial image generation and using less-known nodes like "LTXV Adain Latent" for style capture.
*   5. [Built a reference-first image workflow (90s demo) - looking for SD workflow feedback](https://v.redd.it/6w897maoipkg1) (Score: 3)
    *   This thread is a call for feedback on a newly built reference-first image workflow, presented as a 90s-style demo. The workflow operates on a Brood canvas, takes 1-3 reference images, generates real-time proposals, and writes reproducible artifacts, though currently limited to macOS.
*   6. [If I want to do local video on my machine, do I need to learn Comfy?](https://www.reddit.com/r/StableDiffusion/comments/1ra529o/if_i_want_to_do_local_video_on_my_machine_do_i/) (Score: 3)
    *   This thread discusses whether learning ComfyUI is necessary for local video generation. While alternatives like Wan2GP exist, the general sentiment is that ComfyUI, despite initial setup challenges, is highly recommended for its versatility and node-based approach, which is crucial for advanced workflows.
*   7. [Using Shuttle-3-Diffusion-BF16.gguf, Forge Neo, controlnet will not work](https://www.reddit.com/r/StableDiffusion/comments/1ra6r8m/using_shuttle3diffusionbf16gguf_forge_neo/) (Score: 1)
    *   A user is seeking help because ControlNet is not working with Shuttle-3-Diffusion-BF16.gguf and Forge Neo. A clarifying question was asked about the ControlNet model's compatibility.
*   8. [Seeking advice for specific image generation questions (not "how do I start" questions)](https://www.reddit.com/r/StableDiffusion/comments/1ra37rg/seeking_advice_for_specific_image_generation/) (Score: 1)
    *   Users are seeking advanced advice on specific image generation challenges. One suggestion was to explore the Pony model, particularly Cyberrealistic for photo-realistic results, especially for unusual anthropomorphic characters.
*   9. [Help to make the jump to Klein 9b.](https://www.reddit.com/r/StableDiffusion/comments/1ra1pfc/help_to_make_the_jump_to_klein_9b/) (Score: 1)
    *   This thread is about migrating or leveraging Klein 9b for image generation. Discussions include a method for face swapping using SAM3 with Klein-9B and the desire to use Adetailer models with Klein for gender recognition and multi-face LoRA application.
*   10. [Natural language captions?](https://www.reddit.com/r/StableDiffusion/comments/1ra4ub8/natural_language_captions/) (Score: 1)
    *   The thread asks for recommendations on generating natural language captions. Suggestions included "JoyCaption Batch" and "seansomnitagprocessor v2," with a preference for the latter due to its Qwen3 base and customizable system prompts for LTX-2 training.
*   11. [Help with img2img with ip-adapter](https://www.reddit.com/r/StableDiffusion/comments/1ra0q3j/help_with_img2img_with_ipadapter/) (Score: 1)
    *   This thread seeks assistance with img2img processes using ip-adapter. A user highlighted Klein-9B's effectiveness in cleaning noisy images, specifically its ability to discern eyes through sunglasses and recommended using it to remove glasses.
*   12. [z image BASE controlnet workflow?](https://www.reddit.com/r/StableDiffusion/comments/1ra1mcb/z_image_base_controlnet_workflow/) (Score: 1)
    *   A user is asking about a "z image BASE controlnet workflow." A commenter questioned why the post was a duplicate of a recently deleted one.
*   13. [Help with stable diffusion](https://www.reddit.com/r/StableDiffusion/comments/1ra3f8u/help_with_stable_diffusion/) (Score: 0)
    *   This thread is a request for help with Stable Diffusion issues. Advice given includes manually installing CLIP and a strong recommendation to switch to ComfyUI over AUTOMATIC1111, citing ComfyUI + Z-Image Turbo as the current meta.
*   14. [LTX-2 Wan2gp (or comfyui) what are your best settings, best CFG, modality guidance, negative prompts? What works best for you?](https://www.reddit.com/r/StableDiffusion/comments/1ra1zaw/ltx2_wan2gp_or_comfyui_what_are_your_best/) (Score: 0)
    *   This thread inquires about optimal settings for LTX-2 with Wan2gp or ComfyUI. Commenters generally dismissed the idea of universal "best settings" and urged the original poster to ask more specific, granular questions outlining their desired creative outcomes.
*   15. [Which AI image generator is the most realistic?](https://www.reddit.com/r/StableDiffusion/comments/1ra2l67/which_ai_image_generator_is_the_most_realistic/) (Score: 0)
    *   This thread seeks recommendations for the most realistic AI image generators. Zimage Turbo and CyberrealisticPonyXL were suggested for realism, alongside skepticism and warnings about Higgsfield as a potentially overpriced and unreliable platform.
*   16. [Please I really want to know how this was pulled off cause it’s too good](https://v.redd.it/pnr3ou845pkg1) (Score: 0)
    *   Users in this thread are expressing awe at a highly realistic AI-generated video and are seeking to understand the techniques behind its creation. Speculation includes using LoRAs, image-to-video techniques, and discussions about the specific models for character creation, while also noting the impressive realism.
*   17. [Which AI do you recommend for anime images?](https://www.reddit.com/r/StableDiffusion/comments/1ra40is/which_ai_do_you_recommend_for_anime_images/) (Score: 0)
    *   This thread asks for recommendations for AI models best suited for generating anime images. Anima and Illustrious SDXL were frequently recommended, with distinctions drawn between Anima's prompt adherence and Illustrious's wealth of resources and LoRAs.
*   18. [Most creators waste hours filming and editing.](https://v.redd.it/oq6atpvpnpkg1) (Score: 0)
    *   This thread features a strong disagreement with the premise that creators waste hours filming and editing, with a direct negative response indicating rejection of the statement.
*   19. [How are these videos made? So fire](https://v.redd.it/jhb2run4npkg1) (Score: 0)
    *   Users are asking about the creation process of impressive videos. One commenter attributed the creation to higgsfield.ai, while another directly refuted that the video was AI-generated, creating a point of contention.

# Detailed Analysis by Thread
**[WAN VACE Example Extended to 1 Min Short (Score: 47)](https://v.redd.it/y2pp6gj0ookg1)**
*  **Summary:** This thread features a 1-minute WAN VACE example video. Users expressed high praise for its quality and originality, describing it as "Excellent" and "Loved it :D". Many found it "much more funny and original than those Ai slop with seedance 2 copycat of famous actors fighting". There was also curiosity about the technical implementation, with one user asking, "How'd you manage to get the combat animations working? By default I've never gotten Wan to be able to make anything resembling a solid hit".
*  **Emotion:** The overall emotional tone is predominantly positive, with comments expressing positive (avg score: 0.95) - 2 comments and neutral (avg score: 0.71) - 2 comments.
*  **Top 3 Points of View:**
    * Users praised the video for being funny, original, and enjoyable.
    * Questions were raised about the technical process, specifically how combat animations were achieved, as this is typically difficult with 'Wan' models.
    * There's an appreciation for the video's quality compared to 'AI slop' copying famous actors.

**[Using AI chat to refine Stable Diffusion prompt structure (Score: 34)](https://www.reddit.com/r/StableDiffusion/comments/1ra102a/using_ai_chat_to_refine_stable_diffusion_prompt/)**
*  **Summary:** This thread discusses the utility of using AI chat to refine Stable Diffusion prompt structures. Users reported mixed experiences, with one noting that piping prompts into an LLM via a ComfyUI node has been "hit or miss" and can add overhead, suggesting a need for more powerful hardware like a "5090" for better performance. Another user frequently uses ChatGPT to generate 500-600 word prompts for ideas or modifications. It was highlighted that "refining prompts through AI chat is often the key to better Stable Diffusion images," with a helpful resource sheet shared.
*  **Emotion:** The overall emotional tone is strongly neutral.
*  **Top 3 Points of View:**
    * Refining prompts through AI chat (LLMs like ChatGPT) is seen as a key strategy for improving Stable Diffusion images, with resources like spreadsheets shared for tips.
    * Some users find the process of piping prompts into LLMs (e.g., via LM Studio in ComfyUI) to be hit-or-miss and potentially slow without powerful hardware (e.g., 5090 GPU).
    * Others regularly use ChatGPT for generating long, detailed prompts (500-600 words) for image ideas or modifications.

**[LTX-2 - Avoid Degradation (Score: 14)](https://v.redd.it/aqr2zweouokg1)**
*  **Summary:** This thread discusses strategies for avoiding degradation when using LTX-2. Comments include praise like "Great video" and humorous observations such as "I like how its eye slowly morphs into an eldritch horror." Technical advice was offered, specifically to "Don't use the last frame, that one's always bad. Let the gen run for a second longer, then cut off that last second and use the new last frame," and noting that "the higher you gen, the better coherence usually is (which, of course, is often a question of VRAM)." Limitations were also discussed: "Wan has SVI pro now, which works ok, but not for lip syncing. We’re still stuck in sub-30 second land for character consistency." A user also asked for the prompt used.
*  **Emotion:** The emotional tone is mixed, with comments expressing positive (avg score: 0.68) - 2 comments, neutral (avg score: 0.86) - 2 comments, and negative (avg score: 0.52) - 1 comments.
*  **Top 3 Points of View:**
    * The video quality was praised, with some noting interesting visual effects like an eye morphing into an 'eldritch horror'.
    * Technical advice was given on how to avoid degradation, such as cutting off the last frame and generating for longer to improve coherence (which is VRAM-dependent).
    * Limitations of current tools were discussed, noting 'Wan has SVI pro' but lacks good lip-syncing, and character consistency is still limited to sub-30 second videos.

**[Multi-Image References using LTX2 in ComfyUI (Score: 6)](https://www.reddit.com/r/StableDiffusion/comments/1ra31ft/multiimage_references_using_ltx2_in_comfyui/)**
*  **Summary:** This thread explores methods for using multiple image references with LTX2 in ComfyUI. One user suggested a workflow involving "Flux2 Klein 9B" for combining multiple image references with a prompt to generate a starting image, then using an I2V workflow and trimming the initial morphing frames. Another comment pointed to the "LTXV Adain Latent" node in ComfyUI-LTXVideo as a related tool that captures style from reference latents, noting its subtle but positive effect.
*  **Emotion:** The overall emotional tone is strongly neutral.
*  **Top 3 Points of View:**
    * A suggestion for a workflow using Flux2 Klein 9B to combine multiple image references and a prompt, then refining it with I2V workflows by trimming initial morphing frames.
    * Mention of the less-known 'LTXV Adain Latent' node in ComfyUI that subtly captures style from reference latents, with documentation provided.

**[Built a reference-first image workflow (90s demo) - looking for SD workflow feedback (Score: 3)](https://v.redd.it/6w897maoipkg1)**
*  **Summary:** This thread introduces a "reference-first image workflow" presented as a "90s demo" and seeks feedback. The workflow details include it being a "Brood canvas workflow (not a Comfy node graph yet)" with input of "1–3 reference images on canvas". It involves a loop of arranging references, real-time proposals, selection, generation, and iteration, writing reproducible artifacts. A current limitation is that it is "macOS-only right now". Users commented on the aesthetic, with one saying, "The UI is kinda dope, has its own vibe," and another asking, "Inspired by Starcraft?"
*  **Emotion:** The emotional tone is mixed, with comments expressing neutral (avg score: 0.81) - 2 comments and positive (avg score: 0.50) - 1 comments.
*  **Top 3 Points of View:**
    * Feedback on the workflow details, noting it's a 'Brood canvas workflow' (not Comfy node graph yet) that uses 1-3 reference images, generates proposals, and iterates.
    * Users expressed appreciation for the 'dope' UI with its unique vibe.
    * A question about whether the workflow was inspired by Starcraft.

**[If I want to do local video on my machine, do I need to learn Comfy? (Score: 3)](https://www.reddit.com/r/StableDiffusion/comments/1ra529o/if_i_want_to_do_local_video_on_my_machine_do_i/)**
*  **Summary:** This thread poses the question of whether learning ComfyUI is essential for local video generation. Responses indicated that while one doesn't "have to" (Wan2GP is an alternative), it is highly recommended. Users described ComfyUI as simpler than alternatives once workflows are loaded, despite "installing it was a pain but it's worth it." It's noted that "comfy's nodes approach seems to be what's needed for versatility when you dont have the huge multimodal foundation models" and that "Comfy is where it's at. Don't put it off forever," especially for complex video inference workflows like LTX-2.
*  **Emotion:** The emotional tone is predominantly neutral, with comments expressing neutral (avg score: 0.78) - 5 comments and negative (avg score: 0.23) - 1 comments.
*  **Top 3 Points of View:**
    * Consensus that while you don't *have* to learn ComfyUI for local video generation (Wan2GP is an alternative), it is highly recommended for its versatility and future relevance.
    * ComfyUI is seen as simpler than alternatives once workflows are loaded, despite initial installation difficulties being a 'pain'.
    * Learning ComfyUI is deemed worthwhile for its nodes approach, which is essential for versatility without huge multimodal foundation models, especially for complex video inference workflows like LTX-2.

**[Using Shuttle-3-Diffusion-BF16.gguf, Forge Neo, controlnet will not work (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1ra6r8m/using_shuttle3diffusionbf16gguf_forge_neo/)**
*  **Summary:** This thread addresses a technical issue where ControlNet is not functioning with Shuttle-3-Diffusion-BF16.gguf and Forge Neo. A user responded by asking for clarification: "Which controlnet model are you using? Is it compatible with this model?"
*  **Emotion:** The overall emotional tone is strongly neutral.
*  **Top 3 Points of View:**
    * A user asked a clarifying question about the compatibility of the ControlNet model with the main diffusion model being used.

**[Seeking advice for specific image generation questions (not "how do I start" questions) (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1ra37rg/seeking_advice_for_specific_image_generation/)**
*  **Summary:** This thread is for users seeking specific, advanced advice on image generation rather than beginner questions. One piece of advice offered was to consider the "SDXL-based Pony model," particularly its "Cyberrealistic" fine-tune, for generating unusual anthropomorphic/furry characters and achieving good photo-realistic results in a multi-model chained workflow.
*  **Emotion:** The overall emotional tone is strongly neutral.
*  **Top 3 Points of View:**
    * A suggestion to try the SDXL-based Pony model, specifically Cyberrealistic, for generating unusual anthropomorphic/furry characters and later for photo-realistic results in multi-model chained workflows.

**[Help to make the jump to Klein 9b. (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1ra1pfc/help_to_make_the_jump_to_klein_9b/)**
*  **Summary:** This thread is focused on getting assistance with transitioning to or effectively using Klein 9b. One user shared a successful technique for face swapping: drawing on the target image in the desired swap area makes the model understand, reducing blending, and automating this with SAM3 (e.g., querying "face") works about 60% of the time, almost guaranteed with a batch size of 4. Another user expressed a desire to use Klein similarly, leveraging Adetailer models that recognize gender and apply different LoRAs to multiple faces in continuously generated images.
*  **Emotion:** The emotional tone is mixed, with comments expressing positive (avg score: 0.61) - 1 comments and neutral (avg score: 0.67) - 1 comments.
*  **Top 3 Points of View:**
    * One user shared a technique for face swapping with Klein-9B using SAM3 for segmentation masks and drawing on the target image, achieving success 60% of the time, almost guaranteed with a batch size of 4.
    * Another user expressed a desire to replicate an advanced workflow with Klein, involving Adetailer models capable of gender recognition and applying LoRAs for multiple faces in continuously generated images.

**[Natural language captions? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1ra4ub8/natural_language_captions/)**
*  **Summary:** This thread seeks recommendations for generating natural language captions. One user suggested using "[JoyCaption Batch](https://github.com/MNeMoNiCuZ/joy-caption-batch/)" with 'llama-joycaption-alpha-two-hf-llava'. Another recommended a newer tool, "[seansomnitagprocessor_v2](https://www.reddit.com/r/StableDiffusion/comments/1r5crcy/seansomnitagprocessor_v2_batch_foldersingle_video/)", which is based on Qwen3 and allows for customizable system prompts to match training guidelines for models like LTX-2, deeming it superior to Joycaption.
*  **Emotion:** The overall emotional tone is strongly neutral.
*  **Top 3 Points of View:**
    * Recommendation of 'JoyCaption Batch' with 'llama-joycaption-alpha-two-hf-llava' for generating natural language captions.
    * Another suggestion for 'seansomnitagprocessor v2' (based on Qwen3), highlighting its customization options for system prompts to match training guidelines (e.g., LTX-2) and its superiority over Joycaption's underlying model.

**[Help with img2img with ip-adapter (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1ra0q3j/help_with_img2img_with_ipadapter/)**
*  **Summary:** This thread is a request for help with img2img processes using an ip-adapter. A user shared their experience using Klein-9B to clean up "crusty" dataset images, noting that "K-9B is very good at seeing eyes through noisy sunglasses" and suggesting to "Try asking it to remove the glasses on your images."
*  **Emotion:** The overall emotional tone is strongly neutral.
*  **Top 3 Points of View:**
    * A user noted Klein-9B's effectiveness in cleaning up noisy dataset images, specifically its ability to 'see eyes through noisy sunglasses' and recommended using it to remove glasses.

**[z image BASE controlnet workflow? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1ra1mcb/z_image_base_controlnet_workflow/)**
*  **Summary:** This thread asks about a "z image BASE controlnet workflow." A commenter questioned the original poster, asking, "Why did you delete your previous post from an hour ago asking the same question? Did you try my answer?"
*  **Emotion:** The overall emotional tone is strongly neutral.
*  **Top 3 Points of View:**
    * A user inquired why the original poster deleted a previous, similar post, suggesting they might have already answered it.

**[Help with stable diffusion (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1ra3f8u/help_with_stable_diffusion/)**
*  **Summary:** This thread is a request for assistance with Stable Diffusion. Advice included needing to "install CLIP manually" by following a specific GitHub guide (step 4 for fixing installation failures). Another strong recommendation was to "Install ComfyUI instead. A1111 is dead, and has been for a long time now. Current meta is ComfyUI + models like Z-Image Turbo."
*  **Emotion:** The overall emotional tone is strongly neutral.
*  **Top 3 Points of View:**
    * Advice to manually install CLIP by following a specific GitHub guide to fix installation failures.
    * A strong recommendation to switch to ComfyUI, as AUTOMATIC1111 is considered "dead", and ComfyUI combined with models like Z-Image Turbo represents the current meta.

**[LTX-2 Wan2gp (or comfyui) what are your best settings, best CFG, modality guidance, negative prompts? What works best for you? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1ra1zaw/ltx2_wan2gp_or_comfyui_what_are_your_best/)**
*  **Summary:** This thread seeks "best settings" for LTX-2, Wan2gp, or ComfyUI, including CFG, modality guidance, and negative prompts. Commenters largely dismissed the concept of universally "best settings," explaining that if such settings existed, they wouldn't be optional. They advised the original poster to ask more specific questions, detailing the type of content they wish to create (e.g., photorealism, animation), desired outcomes, and examples for more helpful responses.
*  **Emotion:** The overall emotional tone is strongly neutral.
*  **Top 3 Points of View:**
    * The idea of universally 'best settings' for AI image generation is dismissed as unrealistic; settings depend heavily on desired outcomes and specific creative goals.
    * Users were encouraged to ask more granular questions, specifying desired output (photorealism, animation, etc.) and providing examples to get helpful advice, implying that general 'best settings' questions are too vague.

**[Which AI image generator is the most realistic? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1ra2l67/which_ai_image_generator_is_the_most_realistic/)**
*  **Summary:** This thread asks for recommendations on the most realistic AI image generators. Recommendations included "Zimage Turbo" and "CyberrealisticPonyXL," with a detailed explanation of using the latter for figures, followed by upscaling and inpainting faces, hands, feet, and eyes with specific LoRAs for "incredible detail" leading to "people that look like they’re behind glass." There was significant skepticism towards "Higgsfield," with comments calling it an "advertisement" and criticizing its "proprietary models" as "half baked and overpriced," warning users to check their Discord for complaints. Similar negative sentiment was directed at "Soul," perceiving mentions of it as repeated advertising.
*  **Emotion:** The emotional tone is mixed, with comments expressing positive (avg score: 0.81) - 2 comments and neutral (avg score: 0.69) - 5 comments.
*  **Top 3 Points of View:**
    * Recommendations for realistic image generators include Zimage Turbo and CyberrealisticPonyXL, with specific techniques like inpainting faces, hands, feet, and eyes to achieve high detail.
    * Skepticism and criticism towards 'Higgsfield' as a potential advertisement, with warnings about their proprietary models being 'half baked and overpriced' and encouraging checks of their Discord for complaints.
    * General negative sentiment and dismissal towards 'Soul' as a realistic image generator, perceiving it as repeated advertising.

**[Please I really want to know how this was pulled off cause it’s too good (Score: 0)](https://v.redd.it/pnr3ou845pkg1)**
*  **Summary:** This thread expresses strong admiration for an AI-generated video, with the original poster stating, "Please I really want to know how this was pulled off cause it’s too good." Users speculated on the creation process, suggesting "several clips edited together using a lora for the girl and mostly the same prompt," possibly with "image 2 video." There were inquiries about identifying the specific model used for the character due to a "weird noise on it." Comments also highlighted the impressive realism, with one user questioning, "Jesus, is this ai? Am I so isolated and have been looking at ai so much that fake looks real, or is this just very realistic looking?"
*  **Emotion:** The overall emotional tone is strongly neutral.
*  **Top 3 Points of View:**
    * Speculation that the video was created by editing several clips together using a LoRA for the character and consistent prompting, possibly with image-to-video techniques.
    * Questions about identifying the specific model used to create the realistic character, noting it has a 'weird noise' that might suggest 'nano banyan'.
    * Remarks on the impressive realism of the AI-generated content, with some questioning if it's truly AI or just very realistic, and a suggestion for a dedicated subreddit for "how was this made" posts.

**[Which AI do you recommend for anime images? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1ra40is/which_ai_do_you_recommend_for_anime_images/)**
*  **Summary:** This thread asks for recommendations on AI models for generating anime images. "Anima" and "Illustrious SDXL" were frequently suggested. Anima was noted for its "super good prompt adherence" despite being newer and only in preview, while Illustrious SDXL was highlighted for being "harder to prompt but has a ton of LoRAs and other resources." Other recommendations included "NoobAI and its finetunes," and the general advice to "just browse civitai images till u see one you like and use w/e model they're running."
*  **Emotion:** The emotional tone is mixed, with comments expressing neutral (avg score: 0.90) - 3 comments and positive (avg score: 0.59) - 3 comments.
*  **Top 3 Points of View:**
    * Anima and Illustrious SDXL are highly recommended, with Anima praised for prompt adherence (though still in preview) and Illustrious for its extensive LoRAs and resources.
    * Users suggest browsing Civitai images to find models that match desired styles, and generally endorse Illustrious merges.
    * NoobAI and its finetunes are also mentioned as good options.

**[Most creators waste hours filming and editing. (Score: 0)](https://v.redd.it/oq6atpvpnpkg1)**
*  **Summary:** This thread's title posits that "Most creators waste hours filming and editing." The only comment simply states "no," indicating a direct and strong disagreement with the original post's premise.
*  **Emotion:** The overall emotional tone is strongly negative.
*  **Top 3 Points of View:**
    * A direct refutation of the post's title, implying that creators do not 'waste' hours filming and editing, or that the premise is incorrect.

**[How are these videos made? So fire (Score: 0)](https://v.redd.it/jhb2run4npkg1)**
*  **Summary:** This thread expresses strong admiration for some videos, asking "How are these videos made? So fire." One commenter provided a link to an Instagram creator's page, stating that "[higgsfield.ai](http://higgsfield.ai) is in use here." However, another user offered a contradictory view, simply stating, "This is not AI."
*  **Emotion:** The emotional tone is mixed, with comments expressing neutral (avg score: 0.97) - 1 comments and negative (avg score: 0.56) - 1 comments.
*  **Top 3 Points of View:**
    * One user pointed to 'higgsfield.ai' as the tool used by the creator of the video, referencing an Instagram page.
    * Another user explicitly stated that the video is 'not AI', contradicting the previous claim or the general assumption of the post.
