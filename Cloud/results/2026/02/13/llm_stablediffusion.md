---
title: "Stable Diffusion Subreddit"
date: "2026-02-13"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["StableDiffusion", "AI", "DiffusionModels", "AIArt", "GenerativeAI"]
---

# Overall Ranking and Top Discussions
*   1. [My humble study on the effects of prompting nonexistent words on CLIP-based diffusion models.](https://drive.google.com/file/d/178pCtLQcOSvwvE7BxgBVeyAicQNKSnlw/) (Score: 29)
    *   A study exploring how nonexistent words influence CLIP-based diffusion models, receiving positive feedback and technical discussion on tokenization and model biases.
*   2. [Contest: Night of the Living Dead - The Community Cut](https://www.reddit.com/r/StableDiffusion/comments/1r3ynbt/contest_night_of_the_living_dead_the_community_cut/) (Score: 11)
    *   A community contest to collaboratively restyle 'Night of the Living Dead,' generating enthusiasm for participation and potential prizes.
*   3. [Video generation with camera control using LingBot-World](https://v.redd.it/pgsrsim9pajg1) (Score: 3)
    *   Demonstration of video generation with camera control using LingBot-World, prompting questions about hardware requirements.
*   4. [Ace-Step 1.5: "Auto" mode for BPM and keyscale?](https://www.reddit.com/r/StableDiffusion/comments/1r3xzla/acestep_15_auto_mode_for_bpm_and_keyscale/) (Score: 2)
    *   Discussion on implementing 'Auto' mode for BPM and keyscale in Ace-Step 1.5, with users sharing technical workflows and musical advice.
*   5. [There's a CFG distill lora now for Anima-preview (RDBT - Anima by reakaakasky)](https://www.reddit.com/gallery/1r3zyf8) (Score: 2)
    *   Analysis of a new CFG distill Lora for Anima-preview, noting its impact on image quality, variety, and the effectiveness of negative prompts.
*   6. [Daily dose of Absolute slop](https://v.redd.it/fwlg6fujpbjg1) (Score: 2)
    *   A casual post showcasing 'Daily dose of Absolute slop' video content, with lighthearted comments on rendering quality and dream-like experiences.
*   7. [ComfyUI RTX 5090 incredibly slow image-to-video what am I doing wrong here? (text to video was very fast)](https://www.reddit.com/r/StableDiffusion/comments/1r40t9g/comfyui_rtx_5090_incredibly_slow_imagetovideo/) (Score: 1)
    *   Troubleshooting thread for slow image-to-video generation in ComfyUI with an RTX 5090, seeking advice on configurations and hardware optimization.
*   8. [What about Qwen Image Edit 2601?](https://www.reddit.com/r/StableDiffusion/comments/1r3zdvv/what_about_qwen_image_edit_2601/) (Score: 1)
    *   Discussion about the Qwen Image Edit 2601 model, its current testing status, and community hopes for open-weighted future versions.
*   9. [First & last image - video generation with 8gb vram & 24gb ram ?](https://www.reddit.com/r/StableDiffusion/comments/1r3zpia/first_last_image_video_generation_with_8gb_vram/) (Score: 1)
    *   Inquiry and confirmation regarding the feasibility of video generation with limited VRAM (8GB) and system RAM (24GB).
*   10. [Question about LTX2](https://www.reddit.com/r/StableDiffusion/comments/1r3znv5/question_about_ltx2/) (Score: 1)
    *   Questions about LTX2 model usage, specifically concerning prompt length requirements and the effectiveness of image-to-video (i2v) workflows.
*   11. [Be honest does he have a point? LOL](https://i.redd.it/0ei648w60bjg1.png) (Score: 0)
    *   A debate on VRAM requirements for AI models, with users sharing experiences on running large models with limited hardware vs. those with high-end setups.
*   12. [Could lora, which uses video training to generate images, emerge in the future](https://www.reddit.com/r/StableDiffusion/comments/1r3ysod/could_lora_which_uses_video_training_to_generate/) (Score: 0)
    *   Discussion clarifying LoRA's role in video training for image generation, explaining it as a fine-tuning technique and practical methods for using video frames.
*   13. [Can I run wan or ltx with 5060ti 16g + 16g ram ?](https://www.reddit.com/r/StableDiffusion/comments/1r3xu9e/can_i_run_wan_or_ltx_with_5060ti_16g_16g_ram/) (Score: 0)
    *   Users seeking advice on running Wan or LTX models with specific hardware (5060ti 16G + 16G RAM) and discussing RAM as a bottleneck.
*   14. [Can anyone who’ve successfully made a lora for the Anima model mind posting their config file?](https://www.reddit.com/r/StableDiffusion/comments/1r3zn3b/can_anyone_whove_successfully_made_a_lora_for_the/) (Score: 0)
    *   A request for and sharing of a successful configuration file for training a LoRA model specifically for the Anima AI model.
*   15. [forgot the name of a specific AI image website](https://www.reddit.com/r/StableDiffusion/comments/1r3wmsg/forgot_the_name_of_a_specific_ai_image_website/) (Score: 0)
    *   A user seeking help to identify a forgotten AI image generation website based on its signup process and credit system.

# Detailed Analysis by Thread
**[My humble study on the effects of prompting nonexistent words on CLIP-based diffusion models. (Score: 29)](https://drive.google.com/file/d/178pCtLQcOSvwvE7BxgBVeyAicQNKSnlw/)**
*  **Summary:** My humble study on the effects of prompting nonexistent words on CLIP-based diffusion models. This was interesting, fun and well-written. It was limited to CLIP-based models that can use prompt weighting. This is very interesting. I noticed this behavior with SD1.5, when you give it set of random letters and some stable concept emerges. Thanks for effort and for sharing! I've done this a while since SDXL. I would actually name the mix after the random word that did the coolest look. Sometimes, you don't even need words to get the tokens to do fun things. I used to dump random things into SDXL DMD2 models. great stuff,   one of the exciting things for me,  is to be surprised by the 'occult' imagery in the model. i try with the various tools and crazy prompts - look forwards to see your findings.  thanks! Fascinating subject and a marvelously written report. I haven't done much of this kind of creative exploration of the latent space. [Humans are good in finding patterns](https://www.reddit.com/media?url=https%3A%2F%2Fi.redd.it%2Fu7bayyendeb51.jpg) This feels weirdly like watching a wizard experiment with spell creation.
*  **Emotion:** The overall tone is predominantly positive, with some neutral sentiments.
*  **Top 3 Points of View:**
    * Users expressed strong appreciation for the study's interesting findings and well-written presentation.
    * Technical suggestions were made, such as investigating tokenization, cross-model consistency, and the impact of inference settings (CFG, steps) on the observed effects.
    * Several users shared personal experiences of encountering similar emergent behaviors in models, like stable concepts from random letters or the 'occult' imagery generated by AI.

**[Contest: Night of the Living Dead - The Community Cut (Score: 11)](https://www.reddit.com/r/StableDiffusion/comments/1r3ynbt/contest_night_of_the_living_dead_the_community_cut/)**
*  **Summary:** Contest: Night of the Living Dead - The Community Cut wow, i really would love to win 3 DGX Sparks. I had this idea! But now that I’m reading this it would be crazy to restyle the whole film by myself. A collaborative effort with multiple styles would be so much better. Count me in! Send me a scene
*  **Emotion:** The overall tone is predominantly positive, with some neutral sentiments.
*  **Top 3 Points of View:**
    * Participants expressed excitement about the contest and the potential to win prizes.
    * There's enthusiasm for a collaborative approach to restyling the film, rather than individual efforts, with users offering to contribute.

**[Video generation with camera control using LingBot-World (Score: 3)](https://v.redd.it/pgsrsim9pajg1)**
*  **Summary:** Video generation with camera control using LingBot-World I’ve been looking at this a bit but haven’t played with it yet. Is a 5090 backed by 96gb system ram enough?
*  **Emotion:** The overall tone is consistently neutral.
*  **Top 3 Points of View:**
    * A user inquired about the necessary hardware specifications, specifically GPU and system RAM (e.g., 5090 with 96GB RAM), for running LingBot-World.

**[Ace-Step 1.5: "Auto" mode for BPM and keyscale? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1r3xzla/acestep_15_auto_mode_for_bpm_and_keyscale/)**
*  **Summary:** Ace-Step 1.5: "Auto" mode for BPM and keyscale? A user named Tremolo28 posted a workflow a few days ago where they had this working. I borrowed the trick from that workflow. Ask LLM about songs and their keys to use for reference. Ace Step docs say that particular scales are handled better than others.
*  **Emotion:** The overall tone is consistently neutral.
*  **Top 3 Points of View:**
    * Users provided technical guidance on how to set up an 'auto' mode for BPM and keyscale, leveraging existing workflow tricks and text embed nodes for random selection.
    * Advice was given on the musical aspects, suggesting to consult LLMs for song keys, understand emotional connotations of major/minor scales, and start with common scales like E, D, or C with 4/4 or 3/4 time signatures.

**[There's a CFG distill lora now for Anima-preview (RDBT - Anima by reakaakasky) (Score: 2)](https://www.reddit.com/gallery/1r3zyf8)**
*  **Summary:** There's a CFG distill lora now for Anima-preview (RDBT - Anima by reakaakasky) I find there is just generally a massive drop-off in quality. negative prompts are very strong with Anima. >The primary drawback is that it makes many artists much weaker. And variety between different seeds is much lower.
*  **Emotion:** The overall tone is consistently neutral.
*  **Top 3 Points of View:**
    * The CFG distill Lora for Anima-preview results in a general drop in output quality.
    * This Lora reduces the variety of images generated from different seeds.
    * Negative prompts are particularly effective and strong when used with the Anima model.

**[Daily dose of Absolute slop (Score: 2)](https://v.redd.it/fwlg6fujpbjg1)**
*  **Summary:** Daily dose of Absolute slop Rendered in 1920x1920 so finally Reddit let's it be 1080p, noice lol me trying to run in a dream
*  **Emotion:** The overall tone is consistently neutral.
*  **Top 3 Points of View:**
    * A user acknowledged the improved video quality due to higher resolution rendering.
    * A humorous observation comparing the video content to the common experience of 'trying to run in a dream'.

**[ComfyUI RTX 5090 incredibly slow image-to-video what am I doing wrong here? (text to video was very fast) (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1r40t9g/comfyui_rtx_5090_incredibly_slow_imagetovideo/)**
*  **Summary:** ComfyUI RTX 5090 incredibly slow image-to-video what am I doing wrong here? (text to video was very fast) How much system RAM you got? Well first off the --gpu-only flag probably shouldn't be used when running LTX2 on a 5090 Did you use a script for the portable to ensure the correct wheels for torch and cuda were installed? You could also try using stability matrix to install comfyui since I think it automatically detects and installs the proper wheels for your hardware.
*  **Emotion:** The overall tone is consistently neutral.
*  **Top 3 Points of View:**
    * Users highlighted system RAM as a potential bottleneck for image-to-video generation.
    * One suggestion was to avoid the `--gpu-only` flag when running LTX2 on a 5090.
    * Advice included checking for correct Torch and CUDA wheel installations or using tools like Stability Matrix for proper ComfyUI setup.

**[What about Qwen Image Edit 2601? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1r3zdvv/what_about_qwen_image_edit_2601/)**
*  **Summary:** What about Qwen Image Edit 2601? They keep dribbling out more information, but there was a tweet a day or so ago about how they fixed an issue with the model with how it handled something, so they might be using the qwen chat as a means of quality assurance testing. It's free with no login to try until the weights are out: [https://chat.qwen.ai/?spm=a2ty\_o06.30285417.0.0.68a2c921Kxdkvc&inputFeature=t2i](https://chat.qwen.ai/?spm=a2ty_o06.30285417.0.0.68a2c921Kxdkvc&inputFeature=t2i) They just talked about Qwen2... Qwen1 imo is ded. No one is going to develop for it compared to the much lighter Klein and Z-Image currently. I hope they open weight Qwen2 and it's a third model in the "consumer" category.
*  **Emotion:** The overall tone is consistently neutral.
*  **Top 3 Points of View:**
    * Qwen's current model (Qwen Image Edit 2601) is possibly being used for quality assurance via the chat interface before weights are released.
    * Qwen1 is seen as outdated, with the community looking towards Qwen2 and lighter alternatives like Klein and Z-Image.
    * There is an expressed hope for Qwen2 to be open-weighted, making it accessible as a strong contender in the consumer model market.

**[First & last image - video generation with 8gb vram & 24gb ram ? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1r3zpia/first_last_image_video_generation_with_8gb_vram/)**
*  **Summary:** First & last image - video generation with 8gb vram & 24gb ram ? Yes, I've got it to work at 480p resolution for 3 seconds.
*  **Emotion:** The overall tone is consistently positive.
*  **Top 3 Points of View:**
    * It is possible to achieve video generation with 8GB VRAM and 24GB RAM, albeit at lower resolutions and short durations.

**[Question about LTX2 (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1r3znv5/question_about_ltx2/)**
*  **Summary:** Question about LTX2 use i2v and feed it with a starting image of the breed you want. Try using their Gemma prompt enhancer node. LTX needs long prompts, didn’t have much luck with short prompts like the one you’ve described. Also, did you try i2v workflows? Probably could help you get that specific breed more accurately.
*  **Emotion:** The overall tone is consistently neutral.
*  **Top 3 Points of View:**
    * Users suggested utilizing image-to-video (i2v) workflows and providing a starting image to achieve more accurate generation of specific breeds with LTX2.
    * Advice was given to use the Gemma prompt enhancer node and to employ longer, more descriptive prompts, as LTX2 may not perform well with short prompts.

**[Be honest does he have a point? LOL (Score: 0)](https://i.redd.it/0ei648w60bjg1.png)**
*  **Summary:** Be honest does he have a point? LOL Nah. A lot of WF and LoRA's are trained by people who have 24GB+ VRAM so there are plenty of ressources for that. Some casess are a content desert, I have 96GB VRAM (6000 PRO Blackwell) and would like to push LTX-2 to it's limits. Not a lot of ressource for that sort of thing. By "lenovo lora" they mean a checkpoint which is a model that has been refined/distilled/fine tuned to produce a smaller file size while retaining reasonably decent results. Flux is too big for this sub. He will wait for distill loras and maybe more optimizations in comfyui to make it faster. Lora ran great on my 24gb card even though I explained the entire process. Even with the RAM apocalypse, 16GB NVIDIA GPUs are still within reach for a lot of people. I run everything locally on a 4060 Ti 16GB and 64GB of RAM. You can rent an RTX PRO 6000 Blackwell for about $0.99/hour. Meh, a lot of people who don't have a large amount of VRAM, like me, simply offload it to RAM or even swap on disk. I mean, even I use Flux 2 Dev (GGUF) with my 10GB VRAM and 32GB RAM, which is just about the same speed as Z-Image model without any accelerations. He has a point. Most people try to run things with 8GB VRAM. A lot of posts are "What can I do with my hamster powered calculator"
*  **Emotion:** The overall tone is predominantly neutral, with mixed sentiments including negative and positive.
*  **Top 3 Points of View:**
    * Many users with limited VRAM (e.g., 8GB) struggle to run large AI models, leading to frustration and a perception of hardware inadequacy.
    * Conversely, some argue that 16GB VRAM GPUs are reasonably accessible, and solutions like RAM offloading, disk swapping, or renting cloud GPUs offer viable alternatives for users with less powerful local hardware.
    * There's a debate about model size vs. efficiency: some larger models are criticized for being excessively large when smaller, optimized models can produce comparable results, while Lora creators feel pressured to make models smaller despite the quality trade-offs.

**[Could lora, which uses video training to generate images, emerge in the future (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1r3ysod/could_lora_which_uses_video_training_to_generate/)**
*  **Summary:** Could lora, which uses video training to generate images, emerge in the future Microsoft made deals with OpenAI to accquire the rights to GPT 3 and future variants. Microsoft formed a task-force to investigate the ways in which their newly acquired beast can be tamed efficiently. They came up with a solution called "Low Rank Adaptation for Large Language Model What do you think a LoRa is exactly? Not sure what your exact angle is here but you can do this with Wan Video 2.2. You can feed video to the lora trainer and then set 1 frame when running the video model. a video is just a series of images, and since you don't need any of the temporal data, you can just use still images from the video for your image lora training.
*  **Emotion:** The overall tone is consistently neutral.
*  **Top 3 Points of View:**
    * LoRA (Low Rank Adaptation) is a parameter-efficient fine-tuning technique applicable to various deep learning models, including those for video and images, rather than a method that uses video training to directly generate images.
    * The process of using video for image generation involves treating video as a sequence of still images, from which individual frames can be extracted and used for image LoRA training, eliminating the need for temporal data.
    * Tools like Wan Video 2.2 already support this functionality, allowing users to train a LoRA with video input and then generate single images using the video model.

**[Can I run wan or ltx with 5060ti 16g + 16g ram ? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1r3xu9e/can_i_run_wan_or_ltx_with_5060ti_16g_16g_ram/)**
*  **Summary:** Can I run wan or ltx with 5060ti 16g + 16g ram ? The ram is going to be your bottleneck, I have 24Gb and the same GPU as you and I am absolutely struggling. With 32GB I’d be in better shape, but it honestly seems 64gb is the minimum requirement for reliable generations You can run LTX 2 on 16GB of RAM, but it's going to be tight. Video generation is a resource hog, so keep an eye on the RAM, SSD and GPU tabs in your task manager.
*  **Emotion:** The overall tone is predominantly neutral, with some negative sentiments.
*  **Top 3 Points of View:**
    * 16GB of system RAM is identified as a significant bottleneck for running demanding models like LTX 2 and Wan, with 64GB often being recommended for reliable video generation.
    * Running with insufficient system RAM, even with a capable GPU, will likely lead to heavy use of SSD swap space, which can negatively impact the SSD's lifespan.
    * While challenging, it is possible to run LTX 2 on a 16GB VRAM GPU with 16GB system RAM, but it will be a constrained experience, limited to shorter, lower-resolution videos, and users should monitor hardware usage closely.

**[Can anyone who’ve successfully made a lora for the Anima model mind posting their config file? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1r3zn3b/can_anyone_whove_successfully_made_a_lora_for_the/)**
*  **Summary:** Can anyone who’ve successfully made a lora for the Anima model mind posting their config file? My config for anima_train_network.py was probably far from ideal parameters, but it did train a multi-subject LoRA.
*  **Emotion:** The overall tone is consistently neutral.
*  **Top 3 Points of View:**
    * A user provided a detailed `anima_train_network.py` configuration file, which successfully trained a multi-subject LoRA for the Anima model, even if the parameters were considered non-optimal.

**[forgot the name of a specific AI image website (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1r3wmsg/forgot_the_name_of_a_specific_ai_image_website/)**
*  **Summary:** forgot the name of a specific AI image website also more info you had to sign up to use it and it had a "Credit system" and said that "1 image was 1 credit" also the sign up was just - email - password - confirm password (type it again to make sure you can remember it) this is basically all I remember tensorart? [civitai](https://civitai.com/) maybe?
*  **Emotion:** The overall tone is consistently neutral.
*  **Top 3 Points of View:**
    * The original poster described a forgotten AI image website as having an email/password signup, a credit system where '1 image was 1 credit,' and that's all they remembered.
    * Community members attempted to identify the website, suggesting names like 'Tensor.art' and 'Civitai' based on the provided details.
