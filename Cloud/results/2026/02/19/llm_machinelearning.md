---
title: "Machine Learning Subreddit"
date: "2026-02-19"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machine learning", "AI", "research", "data science", "competitions", "career"]
---

# Overall Ranking and Top Discussions
*   1. [[D] Why are serious alternatives to gradient descent not being explored more?](https://www.reddit.com/r/MachineLearning/comments/1r8l11x/d_why_are_serious_alternatives_to_gradient/) (Score: 113)
    * There are experiments with non-gradient descent approaches, genetic algorithms come to mind. I'm sure there are more and others. I don't think the problem is that no one explores them, I think it's rather that gradient descent is very hard to beat and until now the best we have. If you look at the optimization literature, it's rather wide (you have 2nd order methods, natural gradient methods, non-gradient methods, Bayesian methods, and so forth). First-order methods are popular not because alternatives don't exist, they are popular because empirically they have shown the best performance on the widest range of problems. I worked on evolutionary algorithms (my phd was on this), and as others have said, EA performs well but gradient descent still outperforms EA. EA takes way longer to converge as compared to gradient descent. Defining terms before good-faith argument: how would you define "gradient descent"? Would you consider [Fisher Scoring](https://en.wikipedia.org/wiki/Scoring_algorithm) gradient descent? [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method)? People are exploring. No one has found anything that is a clear improvement. This AI industry only cares about impact. There isn‚Äôt much money put into alternative training methods. Backprop + GD and variants continue to perform the best in empirical results of model performance vs. compute efficiency. Geoff Hinton doesn't think they are the golden path today. There has been research on forward forward methods. As a professor, I see many common dogmas from students. Stochastic gradient descent is so simple, surely it can't work as well as <complicated method X>. Optimizing on random batches is simple. Writing everything in Pytorch is restrictive. Backprop is flawed, but there is no clear explanation as to what makes it flawed. If the real question is how to solve continual learning, then let's tackle that directly. For now the cost doesn't justify it.
*   2. [[R] Analysis of 350+ ML competitions in 2025](https://www.reddit.com/r/MachineLearning/comments/1r8y1ha/r_analysis_of_350_ml_competitions_in_2025/) (Score: 90)
    * Thank you for this report - very interesting read. I'm hoping people start sharing more on agentic coding tools / setup in the coming year. Did you notice winning solutions mention - Claude / Codex / GPT etc ? I'm surprised the proportion of projects using lightning is not higher. I wonder if this is because a lot of solutions are probably using huggingface models, which you then have to wrap in order to use lightning, but if you aren't building the model yourself, there maybe isn't a reason to do that. Thanks for the positive feedback! If you‚Äôd like to support this work, please do [share it](https://mlcontests.com/state-of-machine-learning-competitions-2025/) with anyone else who might find it interesting, or check out my [cloud GPU comparison page](https://cloud-gpus.com/?ref=state25reddit) or [ML-focused online magazine](https://joltml.com/?ref=state25reddit). Polars found that competition winners didn't adopt tabular foundation model (TabPFN) meaningfully. TabPFN is one of the trends to watch for 2025. Do you possibly have data points regarding audio competitions featuring non-human sounds? Like music genre classification etc. Really interesting to read - good to see people are still winning competitions with older hardware!
*   3. [[D] CVPR Decisions](https://www.reddit.com/r/MachineLearning/comments/1r92ln9/d_cvpr_decisions/) (Score: 29)
    * Will I be going to Denver or will it be Sweden‚Ä¶.legends know it may just be Budapest üòå The BR was one of the shortest, lowest quality reviews I've ever received. The WR seemed to generally like the paper and provided a thorough review. I think I submitted a good rebuttal. Last year the link below could be used to see results. Only accepted authors could access it. We can try it tomorrow :) [https://openreview.net/group/info?id=thecvf.com/CVPR/2026/Conference/Authors/Accepted](https://openreview.net/group/info?id=thecvf.com/CVPR/2026/Conference/Authors/Accepted) Results out in how many hours?
*   4. [[R] The "Data Scientist" title is the worst paying title in ML (EMEA).](https://www.reddit.com/r/MachineLearning/comments/1r97em2/r_the_data_scientist_title_is_the_worst_paying/) (Score: 10)
    * You won't find many DS paid 110k in France. You usually start around 40k and will plateau around 80k at the end of your career. Only companies that pay that much are non-French companies, or a few startups like Mistral, Huggingface etc. Data Scientists are now just data analysts. ML is optional but you need to know your SQL and statistics. Data Scientist has always been a generalist role, doing a wide range of stuff like data munging, statistics, ML, visualization etc. All that's changed is that ML has become so big it merits its own career track now. The term "Data Scientist" has become a catch-all term. Every company has diluted the Data Scientist role responsibilities. Some companies are fragmenting the role more than others. HR at most orgs have no real clue about this. HR will pull compensation data from. My guess is that tech salaries being higher than non-tech industries is a big factor. Data scientists are less often in the tech industry than the others. Data Scientist gets paid more than a geographer with a jupyter notebook. Average salary for data scientists in EMEA is definitely not 127k. Average total comp for big tech data scientists in top cities, maybe. Can you provide a source? is this self reported? A data scientist does not typically make close to 125k EUR in Stockholm. This must be a very specific set of high paying companies you are looking at. This makes sense to me. MLE gets paid more than DS because it's more specialized and requires more experience and SWE chops. Just like how DS is paid more than DA - it's a more advanced role. Where did you even get these numbers? These are not the salaries people are getting in any of these cities. There would be some exceptional cases, but this is 100% not the average.
*   5. [[D] Research on Self-supervised fine tunning of "sentence" embeddings?](https://www.reddit.com/r/MachineLearning/comments/1r8y58b/d_research_on_selfsupervised_fine_tunning_of/) (Score: 6)
    * Graph neural networks (GNNs) and graph transformers are useful for learning. In the past, when working with encoder models in a research lab, we would concatenate the mean, min and max of the token embeddings to create a sentence embedding. If you don't want feature explosion, you can apply SVD or NMF. * [Deep Sets](https://arxiv.org/pdf/1703.06114). The invariant version is super simple, just MLP(sum(MLP(x\_i)) * Learnable query token. Like [CLS] but completely general and could be fine-tuned. * A [PNA](https://arxiv.org/pdf/2004.05718)\-like aggregator. Basically just gathering higher-order statistics along each feature dimension.
*   6. [[R] Predicting Edge Importance in GPT-2's Induction Circuit from Weights Alone (œÅ=0.623, 125x speedup)](https://www.reddit.com/r/MachineLearning/comments/1r94poz/r_predicting_edge_importance_in_gpt2s_induction/) (Score: 6)
    * Too long to read, put up a GitHub, write it up, submit to a workshop and then disseminate it. The process will give you some feedback and structure your work better than a bill wall of text. The subreddit is not your personal research journal.
*   7. [[D] Which hyperparameters search library to use?](https://www.reddit.com/r/MachineLearning/comments/1r8v4fn/d_which_hyperparameters_search_library_to_use/) (Score: 4)
    * Katib, PyCaret, and basically [this](https://github.com/askery/automl-list) Optuna is the most convenient in my opinion. Optuna is very simple to use, and the default TPE sampler performs very well. Mljar I‚Äôd go with optuna. They have a decent documentation.
*   8. [[P] I Made my first Transformer architecture code](https://www.reddit.com/r/MachineLearning/comments/1r8v5x2/p_i_made_my_first_transformer_architecture_code/) (Score: 0)
    * Not the place my dude, try r/learnmachinelearning

# Detailed Analysis by Thread
**[ [D] Why are serious alternatives to gradient descent not being explored more? (Score: 113)](https://www.reddit.com/r/MachineLearning/comments/1r8l11x/d_why_are_serious_alternatives_to_gradient/)**
*  **Summary:** There are experiments with non-gradient descent approaches, genetic algorithms come to mind. I'm sure there are more and others. I don't think the problem is that no one explores them, I think it's rather that gradient descent is very hard to beat and until now the best we have. If you look at the optimization literature, it's rather wide (you have 2nd order methods, natural gradient methods, non-gradient methods, Bayesian methods, and so forth). First-order methods are popular not because alternatives don't exist, they are popular because empirically they have shown the best performance on the widest range of problems. I worked on evolutionary algorithms (my phd was on this), and as others have said, EA performs well but gradient descent still outperforms EA. EA takes way longer to converge as compared to gradient descent. Defining terms before good-faith argument: how would you define "gradient descent"? Would you consider [Fisher Scoring](https://en.wikipedia.org/wiki/Scoring_algorithm) gradient descent? [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method)? People are exploring. No one has found anything that is a clear improvement. This AI industry only cares about impact. There isn‚Äôt much money put into alternative training methods. Backprop + GD and variants continue to perform the best in empirical results of model performance vs. compute efficiency. Geoff Hinton doesn't think they are the golden path today. There has been research on forward forward methods. As a professor, I see many common dogmas from students. Stochastic gradient descent is so simple, surely it can't work as well as <complicated method X. Optimizing on random batches is simple. Writing everything in Pytorch is restrictive. Backprop is flawed, but there is no clear explanation as to what makes it flawed. If the real question is how to solve continual learning, then let's tackle that directly. For now the cost doesn't justify it.
*  **Emotion:** Mostly Neutral, with some positive and negative sentiments.
*  **Top 3 Points of View:**
    * Gradient descent (GD) and its variants (like backprop) are empirically the most effective and efficient methods for a wide range of problems, making them hard to beat.
    * Alternatives like genetic algorithms or 2nd-order methods are being explored but often take longer to converge or haven't shown clear improvements that justify their cost/complexity.
    * The AI industry prioritizes impact and performance; without clear advantages, funding and adoption for alternative training methods are limited.
**[ [R] Analysis of 350+ ML competitions in 2025 (Score: 90)](https://www.reddit.com/r/MachineLearning/comments/1r8y1ha/r_analysis_of_350_ml_competitions_in_2025/)**
*  **Summary:** Thank you for this report - very interesting read. I'm hoping people start sharing more on agentic coding tools / setup in the coming year. Did you notice winning solutions mention - Claude / Codex / GPT etc ? I'm surprised the proportion of projects using lightning is not higher. I wonder if this is because a lot of solutions are probably using huggingface models, which you then have to wrap in order to use lightning, but if you aren't building the model yourself, there maybe isn't a reason to do that. Thanks for the positive feedback! If you‚Äôd like to support this work, please do [share it](https://mlcontests.com/state-of-machine-learning-competitions-2025/) with anyone else who might find it interesting, or check out my [cloud GPU comparison page](https://cloud-gpus.com/?ref=state25reddit) or [ML-focused online magazine](https://joltml.com/?ref=state25reddit). Polars found that competition winners didn't adopt tabular foundation model (TabPFN) meaningfully. TabPFN is one of the trends to watch for 2025. Do you possibly have data points regarding audio competitions featuring non-human sounds? Like music genre classification etc. Really interesting to read - good to see people are still winning competitions with older hardware!
*  **Emotion:** Primarily Positive.
*  **Top 3 Points of View:**
    * The report provides interesting insights into ML competitions, with appreciation for the data and analysis.
    * Observations on specific tools: Polars hasn't seen significant adoption among competition winners due to Pandas' integration with scikit-learn, while the use of Lightning is surprisingly not higher.
    * Interest in future trends includes agentic coding tools (like Claude/Codex/GPT) and the potential of tabular foundation models (TabPFN) to challenge XGBoost.
**[ [D] CVPR Decisions (Score: 29)](https://www.reddit.com/r/MachineLearning/comments/1r92ln9/d_cvpr_decisions/)**
*  **Summary:** Will I be going to Denver or will it be Sweden‚Ä¶.legends know it may just be Budapest üòå The BR was one of the shortest, lowest quality reviews I've ever received. The WR seemed to generally like the paper and provided a thorough review. I think I submitted a good rebuttal. Last year the link below could be used to see results. Only accepted authors could access it. We can try it tomorrow :) [https://openreview.net/group/info?id=thecvf.com/CVPR/2026/Conference/Authors/Accepted](https://openreview.net/group/info?id=thecvf.com/CVPR/2026/Conference/Authors/Accepted) Results out in how many hours?
*  **Emotion:** Overwhelmingly Neutral.
*  **Top 3 Points of View:**
    * Researchers are awaiting the CVPR decisions, with speculation about acceptance locations (Denver, Sweden, Budapest).
    * Review experiences vary, with some receiving low-quality reviews and others hopeful for score increases after rebuttals.
    * There's anticipation for where and when results will be officially released, referencing a previous year's link for accepted authors.
**[ [R] The "Data Scientist" title is the worst paying title in ML (EMEA). (Score: 10)](https://www.reddit.com/r/MachineLearning/comments/1r97em2/r_the_data_scientist_title_is_the_worst_paying/)**
*  **Summary:** You won't find many DS paid 110k in France. You usually start around 40k and will plateau around 80k at the end of your career. Only companies that pay that much are non-French companies, or a few startups like Mistral, Huggingface etc. Data Scientists are now just data analysts. ML is optional but you need to know your SQL and statistics. Data Scientist has always been a generalist role, doing a wide range of stuff like data munging, statistics, ML, visualization etc. All that's changed is that ML has become so big it merits its own career track now. The term "Data Scientist" has become a catch-all term. Every company has diluted the Data Scientist role responsibilities. Some companies are fragmenting the role more than others. HR at most orgs have no real clue about this. HR will pull compensation data from. My guess is that tech salaries being higher than non-tech industries is a big factor. Data scientists are less often in the tech industry than the others. Data Scientist gets paid more than a geographer with a jupyter notebook. Average salary for data scientists in EMEA is definitely not 127k. Average total comp for big tech data scientists in top cities, maybe. Can you provide a source? is this self reported? A data scientist does not typically make close to 125k EUR in Stockholm. This must be a very specific set of high paying companies you are looking at. This makes sense to me. MLE gets paid more than DS because it's more specialized and requires more experience and SWE chops. Just like how DS is paid more than DA - it's a more advanced role. Where did you even get these numbers? These are not the salaries people are getting in any of these cities. There would be some exceptional cases, but this is 100% not the average.
*  **Emotion:** Mostly Neutral, with some positive and negative sentiments.
*  **Top 3 Points of View:**
    * The 'Data Scientist' role has become a broad, catch-all term, leading to dilution of responsibilities and varying expectations, often blending with Data Analyst duties.
    * Compensation for Data Scientists, especially in EMEA, might be lower than perceived averages or specific high-paying tech roles, with actual salaries varying significantly by country and company type (tech vs. non-tech).
    * More specialized roles like Machine Learning Engineers (MLEs) are typically better compensated due to requiring stronger software engineering skills and expertise in cutting-edge ML architectures.
**[ [D] Research on Self-supervised fine tunning of "sentence" embeddings? (Score: 6)](https://www.reddit.com/r/MachineLearning/comments/1r8y58b/d_research_on_selfsupervised_fine_tunning_of/)**
*  **Summary:** Graph neural networks (GNNs) and graph transformers are useful for learning. In the past, when working with encoder models in a research lab, we would concatenate the mean, min and max of the token embeddings to create a sentence embedding. If you don't want feature explosion, you can apply SVD or NMF. * [Deep Sets](https://arxiv.org/pdf/1703.06114). The invariant version is super simple, just MLP(sum(MLP(x\_i)) * Learnable query token. Like [CLS] but completely general and could be fine-tuned. * A [PNA](https://arxiv.org/pdf/2004.05718)\-like aggregator. Basically just gathering higher-order statistics along each feature dimension.
*  **Emotion:** Overwhelmingly Neutral.
*  **Top 3 Points of View:**
    * Graph Neural Networks (GNNs) and Graph Transformers are relevant areas for self-supervised fine-tuning of embeddings, particularly for pooling operations.
    * Methods for creating sentence embeddings include concatenating mean, min, and max of token embeddings, potentially followed by dimensionality reduction (SVD, NMF).
    * Other advanced aggregation techniques for learning on sets include Deep Sets, learnable query tokens (like a generalizable [CLS] token), and PNA-like aggregators for higher-order statistics.
**[ [R] Predicting Edge Importance in GPT-2's Induction Circuit from Weights Alone (œÅ=0.623, 125x speedup) (Score: 6)](https://www.reddit.com/r/MachineLearning/comments/1r94poz/r_predicting_edge_importance_in_gpt2s_induction/)**
*  **Summary:** Too long to read, put up a GitHub, write it up, submit to a workshop and then disseminate it. The process will give you some feedback and structure your work better than a bill wall of text. The subreddit is not your personal research journal.
*  **Emotion:** Overall Neutral.
*  **Top 3 Points of View:**
    * The research post discusses predicting edge importance in GPT-2's induction circuit with claimed significant speedup.
    * The community suggests that detailed research explanations are better suited for dedicated platforms like GitHub or academic venues (workshops, journals) for proper feedback and structure.
    * Reddit is perceived as unsuitable for lengthy, unstructured research dumps, serving a different purpose than a personal research journal.
**[ [D] Which hyperparameters search library to use? (Score: 4)](https://www.reddit.com/r/MachineLearning/comments/1r8v4fn/d_which_hyperparameters_search_library_to_use/)**
*  **Summary:** Katib, PyCaret, and basically [this](https://github.com/askery/automl-list) Optuna is the most convenient in my opinion. Optuna is very simple to use, and the default TPE sampler performs very well. Mljar I‚Äôd go with optuna. They have a decent documentation.
*  **Emotion:** Generally Positive.
*  **Top 3 Points of View:**
    * Optuna is a highly recommended library due to its convenience, simplicity, good documentation, and effective default TPE sampler.
    * Other suggested libraries include Katib, PyCaret, and Mljar, often found in broader AutoML lists.
    * The discussion focuses on identifying user-friendly and effective tools for hyperparameter optimization.
**[ [P] I Made my first Transformer architecture code (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1r8v5x2/p_i_made_my_first_transformer_architecture_code/)**
*  **Summary:** Not the place my dude, try r/learnmachinelearning
*  **Emotion:** Overall Neutral.
*  **Top 3 Points of View:**
    * The original poster shared their first Transformer architecture code.
    * A comment advises that such personal coding projects are better suited for `r/learnmachinelearning` rather than `r/MachineLearning`.
