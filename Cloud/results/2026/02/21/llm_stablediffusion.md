---
title: "Stable Diffusion Subreddit"
date: "2026-02-21"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["StableDiffusion", "AIModels", "ImageGeneration"]
---

# Overall Ranking and Top Discussions
1.  [I built the first Android app in the world that detects AI content locally and offline over any app using a Quick Tile](<https://v.redd.it/4yxsdzl3xvkg1>) (Score: 147)
    * Users discuss the reliability and validity of an Android app designed to detect AI-generated content locally and offline, with many expressing skepticism and requesting benchmarks.
2.  [FLUX2 Klein 9B LoKR Training – My Ostris AI Toolkit Configuration & Observations](<https://www.reddit.com/r/StableDiffusion/comments/1rayrbj/flux2_klein_9b_lokr_training_my_ostris_ai_toolkit/>) (Score: 15)
    * Discussion revolves around FLUX2 Klein 9B LoKR training, sharing configurations, observations on training times, memory issues, and comparisons with other LoRA methods.
3.  [Wan 2.2 HuMo + SVI Pro + ACE-Step 1.5 Turbo](<https://v.redd.it/7ps7yxmbjvkg1>) (Score: 14)
    * A post showcasing 'Wan 2.2 HuMo + SVI Pro + ACE-Step 1.5 Turbo' sparks comments on its quality, render time, and technical accessibility.
4.  [ZIRME: My own version of BIRME](<https://www.reddit.com/r/StableDiffusion/comments/1rasfdc/zirme_my_own_version_of_birme/>) (Score: 3)
    * A user introduces 'ZIRME', their version of 'BIRME', with a suggestion to offer local, offline functionality similar to its predecessor.
5.  [I'm completely done with Z-Image character training... exhausted](<https://www.reddit.com/r/StableDiffusion/comments/1rb0uh8/im_completely_done_with_zimage_character_training/>) (Score: 2)
    * Users express frustration and seek advice regarding 'Z-Image character training', discussing model effectiveness, memory issues, and alternative training approaches like Flux 9B.
6.  [Simple way to remove person and infill background in ComfyUI](<https://www.reddit.com/r/StableDiffusion/comments/1raurkj/simple_way_to_remove_person_and_infill_background/>) (Score: 1)
    * The thread focuses on simple methods to remove people and infill backgrounds in ComfyUI, with suggestions pointing to Flux Klein and Qwen Image Edit models.
7.  [Simple controlnet option for Flux 2 klein 9b?](<https://www.reddit.com/r/StableDiffusion/comments/1rav809/simple_controlnet_option_for_flux_2_klein_9b/>) (Score: 1)
    * Users discuss options for ControlNet with Flux 2 Klein 9B, with some suggesting that Klein's editing capabilities might negate the need for traditional ControlNets.
8.  [From automatic1111 to forge neo](<https://www.reddit.com/r/StableDiffusion/comments/1razzs5/from_automatic1111_to_forge_neo/>) (Score: 1)
    * A user transitions from Automatic1111 to Forge Neo, seeking advice on troubleshooting issues by suggesting A/B testing hardware/software.
9.  [I used Chatgpt and gave it a story, and its pretty!! Did someone try to give a story during image generation and gave good results??](<https://www.reddit.com/gallery/1rawddn>) (Score: 0)
    * The discussion centers on using large language models (like ChatGPT) to generate story-like prompts for image generation, with users noting that newer models handle wordier text better than older SDXL models.
10. [I've been looking for local AI workflow that can do something like Kling's Omni where you input reference images and refer to those images in a prompt to create a new image.](<https://www.reddit.com/r/StableDiffusion/comments/1rayi33/ive_been_looking_for_local_ai_workflow_that_can/>) (Score: 0)
    * A user seeks a local AI workflow capable of Kling's Omni-like functionality, where reference images are used in prompts to create new images, with Qwen Image Edit being recommended.
11. [[ACE-STEP]Does Claude made better implementation of training than the official UI?](<https://www.reddit.com/r/StableDiffusion/comments/1raxoln/acestepdoes_claude_made_better_implementation_of/>) (Score: 0)
    * The post discusses whether Claude offers a better implementation for training compared to the official UI, with comments on using Claude for scripting training, LoRA quality, and optimal training steps.
12. [Is there a anime model that doesnt make flat/bland illustrations like these?](<https://i.redd.it/j2r8rmh3yvkg1.jpeg>) (Score: 0)
    * Users discuss how to achieve less 'flat/bland' anime-style illustrations, suggesting techniques like using artist styles in prompts, LoRAs, and detailer nodes for improved texture and sharpness.
13. [How you use AI?](<https://www.reddit.com/r/StableDiffusion/comments/1razjg4/how_you_use_ai/>) (Score: 0)
    * Users share their preferred methods for using AI, primarily for image generation, with ComfyUI local and Wan2gp being mentioned as popular choices.
14. [Using stable diffusion to create realistic images of buildings](<https://www.reddit.com/r/StableDiffusion/comments/1raxlh1/using_stable_diffusion_to_create_realistic_images/>) (Score: 0)
    * The thread discusses using Stable Diffusion to create realistic images of buildings, with advice to use Klein 9B for restoring photos to high quality and generating different views.
15. [Please help with LTX 2 guys! Character will not walk towards the screen :(](<https://i.redd.it/1t38l1pf1wkg1.png>) (Score: 0)
    * Users seek help with LTX 2 to make a character walk towards the screen, brainstorming prompt adjustments and alternative workflows like stealing motion or using Qwen Image Edit for specific frames.

# Detailed Analysis by Thread
**[ I built the first Android app in the world that detects AI content locally and offline over any app using a Quick Tile (Score: 147)](<https://v.redd.it/4yxsdzl3xvkg1>)**
*  **Summary:** open source? The app is interesting, but the statistics are not accurate enough. In the demo you show 3 examples with AI-only media, but you should include in the demo non-AI media with 90% real probability. I'll make an app for this it'll just always say 50/50. I don't trust any AI content detector. Unless the content is generated by a model that builds in watermarking tools, like SynthID, you just cannot reliably detect AI-gen content anymore. Of course, some stuff is obvious, but you don't need a detector for that, and the stuff that's not obvious is not reliably obvious to any detector either. Do you have benchmarks? Otherwise I’m //highly// skeptical. As long as you're always disclosing that your AI detector isn't accurate and makes a best attempt I think it's fair. People who rely on these tools are unfortunately being deceived. The people falling for this are old people who don’t even know how to install an app and just look at YouTube reels lol. Nice snake oil. These don't work reliably. yes yes I want to keep clicking this share w Ai detector app step for every suspicious video I doomscroll on my social media apps not. Why you trying to detect us man?
*  **Emotion:** The overall tone is predominantly neutral (Average score: 0.63). There are also contributions of negative (2) and positive (2).
*  **Top 3 Points of View:**
    * Skepticism about the reliability and accuracy of AI content detectors, often labeling them as 'snake oil' due to unproven claims.
    * Calls for rigorous benchmarks and comprehensive testing, including non-AI media, to validate the app's accuracy and performance over time.
    * Concerns about the practicality and user experience of such an app, questioning the need for manual steps and the target audience.
**[ FLUX2 Klein 9B LoKR Training – My Ostris AI Toolkit Configuration & Observations (Score: 15)](<https://www.reddit.com/r/StableDiffusion/comments/1rayrbj/flux2_klein_9b_lokr_training_my_ostris_ai_toolkit/>)**
*  **Summary:** I wanna go back and train LoKrs for some of the loras I made from the last two years. I always used ai-toolkit, but I saw these comments about musubi being faster. Tomorrow I'll check how long it took me to train with Klein 9B. It took over 12 hours with 23 images and 2.5 hours with OneTrainer. OKR is interesting to me and I want to try it out on ZIB and ZIT LoRAs. Thanks for sharing, will definitely try it out! any findings around training a style instead of a character? Is this only for flux 2 Klein? Or can I use the same steps and saving math's for ZiT? In a 5090 with 30 pics and max res set to 1024 it takes between 35 and 45 mins with 30 x 3 x 6 =540 steps in total. When I try to train 9B I get out of memory errors immediately on my 5070ti 16gb VRAM and 32gb RAM. Not sure what I’m setting up wrong.
*  **Emotion:** The overall tone is predominantly neutral (Average score: 0.68). There are also contributions of positive (2).
*  **Top 3 Points of View:**
    * Interest in LoKR training for various AI models (FLUX2 Klein 9B, ZIB, ZIT) and sharing of configuration settings and observations.
    * Troubleshooting and performance issues, particularly regarding out-of-memory errors on GPUs with limited VRAM during 9B model training.
    * Comparison of training times and efficiency across different toolkits (AI-Toolkit vs. OneTrainer vs. Musubi) and model versions.
**[ Wan 2.2 HuMo + SVI Pro + ACE-Step 1.5 Turbo (Score: 14)](<https://v.redd.it/7ps7yxmbjvkg1>)**
*  **Summary:** Wan quality is still better than ltx2 but render time is pain. It has some node, that is not available. So I can’t use. Insert a scene of Simon Cowell stopping him mid-song and roasting him on how dreadful his lyrics are. Looks like he is singing a love song
*  **Emotion:** The overall tone is predominantly neutral (Average score: 0.56). There are also contributions of negative (2) and positive (1).
*  **Top 3 Points of View:**
    * Positive reception of the visual quality of outputs generated by 'Wan 2.2 HuMo + SVI Pro + ACE-Step 1.5 Turbo'.
    * Concerns about the render time and potential technical barriers (missing nodes) preventing broader adoption or use.
    * Humorous or critical takes on the AI-generated content, likening it to a 'love song' or suggesting 'Simon Cowell' style critique.
**[ ZIRME: My own version of BIRME (Score: 3)](<https://www.reddit.com/r/StableDiffusion/comments/1rasfdc/zirme_my_own_version_of_birme/>)**
*  **Summary:** My suggestion, is that you go the same road they did. BIRME offers a local download option, or they did at one point. Where it all remains local and loads in your local browser offline, without internet connection (I read your stated claim), and/or uploading to you front end, one dataset after another from around the globe. TIA.
*  **Emotion:** The overall tone is predominantly neutral (Average score: 0.94).
*  **Top 3 Points of View:**
    * Suggestion that the new 'ZIRME' app should emulate 'BIRME' by offering a local download option for offline use.
    * Emphasis on the importance of local processing to avoid uploading datasets to a front end.
    * An interest in the functionality of BIRME-like tools for image processing.
**[ I'm completely done with Z-Image character training... exhausted (Score: 2)](<https://www.reddit.com/r/StableDiffusion/comments/1rb0uh8/im_completely_done_with_zimage_character_training/>)**
*  **Summary:** Try flux 9b. I'm actually done with everyone trying to train recently for literally no reason. No decent LoRAs in the thousands of junk. Rather just give up than force ffs. Zimage "Base" was tuned further than the original Zimage distillation to Turbo version, so no matter how hard you train on it, it will work best on base than turbo. If you want to train for turbo, use the adapter or the De-turbo. I had good experience with training face with z-image turbo. bad experience with training artstyle. Forget Turbo, Use 4step distilled lora with Base! First off, what type of character are you creating. Is it a cartoon or anime character, is it based on a real person, or is it a real person? love posts that tell us nothing about the actual workflow, but just "actually stuff is bad." Could you share some results? Also show some reference images and others of the result, because I can't know what you're training for, also for different training bases. Do you mind showing your results? Don’t even try to apply base model lora to turbo. There are 4 step lora available for base model.
*  **Emotion:** The overall tone is predominantly neutral (Average score: 0.70). There are also contributions of positive (2).
*  **Top 3 Points of View:**
    * Frustration with the quality and effectiveness of 'Z-Image character training', especially the lack of decent LoRAs.
    * Recommendations to use specific models or approaches like Flux 9B, 4-step distilled LoRA with Zimage Base, or RedCraft for better results.
    * Requests for the original poster to share their workflow and results for better troubleshooting and understanding.
**[ Simple way to remove person and infill background in ComfyUI (Score: 1)](<https://www.reddit.com/r/StableDiffusion/comments/1raurkj/simple_way_to_remove_person_and_infill_background/>)**
*  **Summary:** Qwen Image Edit worked as it was supposed to. Flux Klein can do that. Don't expect it to work 100% time though (poor woman), some compositions are more complex than others. You have multiple options. One of them are: 1. Manual way, e.g. mask the person and do inpanting, these nodes can help [ComfyUI Inpaint Nodes](https://github.com/Acly/comfyui-inpaint-nodes) 2. Use image edit model like Qwen Image Edit or Flux Klein and just write a prompt "remove person from image". It takes about 2 seconds for Klein on a weak GPU. [https://civitai.com/models/2390013/flux2-klein-ultimate-aio-pro-t2i-i2i-inpaint-replace-remove-swap-edit-segment-manual-auto-none](https://civitai.com/models/2390013/flux2-klein-ultimate-aio-pro-t2i-i2i-inpaint-replace-remove-swap-edit-segment-manual-auto-none). Qwen is content aware and may be easier to use than older content aware models like Stable Diffusion. Flux Klein can do it: [https://blog.comfy.org/p/flux2-klein-4b-fast-local-image-editing](https://blog.comfy.org/p/flux2-klein-4b-fast-local-image-editing) Use the edit workflow Or Qwen Image Edit 2511. The new Klein models are editing powerhouse, and very lightweight.. Use the 9b-distilled model. It needs 8 steps. CFG = 1
*  **Emotion:** The overall tone is predominantly neutral (Average score: 0.88). There are also contributions of positive (2).
*  **Top 3 Points of View:**
    * Using specialized image editing models like Flux Klein or Qwen Image Edit for effective object removal and background infill.
    * Acknowledging that while these tools are powerful, they may not work 100% of the time, especially with complex compositions.
    * Suggesting alternative manual methods like masking and inpainting with dedicated ComfyUI nodes.
**[ Simple controlnet option for Flux 2 klein 9b? (Score: 1)](<https://www.reddit.com/r/StableDiffusion/comments/1rav809/simple_controlnet_option_for_flux_2_klein_9b/>)**
*  **Summary:** What people don’t understand when using input images as ControlNet is that you can’t choose how strongly you want to apply the control, like you *can* with the real ControlNet. Take a look at this, "FLUX.2 Klein Ref Latent Controller": [https://github.com/capitan01R/ComfyUI-Flux2Klein-Enhancer](https://github.com/capitan01R/ComfyUI-Flux2Klein-Enhancer). The powerful editing capablities of Flux2 Klein 9b/4b , negates any need for controlnets . following with 9b. 8 steps .cfg = 1. The powerful editing capablities of Flux2 Klein 9b/4b , negates any need for controlnets . following with 9b. 8 steps .cfg = 1. You can use one of the preprocessors on an image and send it to Klein or QIE directly. They understand depthmaps and openpose images. i dont think there is any controlnet for klein 9b or 4b but those models are decent without control net and does sketch to art or pose to image pretty well without any control by just using image to image
*  **Emotion:** The overall tone is predominantly neutral (Average score: 0.85).
*  **Top 3 Points of View:**
    * Questioning the necessity of ControlNets with advanced models like Flux2 Klein 9B/4B due to their inherent editing capabilities.
    * Suggestions to use preprocessors for depthmaps and openpose images as input for Klein or QIE directly.
    * Highlighting the limitation of simply using input images for control compared to true ControlNet, which allows control over application strength.
**[ From automatic1111 to forge neo (Score: 1)](<https://www.reddit.com/r/StableDiffusion/comments/1razzs5/from_automatic1111_to_forge_neo/>)**
*  **Summary:** You should conduct an A/B test to isolate the cause. If the same issue occurs on your current PC using automatic1111 with identical settings, it indicates a problem with the PC's hardware or software.
*  **Emotion:** The overall tone is predominantly neutral (Average score: 0.92).
*  **Top 3 Points of View:**
    * Recommendation to conduct an A/B test when switching between Automatic1111 and Forge Neo to identify the root cause of issues.
    * Consideration that issues might stem from PC hardware or software rather than the AI software itself.
    * General advice on troubleshooting technical problems in AI environments.
**[ I used Chatgpt and gave it a story, and its pretty!! Did someone try to give a story during image generation and gave good results?? (Score: 0)](<https://www.reddit.com/gallery/1rawddn>)**
*  **Summary:** The newer models (Flux, Chroma, etc.) use bigger LLMs for working on the prompt and have been trained with wordy text. With SDXL models, you had the 75 token limit, plus some were trained on keywords, so you had to keep things. cool story bro. We don't use Chat gpt here, this subreddit is about local model.
*  **Emotion:** The overall tone is predominantly neutral (Average score: 0.75). There are also contributions of positive (1).
*  **Top 3 Points of View:**
    * Advising that the subreddit is primarily for local models, implying a preference against (or rule about) discussing online tools like ChatGPT.
    * Observing that newer AI models (Flux, Chroma) are better at handling descriptive, story-like prompts due to being trained with larger LLMs.
    * Noting the shift from concise, keyword-based prompts (typical of older SDXL models) to more verbose, story-driven prompts.
**[ I've been looking for local AI workflow that can do something like Kling's Omni where you input reference images and refer to those images in a prompt to create a new image. (Score: 0)](<https://www.reddit.com/r/StableDiffusion/comments/1rayi33/ive_been_looking_for_local_ai_workflow_that_can/>)**
*  **Summary:** Qwen image edit can do it. Wan phantom does similar but not exceptionally well. Qwen Edit 2511
*  **Emotion:** The overall tone is predominantly neutral (Average score: 0.85).
*  **Top 3 Points of View:**
    * Seeking local AI workflows that can use reference images within prompts to generate new images, similar to Kling's Omni.
    * Recommendation of Qwen Image Edit as a capable tool for such reference-based image generation.
    * Mention of Wan Phantom as another option, though noted as less exceptional.
**[[ACE-STEP]Does Claude made better implementation of training than the official UI? (Score: 0)](<https://www.reddit.com/r/StableDiffusion/comments/1raxoln/acestepdoes_claude_made_better_implementation_of/>)**
*  **Summary:** Claude is writing scripts to train and stuff. Lokr is *** for quality. You can split your instrumentals into individual stems using uvr from GitHub.
*  **Emotion:** The overall tone is predominantly neutral (Average score: 0.89).
*  **Top 3 Points of View:**
    * Exploring whether Claude improves AI model training implementation compared to official UIs.
    * Discussion of technical training aspects like VRAM limitations, LoRA dimensions, finetuning, and optimal steps for inference.
    * Tips for improving quality, such as splitting instrumentals into stems and proper captioning for better training outcomes.
**[ Is there a anime model that doesnt make flat/bland illustrations like these? (Score: 0)](<https://i.redd.it/j2r8rmh3yvkg1.jpeg>)**
*  **Summary:** It’s either you look for a Lora with that style or start looking at artist styles on danbooru whether it be one or combined that best match your goal style. Or Lora mixed with artists. "very flat, lacking texture"? that's just how anime looks. "nail is lacking shine"? glossy nails in positive prompt. not enough detail, not sharp enough? aDetailer/face detailer node, or manual inpainting. it's not about the model, it's the style you choose to use. Easy answer. Use a hand detailer. I use hand detail right after face detail after the upscale. Not quite illustrious, but have you checked any of the ones based on noobai? Some of the vpred ones are quite impressive! find a famous artist style that you like, put their name in the tags of your prompt, illustrious will understand the style you're looking for. So… you are not looking for a anime model. Gotcha.
*  **Emotion:** The overall tone is predominantly neutral (Average score: 0.77). There are also contributions of positive (2).
*  **Top 3 Points of View:**
    * Addressing the 'flat/bland' appearance in anime models by suggesting the use of artist styles and LoRAs in prompts.
    * Recommendation of detailer nodes (face, hand) and manual inpainting for adding texture, sharpness, and gloss.
    * Clarification that the issue might be related to chosen style rather than the model itself, suggesting exploration of models based on noobai.
**[ How you use AI? (Score: 0)](<https://www.reddit.com/r/StableDiffusion/comments/1razjg4/how_you_use_ai/>)**
*  **Summary:** Comfyui local. Better because I can do what I want, locally. Easiest I found is Wan2gp, alot easier to use than comfyui. ???? What do you want the AI to do? SD is an image gen model
*  **Emotion:** The overall tone is predominantly neutral (Average score: 0.66). There are also contributions of positive (1).
*  **Top 3 Points of View:**
    * Preference for local AI setups, specifically ComfyUI, for greater control and customization in image generation.
    * Mention of Wan2gp as an easier-to-use alternative to ComfyUI for image generation tasks.
    * General inquiry about how users are leveraging AI models for various purposes.
**[ Using stable diffusion to create realistic images of buildings (Score: 0)](<https://www.reddit.com/r/StableDiffusion/comments/1raxlh1/using_stable_diffusion_to_create_realistic_images/>)**
*  **Summary:** Use Klein 9B to restore the photos to high quality. Use klein 9b to create different views from high quliaty restored imgaes.
*  **Emotion:** The overall tone is predominantly neutral (Average score: 0.96).
*  **Top 3 Points of View:**
    * Recommendation to use Klein 9B for restoring photos to high quality.
    * Suggestion to use Klein 9B for generating different views from high-quality restored images.
    * Focus on the practical application of Stable Diffusion for realistic architectural visualization.
**[ Please help with LTX 2 guys! Character will not walk towards the screen :( (Score: 0)](<https://i.redd.it/1t38l1pf1wkg1.png>)**
*  **Summary:** You could also try “running towards the viewer”. The tracking and dolly prompts may not be required as I think the model may naturally apply a dolly back if you were able to get her running towards it. **woman is running towards us maintaining eye contact. camera is tracking her camera tracking shot**. **using your promt she want the other way for some reason**. No pause. No zoom-out. No cut. ? never use what you dont want to see in positive prompt. even my tool wouldn't *** up as much to put pause in it. lol. There is no LTX2 equivalent of Wans first to last frame workflow. I would use Qwen Image Edit to generate the frame at the other end of the video and use it in 1st to the last frame.
*  **Emotion:** The overall tone is predominantly neutral (Average score: 0.83).
*  **Top 3 Points of View:**
    * Struggling with getting characters to move in a specific direction (towards the screen) using LTX 2.
    * Brainstorming prompt-based solutions like 'running towards the viewer' and negative prompts to prevent unwanted camera movements.
    * Suggesting advanced workflows such as stealing motion from other videos or using Qwen Image Edit to generate specific frames for 'first-to-last frame' workflows.
