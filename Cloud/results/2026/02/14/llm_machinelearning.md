---
title: "Machine Learning Subreddit"
date: "2026-02-14"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["Machine Learning", "NLP", "Job Market", "Research", "Deep Learning", "Reviews", "AI Ethics"]
---

# Overall Ranking and Top Discussions
*   1. **[[D] Struggling on the NLP job market as a final-year PhD , looking for advice](https://www.reddit.com/r/MachineLearning/comments/1r467ra/d_struggling_on_the_nlp_job_market_as_a_finalyear/)** (Score: 119)
    * Definitely study leetcode if you’re not good at it. It’s frustrating but it’s still the process. Some places it’s the only process. You're not going to get tenure track positions before finishing a PhD without a postdoc or industry experience In industry hiring right now, people are generally not hiring for NLP tasks, but for ability to contribute to the model / system stack itself. Research narrative and presenting 2-3 papers as a coherent story about improving model capability was more important than publication count. Even for research I am genuinely curious how your research is considered outdated. Because everywhere I look there’s still NLP/LLM roles. You can cold email about post docs, but where is your advisor in all of this? Can they introduce you to people? What are your skills? What models, architectures, datasets, tasks do you have expertise in? Are you using encoders mainly? There is a lack of experience in the field of NLP in my team. I know it might be hard to hear, so don't throw away your work just yet. Focus on positions that have been posted recently. You have 17 papers at good venues and 430 citations on your profile. You have 8 interviews with no offers. You should prepare a CV, Code, Clickable projects, make the code open source, add well written documentation for it, make it modular, apply clean code principles This is disheartening to hear. Yes, it's outdated. There were 200 applications and 8 interviews, but no offers. My research area is seen as too narrow or outdated (summarization) or not aligned with what the team currently needs. PhDs in astrophysics and molecular biology and computer science are supposed to have one thing in After 20 years of working in AI/ML as an academic, he is now hiring at a top-level place. He had 200 applications and 8 interviews, but no offers. He advises his students to work their network and get practice interviews with people who are in industry.
*   2. **[[P] I trained YOLOX from scratch to avoid Ultralytics' AGPL (aircraft detection on iOS)](https://austinsnerdythings.com/2026/02/13/training-yolox-aircraft-detection-mit-license/)** (Score: 21)
    * Why not trying DETR-like architectures ? Nice workaround avoiding AGPL. I've always gotten better mobile performance by quantizing the model before iOS deployment. How'd you handle those false positives during real-world testing? What dataset did you use? How are the numbers? Does Ultralytics actively patrol this sub? I've upvoted every comment so far and most are sitting at 1 (meaning they got at least 1 downvote each).
*   3. **[[D] ICML assigned me a paper that I reviewed in ICLR](https://www.reddit.com/r/MachineLearning/comments/1r4umpo/d_icml_assigned_me_a_paper_that_i_reviewed_in_iclr/)** (Score: 11)
    * Woah what are the chances. You should probably let your chair know. I personally don't think reviewing it again is an issue but maybe it is 2 years ago I reviewed the same paper at NeurIPS and a few months later at AISTATS and it was rejected again. Assert dominance by giving the same review and same rating (I suppose ICML ratings are from 1-6) I'm just kidding but I think you should area chair or someone. Disclose it.
*   4. **[[D] ARR Jan ARR Discussion](https://www.reddit.com/r/MachineLearning/comments/1r4fotm/d_arr_jan_arr_discussion/)** (Score: 9)
    * I got a pile of trash this cycle. So disappointed with the mediocre paper quality this day. I personally really like the papers I reviewed, they are high quality and interesting. I gave 3-4 for most of them besides one, which I gave a 2.
*   5. **[[D] Average Number of Interviews to Get a Job (US)](https://www.reddit.com/r/MachineLearning/comments/1r4tnv4/d_average_number_of_interviews_to_get_a_job_us/)** (Score: 9)
    * lol ,there are no jobs man... Do something else, or nothing at all. If you've gotten 23 interviews without an offer, I feel like you're more on the side of messing up interviews than not being qualified/impressive. Because, jeez, that's a *lot*, given how much filtering occurs before any interviewing these days. u didnt train any LLM from scratch, good luck finding any job Some resume items sound very impressive. Perhaps you have inflated some of them and interviewers are calling your bluff?
*   6. **[[D] Minimax 2.5 is out, considering local deployment](https://www.reddit.com/r/MachineLearning/comments/1r48dvk/d_minimax_25_is_out_considering_local_deployment/)** (Score: 5)
    * If you have the hardware to run llama, do it. If you don't, spend the money on API calls instead. if u are thinking about local, start with smaller checkpoints to see if performance and memory are manageable. a lot of these new models look good in demos but hit hardware limits fast. check runtime support and quantization options before committing to big setups.
*   7. **[[R] Lrec 26 acceptance emails](https://www.reddit.com/r/MachineLearning/comments/1r47czy/r_lrec_26_acceptance_emails/)** (Score: 3)
    * I received an acceptance email on the 12th along with a few other colleagues. Tough wait. Reminds me how the global research map is redrawing. Some new hubs are moving incredibly fast. You tell me what that means.
*   8. **[[D] Interesting Gradient Norm Goes Down-Up-Down](https://www.reddit.com/r/MachineLearning/comments/1r4bbbd/d_interesting_gradient_norm_goes_downupdown/)** (Score: 3)
    * That's not abnormal. Though it does suggest a need for an HPO study. It looks like a phase transition between memorization and generalization. How's the test error look? Have you thought about how regularization might affect the grad norm? double descent? I don't know exactly your set up but there are different cases if you view the weighted sum as associative memory. Under capacity, capacity, over capacity. So you would expect corresponding changes in the norm(s). this is solid — gradient norm dipping then spiking then smoothing out usually means the optimizer hit a weird saddle point or sharp curvature early on. nice work catching that pattern. [https://en.wikipedia.org/wiki/Grokking\_(machine\_learning)](https://en.wikipedia.org/wiki/Grokking_(machine_learning))
*   9. **[[D] Asymmetric consensus thresholds for multi-annotator NER — valid approach or methodological smell?](https://i.redd.it/aukkp6r9kfjg1.png)** (Score: 1)
    * While I appreciate technical questions, the clearly fully ai written post is a real turn off. Good call on asymmetric thresholds given your annotators' known strengths. Documenting the per-category rationale would help reviewers see this isn't arbitrary. How's the inter-annotator agreement looking for the trickier PII types?

# Detailed Analysis by Thread

**[[D] Struggling on the NLP job market as a final-year PhD , looking for advice (Score: 119)](https://www.reddit.com/r/MachineLearning/comments/1r467ra/d_struggling_on_the_nlp_job_market_as_a_finalyear/)**
*  **Summary:** Definitely study leetcode if you’re not good at it. It’s frustrating but it’s still the process. Some places it’s the only process. You're not going to get tenure track positions before finishing a PhD without a postdoc or industry experience In industry hiring right now, people are generally not hiring for NLP tasks, but for ability to contribute to the model / system stack itself. Research narrative and presenting 2-3 papers as a coherent story about improving model capability was more important than publication count. Even for research I am genuinely curious how your research is considered outdated. Because everywhere I look there’s still NLP/LLM roles. You can cold email about post docs, but where is your advisor in all of this? Can they introduce you to people? What are your skills? What models, architectures, datasets, tasks do you have expertise in? Are you using encoders mainly? There is a lack of experience in the field of NLP in my team. I know it might be hard to hear, so don't throw away your work just yet. Focus on positions that have been posted recently. You have 17 papers at good venues and 430 citations on your profile. You have 8 interviews with no offers. You should prepare a CV, Code, Clickable projects, make the code open source, add well written documentation for it, make it modular, apply clean code principles This is disheartening to hear. Yes, it's outdated. There were 200 applications and 8 interviews, but no offers. My research area is seen as too narrow or outdated (summarization) or not aligned with what the team currently needs. PhDs in astrophysics and molecular biology and computer science are supposed to have one thing in After 20 years of working in AI/ML as an academic, he is now hiring at a top-level place. He had 200 applications and 8 interviews, but no offers. He advises his students to work their network and get practice interviews with people who are in industry.
*  **Emotion:** The thread generally has a neutral tone (with 6 comment(s) averaging a score of 0.73). There are also discussions with negative sentiment (3 comment(s) averaging 0.72); positive sentiment (1 comment(s) averaging 0.39).
*  **Top 3 Points of View:**
    * The NLP job market emphasizes foundational ML skills (system stack contributions, pretraining, RL, eval) over narrow NLP tasks like summarization. Candidates need to demonstrate broader capabilities.
    * Technical interview proficiency (LeetCode, DSA, System Design) and soft skills (networking, self-marketing) are critical for securing offers, even with strong research profiles. Internships not converting indicate issues beyond technical merit.
    * Reframing research expertise (e.g., summarization as 'Information Compression' or 'Context Management for LLMs') to align with current industry needs can improve job prospects.

**[[P] I trained YOLOX from scratch to avoid Ultralytics' AGPL (aircraft detection on iOS) (Score: 21)](https://austinsnerdythings.com/2026/02/13/training-yolox-aircraft-detection-mit-license/)**
*  **Summary:** Why not trying DETR-like architectures ? Nice workaround avoiding AGPL. I've always gotten better mobile performance by quantizing the model before iOS deployment. How'd you handle those false positives during real-world testing? What dataset did you use? How are the numbers? Does Ultralytics actively patrol this sub? I've upvoted every comment so far and most are sitting at 1 (meaning they got at least 1 downvote each).
*  **Emotion:** The thread generally has a neutral tone (with 4 comment(s) averaging a score of 0.57).
*  **Top 3 Points of View:**
    * Training custom models (YOLOX) from scratch is a valid strategy to bypass restrictive open-source licenses like AGPL for commercial projects.
    * Users inquire about technical aspects such as alternative architectures (DETR-like), specific datasets used, model performance, and optimization for mobile deployment (quantization, handling false positives).
    * There's community suspicion regarding potential downvoting by entities associated with competing software (Ultralytics) in discussions about license avoidance.

**[[D] ICML assigned me a paper that I reviewed in ICLR (Score: 11)](https://www.reddit.com/r/MachineLearning/comments/1r4umpo/d_icml_assigned_me_a_paper_that_i_reviewed_in_iclr/)**
*  **Summary:** Woah what are the chances. You should probably let your chair know. I personally don't think reviewing it again is an issue but maybe it is 2 years ago I reviewed the same paper at NeurIPS and a few months later at AISTATS and it was rejected again. Assert dominance by giving the same review and same rating (I suppose ICML ratings are from 1-6) I'm just kidding but I think you should area chair or someone. Disclose it.
*  **Emotion:** The thread generally has a neutral tone (with 4 comment(s) averaging a score of 0.72).
*  **Top 3 Points of View:**
    * Reviewers are ethically obliged to disclose when they are assigned a paper they have previously reviewed for a different conference.
    * Conference Area Chairs may permit reviewing the paper again, but advise treating it as a fresh submission. However, paper quality and reviewer consistency remain concerns.
    * The situation highlights the overlap and resubmission practices between major ML conferences like ICML, ICLR, NeurIPS, and AISTATS.

**[[D] ARR Jan ARR Discussion (Score: 9)](https://www.reddit.com/r/MachineLearning/comments/1r4fotm/d_arr_jan_arr_discussion/)**
*  **Summary:** I got a pile of trash this cycle. So disappointed with the mediocre paper quality this day. I personally really like the papers I reviewed, they are high quality and interesting. I gave 3-4 for most of them besides one, which I gave a 2.
*  **Emotion:** The thread generally has a negative tone (with 1 comment(s) averaging a score of 0.84). There are also discussions with positive sentiment (1 comment(s) averaging 0.78).
*  **Top 3 Points of View:**
    * There is a polarized view on the quality of papers submitted to ARR (ACL Rolling Review) during the January cycle, with some reviewers praising high quality and others criticizing mediocre submissions.
    * Reviewers express disappointment regarding a perceived decline in overall paper quality in the current cycle.
    * Conversely, some reviewers found their assigned papers to be high quality and interesting, suggesting variability in submissions or reviewer assignments.

**[[D] Average Number of Interviews to Get a Job (US) (Score: 9)](https://www.reddit.com/r/MachineLearning/comments/1r4tnv4/d_average_number_of_interviews_to_get_a_job_us/)**
*  **Summary:** lol ,there are no jobs man... Do something else, or nothing at all. If you've gotten 23 interviews without an offer, I feel like you're more on the side of messing up interviews than not being qualified/impressive. Because, jeez, that's a *lot*, given how much filtering occurs before any interviewing these days. u didnt train any LLM from scratch, good luck finding any job Some resume items sound very impressive. Perhaps you have inflated some of them and interviewers are calling your bluff?
*  **Emotion:** The thread generally has a neutral tone (with 2 comment(s) averaging a score of 0.80). There are also discussions with positive sentiment (2 comment(s) averaging 0.66).
*  **Top 3 Points of View:**
    * A high number of interviews without an offer (e.g., 23) points to issues in interview performance rather than a lack of qualifications or impressive resume items.
    * The current job market for ML roles is perceived as extremely challenging, with suggestions that advanced skills like training LLMs from scratch are now expected.
    * Some speculate that candidates might be inflating their resume experiences, leading interviewers to probe for depth that isn't present.

**[[D] Minimax 2.5 is out, considering local deployment (Score: 5)](https://www.reddit.com/r/MachineLearning/comments/1r48dvk/d_minimax_25_is_out_considering_local_deployment/)**
*  **Summary:** If you have the hardware to run llama, do it. If you don't, spend the money on API calls instead. if u are thinking about local, start with smaller checkpoints to see if performance and memory are manageable. a lot of these new models look good in demos but hit hardware limits fast. check runtime support and quantization options before committing to big setups.
*  **Emotion:** The thread generally has a neutral tone (with 2 comment(s) averaging a score of 0.74).
*  **Top 3 Points of View:**
    * The decision to deploy large language models locally depends on existing hardware; using API calls is more cost-effective if new hardware purchases are required, given rapid model evolution.
    * Testing with smaller model checkpoints is crucial to assess performance and memory constraints before committing to large-scale local setups.
    * Runtime support, quantization options, and community resources (e.g., `r/LocalLlama`) are important considerations for successful local deployment.

**[[R] Lrec 26 acceptance emails (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1r47czy/r_lrec_26_acceptance_emails/)**
*  **Summary:** I received an acceptance email on the 12th along with a few other colleagues. Tough wait. Reminds me how the global research map is redrawing. Some new hubs are moving incredibly fast. You tell me what that means.
*  **Emotion:** The thread generally has a neutral tone (with 1 comment(s) averaging a score of 0.57). There are also discussions with positive sentiment (1 comment(s) averaging 0.29).
*  **Top 3 Points of View:**
    * Researchers are receiving acceptance notifications for the LREC 2026 conference, indicating the progression of the academic review cycle.
    * The period of waiting for conference acceptance results can be a stressful time for authors.
    * The discussion subtly touches upon the evolving landscape of global research and the emergence of fast-moving new research hubs.

**[[D] Interesting Gradient Norm Goes Down-Up-Down (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1r4bbbd/d_interesting_gradient_norm_goes_downupdown/)**
*  **Summary:** That's not abnormal. Though it does suggest a need for an HPO study. It looks like a phase transition between memorization and generalization. How's the test error look? Have you thought about how regularization might affect the grad norm? double descent? I don't know exactly your set up but there are different cases if you view the weighted sum as associative memory. Under capacity, capacity, over capacity. So you would expect corresponding changes in the norm(s). this is solid — gradient norm dipping then spiking then smoothing out usually means the optimizer hit a weird saddle point or sharp curvature early on. nice work catching that pattern. [https://en.wikipedia.org/wiki/Grokking\_(machine\_learning)](https://en.wikipedia.org/wiki/Grokking_(machine_learning))
*  **Emotion:** The thread generally has a neutral tone (with 4 comment(s) averaging a score of 0.75). There are also discussions with positive sentiment (1 comment(s) averaging 0.51).
*  **Top 3 Points of View:**
    * Non-monotonic behavior in gradient norm (e.g., down-up-down) is not necessarily abnormal and can be indicative of underlying training dynamics like phase transitions or double descent.
    * Such patterns suggest the need for thorough hyperparameter optimization studies and investigating the impact of regularization on gradient norms.
    * The phenomenon might be explained by the optimizer navigating complex loss landscapes, potentially encountering saddle points or sharp curvatures early in the optimization process.

**[[D] Asymmetric consensus thresholds for multi-annotator NER — valid approach or methodological smell? (Score: 1)](https://i.redd.it/aukkp6r9kfjg1.png)**
*  **Summary:** While I appreciate technical questions, the clearly fully ai written post is a real turn off. Good call on asymmetric thresholds given your annotators' known strengths. Documenting the per-category rationale would help reviewers see this isn't arbitrary. How's the inter-annotator agreement looking for the trickier PII types?
*  **Emotion:** The thread generally has a positive tone (with 1 comment(s) averaging a score of 0.70). There are also discussions with neutral sentiment (1 comment(s) averaging 0.60).
*  **Top 3 Points of View:**
    * Asymmetric consensus thresholds in multi-annotator NER can be a valid methodological choice, particularly when leveraged to account for known annotator strengths in specific categories.
    * Robust documentation of the rationale behind per-category thresholds is essential to ensure transparency and justify the approach to reviewers.
    * There is a sentiment within the community that posts perceived as entirely AI-generated can be a turn-off, even if the technical question is valid.
