---
title: "Data Engineering Subreddit"
date: "2026-02-12"
description: "Analysis of top discussions and trends in the dataengineering subreddit"
tags: ["data engineering", "career advice", "AI", "SQL", "cloud computing"]
---

# Overall Ranking and Top Discussions
1.  [I built a website to centralize articles, events and podcasts about data](https://i.redd.it/0ovtcb4jc0jg1.png) (Score: 51)
    *   A user shared a new website they built to centralize data-related articles, events, and podcasts, receiving overwhelmingly positive feedback and suggestions for new features.
2.  [Should I prioritize easy/medium or hard questions from DataLemur as a new graduate?](https://www.reddit.com/r/dataengineering/comments/1r2gu5c/should_i_prioritize_easymedium_or_hard_questions/) (Score: 35)
    *   A new graduate asked for advice on tackling DataLemur questions for interview prep, with responses recommending hard questions to strengthen skills and emphasizing the importance of portfolio projects.
3.  [Am I cooked?](https://www.reddit.com/r/dataengineering/comments/1r2nrv3/am_i_cooked/) (Score: 24)
    *   A junior data engineer questioned their career trajectory, leading to a wave of supportive responses offering reassurance, advice on skill development, and career progression strategies.
4.  [My brain freezes while solving or writing SQL queries.](https://www.reddit.com/r/dataengineering/comments/1r2j1ae/my_brain_freezes_while_solving_or_writing_sql/) (Score: 21)
    *   Users discussed solutions for overcoming mental blocks when writing SQL queries, especially during interviews, focusing on practice, conceptual understanding, and interview-specific strategies.
5.  [Being pushed out of job, trying to plan next steps](https://www.reddit.com/r/dataengineering/comments/1r2wvxg/being_pushed_out_of_job_trying_to_plan_next_steps/) (Score: 16)
    *   A user facing job loss sought advice on transitioning to cloud data engineering, receiving practical tips on learning resources, free trials, and navigating job market demands.
6.  [Am I being anxious too early?](https://www.reddit.com/r/dataengineering/comments/1r2uova/am_i_being_anxious_too_early/) (Score: 6)
    *   The thread explored early career anxiety, with discussions emphasizing the importance of theoretical knowledge over tool-specific learning and acknowledging the broad responsibilities of modern engineers.
7.  [11 Compaction Strategies for Iceberg Data Lakes](https://overcast.blog/11-compaction-strategies-for-iceberg-data-lakes-906d347af0f9) (Score: 5)
    *   An article on Iceberg Data Lake compaction strategies was shared, prompting a technical discussion on the complexity and justification for data lakes and practical compaction approaches.
8.  [ERP sysadmin vs Data Engineering](https://www.reddit.com/r/dataengineering/comments/1r2pzwc/erp_sysadmin_vs_data_engineering/) (Score: 5)
    *   Users compared career paths between ERP sysadmin and data engineering, generally favoring DE for its broader opportunities, stimulating work, and higher pay potential.
9.  [jack of all trades VS a master of one, how should I learn as a junior engineer?](https://www.reddit.com/r/dataengineering/comments/1r2xnf6/jack_of_all_trades_vs_a_master_of_one_how_should/) (Score: 4)
    *   A junior engineer asked about specialization vs. generalization, leading to a consensus favoring being a "jack of all trades" in the diverse and evolving data engineering field.
10. [I want to practise Dimensional Data Modelling but im lost](https://www.reddit.com/r/dataengineering/comments/1r2shmx/i_want_to_practise_dimensional_data_modelling_but/) (Score: 3)
    *   A user looking for dimensional data modeling practice was recommended the Fantasy Premier League API as a free and accessible learning resource.
11. [What to learn besides DE](https://www.reddit.com/r/dataengineering/comments/1r2svde/what_to_learn_besides_de/) (Score: 3)
    *   The thread discussed additional skills beyond core data engineering, highlighting data infrastructure (Docker, Kubernetes), theoretical knowledge, and business domain expertise.
12. [Matching Records](https://www.reddit.com/r/dataengineering/comments/1r2tpnt/matching_records/) (Score: 3)
    *   Users discussed the challenging problem of matching records in small-to-medium companies, suggesting graph databases or Python-based iterative solutions depending on resources.
13. [How do you push data from one api to another](https://www.reddit.com/r/dataengineering/comments/1r2ann0/how_do_you_push_data_from_one_api_to_another/) (Score: 2)
    *   The discussion revolved around transferring data between APIs, offering advice on using AI for basic code, pseudocode examples, and relevant data engineering patterns like pub/sub.
14. [Azure data engineering course](https://www.reddit.com/r/dataengineering/comments/1r2w945/azure_data_engineering_course/) (Score: 2)
    *   A bot responded to a query about Azure data engineering courses by linking to a community-submitted list of learning resources.
15. [Got tired of spinning up Flink to power a live dashboard, so I built a minimal Arrow-compatible data engine in Rust. Would love to hear your thoughts.](https://www.reddit.com/r/dataengineering/comments/1r2f8bf/got_tired_of_spinning_up_flink_to_power_a_live/) (Score: 2)
    *   A user presented a new Rust-based data engine, drawing critical feedback regarding its comparison to Flink and suggestions to better position its unique advantages.
16. [AI For Data Modelling??](https://www.reddit.com/r/dataengineering/comments/1r2uicu/ai_for_data_modelling/) (Score: 0)
    *   The post questioned the use of AI for data modeling, eliciting mixed responses ranging from practical applications to skepticism, fundamental criticisms, and strong negative reactions to perceived AI-generated content.
17. [How are you protecting your repos in the age of AI, especially in data engineering?](https://www.reddit.com/r/dataengineering/comments/1r2p04z/how_are_you_protecting_your_repos_in_the_age_of/) (Score: 0)
    *   The thread discussed strategies for securing repositories against AI-generated code, highlighting the limitations of AI in catching logical bugs and emphasizing robust testing and CI/CD pipelines.
18. [AWS Data Engineering services and Prep](https://www.reddit.com/r/dataengineering/comments/1r2xcx5/aws_data_engineering_services_and_prep/) (Score: 0)
    *   A user inquired about AWS data engineering, receiving an ironic suggestion to look at other cloud providers and a bot-provided link to general learning resources.
19. [How is Agentic AI going to change data engineering?](https://www.reddit.com/r/dataengineering/comments/1r2zwqv/how_is_agentic_ai_going_to_change_data_engineering/) (Score: 0)
    *   The discussion focused on the potential impact of Agentic AI on data engineering, with responses ranging from practical applications to skepticism about current LLM capabilities and humorous remarks.

# Detailed Analysis by Thread
**[I built a website to centralize articles, events and podcasts about data (Score: 51)](https://i.redd.it/0ovtcb4jc0jg1.png)**
*   **Summary:** The thread discusses a user-built website centralizing data-related articles, events, and podcasts. The community expressed overwhelmingly positive feedback and gratitude, with several users highlighting the usefulness of such a resource. There were also constructive suggestions for enhancements like an RSS feed and potential integration with LLMs, as well as questions about the article sourcing methodology.
*   **Emotion:** The overall emotional tone is highly **Positive and Enthusiastic**. Comments show appreciation ("amazing," "super cool," "great idea") and excitement, with suggestions and questions maintaining a neutral but engaged tone.
*   **Top 3 Points of View:**
    *   Strong appreciation for the creation of a centralized resource for data-related content, seen as very useful by the community.
    *   Constructive suggestions for future features and improvements, such as adding an RSS feed or integrating LLM capabilities.
    *   Curiosity about the technical implementation, specifically how the articles and other content are sourced.

**[Should I prioritize easy/medium or hard questions from DataLemur as a new graduate? (Score: 35)](https://www.reddit.com/r/dataengineering/comments/1r2gu5c/should_i_prioritize_easymedium_or_hard_questions/)**
*   **Summary:** A new graduate sought advice on whether to prioritize easy/medium or hard questions from DataLemur for interview preparation. The consensus, including input from the DataLemur founder, suggested tackling hard questions after mastering the basics, as they build stronger skills applicable to the medium-level questions commonly found in interviews. Additionally, having a robust, end-to-end portfolio project was emphasized as crucial for data engineering interviews.
*   **Emotion:** The emotional tone is predominantly **Neutral and Informative**, offering practical career guidance. There's an underlying positive encouragement for the new graduate's efforts.
*   **Top 3 Points of View:**
    *   After mastering easy/medium problems, prioritize hard questions from platforms like DataLemur, as they strengthen skills and improve pattern recognition for all difficulty levels.
    *   Most data engineering interviews typically feature medium-level questions, but practicing hard problems provides a strong foundation.
    *   Beyond problem-solving platforms, having a proper, end-to-end, deployed portfolio project is critically important for demonstrating practical data engineering skills in interviews.

**[Am I cooked? (Score: 24)](https://www.reddit.com/r/dataengineering/comments/1r2nrv3/am_i_cooked/)**
*   **Summary:** The original poster, a junior data engineer three years into their career, expressed concern (asking "Am I cooked?"). The community provided extensive reassurance and support, dismissing the notion that they were "cooked." Advice focused on strategies for technical skill improvement, such as practicing coding from scratch and prioritizing conceptual understanding over syntax memorization. Users also shared insights on career progression, including the value of job hopping, communicating growth interests to management, and acknowledging imposter syndrome as a common experience in data engineering. Some comments also touched on the perceived "boring" nature of DE work after initial setup.
*   **Emotion:** The dominant emotion is **Supportive and Encouraging**, offering reassurance to the anxious poster. There are elements of candid realism about the nature of the job and imposter syndrome, contributing to a balanced but ultimately helpful tone.
*   **Top 3 Points of View:**
    *   Strong reassurance that the user is "definitely not cooked" at 3 years experience; imposter syndrome is common and manageable.
    *   Focus on improving technical skills through deliberate practice (coding from scratch, using references only when stuck) and understanding core concepts rather than just memorizing syntax.
    *   Career growth often involves job hopping around the 2-3 year mark to broaden experience and increase salary, or actively communicating growth interests to current management.

**[My brain freezes while solving or writing SQL queries. (Score: 21)](https://www.reddit.com/r/dataengineering/comments/1r2j1ae/my_brain_freezes_while_solving_or_writing_sql/)**
*   **Summary:** The thread addresses the common challenge of "brain freezing" when solving or writing SQL queries, particularly in high-pressure situations like interviews. Users recommended extensive practice with sample databases and platforms like DataLemur, focusing on understanding SQL's internal order of operations and query profiles rather than rote memorization. Key advice highlighted the difference between iterative real-world SQL writing and the "compile in your head" skill needed for interviews, suggesting practice with common patterns and verbalizing thought processes. The use of AI tools for syntax assistance was also mentioned.
*   **Emotion:** The tone is primarily **Instructive and Supportive**, providing practical advice and empathy for a common technical struggle. While one comment expressed a personal dislike for programming compared to SQL, the overall sentiment is geared towards problem-solving.
*   **Top 3 Points of View:**
    *   Consistent practice (e.g., using platforms like DataLemur, downloading sample databases) is the most effective way to overcome mental blocks and build muscle memory for SQL.
    *   Understanding the underlying logic of SQL, such as the order of operations and query profiles, helps in conceptualizing queries and writing them more effectively.
    *   Interview performance in SQL requires a different skill ("compile in your head") than real-world iterative coding; practicing common patterns and vocalizing thought processes can help.

**[Being pushed out of job, trying to plan next steps (Score: 16)](https://www.reddit.com/r/dataengineering/comments/1r2wvxg/being_pushed_out_of_job_trying_to_plan_next_steps/)**
*   **Summary:** The user, facing job displacement, sought guidance on next steps, particularly regarding transitioning to cloud data engineering. The community provided practical advice on leveraging free trials from vendors like Snowflake and dbt for hands-on learning, suggesting resources like John Savill's YouTube channel for cloud concepts. Personal anecdotes shared the frustration of needing direct experience with specific tech stacks for job interviews, but also offered encouragement that core DE principles remain transferable.
*   **Emotion:** The emotional tone is **Empathetic and Practical**. While some comments share past negative experiences (job loss), the overall advice is constructive and supportive, focusing on actionable steps for career recovery and growth.
*   **Top 3 Points of View:**
    *   Utilize free trials from cloud vendors (Snowflake, Databricks, dbt) to gain hands-on experience and build portfolio projects for learning new cloud technologies.
    *   The transition to cloud isn't overly difficult; core data engineering principles apply, but understanding specific solutions, RBAC, and general cloud architecture (e.g., via John Savill's videos) is key.
    *   Job searching can be frustrating as companies often prioritize candidates with direct experience in their exact tech stack, making continuous hands-on learning important.

**[Am I being anxious too early? (Score: 6)](https://www.reddit.com/r/dataengineering/comments/1r2uova/am_i_being_anxious_too_early/)**
*   **Summary:** A user inquired about early career anxiety. Responses underscored the critical importance of understanding the theoretical "why" behind data technologies and concepts (e.g., ACID, IAC) rather than just learning specific tools, as technologies constantly evolve. Comments also highlighted that AI can manage syntax, further freeing engineers to focus on theoretical knowledge. One user vividly described the significant, broad responsibilities of modern engineers, leading to anxiety, suggesting that such stress might be company-specific.
*   **Emotion:** The emotional tone is **Mixed**, combining thoughtful guidance on long-term learning strategies with expressions of significant job-related anxiety. It's largely neutral in its advice but acknowledges the emotional toll of the profession.
*   **Top 3 Points of View:**
    *   Focus on learning the underlying theory and "why" behind technologies (e.g., ACID, IAC) rather than just the tools, as theoretical knowledge is evergreen while tools change.
    *   AI tools can handle syntax, making conceptual and theoretical understanding even more crucial for engineers in the age of AI.
    *   Modern engineering roles can be a source of significant anxiety due to broad responsibilities that extend beyond core tasks, potentially indicating a systemic issue in some companies.

**[11 Compaction Strategies for Iceberg Data Lakes (Score: 5)](https://overcast.blog/11-compaction-strategies-for-iceberg-data-lakes-906d347af0f9)**
*   **Summary:** This thread discusses compaction strategies for Iceberg Data Lakes, triggered by an article. Comments emphasized that data lakes typically require substantial scale to justify their operational complexity. Practical advice was given on tailoring compaction strategies to specific read patterns and data loads (streaming versus batch), with a specific recommendation for hourly bin-packing and nightly sort compaction for high-throughput ingestion to maintain query performance without excessive compute costs.
*   **Emotion:** The emotional tone is **Informative and Technical**. Discussions are factual and focused on best practices and justifications for specific architectural choices in data lakes.
*   **Top 3 Points of View:**
    *   Data lakes introduce complexity that is only justified when dealing with data at significant scale.
    *   Effective compaction strategies must be chosen based on specific read patterns and the nature of data loads (streaming updates vs. batch loads).
    *   For high-throughput ingestion, a combination of hourly bin-packing for small files and nightly sort compaction can maintain stable query performance efficiently.

**[ERP sysadmin vs Data Engineering (Score: 5)](https://www.reddit.com/r/dataengineering/comments/1r2pzwc/erp_sysadmin_vs_data_engineering/)**
*   **Summary:** The thread compares career paths between an ERP sysadmin role and data engineering. The consensus leaned heavily towards data engineering, citing its wider range of opportunities, diverse specializations, more stimulating work, and generally higher earning potential. While ERP sysadmin roles were acknowledged for their security, they were also noted for potentially offering less stimulating work and lower pay.
*   **Emotion:** The tone is **Neutral and Comparative**, providing objective (though often opinionated) perspectives on career choices within tech.
*   **Top 3 Points of View:**
    *   Data Engineering generally offers more diverse opportunities, specializations, and a broader array of tools to learn compared to an ERP sysadmin role.
    *   Data Engineering is perceived as more stimulating and financially rewarding, offering better payment potential.
    *   While ERP technical skills can provide job security, the compensation might be lower and the work less stimulating than in data engineering.

**[jack of all trades VS a master of one, how should I learn as a junior engineer? (Score: 4)](https://www.reddit.com/r/dataengineering/comments/1r2xnf6/jack_of_all_trades_vs_a_master_of_one_how_should/)**
*   **Summary:** A junior engineer asked whether to focus on being a "jack of all trades" or a "master of one" in data engineering. The strong sentiment from the community favored becoming a "jack of all trades," citing the broad and constantly evolving nature of the DE field where deep specialization in a single tool is often impractical. Advice included concentrating on evergreen skills like Python, SQL, databases, and fundamental tools (Airflow, Docker, GitHub, Kubernetes, DBT). Some frustration was expressed regarding company structures that don't always foster deep technical growth and the perceived disparity in salary growth between technical and non-technical roles.
*   **Emotion:** The emotional tone is **Practical and Realistic**, with an underlying current of **Frustration** regarding career development challenges and industry trends.
*   **Top 3 Points of View:**
    *   Data Engineers should aim to be "jacks of all trades" due to the field's vastness and constant evolution, making deep specialization in a single area nearly impossible and less beneficial.
    *   Focus on mastering evergreen foundational skills such as Python (with data packages), SQL, databases (RDBMS, NoSQL), data warehousing, and key tools like Airflow, Docker, GitHub, and Kubernetes.
    *   There's a shared frustration that companies often imply a need for mastery in job descriptions while actual roles demand broad skills, coupled with concerns about limited technical growth opportunities and salary stagnation compared to non-technical roles.

**[I want to practise Dimensional Data Modelling but im lost (Score: 3)](https://www.reddit.com/r/dataengineering/comments/1r2shmx/i_want_to_practise_dimensional_data_modelling_but/)**
*   **Summary:** A user seeking to practice dimensional data modeling was given a helpful recommendation: the Fantasy Premier League API. This API was highlighted as a suitable, free-to-use resource that requires no authentication and has not shown rate limiting, making it ideal for learning projects, despite its documentation being limited to a linked Medium article.
*   **Emotion:** The tone is **Helpful and Instructive**, offering a direct and practical solution to the user's query.
*   **Top 3 Points of View:**
    *   The Fantasy Premier League API is an excellent, free, and accessible resource for practicing dimensional data modeling.
    *   It offers real-world data without requiring authentication, making it easy to integrate into learning projects.
    *   While official documentation is limited, a linked Medium article serves as a helpful guide for understanding the API's endpoints.

**[What to learn besides DE (Score: 3)](https://www.reddit.com/r/dataengineering/comments/1r2svde/what_to_learn_besides_de/)**
*   **Summary:** In response to a query about learning beyond core data engineering, users recommended expanding into data infrastructure skills, particularly Docker and Kubernetes, for prototyping and cloud-agnostic deployment. Emphasized was the importance of developing a deeper theoretical understanding of data-intensive systems (e.g., through "Designing Data Intensive Applications"). Additionally, understanding the business context and specializing in key industries like Telecom, Banking, Fintech, or Government was suggested for career advancement and higher salaries. One humorous comment simply advised learning "that AI can't replace you."
*   **Emotion:** The tone is **Instructive and Forward-looking**, offering strategic advice for long-term career growth in data engineering. It's largely neutral, focusing on practical learning paths.
*   **Top 3 Points of View:**
    *   Develop data infrastructure skills (Docker, Kubernetes, networking concepts) to enable rapid prototyping, cloud-agnostic deployment, and effective troubleshooting.
    *   Gain a deep theoretical understanding of data-intensive applications and systems, moving beyond just tool-specific knowledge.
    *   Cultivate business acumen and specialize in an industry (e.g., Telecom, Banking) to become an "Industry Specialized Data Engineer," which can lead to significant salary increases.

**[Matching Records (Score: 3)](https://www.reddit.com/r/dataengineering/comments/1r2tpnt/matching_records/)**
*   **Summary:** The discussion centers on the difficulty of "matching records" in small-to-medium companies. The primary advice suggested using graph databases as the easiest solution if resources permit. For situations with limited resources, a Python-based iterative approach, scrutinizing identifiers on events, was recommended for non-"Big Data" scenarios. This problem was described as one of the most challenging in the user's experience.
*   **Emotion:** The tone is **Problem-Focused and Practical**, acknowledging the difficulty of the task but offering actionable solutions. There's an underlying negative sentiment about the problem's inherent challenge.
*   **Top 3 Points of View:**
    *   Record matching is a notoriously difficult problem for small-to-medium companies, often requiring "overwrought" solutions.
    *   Graph databases offer the easiest solution if budget and resources are available, even though the functionality can be simulated with complex SQL.
    *   For resource-constrained scenarios with non-"Big Data," Python can be used for iterative processing, scrutinizing identifiers to assign master IDs and match records.

**[How do you push data from one api to another (Score: 2)](https://www.reddit.com/r/dataengineering/comments/1r2ann0/how_do_you_push_data_from_one_api_to_another/)**
*   **Summary:** The thread addressed the task of pushing data from one API to another. Responses included suggesting AI tools for generating basic code, providing a pseudocode example for simple GET and POST operations, and clarifying that while direct API-to-API pushes might fall outside the typical scope of data engineers, understanding event-driven architectures like pub/sub is highly relevant for managing data flows between systems.
*   **Emotion:** The tone is **Instructive and Clarifying**, breaking down the problem and providing different perspectives on how to approach it, from basic coding to architectural considerations.
*   **Top 3 Points of View:**
    *   AI tools (LLMs) can provide a quick starting point or "vibe code" for basic API interaction logic.
    *   A straightforward programming approach involves fetching data from one API (`doGet`) and then posting it to another (`doPost`).
    *   From a data engineering perspective, direct API-to-API pushes are often handled by software engineers, but event-driven patterns like publish/subscribe are relevant for managing data flows between systems.

**[Azure data engineering course (Score: 2)](https://www.reddit.com/r/dataengineering/comments/1r2w945/azure_data_engineering_course/)**
*   **Summary:** A user inquired about Azure data engineering courses. The response was an automated bot directing them to a community-submitted list of learning resources available on the dataengineering wiki.
*   **Emotion:** The tone is purely **Informative**, delivered by an automated system.
*   **Top 3 Points of View:**
    *   A community-curated list of learning resources is available for data engineering topics, including potentially Azure-related content. (Only one distinct POV as it's a bot response.)

**[Got tired of spinning up Flink to power a live dashboard, so I built a minimal Arrow-compatible data engine in Rust. Would love to hear your thoughts. (Score: 2)](https://www.reddit.com/r/dataengineering/comments/1r2f8bf/got_tired_of_spinning_up_flink_to_power_a_live/)**
*   **Summary:** A user presented a new minimal Arrow-compatible data engine built in Rust, positioned as an alternative to Flink for live dashboards. The primary comment provided strong criticism regarding the comparison to Flink, arguing that Flink is an industry standard for streaming and any new solution needs to clearly articulate its specific advantages for lightweight real-time dashboards without directly attempting to compete with such an established "beast."
*   **Emotion:** The tone is **Critical and Direct**, offering pointed feedback on product positioning and competitive strategy, but ultimately aiming for constructive criticism.
*   **Top 3 Points of View:**
    *   Comparing a new, minimal data engine directly to Flink is a strategic mistake, as Flink is the de facto standard for streaming processing and significantly more robust.
    *   A new lightweight solution needs to clearly define its unique advantages and specific use cases, focusing on what it does better without trying to compete head-on with established, powerful systems like Flink.
    *   Lightweight real-time dashboards can often be powered by simpler, existing technologies (e.g., PostgreSQL and a script), implying that a new engine needs a strong justification for its existence.

**[AI For Data Modelling?? (Score: 0)](https://www.reddit.com/r/dataengineering/comments/1r2uicu/ai_for_data_modelling/)**
*   **Summary:** The post questioned the utility of "AI For Data Modelling." The discussion encompassed diverse viewpoints: some advocated for using AI as a helpful tool (e.g., for exploring metrics or playing different roles), while others expressed skepticism about generative AI's ability to produce deterministic results and stressed the foundational necessity of ERDs for data modeling. There was also a strong negative reaction from one user, accusing the post of being an AI-generated advert. Technical points about common data modeling pitfalls, like assumptions about data cleanliness and issues with "WIDE tables" and data grain, were also raised.
*   **Emotion:** The emotional tone is **Mixed, ranging from Practical and Informative to Skeptical and Strongly Critical**. The presence of an accusatory comment adds a significant negative spike, creating a somewhat contentious atmosphere.
*   **Top 3 Points of View:**
    *   AI can be a useful assistant for data modeling by allowing users to simulate different roles (e.g., business SME, data steward) and explore funnel metrics or data quality issues.
    *   There is skepticism about generative AI's ability to produce reliable and deterministic data models, emphasizing that foundational elements like Entity-Relationship Diagrams (ERDs) remain critical.
    *   Concerns exist about the quality and intent of AI-generated content (e.g., accusations of AI-written adverts) and traditional data modeling pitfalls like incorrect assumptions about data cleanliness or issues with wide tables and data grain.

**[How are you protecting your repos in the age of AI, especially in data engineering? (Score: 0)](https://www.reddit.com/r/dataengineering/comments/1r2p04z/how_are_you_protecting_your_repos_in_the_age_of/)**
*   **Summary:** The thread explored methods for protecting repositories in data engineering projects, especially concerning AI-generated code. The consensus was that while AI (like Claude Code) can aid with code style, it frequently fails to catch logical bugs. Effective protection strategies highlighted involved extensive custom testing, specifically singular tests designed to encode precise business rules, and requiring example queries with expected outputs in Pull Request descriptions for manual verification. Continuous Integration (CI) pipelines with enforced schema and singular tests were identified as crucial guardrails for maintaining code quality, particularly for open-source contributions.
*   **Emotion:** The tone is **Informative and Practical**, focusing on concrete strategies and learned experiences in managing AI-assisted development and ensuring code quality.
*   **Top 3 Points of View:**
    *   AI tools assist with code style but are ineffective at catching logical bugs and violations of business rules in data engineering code.
    *   Effective protection relies heavily on robust testing, particularly custom singular tests that encode specific business rules (e.g., "revenue should never be negative").
    *   Continuous Integration (CI) pipelines, which require schema tests and singular tests before merging, serve as essential guardrails for maintaining code quality, especially for open-source projects.

**[AWS Data Engineering services and Prep (Score: 0)](https://www.reddit.com/r/dataengineering/comments/1r2xcx5/aws_data_engineering_services_and_prep/)**
*   **Summary:** A user inquired about AWS Data Engineering services and preparation. The responses included an ironic suggestion to start with documentation from alternative cloud providers like GCP and Azure, alongside an automated bot response directing to general data engineering learning resources available on a community wiki.
*   **Emotion:** The tone is **Neutral**, with a hint of dry humor or irony in one of the suggestions, balanced by a straightforward informational bot response.
*   **Top 3 Points of View:**
    *   (Ironic) When seeking information about a specific cloud provider (AWS), some suggest exploring documentation from competing providers (GCP, Azure) for a broader perspective.
    *   Community-submitted learning resources are available on a dedicated wiki for general data engineering topics.
    *   The original post is a direct query for specific platform knowledge.

**[How is Agentic AI going to change data engineering? (Score: 0)](https://www.reddit.com/r/dataengineering/comments/1r2zwqv/how_is_agentic_ai_going_to_change_data_engineering/)**
*   **Summary:** The discussion revolved around how "Agentic AI" might change data engineering. Responses offered a mixed perspective: some acknowledged AI's potential to assist with pipeline development, SQL tuning, and optimization of Spark clusters, but tempered expectations by noting that current LLMs are not as advanced as often perceived. Other comments introduced elements of humor or skepticism, with one questioning if the original poster was a bot and another jokingly suggesting a "self destruct sequence."
*   **Emotion:** The emotional tone is **Mixed and Speculative**, blending practical insights with a degree of skepticism and humor regarding the transformative potential of Agentic AI.
*   **Top 3 Points of View:**
    *   Agentic AI and LLMs have the potential to assist data engineers in various tasks such as pipeline development, SQL tuning, and optimizing data processing clusters (e.g., Spark).
    *   A realistic perspective suggests that current LLMs are not as advanced as general perception might indicate, implying their utility is still somewhat limited or partial.
    *   There's a humorous or skeptical undercurrent regarding the nature of the original question or the poster, with comments questioning if the user is a bot.
