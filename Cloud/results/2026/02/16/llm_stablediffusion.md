---
title: "Stable Diffusion Subreddit"
date: "2026-02-16"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["StableDiffusion", "AI Models", "Image Generation", "LoRA Training", "AI Tools"]
---

# Overall Ranking and Top Discussions
1.  [Just for fun, created with ZIT and WAN](https://v.redd.it/tg1w1bxbnwjg1) (Score: 79)
    *   This thread features a video created using ZIT and WAN, with users expressing enjoyment and asking technical questions about the workflow and speed.
2.  [Switching to OneTrainer made me realize how overfitted my AI-Toolkit LoRAs were](https://www.reddit.com/r/StableDiffusion/comments/1r6fgzx/switching_to_onetrainer_made_me_realize_how/) (Score: 56)
    *   Discussion around the benefits of OneTrainer over AI-Toolkit for LoRA training, focusing on overfitting detection, alongside user experience feedback (both positive and negative) and technical queries.
3.  [LTX-2 is addictive (LTX-2 A+T2V)](https://v.redd.it/rc3w0cdi9wjg1) (Score: 14)
    *   A post showcasing a creation made with LTX-2 (A+T2V), leading to positive reactions about the style and a question about the specific workflow used.
4.  [Boulevard du Temple (one of the world's oldest photos) restored using Flux 2](https://www.reddit.com/gallery/1r6kbfb) (Score: 10)
    *   A restored historical photo using Flux 2 sparks discussion on restoration prompts and ethical concerns about altering historical images by removing elements.
5.  [Hey everyone did anyone tried the new deepgen1.0 ?](https://huggingface.co/deepgenteam/DeepGen-1.0) (Score: 7)
    *   Users share their experiences and concerns about the DeepGen-1.0 model, highlighting issues like broken code, security risks, outdated file formats, and resolution limitations.
6.  [I wondered what kind of PC specification they have for this real-time lipsync ðŸ¤”](https://www.reddit.com/r/StableDiffusion/comments/1r6jpte/i_wondered_what_kind_of_pc_specification_they/) (Score: 3)
    *   A user inquires about the hardware requirements for real-time lipsync technology, with a comment suggesting it might be filter-based and less demanding than expected.
7.  [Does klein 9b base lora works on non base model?](https://www.reddit.com/r/StableDiffusion/comments/1r6hv3z/does_klein_9b_base_lora_works_on_non_base_model/) (Score: 3)
    *   Users confirm and explain that Klein 9b base LoRAs are indeed compatible and effective with non-base models, which is their intended use.
8.  [Best config to use Flux.2 Klein on Forge-Neo?](https://www.reddit.com/r/StableDiffusion/comments/1r6cvrc/best_config_to_use_flux2_klein_on_forgeneo/) (Score: 2)
    *   A request for optimal configuration settings for Flux.2 Klein on Forge-Neo, receiving advice on experimenting with step counts and a clarifying question about the Klein variant.
9.  [How to update multiple WanImagetoVideo from one node?](https://www.reddit.com/r/StableDiffusion/comments/1r6jh8r/how_to_update_multiple_wanimagetovideo_from_one/) (Score: 2)
    *   A user asks for methods to update multiple WanImagetoVideo nodes efficiently, receiving a suggestion to use an image scaling node.
10. [Questions about LoRA training](https://www.reddit.com/r/StableDiffusion/comments/1r6eckx/questions_about_lora_training/) (Score: 1)
    *   Discussion providing advice on offline LoRA training, recommended tools (Z-Image Turbo, OneTrainer, SimpleTuner), and model choices for specific output types.
11. [SD on macs](https://www.reddit.com/r/StableDiffusion/comments/1r6hxd2/sd_on_macs/) (Score: 1)
    *   Recommendations for running Stable Diffusion on Macs, with "Draw Things" app being the primary suggestion due to its compatibility with various AI models.
12. [Best model stack for hair/beard/brow/makeup local edits without changing face or background?](https://www.reddit.com/r/StableDiffusion/comments/1r6ifli/best_model_stack_for_hairbeardbrowmakeup_local/) (Score: 1)
    *   Users seek and provide advice on models for localized facial edits, with "Klein" and masking techniques highlighted as effective solutions.
13. [Yennefer z Vengerbergu. Witcher 3 Remake Art.](https://www.reddit.com/gallery/1r6h6lm) (Score: 0)
    *   A user shares AI-generated art of Yennefer, leading to an observation about how minor AI changes can result in significant visual transformations, even gender shifts.
14. [I tested the same prompt across 5 different AI models - here are the surprising results](https://i.redd.it/lwlrzi0cyvjg1.png) (Score: 0)
    *   A comparative analysis of AI models for image generation, sparking discussions on model performance, the importance of detailed prompts, and constructive critiques of testing methodologies.
15. [best local tool for image editing?](https://www.reddit.com/r/StableDiffusion/comments/1r6fh1s/best_local_tool_for_image_editing/) (Score: 0)
    *   A query about local image editing tools receives a recommendation for InvokeAI for Nvidia GPUs and a clarifying question about the term "tool."
16. [what AI Should download for for generating videos and pictures?](https://www.reddit.com/r/StableDiffusion/comments/1r6hfa6/what_ai_should_download_for_for_generating_videos/) (Score: 0)
    *   A user asks for AI recommendations for video and picture generation, receiving a comprehensive list of beginner-friendly and advanced tools/models.
17. [Is there anywhere I can download real people Klein Loras from considered very good ? I tried training and the results are awful. I don't know if I'm being too critical or if my blondes really are terrible.](https://www.reddit.com/r/StableDiffusion/comments/1r6lf5o/is_there_anywhere_i_can_download_real_people/) (Score: 0)
    *   A user expresses frustration with their own LoRA training for "real people" and seeks recommendations for high-quality pre-trained Klein LoRAs, receiving a link to a Hugging Face resource.

# Detailed Analysis by Thread
**[Just for fun, created with ZIT and WAN (Score: 79)](https://v.redd.it/tg1w1bxbnwjg1)**
*   **Summary:** Users are discussing a fun video creation made with ZIT and WAN, expressing enjoyment and asking for technical details about the generation process.
*   **Emotion:** Predominantly Positive, characterized by appreciation and enthusiasm ("love it," "cute," emojis), alongside a significant undercurrent of Neutral, driven by technical curiosity and questions about the tools and workflow.
*   **Top 3 Points of View:**
    *   Users express strong appreciation and enjoyment for the creative output, finding it fun and cute.
    *   There's considerable technical interest in the specific tools and methods used, such as how to achieve natural speed with WAN or the workflow (e.g., "flf workflow or only image2video?").
    *   A minor point of view expresses lighthearted curiosity about the nature of the content, referring to "funny human videos."

**[Switching to OneTrainer made me realize how overfitted my AI-Toolkit LoRAs were (Score: 56)](https://www.reddit.com/r/StableDiffusion/comments/1r6fgzx/switching_to_onetrainer_made_me_realize_how/)**
*   **Summary:** This thread discusses the user's experience switching from AI-Toolkit to OneTrainer for LoRA training, highlighting OneTrainer's advantages in detecting overfitting. The conversation includes both praise and criticism for OneTrainer's usability, along with numerous technical questions.
*   **Emotion:** Mixed. There's enthusiasm and Positive sentiment for OneTrainer's performance (speed, memory, defaults) and ease of setup, but also significant Negative sentiment regarding its user interface, scalability on HiDPI, and workflow complexity. Many comments are Neutral, focusing on technical inquiries and comparisons.
*   **Top 3 Points of View:**
    *   **OneTrainer is a superior tool for LoRA training:** Many users praise OneTrainer for its effectiveness in detecting overfitting, good default templates, speed, and memory management, making it a strong alternative to other tools like AI-Toolkit.
    *   **OneTrainer suffers from usability and UI issues:** A significant portion of feedback highlights frustration with OneTrainer's outdated UI, poor scaling on high-resolution displays (especially on Linux), and a perceived overly complex workflow for basic tasks like adding datasets.
    *   **Users seek specific technical configurations and compatibility information:** There are numerous questions about optimal training configurations, hardware requirements (e.g., "train 9b with 5060ti?"), compatibility with specific models (Wan2.2), and where to find templates or run it on cloud platforms.

**[LTX-2 is addictive (LTX-2 A+T2V) (Score: 14)](https://v.redd.it/rc3w0cdi9wjg1)**
*   **Summary:** A user showcases a video created with LTX-2 (specifically LTX-2 A+T2V), expressing how "addictive" it is. Comments largely appreciate the visual style, with one asking about the workflow used.
*   **Emotion:** Strongly Positive, reflecting widespread enjoyment and appreciation for the style and output of the LTX-2 creation. A single Neutral comment queries the technical workflow.
*   **Top 3 Points of View:**
    *   Enthusiastic appreciation for the aesthetic quality and style of the video generated with LTX-2.
    *   Curiosity regarding the specific technical workflow or methods employed to achieve the presented results (e.g., "Vrgirl workflow?").
    *   The post itself implicitly conveys that LTX-2 is a highly engaging and enjoyable tool, potentially "addictive" for creators.

**[Boulevard du Temple (one of the world's oldest photos) restored using Flux 2 (Score: 10)](https://www.reddit.com/gallery/1r6kbfb)**
*   **Summary:** A Reddit user shares a restoration of the historic "Boulevard du Temple" photograph using Flux 2. The discussion includes a user sharing their prompt for similar restorations and another user raising a critical point about the potential erasure of historical figures through AI restoration.
*   **Emotion:** Mixed. There's enthusiasm for AI photo restoration capabilities, but also a strong sense of concern and critique regarding the ethical implications of AI altering historical context by removing iconic elements or figures.
*   **Top 3 Points of View:**
    *   Admiration and enthusiasm for using AI models like Flux 2 and Klein 9b to restore and enhance old photographs to a modern, high-resolution aesthetic.
    *   Users are keen to share and learn specific prompts and configurations for achieving desired restoration effects (e.g., "clean it, sharpen it, neutral colors").
    *   A significant concern is raised about the ethical implications of AI restoration, particularly when it inadvertently (or explicitly) removes historically significant elements or individuals, likening it to a "1984" rewriting of history.

**[Hey everyone did anyone tried the new deepgen1.0 ? (Score: 7)](https://huggingface.co/deepgenteam/DeepGen-1.0)**
*   **Summary:** This thread initiates a discussion about the DeepGen-1.0 model. Users who have tried it report a range of issues, including broken sample code, concerns over `.pt` file security, outdated distribution formats, and restrictive output resolutions.
*   **Emotion:** Predominantly Negative and Skeptical. Users express frustration with the model's functionality, security concerns regarding file types, and disappointment over its limitations, contributing to a generally unfavorable impression.
*   **Top 3 Points of View:**
    *   DeepGen-1.0 has significant functional problems, including broken sample Python code that prevents users from generating content.
    *   There are security concerns about running `.pt` files, with users expressing a preference for safer formats like `.safetensors` to mitigate risks of malicious code.
    *   The model is criticized for using outdated packaging formats (ckpt and zip in 2026) and for being restricted to a low output resolution (512x512), which makes it "not worth using" for many.

**[I wondered what kind of PC specification they have for this real-time lipsync ðŸ¤” (Score: 3)](https://www.reddit.com/r/StableDiffusion/comments/1r6jpte/i_wondered_what_kind_of_pc_specification_they/)**
*   **Summary:** The original poster expresses curiosity about the PC specifications required for real-time lipsync technology. The sole comment suggests that the technology might be based on filters rather than full AI, implying it could run on less powerful devices like smartphones.
*   **Emotion:** Neutral. The thread is driven by an inquisitive tone from the original poster, met with a pragmatic and speculative, yet informative, response.
*   **Top 3 Points of View:**
    *   Curiosity about the demanding hardware specifications needed to run advanced real-time AI features like lipsync.
    *   Speculation that the real-time lipsync technology might be filter-based or simpler algorithms rather than resource-intensive full AI.
    *   The possibility that such technology could be accessible on consumer devices like smartphones, suggesting lower hardware requirements than initially imagined.

**[Does klein 9b base lora works on non base model? (Score: 3)](https://www.reddit.com/r/StableDiffusion/comments/1r6hv3z/does_klein_9b_base_lora_works_on_non_base_model/)**
*   **Summary:** A user asks whether Klein 9b base LoRAs are compatible with non-base models. The community provides affirmative answers, clarifying that this compatibility is precisely the intended design and that it works effectively.
*   **Emotion:** Positive and Informative. The overall tone is helpful and reassuring, with users confidently confirming the functionality and explaining the purpose of base LoRAs.
*   **Top 3 Points of View:**
    *   Klein 9b base LoRAs are indeed compatible with non-base models.
    *   This compatibility is the fundamental purpose of base LoRAs: to be trained with a base model and then applied to a distilled, non-base model.
    *   The general consensus is that this functionality works "very well" in practice.

**[Best config to use Flux.2 Klein on Forge-Neo? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1r6cvrc/best_config_to_use_flux2_klein_on_forgeneo/)**
*   **Summary:** A user is seeking advice on the optimal configuration for running Flux.2 Klein on the Forge-Neo platform. Responses include a suggestion to experiment with different step counts for editing and a request for clarification on the specific Klein variant being used.
*   **Emotion:** Neutral. The thread revolves around technical problem-solving and information gathering, with no strong emotional expressions.
*   **Top 3 Points of View:**
    *   Users are seeking specific configuration advice for combining Flux.2 Klein with Forge-Neo to achieve optimal results.
    *   A practical suggestion involves trying different step counts (e.g., 2, 3, 4, 8) when editing, as this can lead to varied outcomes.
    *   There's a need for clarification regarding the exact variant of Klein being used to provide more precise and relevant configuration advice.

**[How to update multiple WanImagetoVideo from one node? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1r6jh8r/how_to_update_multiple_wanimagetovideo_from_one/)**
*   **Summary:** A user asks for a method to update multiple WanImagetoVideo nodes from a single source node. The lone comment advises sharing the current workflow and suggests using an `imagescaletoratio` node for input/reference image scaling as a common solution.
*   **Emotion:** Neutral. The discussion is purely technical, focused on a workflow challenge and a pragmatic solution.
*   **Top 3 Points of View:**
    *   The user is looking for an efficient way to manage and update multiple video generation nodes from a single input source.
    *   A common approach to achieve this is by scaling the input or reference image using an `imagescaletoratio` node.
    *   To provide more specific help, it's necessary to understand the user's existing workflow.

**[Questions about LoRA training (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1r6eckx/questions_about_lora_training/)**
*   **Summary:** This thread addresses general inquiries about LoRA training, covering offline training capabilities, suitable software, and how to achieve "better output." Comments offer recommendations for various training tools and discuss model choices for specific aesthetic goals.
*   **Emotion:** Positive and Informative. The tone is largely helpful, with community members sharing their knowledge and positive experiences with different LoRA training methods and tools.
*   **Top 3 Points of View:**
    *   Offline LoRA training is highly feasible and effective even with mid-range GPUs (e.g., 3060), and even more so with higher-end cards (e.g., 5090).
    *   Multiple software options are recommended for LoRA training, including Z-Image Turbo, AI-Toolkit, OneTrainer, and SimpleTuner, each with unique features and user bases.
    *   The definition of "better output" for LoRAs is subjective and depends on the desired outcome; for realistic photos, Z-Image Turbo is suggested, while Flux2 Klein is also noted as a promising option.

**[SD on macs (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1r6hxd2/sd_on_macs/)**
*   **Summary:** A user asks for advice on running Stable Diffusion on Mac computers. The responses consistently recommend the "Draw Things" app, highlighting its ability to run various popular AI models.
*   **Emotion:** Neutral and Informative. The thread directly addresses a technical query with clear, concise, and consistent recommendations.
*   **Top 3 Points of View:**
    *   The "Draw Things" app is the go-to recommendation for running Stable Diffusion on Mac devices.
    *   "Draw Things" offers broad compatibility, supporting various popular AI models like Flux, Qwen, and Z Image.
    *   Users looking to utilize Stable Diffusion on macOS should explore the Draw Things app and its dedicated subreddit for further resources.

**[Best model stack for hair/beard/brow/makeup local edits without changing face or background? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1r6ifli/best_model_stack_for_hairbeardbrowmakeup_local/)**
*   **Summary:** A user seeks recommendations for the optimal model stack to perform localized edits on facial features such as hair, beard, brows, or makeup, specifically without altering the surrounding face or background. Responses suggest using "Klein" in conjunction with masking techniques and careful parameter tuning.
*   **Emotion:** Neutral. The discussion is purely technical, focused on seeking and providing specific advice for a detailed image editing task.
*   **Top 3 Points of View:**
    *   Klein is a highly recommended model for performing precise, localized edits on facial features.
    *   Utilizing a mask with Klein is crucial for ensuring that edits are confined to the target area (e.g., hair, beard) and do not inadvertently affect the rest of the face or background.
    *   When using Klein for such edits, it's important to be mindful of parameters and avoid changing broader elements like lighting or color to maintain consistency.

**[Yennefer z Vengerbergu. Witcher 3 Remake Art. (Score: 0)](https://www.reddit.com/gallery/1r6h6lm)**
*   **Summary:** This thread features AI-generated art depicting Yennefer from The Witcher 3, presented as "Remake Art." A comment observes how subtle changes in the AI's output can lead to surprising transformations, specifically noting a shift in perceived gender between images.
*   **Emotion:** Neutral and Observational. The primary tone is descriptive, highlighting an interesting outcome of AI generation rather than expressing strong positive or negative feelings.
*   **Top 3 Points of View:**
    *   The post showcases AI-generated interpretations of a popular character from The Witcher 3, framed as concept art for a remake.
    *   AI models can produce surprising and sometimes significant visual alterations from seemingly minor changes in input or iteration, even leading to changes in perceived gender.
    *   There's an implicit interest in the transformative capabilities and potential "imperfections" or unexpected outputs of AI art generation.

**[I tested the same prompt across 5 different AI models - here are the surprising results (Score: 0)](https://i.redd.it/lwlrzi0cyvjg1.png)**
*   **Summary:** A user shares the results of testing a single prompt across five different AI models. The subsequent discussion involves evaluating the models, critiquing the brevity of the prompt used for the test, and offering a highly detailed example of an expanded prompt for high-quality product photography.
*   **Emotion:** Predominantly Neutral and Analytical. The thread is characterized by objective model comparison, constructive criticism regarding testing methodology, and helpful contributions of advanced prompting techniques. There's some subtle positive evaluation of specific models.
*   **Top 3 Points of View:**
    *   Evaluation of AI model performance: Flux 2 Pro is considered superior for commercial/business applications, and GPT Image 1.5 is also highly regarded. There's curiosity about the relative standing of models like Google's Nano Banana.
    *   Critique of prompt brevity: Many users argue that the original prompt was too short and lacked sufficient detail for a conclusive test, leaving too much to the models' interpretation and making it difficult for models to genuinely "fail."
    *   The importance of detailed prompting: An extensive, highly descriptive prompt example for "Ultimate Luxury Watch Product Photography" is shared, demonstrating how granular detail is necessary to achieve professional, high-quality, and controlled AI-generated images.

**[best local tool for image editing? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1r6fh1s/best_local_tool_for_image_editing/)**
*   **Summary:** A user seeks recommendations for the best local tool for image editing. The responses include a recommendation for InvokeAI, specifically for users with Nvidia GPUs and CUDA, and a clarifying question to ascertain if the user is asking about software tools or AI models.
*   **Emotion:** Neutral. The thread is purely informative, focusing on providing technical guidance and seeking clarification for better assistance.
*   **Top 3 Points of View:**
    *   The "best" local image editing tool is hardware-dependent; InvokeAI is recommended for users with Nvidia GPUs and CUDA.
    *   There's a need to clarify whether the user is seeking a software application ("tool") or a specific AI model for image editing.
    *   InvokeAI is presented as a viable free community option for local image editing, with a link provided for more information.

**[what AI Should download for for generating videos and pictures? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1r6hfa6/what_ai_should_download_for_for_generating_videos/)**
*   **Summary:** A user asks for recommendations on which AI tools to download for generating both videos and pictures. The response provides a comprehensive guide, suggesting beginner-friendly options like Wan2GP and more advanced tools like ComfyUI, along with specific model recommendations for video and image generation.
*   **Emotion:** Highly Informative and Positive. The tone is very helpful and appreciative, as a detailed list of resources is provided, which is met with gratitude.
*   **Top 3 Points of View:**
    *   For beginners, Wan2GP is recommended as a starting point, easily installable via direct GitHub link or Pinokio.co's one-click installer.
    *   More advanced users or those seeking greater flexibility should consider ComfyUI, also available as a portable version through Pinokio.co.
    *   Specific model recommendations are provided for video generation (LTX-2, Wan 2.2) and image generation/editing (Flux Klein, Flux 2 Dev, Z Image, Z Image Turbo, Qwen Image variants), offering a comprehensive toolkit.

**[Is there anywhere I can download real people Klein Loras from considered very good ? I tried training and the results are awful. I don't know if I'm being too critical or if my blondes really are terrible. (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1r6lf5o/is_there_anywhere_i_can_download_real_people/)**
*   **Summary:** A user expresses frustration with their own attempts at training Klein LoRAs for "real people," describing their results as "awful," and seeks recommendations for high-quality, pre-trained models. A helpful response provides a link to a Hugging Face Space as a potential resource.
*   **Emotion:** Neutral, with an underlying tone of frustration from the original poster about their training challenges, met by a concise and helpful informational response from the community.
*   **Top 3 Points of View:**
    *   The user is experiencing significant difficulty and dissatisfaction with their personal LoRA training efforts, particularly for generating realistic "real people."
    *   There is a strong desire to find high-quality, pre-trained Klein LoRAs for realistic human subjects, suggesting that readily available, well-made models are preferred over struggling with self-training.
    *   The community offers a specific resource (a Hugging Face Space by malcolmrey) as a potential source for such high-quality LoRAs.
