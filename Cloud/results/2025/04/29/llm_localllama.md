---
title: "LocalLLaMA Subreddit"
date: "2025-04-29"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "Local Models", "Qwen3", "Llama"]
---

# Overall Ranking and Top Discussions
1. [[D] No new models in LlamaCon announced](https://ai.meta.com/blog/llamacon-llama-news/) (Score: 107)
    *   Discusses the disappointment that no new models were announced at LlamaCon, focusing on Llama API, inference speeds, security tools, integrations and grants.
2. [Qwen3 vs Gemma 3](https://www.reddit.com/r/LocalLLaMA/comments/1kau30f/qwen3_vs_gemma_3/) (Score: 66)
    *   Compares the performance and capabilities of Qwen3 and Gemma 3 models in various tasks, including creative writing, language support, and factual knowledge retrieval.
3. [Qwen3 on Fiction.liveBench for Long Context Comprehension](https://i.redd.it/fpnum6crstxe1.png) (Score: 15)
    *   Discusses the performance of Qwen3 models on the Fiction.liveBench for long context comprehension, noting that they underperform compared to qwq-32b.
4. [Qwen3-235B-A22B is now available for free on HuggingChat!](https://hf.co/chat/models/Qwen/Qwen3-235B-A22B) (Score: 13)
    *   Announces the availability of Qwen3-235B-A22B on HuggingChat and encourages users to try it out and provide feedback.
5. [Qwen3-32B - Testing the limits of massive context sizes using a 107,142 tokens prompt](https://www.reddit.com/r/LocalLLaMA/comments/1kaw33r/qwen332b_testing_the_limits_of_massive_context/) (Score: 6)
    *   Explores the performance of Qwen3-32B with massive context sizes, specifically using a 107,142 tokens prompt.
6. [Qwen3-235B-A22B => UD-Q3_K_XL GGUF @12t/s with 4x3090 and old Xeon](https://www.reddit.com/r/LocalLLaMA/comments/1kawzia/qwen3235ba22b_udq3_k_xl_gguf_12ts_with_4x3090_and/) (Score: 6)
    *   Reports the performance of Qwen3-235B-A22B using UD-Q3_K_XL GGUF at 12t/s with 4x3090 and an old Xeon processor.
7. [How do you uncensor qwen3?](https://www.reddit.com/r/LocalLLaMA/comments/1kavfkv/how_do_you_uncensor_qwen3/) (Score: 5)
    *   Asks about methods to uncensor Qwen3 models, with suggestions focusing on system prompts.
8. [M4 Pro (48GB) Qwen3-30b-a3b gguf vs mlx](https://www.reddit.com/r/LocalLLaMA/comments/1kavr8r/m4_pro_48gb_qwen330ba3b_gguf_vs_mlx/) (Score: 5)
    *   Compares the performance of Qwen3-30b-a3b using GGUF on an M4 Pro (48GB) against MLX.
9. [Speech to Speech Interactive Model with tool calling support](https://www.reddit.com/r/LocalLLaMA/comments/1kaw484/speech_to_speech_interactive_model_with_tool/) (Score: 4)
    *   Discusses which company is most likely to create a speech-to-speech interactive model with tool calling support, speculating that OpenAI is the most likely candidate.
10. [So no new llama model today?](https://www.reddit.com/r/LocalLLaMA/comments/1kaweax/so_no_new_llama_model_today/) (Score: 4)
    *   Expresses disappointment that no new Llama model was announced, with some users hoping for a surprise announcement later in the day.
11. [ðŸ˜² M3Max vs 2xRTX3090 with Qwen3 MoE Against Various Prompt Sizes!](https://www.reddit.com/r/LocalLLaMA/comments/1kavlkz/m3max_vs_2xrtx3090_with_qwen3_moe_against_various/) (Score: 1)
    *   Presents a comparison of M3 Max versus 2xRTX3090 performance with Qwen3 MoE across various prompt sizes.
12. [Out of the game for 12 months, what's the goto?](https://www.reddit.com/r/LocalLLaMA/comments/1kawnki/out_of_the_game_for_12_months_whats_the_goto/) (Score: 1)
    *   Asks for recommendations on current tools and models for local LLM development after being out of the loop for a year.

# Detailed Analysis by Thread
**[ [D] No new models in LlamaCon announced (Score: 107)](https://ai.meta.com/blog/llamacon-llama-news/)**
*  **Summary:** The thread discusses the LlamaCon announcements, focusing on the absence of new model releases and the introduction of the Llama API, faster inference options, security tools, Llama Stack integrations, and impact grants.
*  **Emotion:** Predominantly neutral, with instances of negative sentiment due to disappointment over the lack of new model announcements.
*  **Top 3 Points of View:**
    *   Disappointment that no new Llama models were announced.
    *   Interest in the new Llama API and its potential benefits.
    *   Speculation on whether Llama 4 17B is worse than Qwen 3 14B.

**[Qwen3 vs Gemma 3 (Score: 66)](https://www.reddit.com/r/LocalLLaMA/comments/1kau30f/qwen3_vs_gemma_3/)**
*  **Summary:** This thread compares Qwen3 and Gemma 3 models, discussing their performance in creative writing, language support, factual knowledge, and overall stability.
*  **Emotion:** Mixed, with positive sentiment towards improvements in Qwen3, but also negative sentiment regarding its factual knowledge and consistency compared to Gemma 3.
*  **Top 3 Points of View:**
    *   Qwen3 has improved Japanese support and is faster than Gemma 3.
    *   Gemma 3 is better overall, especially in creative writing.
    *   Qwen3's factual knowledge is lacking, and it hallucinates badly in some cases.

**[Qwen3 on Fiction.liveBench for Long Context Comprehension (Score: 15)](https://i.redd.it/fpnum6crstxe1.png)**
*  **Summary:** This thread discusses the performance of Qwen3 models on the Fiction.liveBench for long context comprehension, noting that they underperform compared to qwq-32b.
*  **Emotion:** Predominantly neutral, with factual observations about model performance.
*  **Top 3 Points of View:**
    *   Qwen3 models are competitive against o3-mini and grok-3-mini on the Fiction.liveBench.
    *   Qwen3 models underperform qwq-32b on the Fiction.liveBench.
    *   Model performance seems to scale according to their active parameters.

**[Qwen3-235B-A22B is now available for free on HuggingChat! (Score: 13)](https://hf.co/chat/models/Qwen/Qwen3-235B-A22B)**
*  **Summary:** This thread announces the availability of Qwen3-235B-A22B on HuggingChat and encourages users to try it out and provide feedback.
*  **Emotion:** Neutral, with a positive undertone due to the announcement of the model's availability.
*  **Top 3 Points of View:**
    *   Encouragement to try out Qwen3-235B-A22B on HuggingChat.
    *   Information about HuggingChat being built on top of chat-ui.
    *   Anticipation for future developments and model releases.

**[Qwen3-32B - Testing the limits of massive context sizes using a 107,142 tokens prompt (Score: 6)](https://www.reddit.com/r/LocalLLaMA/comments/1kaw33r/qwen332b_testing_the_limits_of_massive_context/)**
*  **Summary:** This thread explores the performance of Qwen3-32B with massive context sizes, specifically using a 107,142 tokens prompt.
*  **Emotion:** Neutral, focusing on technical aspects and performance observations.
*  **Top 3 Points of View:**
    *   Qwen3 models break when using q4 cache but work fine with q8.
    *   128k context was just out of reach for P40 24GB.
    *   Suggestion to try fp8 and fp16 versions of the model.

**[Qwen3-235B-A22B => UD-Q3_K_XL GGUF @12t/s with 4x3090 and old Xeon (Score: 6)](https://www.reddit.com/r/LocalLLaMA/comments/1kawzia/qwen3235ba22b_udq3_k_xl_gguf_12ts_with_4x3090_and/)**
*  **Summary:** Reports the performance of Qwen3-235B-A22B using UD-Q3_K_XL GGUF at 12t/s with 4x3090 and an old Xeon processor.
*  **Emotion:** Neutral, focusing on technical details and hardware configurations.
*  **Top 3 Points of View:**
    *   Inquiry on how the model was loaded onto 4x RTX cards.

**[How do you uncensor qwen3? (Score: 5)](https://www.reddit.com/r/LocalLLaMA/comments/1kavfkv/how_do_you_uncensor_qwen3/)**
*  **Summary:** Asks about methods to uncensor Qwen3 models, with suggestions focusing on system prompts.
*  **Emotion:** Neutral, seeking technical advice.
*  **Top 3 Points of View:**
    *   Question about what prompts are being used to trigger censorship.
    *   Suggestion to use a system prompt that drives home that it should be uncensored.

**[M4 Pro (48GB) Qwen3-30b-a3b gguf vs mlx (Score: 5)](https://www.reddit.com/r/LocalLLaMA/comments/1kavr8r/m4_pro_48gb_qwen330ba3b_gguf_vs_mlx/)**
*  **Summary:** Compares the performance of Qwen3-30b-a3b using GGUF on an M4 Pro (48GB) against MLX.
*  **Emotion:** Neutral, focusing on technical performance and comparisons.
*  **Top 3 Points of View:**
    *   Acknowledgement that speed is good with MLX.
    *   Concern that MLX 4-bit quants quality is far worse compared to GGUF.

**[Speech to Speech Interactive Model with tool calling support (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1kaw484/speech_to_speech_interactive_model_with_tool/)**
*  **Summary:** Discusses which company is most likely to create a speech-to-speech interactive model with tool calling support, speculating that OpenAI is the most likely candidate.
*  **Emotion:** Neutral, based on speculation and reasoning.
*  **Top 3 Points of View:**
    *   OpenAI is the most likely candidate due to resources and a highly skilled team.

**[So no new llama model today? (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1kaweax/so_no_new_llama_model_today/)**
*  **Summary:** Expresses disappointment that no new Llama model was announced, with some users hoping for a surprise announcement later in the day.
*  **Emotion:** Neutral, with some disappointment and a glimmer of hope.
*  **Top 3 Points of View:**
    *   Disappointment over the lack of a new Llama model announcement.
    *   Hope for a later announcement.
    *   Belief that no model is better than a poor model.

**[ðŸ˜² M3Max vs 2xRTX3090 with Qwen3 MoE Against Various Prompt Sizes! (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1kavlkz/m3max_vs_2xrtx3090_with_qwen3_moe_against_various/)**
*  **Summary:** Presents a comparison of M3 Max versus 2xRTX3090 performance with Qwen3 MoE across various prompt sizes.
*  **Emotion:** Neutral, data-driven with suggestions for improvement.
*  **Top 3 Points of View:**
    *   Suggestion to use the fp8 model with vLLM for optimum performance.
    *   Recommendation to use VLLM or Exllama for a fair comparison against MLX.
    *   Question about watt usage per system for power analysis.

**[Out of the game for 12 months, what's the goto? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1kawnki/out_of_the_game_for_12_months_whats_the_goto/)**
*  **Summary:** Asks for recommendations on current tools and models for local LLM development after being out of the loop for a year.
*  **Emotion:** Neutral, seeking guidance and updated information.
*  **Top 3 Points of View:**
    *   Recommendation for Ollama and openwebui.
    *   Suggestion to try Qwen 3 8B or Qwen 2.5 7B.
    *   Mention of Gemma3 being good and having image understanding.
