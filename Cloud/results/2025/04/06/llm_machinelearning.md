---
title: "Machine Learning Subreddit"
date: "2025-04-06"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "NLP"]
---

# Overall Ranking and Top Discussions
1.  [[R] NoProp: Training neural networks without back-propagation or forward-propagation](https://www.reddit.com/r/MachineLearning/comments/1jsft3c/r_noprop_training_neural_networks_without/) (Score: 101)
    *   This thread discusses a new method called NoProp for training neural networks without back-propagation or forward-propagation.
2.  [[R] Image classification by evolving bytecode](https://zyme.dev/blog/1_image_classification_by_evolving_bytecode) (Score: 13)
    *   This thread is about image classification using evolved bytecode, with commenters requesting more details on the instruction set.
3.  [[D] Rich Sutton: Self-Verification, The Key to AI](http://www.incompleteideas.net/IncIdeas/KeytoAI.html) (Score: 12)
    *   This thread discusses Rich Sutton's ideas on self-verification as the key to AI, relating it to LLMs and external verifiers.
4.  [[D]IJCAI 2025 reviews and rebuttal discussion](https://www.reddit.com/r/MachineLearning/comments/1jss0lu/dijcai_2025_reviews_and_rebuttal_discussion/) (Score: 7)
    *   This thread is a discussion about the release of IJCAI 2025 reviews and rebuttals.
5.  [[P] anyone working on Arabic OCR?](https://www.reddit.com/r/MachineLearning/comments/1jsclxw/p_anyone_working_on_arabic_ocr/) (Score: 4)
    *   This thread inquires about Arabic OCR projects and suggests using vLLMs like Gemini.
6.  [[D] Everyday examples of non-linearly separable problems](https://www.reddit.com/r/MachineLearning/comments/1jszd7k/d_everyday_examples_of_nonlinearly_separable/) (Score: 3)
    *   This thread discusses and provides examples of non-linearly separable problems, such as XOR, the swiss roll, and examples related to human classification problems like Male vs Female.
7.  [[D] How to handle limited space in RAM when training in Google Colab?](https://www.reddit.com/r/MachineLearning/comments/1jsuyxh/d_how_to_handle_limited_space_in_ram_when/) (Score: 2)
    *   This thread discusses ways to handle limited RAM space when training models in Google Colab, with suggestions to read CSV files in chunks and to use dask-ml or Polars.
8.  [[D] Has anyone else observed structured, persistent linguistic emergence in LLMs?](https://www.reddit.com/r/MachineLearning/comments/1jskfnj/d_has_anyone_else_observed_structured_persistent/) (Score: 0)
    *   This thread inquires about structured linguistic emergence in LLMs, but is criticized for being vague and lacking details.

# Detailed Analysis by Thread
**[[R] NoProp: Training neural networks without back-propagation or forward-propagation (Score: 101)](https://www.reddit.com/r/MachineLearning/comments/1jsft3c/r_noprop_training_neural_networks_without/)**
*   **Summary:** This thread is about a paper introducing NoProp, a method for training neural networks without back-propagation or forward-propagation.
*   **Emotion:** The overall emotional tone is neutral, with curiosity about the method's performance and comparisons to backpropagation.
*   **Top 3 Points of View:**
    *   Inquiry about the performance of NoProp compared to backpropagation.
    *   Concern about why the method wasn't tested on larger datasets.
    *   Skepticism about whether the method truly avoids backpropagation.

**[[R] Image classification by evolving bytecode (Score: 13)](https://zyme.dev/blog/1_image_classification_by_evolving_bytecode)**
*   **Summary:** The thread discusses image classification by evolving bytecode and a commenter mentions that they want to see the description of an instruction set or bytecode.
*   **Emotion:** The overall emotional tone is neutral and interested.
*   **Top 3 Points of View:**
    *   Request for a description of the instruction set and bytecode used.
    *   Suggestion that a language description might not be necessary for genetic algorithm experiments.
    *   Suggestion to also post in r/alife.

**[[D] Rich Sutton: Self-Verification, The Key to AI (Score: 12)](http://www.incompleteideas.net/IncIdeas/KeytoAI.html)**
*   **Summary:** The thread discusses Rich Sutton's ideas on self-verification in AI, specifically in the context of LLMs.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 3 Points of View:**
    *   The idea is relevant to LLMs.
    *   LLMs cannot self-verify.
    *   LLMs can achieve superhuman results when paired with a robust external verifier.

**[[D]IJCAI 2025 reviews and rebuttal discussion (Score: 7)](https://www.reddit.com/r/MachineLearning/comments/1jss0lu/dijcai_2025_reviews_and_rebuttal_discussion/)**
*   **Summary:** The thread is a discussion about the release of IJCAI 2025 reviews and rebuttals.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 3 Points of View:**
    *   Asking if anyone has received their reviews yet.
    *   Wondering if the reviews will be out by the end of the day.

**[[P] anyone working on Arabic OCR? (Score: 4)](https://www.reddit.com/r/MachineLearning/comments/1jsclxw/p_anyone_working_on_arabic_ocr/)**
*   **Summary:** The thread asks if anyone is working on Arabic OCR and suggests using a vLLM like Gemini.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 3 Points of View:**
    *   Inquiry about working on Arabic OCR.
    *   Suggestion to use a vLLM like Gemini.

**[[D] Everyday examples of non-linearly separable problems (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1jszd7k/d_everyday_examples_of_nonlinearly_separable/)**
*   **Summary:** The thread discusses and provides examples of non-linearly separable problems.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 3 Points of View:**
    *   XOR and the swiss roll as classic examples.
    *   Language and the discretization of continuous things as an example.
    *   Real world examples like height vs gender or height and weight vs cat/dog.

**[[D] How to handle limited space in RAM when training in Google Colab? (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1jsuyxh/d_how_to_handle_limited_space_in_ram_when/)**
*   **Summary:** The thread discusses ways to handle limited RAM space when training models in Google Colab.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 3 Points of View:**
    *   Read the csv files in chunks.
    *   Use dask-ml.
    *   Use polars instead of pandas.

**[[D] Has anyone else observed structured, persistent linguistic emergence in LLMs? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1jskfnj/d_has_anyone_else_observed_structured_persistent/)**
*   **Summary:** The thread inquires about structured linguistic emergence in LLMs, but is criticized for being vague and lacking details.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 3 Points of View:**
    *   Criticism of the post for being vague and lacking necessary details (architecture, parameters, training data).
    *   Suggestion that it may just be imitating language.
    *   Suggestion to look at GPT2 research paper.
