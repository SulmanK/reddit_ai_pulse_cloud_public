---
title: "Machine Learning Subreddit"
date: "2025-04-22"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machine learning", "AI", "research"]
---

# Overall Ranking and Top Discussions
1.  [[R] One Embedding to Rule Them All](https://www.reddit.com/r/MachineLearning/comments/1k5b3ni/r_one_embedding_to_rule_them_all/) (Score: 33)
    *   The discussion revolves around a new embedding technique and its potential applications, with some users drawing parallels to Meta's ImageBind and others expressing general dislike for Pinterest.
2.  [[R] [DeepMind] Welcome to the Era of Experience](https://www.reddit.com/r/MachineLearning/comments/1k4zr1i/r_deepmind_welcome_to_the_era_of_experience/) (Score: 31)
    *   This thread discusses a DeepMind paper (preprint of a chapter from Richard Suttonâ€™s upcoming book) advocating for reinforcement learning. Some commenters criticize the paper's lack of equations and experiments.
3.  [[D] How much more improvment can you squeeze out by fine tuning large language models](https://www.reddit.com/r/MachineLearning/comments/1k4tn68/d_how_much_more_improvment_can_you_squeeze_out_by/) (Score: 29)
    *   The discussion centers around the limits of fine-tuning large language models. Users share experiences with LoRA, discuss the saturation of models on standard benchmarks, and suggest alternative approaches like short/mid/long-term memory implementations or reinforcement learning.
4.  [[D] What are the current research gaps on GNN?](https://www.reddit.com/r/MachineLearning/comments/1k4oxgo/d_what_are_the_current_research_gaps_on_gnn/) (Score: 12)
    *   The thread explores research gaps in Graph Neural Networks (GNNs), particularly focusing on addressing the over-smoothing problem in deeper architectures.
5.  [[D] New masters thesis student and need access to cloud GPUs](https://www.reddit.com/r/MachineLearning/comments/1k4za4g/d_new_masters_thesis_student_and_need_access_to/) (Score: 8)
    *   A new master's student is seeking advice on accessing cloud GPUs. The discussion covers various cloud platforms (Runpod, Lambdalabs, vast.ai, Google Colab), GPU recommendations, and alternative compute strategies.
6.  [[D] Two basic questions about GNN](https://www.reddit.com/r/MachineLearning/comments/1k4yay1/d_two_basic_questions_about_gnn/) (Score: 2)
    *   The thread involves two questions about graph neural networks, specifically if GNNs need node features, and how to handle multiple edges between same nodes.
7.  [[D] How is SAE / cross layer transcoder trained?](https://www.reddit.com/r/MachineLearning/comments/1k4tvlu/d_how_is_sae_cross_layer_transcoder_trained/) (Score: 0)
    *   The thread is about how to train SAE / cross layer transcoder models.
8.  [[P] How do I detect cancelled text](https://www.reddit.com/r/MachineLearning/comments/1k51qlv/p_how_do_i_detect_cancelled_text/) (Score: 0)
    *   The discussion is on how to detect cancelled (struck-through) text in images or PDFs. Some suggest standard image processing techniques, while others suggest using object detection models, with some disagreement if machine learning is needed.
9.  Google AI Training Concerns [D](https://www.reddit.com/r/MachineLearning/comments/1k5az2g/google_ai_training_concerns_d/) (Score: 0)
    *   The discussion is about ethical concerns and Google AI training.

# Detailed Analysis by Thread
**[[R] One Embedding to Rule Them All (Score: 33)](https://www.reddit.com/r/MachineLearning/comments/1k5b3ni/r_one_embedding_to_rule_them_all/)**
*  **Summary:**  The discussion revolves around a new embedding technique and its potential applications. Some users draw parallels to Meta's ImageBind and its Lord of the Rings reference, while another expresses general dislike for Pinterest due to dead-end search results.
*  **Emotion:** The emotional tone is mixed. Sentiment is primarily Neutral, with some Negative sentiment expressing dislike for Pinterest.
*  **Top 3 Points of View:**
    *   The embedding technique is interesting and potentially useful.
    *   This is similar to Meta's ImageBind project.
    *   Pinterest is frustrating due to its dead-end links.

**[[R] [DeepMind] Welcome to the Era of Experience (Score: 31)](https://www.reddit.com/r/MachineLearning/comments/1k4zr1i/r_deepmind_welcome_to_the_era_of_experience/)**
*  **Summary:**  This thread discusses a DeepMind paper advocating for reinforcement learning as a key to AI development. Some commenters criticize the paper's lack of equations and experiments, while others debate the role of reinforcement learning versus supervised learning and the importance of memory.
*  **Emotion:** The emotional tone is predominantly Neutral, with some Negative sentiment related to the perceived lack of substance in the paper. A Positive sentiment is expressed about researchers still being friends.
*  **Top 3 Points of View:**
    *   Reinforcement learning is important, but should not be the only focus.
    *   The paper lacks empirical validation and is more akin to "religion" than science.
    *   The paper has errors and is not well written.

**[[D] How much more improvment can you squeeze out by fine tuning large language models (Score: 29)](https://www.reddit.com/r/MachineLearning/comments/1k4tn68/d_how_much_more_improvment_can_you_squeeze_out_by/)**
*  **Summary:**  The discussion centers around the limits of fine-tuning large language models (LLMs). Users share experiences with LoRA, discuss the saturation of models on standard benchmarks, and suggest alternative approaches.
*  **Emotion:** The emotional tone is generally Neutral. Some users express Positive sentiment regarding LoRA's effectiveness.
*  **Top 3 Points of View:**
    *   Fine-tuning is best for specific tasks or output formats.
    *   LoRA can be effective, but requires extensive experimentation.
    *   Small, overtrained models are harder to fine-tune.

**[[D] What are the current research gaps on GNN? (Score: 12)](https://www.reddit.com/r/MachineLearning/comments/1k4oxgo/d_what_are_the_current_research_gaps_on_gnn/)**
*  **Summary:**  The thread explores research gaps in Graph Neural Networks (GNNs), particularly focusing on addressing the over-smoothing problem in deeper architectures.
*  **Emotion:** The emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   Over-smoothing in deep GNNs is a significant research gap.
    *   Mechanisms are needed to overcome the over-smoothing problem.
    *   The over-smoothing problem impacts explainability.

**[[D] New masters thesis student and need access to cloud GPUs (Score: 8)](https://www.reddit.com/r/MachineLearning/comments/1k4za4g/d_new_masters_thesis_student_and_need_access_to/)**
*  **Summary:**  A new master's student is seeking advice on accessing cloud GPUs. The discussion covers various cloud platforms, GPU recommendations, and alternative compute strategies.
*  **Emotion:** The emotional tone is primarily Neutral, with some Positive sentiments toward some platforms.
*  **Top 3 Points of View:**
    *   Explore university-owned clusters first.
    *   Consider cloud GPU providers like Runpod, Lambdalabs, and vast.ai.
    *   If resources are limited, explore alternatives to LLMs.

**[[D] Two basic questions about GNN (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1k4yay1/d_two_basic_questions_about_gnn/)**
*  **Summary:** The thread involves two questions about graph neural networks, specifically if GNNs need node features, and how to handle multiple edges between same nodes.
*  **Emotion:** The emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   Node features are needed, and if not available, they can be generated.
    *   Unsupervised graph embeddings are an alternative to needing node features.
    *   Multiple edges between the same nodes create a multigraph, which requires specific models or transformations.

**[[D] How is SAE / cross layer transcoder trained? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1k4tvlu/d_how_is_sae_cross_layer_transcoder_trained/)**
*  **Summary:**  The thread is about how to train SAE / cross layer transcoder models, pointing to an external resource.
*  **Emotion:** The emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   A github repository is provided with code related to SAE / cross layer transcoders.

**[[P] How do I detect cancelled text (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1k51qlv/p_how_do_i_detect_cancelled_text/)**
*  **Summary:** The discussion is on how to detect cancelled (struck-through) text in images or PDFs.
*  **Emotion:** The emotional tone is mixed, with Neutral, and Positive sentiments.
*  **Top 3 Points of View:**
    *   Machine learning may be overkill, parse the PDF and check the text formatting for a strikethrough.
    *   Standard image processing techniques may be sufficient.
    *   Object detection models may be a useful approach, needing parameter tuning.

**[Google AI Training Concerns [D] (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1k5az2g/google_ai_training_concerns_d/)**
*  **Summary:**  The discussion is about ethical concerns and Google AI training.
*  **Emotion:** The emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   Need more details about the unethical treatment.
    *   The email may not exist if the person has left Google.
    *   Past threads are provided about Ethics concerns and Google.
