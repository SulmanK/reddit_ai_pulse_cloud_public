---
title: "Data Science Subreddit"
date: "2025-04-23"
description: "Analysis of top discussions and trends in the datascience subreddit"
tags: ["data science", "AI", "metrics"]
---

# Overall Ranking and Top Discussions
1.  [[D] How is your teaming using AI for DS?](https://www.reddit.com/r/datascience/comments/1k5ikzd/how_is_your_teaming_using_ai_for_ds/) (Score: 47)
    *   The discussion revolves around how data science teams are leveraging AI, particularly LLMs, for various tasks such as data labeling, coding, generating synthetic data, and extracting information from unstructured data.
2.  [To Interviewers who ask product metrics cases study, what makes you say yes or no to a candidate, do you want complex metrics? Or basic works too?](https://www.reddit.com/r/datascience/comments/1k63zii/to_interviewers_who_ask_product_metrics_cases/) (Score: 12)
    *   The thread discusses what interviewers look for in candidates when asking about product metrics, emphasizing the importance of understanding business outcomes and the ability to align metrics with those outcomes, rather than focusing on complex metrics.
3.  [Is there a sentence cloud? (Something similar to word cloud)](https://www.reddit.com/r/datascience/comments/1k5xj8z/is_there_a_sentence_cloud_something_similar_to/) (Score: 7)
    *   The post inquires about the existence of a "sentence cloud" similar to a word cloud. Responses suggest using topic modeling techniques like Bertopic or leveraging LLMs to summarize and group feedback.
4.  [How can I come up with better feature ideas?](https://www.reddit.com/r/datascience/comments/1k60gey/how_can_i_come_up_with_better_feature_ideas/) (Score: 2)
    *   The discussion focuses on strategies for generating better feature ideas in the context of a credit scoring model. Suggestions include considering the opposite of existing metrics, using behavioral data, and training an overfitted model to estimate peak performance.

# Detailed Analysis by Thread
**[[D] How is your teaming using AI for DS? (Score: 47)](https://www.reddit.com/r/datascience/comments/1k5ikzd/how_is_your_teaming_using_ai_for_ds/)**
*  **Summary:** The discussion centers around the application of AI and LLMs in data science teams. Participants share their experiences using AI for tasks like data labeling, synthetic data generation, coding, and geospatial analysis. They debate the effectiveness of AI versus traditional machine learning techniques and highlight the importance of human oversight.
*  **Emotion:** The emotional tone is mixed, ranging from positive sentiment towards the usefulness of tools like Gemini 2.5 pro to negative sentiment expressing frustration with LLMs. Overall, the tone is mostly neutral, with a focus on practical applications and challenges.
*  **Top 3 Points of View:**
    *   AI/LLMs are useful for automating tasks like data labeling, coding, and generating synthetic data.
    *   While AI tools can be powerful, they require human oversight and domain expertise to ensure accuracy and relevance.
    *   The term "AI" is sometimes just marketing for machine learning models.

**[To Interviewers who ask product metrics cases study, what makes you say yes or no to a candidate, do you want complex metrics? Or basic works too? (Score: 12)](https://www.reddit.com/r/datascience/comments/1k63zii/to_interviewers_who_ask_product_metrics_cases/)**
*  **Summary:** This thread explores the qualities interviewers seek in candidates tackling product metrics case studies. The emphasis is on demonstrating an understanding of business goals, the ability to connect metrics to tangible outcomes, and a solid grasp of statistical fundamentals rather than the complexity of the metrics themselves.
*  **Emotion:** The general emotional tone is neutral. Commenters are focused on providing practical advice and insights into the interview process. A few expressions of doubt and frustration are present.
*  **Top 3 Points of View:**
    *   Interviewers prioritize a candidate's ability to align metrics with business outcomes over the complexity of the metrics.
    *   A strong understanding of statistical fundamentals is crucial for detecting and addressing biases in data.
    *   Candidates should focus on explaining their problem-solving approach and the rationale behind their metric choices.

**[Is there a sentence cloud? (Something similar to word cloud) (Score: 7)](https://www.reddit.com/r/datascience/comments/1k5xj8z/is_there_a_sentence_cloud_something_similar_to/)**
*  **Summary:**  The post asks if there is a way to create a visualization similar to a word cloud, but for sentences. The discussion explores several approaches, including topic modeling, using LLMs for summarization and clustering, and adapting existing word cloud code. The practicality and relevance of such a visualization are also considered.
*  **Emotion:** The overall emotional tone is neutral, with a focus on providing helpful suggestions and technical solutions. There's a hint of positive sentiment in the encouragement and problem-solving.
*  **Top 3 Points of View:**
    *   LLMs can be effectively used to summarize and cluster sentences for a similar effect to a sentence cloud.
    *   Topic modeling techniques like Bertopic are a good alternative for identifying key themes in the text.
    *   Creating a custom solution using text embeddings and clustering is feasible but might require more effort than using existing tools or LLMs.

**[How can I come up with better feature ideas? (Score: 2)](https://www.reddit.com/r/datascience/comments/1k60gey/how_can_i_come_up_with_better_feature_ideas/)**
*  **Summary:**  The thread is a request for advice on how to generate better feature ideas, specifically in the context of a credit scoring model. Commenters suggest exploring the data, considering non-linear relationships, and experimenting with overfitted models to gauge potential performance.
*  **Emotion:**  The emotional tone is primarily neutral, with some positive sentiment in the encouragement and helpful suggestions.
*  **Top 3 Points of View:**
    *   Examine the data for potential issues and consider how it's collected.
    *   Consider using techniques like GLM, GAM, or GBM to handle non-linear relationships.
    *   Experiment with an overfitted model to estimate the peak performance achievable with the current features.
