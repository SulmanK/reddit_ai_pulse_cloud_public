---
title: "Machine Learning Subreddit"
date: "2025-04-23"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "transformers"]
---

# Overall Ranking and Top Discussions
1.  [[D] Spotify 100,000 Podcasts Dataset availability](https://www.reddit.com/r/MachineLearning/comments/1k5zvbf/d_spotify_100000_podcasts_dataset_availability/) (Score: 31)
    * The thread discusses the availability of the Spotify 100,000 Podcasts Dataset.
2.  [[P] I built a self-hosted version of DataBricks for research](https://www.reddit.com/r/MachineLearning/comments/1k5viph/p_i_built_a_selfhosted_version_of_databricks_for/) (Score: 24)
    * The thread features a user sharing their self-hosted version of DataBricks for research purposes.
3.  [[D] Would multiple NVIDIA Tesla P100's be cost effective for model training?](https://www.reddit.com/r/MachineLearning/comments/1k5ley3/d_would_multiple_nvidia_tesla_p100s_be_cost/) (Score: 10)
    * The thread discusses the cost-effectiveness of using multiple NVIDIA Tesla P100s for model training, with users suggesting alternatives.
4.  [[D] Is my take on transformers in time series reasonable / where is it wrong?](https://www.reddit.com/r/MachineLearning/comments/1k63r4a/d_is_my_take_on_transformers_in_time_series/) (Score: 10)
    *  The thread revolves around a discussion about the effectiveness of transformers in time series analysis, questioning the original poster's perspective.
5.  [[R] Can’t Train LoRA + Phi-2 on 2x GPUs with FSDP — Keep Getting PyArrow ArrowInvalid, DTensor, and Tokenization Errors](https://www.reddit.com/r/MachineLearning/comments/1k5jhub/r_cant_train_lora_phi2_on_2x_gpus_with_fsdp_keep/) (Score: 0)
    * The thread is about the user's issues with training LoRA + Phi-2 on 2x GPUs, with errors like PyArrow ArrowInvalid, DTensor, and tokenization problems.
6.  [[P] What AI model should I train for this use case?](https://www.reddit.com/r/MachineLearning/comments/1k5mcj0/p_what_ai_model_should_i_train_for_this_use_case/) (Score: 0)
    * The thread discusses what AI model the user should train for a particular use case.

# Detailed Analysis by Thread
**[[D] Spotify 100,000 Podcasts Dataset availability (Score: 31)](https://www.reddit.com/r/MachineLearning/comments/1k5zvbf/d_spotify_100000_podcasts_dataset_availability/)**
*  **Summary:** The thread discusses the availability of the Spotify 100,000 Podcasts Dataset.
*  **Emotion:** The emotional tone of the thread is Neutral.
*  **Top 3 Points of View:**
    *  A user is inquiring about the dataset

**[[P] I built a self-hosted version of DataBricks for research (Score: 24)](https://www.reddit.com/r/MachineLearning/comments/1k5viph/p_i_built_a_selfhosted_version_of_databricks_for/)**
*  **Summary:** The thread features a user sharing their self-hosted version of DataBricks for research purposes. Other users discuss their interpretations of DataBricks.
*  **Emotion:** The emotional tone of the thread is Positive, indicating enthusiasm and appreciation for the project.
*  **Top 3 Points of View:**
    *  The original poster built a self-hosted DataBricks version.
    *  Another user believes dynamic autoscaling of spark workers is the main value of DataBricks.
    *  A user expressed appreciation for sharing the project.

**[[D] Would multiple NVIDIA Tesla P100's be cost effective for model training? (Score: 10)](https://www.reddit.com/r/MachineLearning/comments/1k5ley3/d_would_multiple_nvidia_tesla_p100s_be_cost/)**
*  **Summary:** The thread discusses the cost-effectiveness of using multiple NVIDIA Tesla P100s for model training, with users suggesting alternatives like newer GPUs or cloud instances.
*  **Emotion:** The emotional tone of the thread is Neutral, reflecting a discussion about technical specifications and cost considerations.
*  **Top 3 Points of View:**
    *  P100s are considered slow and power-inefficient for modern LLMs.
    *  A 3090 is suggested as a better alternative.
    *  Using an EC2 instance is proposed as a cost-effective solution.

**[[D] Is my take on transformers in time series reasonable / where is it wrong? (Score: 10)](https://www.reddit.com/r/MachineLearning/comments/1k63r4a/d_is_my_take_on_transformers_in_time_series/)**
*  **Summary:** The thread revolves around a discussion about the effectiveness of transformers in time series analysis, questioning the original poster's perspective, and comparing time series to language processing.
*  **Emotion:** The emotional tone of the thread is Neutral, indicating a technical discussion and analysis.
*  **Top 3 Points of View:**
    *  Transformers work well in natural language due to multi-scale autocorrelation and rich embedding spaces.
    *  Time series often lack the properties that make transformers effective in language processing.
    *  Transformers correlate elements based on similarity, which might not be suitable for time series.

**[[R] Can’t Train LoRA + Phi-2 on 2x GPUs with FSDP — Keep Getting PyArrow ArrowInvalid, DTensor, and Tokenization Errors (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1k5jhub/r_cant_train_lora_phi2_on_2x_gpus_with_fsdp_keep/)**
*  **Summary:** The thread is about the user's issues with training LoRA + Phi-2 on 2x GPUs, with errors like PyArrow ArrowInvalid, DTensor, and tokenization problems. Other users suggest the post is poorly formatted and lacks necessary information for debugging.
*  **Emotion:** The emotional tone of the thread is Neutral, although there's a hint of frustration in some comments.
*  **Top 3 Points of View:**
    *  The original poster is having trouble training LoRA + Phi-2.
    *  Other users criticize the post's formatting and lack of code.
    *  A user recommends using unsloth for easier training.

**[[P] What AI model should I train for this use case? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1k5mcj0/p_what_ai_model_should_i_train_for_this_use_case/)**
*  **Summary:** The thread discusses what AI model the user should train for a particular use case. Advice ranges from using existing image generators to training autoregressive models.
*  **Emotion:** The emotional tone of the thread is Neutral, as it involves giving technical advice.
*  **Top 3 Points of View:**
    *  The desired outcome is at the edge of current AI capabilities.
    *  Using existing image generators (GPT-4o, Gemini 2.0 Flash) is recommended.
    *  Training an autoregressive model is suggested, although it is acknowledged it would be limited.
