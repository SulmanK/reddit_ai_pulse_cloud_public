---
title: "Machine Learning Subreddit"
date: "2025-04-30"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machine learning", "AI", "deep learning"]
---

# Overall Ranking and Top Discussions
1.  [[D] Incoming ICML results](https://www.reddit.com/r/MachineLearning/comments/1kb21lx/incoming_icml_results_d/) (Score: 27)
    *   Discussion about the upcoming ICML results and people sharing their score expectations.
2.  [[Discussion]I trained a 7B LLM with only 8GB of VRAM using symbolic compression MemoryCore benchmark results](https://www.reddit.com/r/MachineLearning/comments/1kbij9t/discussioni_trained_a_7b_llm_with_only_8gb_of/) (Score: 16)
    *   A user claims to have trained a 7B LLM with limited VRAM and gets a lot of questions as a result.
3.  [Learnable matrices in sequence without nonlinearity - reasons? [R]](https://www.reddit.com/r/MachineLearning/comments/1kbdoig/learnable_matrices_in_sequence_without/) (Score: 13)
    *   Discussion around learnable matrices in sequence without nonlinearity.
4.  [[D] NeurIPS 2025 rebuttal period?](https://www.reddit.com/r/MachineLearning/comments/1kayz63/d_neurips_2025_rebuttal_period/) (Score: 3)
    *   Speculation about the timing and format of the NeurIPS 2025 rebuttal period.
5.  [[D] Consistently Low Accuracy Despite Preprocessing — What Am I Missing?](https://www.reddit.com/r/MachineLearning/comments/1kbg45l/d_consistently_low_accuracy_despite_preprocessing/) (Score: 3)
    *   A user is asking for suggestions on what steps to take to improve their model's accuracy.
6.  [[D] Divergence in a NN, Reinforcement Learning](https://www.reddit.com/r/MachineLearning/comments/1kb0zqa/d_divergence_in_a_nn_reinforcement_learning/) (Score: 2)
    *   A user is asking for help regarding divergence in a NN.
7.  [How to handle imbalanced output scales in PINN/PI-DeepONet loss function? [R]](https://www.reddit.com/r/MachineLearning/comments/1kboofc/how_to_handle_imbalanced_output_scales_in/) (Score: 2)
    *   How to handle imbalanced output scales in PINN/PI-DeepONet loss function
8.  [Whisper Translation Finetuning [P]](https://www.reddit.com/r/MachineLearning/comments/1kbhyon/whisper_translation_finetuning_p/) (Score: 1)
    *   Questions about the quality and size of the data being used for whisper translation.
9.  [[P] Fire detection drone](https://www.reddit.com/r/MachineLearning/comments/1kbd20z/p_fire_detection_drone/) (Score: 0)
    *   Suggestions for using IR band cameras for fire detection.

# Detailed Analysis by Thread

**[[D] Incoming ICML results (Score: 27)](https://www.reddit.com/r/MachineLearning/comments/1kb21lx/incoming_icml_results_d/)**
*  **Summary:**  The thread revolves around the anxiety and anticipation surrounding the incoming ICML (International Conference on Machine Learning) results. Participants are discussing the potential acceptance rates, the meaning of reviewer scores, and sharing their personal experiences with previous conference submissions.
*  **Emotion:** The overall emotional tone is neutral, with elements of anxiety, hope, and resignation. There is a mix of nervousness about the results and attempts to provide realistic expectations based on past experiences.
*  **Top 3 Points of View:**
    *   Speculation about the cutoff score for acceptance, with some suggesting a score around 3.
    *   Discussion on the meaning of reviewer scores and their correlation with the acceptance decision.
    *   Sharing personal experiences and anxieties related to submitting to ICML.

**[[Discussion]I trained a 7B LLM with only 8GB of VRAM using symbolic compression MemoryCore benchmark results (Score: 16)](https://www.reddit.com/r/MachineLearning/comments/1kbij9t/discussioni_trained_a_7b_llm_with_only_8gb_of/)**
*  **Summary:** A user claims to have trained a 7B Large Language Model with only 8GB of VRAM using symbolic compression and shares benchmark results. This sparks a debate about the validity and practicality of the method, with requests for more information and skepticism about the claims.
*  **Emotion:** The thread exhibits a mix of excitement, skepticism, and curiosity. Some users are impressed and eager to learn more, while others question the methodology and results.
*  **Top 3 Points of View:**
    *   Excitement and interest in the possibility of training large models with limited resources, requesting a GitHub link or literature.
    *   Skepticism regarding the claim, with users pointing out inconsistencies and questioning the validity of the approach.
    *   Clarification of the approach, explaining that the model is effectively a LoRA for Qwen.

**[Learnable matrices in sequence without nonlinearity - reasons? [R] (Score: 13)](https://www.reddit.com/r/MachineLearning/comments/1kbdoig/learnable_matrices_in_sequence_without/)**
*  **Summary:** The thread discusses the reasons for using learnable matrices in sequence without nonlinearity in machine learning models. It covers topics like computational efficiency, regularization, and modeling quadratic relationships between input tokens, and also offers insight into transformer architectures.
*  **Emotion:** The thread's emotional tone is mostly neutral, reflecting a technical discussion.
*  **Top 3 Points of View:**
    *   Computational efficiency: breaking down a matrix into a sequence of smaller matrices can reduce the number of parameters and make computations more efficient.
    *   Regularization: Breaking down larger matrices into smaller matrices can act as a form of regularization.
    *   Modeling quadratic relationships: Sequences of learnable matrices allows the modeling of quadratic relationships between the input tokens.

**[[D] NeurIPS 2025 rebuttal period? (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1kayz63/d_neurips_2025_rebuttal_period/)**
*  **Summary:** A discussion about when the rebuttal period for NeurIPS 2025 might be and whether or not NeurIPS would get rid of the rebuttal period all together.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   The NeurIPS 2025 rebuttal period will likely be during the "Discussion and meta-review period: Jul 17, 2025 - Aug 21, 2025 AoE".
    *   Getting rid of the rebuttal period would be too large of a change and will likely not happen without a large-scale survey from the community.
    *   No third point of view available.

**[[D] Consistently Low Accuracy Despite Preprocessing — What Am I Missing? (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1kbg45l/d_consistently_low_accuracy_despite_preprocessing/)**
*  **Summary:** The thread centers on a user seeking advice for improving the accuracy of their machine learning model, which is consistently low despite preprocessing. Other users offer suggestions ranging from trying different algorithms and feature engineering techniques to questioning the dataset's quality and the feasibility of the desired accuracy.
*  **Emotion:** The emotional tone is neutral and supportive.
*  **Top 3 Points of View:**
    *   Try simpler models like XGBoost or LightGBM and minimal preprocessing.
    *   Explore feature engineering based on medical research related to the prediction task (cardiovascular diseases).
    *   Consider the possibility that the dataset itself might be the limiting factor, and the desired accuracy might not be achievable.

**[[D] Divergence in a NN, Reinforcement Learning (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1kb0zqa/d_divergence_in_a_nn_reinforcement_learning/)**
*  **Summary:** A user is asking for help regarding divergence in a NN and reinforcement learning, and the response suggests the agent is not diverging and the tau value is too low.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   The network learns and doesn't diverge, but the tau value is too low.
    *   The agent is always choosing the top action.
    *   A better initialization of the network or making tau closer to 1.

**[How to handle imbalanced output scales in PINN/PI-DeepONet loss function? [R] (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1kboofc/how_to_handle_imbalanced_output_scales_in/)**
*  **Summary:** A discussion about handling imbalanced output scales in PINN/PI-DeepONet loss functions.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Adaptive weight, using a soft Attention-Mechanism
    *   Hard code certain terms which leads them to vanish from your loss function.
    *   Use second-order optimizers

**[Whisper Translation Finetuning [P] (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1kbhyon/whisper_translation_finetuning_p/)**
*  **Summary:** The post is about whisper translation finetuning and questions about the data being used to accomplish this.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   How's the audio quality?
    *   How big is the dataset?
    *   Tried wav2vec2 or wav2vec2 Bert?

**[[P] Fire detection drone (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1kbd20z/p_fire_detection_drone/)**
*  **Summary:** A discussion about using IR band cameras in order to detect fires.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   It's better to use IR band cameras to detect fires.
    *   Use an existing project for fire detection.
    *   No third point of view available.
