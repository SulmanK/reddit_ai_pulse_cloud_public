---
title: "Machine Learning Subreddit"
date: "2025-04-03"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machine learning", "AI", "research"]
---

# Overall Ranking and Top Discussions
1.  [[D] UAI 2025 Reviews Waiting Place](https://www.reddit.com/r/MachineLearning/comments/1jql3hx/d_uai_2025_reviews_waiting_place/) (Score: 8)
    *   A thread where people are waiting to receive reviews for their UAI 2025 submissions.
2.  [[D] Anyone got reviews for the paper submitted to AIED 2025 conference](https://www.reddit.com/r/MachineLearning/comments/1jqevkl/d_anyone_got_reviews_for_the_paper_submitted_to/) (Score: 6)
    *   A thread where people are discussing whether or not they have received their reviews for the AIED 2025 conference.
3.  [[R] Position: Model Collapse Does Not Mean What You Think](https://arxiv.org/abs/2503.03150) (Score: 5)
    *   A discussion about a paper arguing that "model collapse" is not the significant issue some perceive it to be, with some disagreeing and saying that models trained on other model data lead to tool degradation.
4.  [AI tools for ML Research - what am I missing? [D]](https://www.reddit.com/r/MachineLearning/comments/1jqolkh/ai_tools_for_ml_research_what_am_i_missing_d/) (Score: 4)
    *   A thread discussing which AI tools are useful for ML research, with comparisons between OpenAI's and Gemini's research capabilities.
5.  [[D] Time series models with custom loss](https://www.reddit.com/r/MachineLearning/comments/1jqettp/d_time_series_models_with_custom_loss/) (Score: 3)
    *   A thread about time series models with custom loss functions, suggesting Augmented Dynamic Adaptive Models (ADAM) and discussing noise distributions.
6.  [[N] Open-data reasoning model, trained on curated supervised fine-tuning (SFT) dataset, outperforms DeepSeekR1. Big win for the open source community](https://www.reddit.com/r/MachineLearning/comments/1jqqlxc/n_opendata_reasoning_model_trained_on_curated/) (Score: 3)
    *   A thread discussing an open-data reasoning model that outperforms DeepSeekR1, and if it surpasses the QwQ 32b.
7.  [[D] Interpreting Image Patch and Subpatch Tokens for Latent Diffusion](https://www.reddit.com/r/MachineLearning/comments/1jq9yex/d_interpreting_image_patch_and_subpatch_tokens/) (Score: 2)
    *   A thread discussing how images are compressed using CNN-based VAEs for latent diffusion and patch-based tokenization.
8.  [[D] Fine-tuning a fine-tuned YOLO model?](https://www.reddit.com/r/MachineLearning/comments/1jqh8zw/d_finetuning_a_finetuned_yolo_model/) (Score: 1)
    *   A thread asking how to improve a YOLO model depending on the semi-annotated dataset.
9.  [[P] Privately Hosted LLM (HIPAA Compliant)](https://www.reddit.com/r/MachineLearning/comments/1jqn1xb/p_privately_hosted_llm_hipaa_compliant/) (Score: 1)
    *   A thread suggesting the poster try a PoC project using local compute and HF for a privately hosted, HIPAA compliant LLM.
10. [[R] For those of you who are familiar with Kolmogorov Arnold Networks and the Meijer-G function, is representing the B-Spline using a Meijer-G function possible?](https://www.reddit.com/r/MachineLearning/comments/1jqp77y/r_for_those_of_you_who_are_familiar_with/) (Score: 1)
    *   A thread asking if it's possible to represent the B-Spline using a Meijer-G function. The response suggests taking a linear combination of different basis functions and/or combining them multiplicatively.
11. [[D] Are there AIs that are trained only on free and open source datasets that are compatible with each other?](https://www.reddit.com/r/MachineLearning/comments/1jq3eer/d_are_there_ais_that_are_trained_only_on_free_and/) (Score: 0)
    *   A thread asking if there are AIs that are trained only on free and open source datasets. The responses mention HuggingFace's projects and a bill in California that would force developers to post all copyrighted material they used to train the model.
12. [[P] Starting a GPU VPS Hosting Service â€“ Need Your Insights on Pricing, Hardware & Features](https://www.reddit.com/r/MachineLearning/comments/1jqcau7/p_starting_a_gpu_vps_hosting_service_need_your/) (Score: 0)
    *   A thread discussing insights on pricing, hardware, and features for starting a GPU VPS hosting service.

# Detailed Analysis by Thread
**[[D] UAI 2025 Reviews Waiting Place (Score: 8)](https://www.reddit.com/r/MachineLearning/comments/1jql3hx/d_uai_2025_reviews_waiting_place/)**
*   **Summary:** People are checking to see if anyone has received scores for their UAI 2025 submissions.
*   **Emotion:** The emotional tone of the thread is neutral, expressing a state of waiting and anticipation.
*   **Top 3 Points of View:**
    *   Waiting for scores.

**[[D] Anyone got reviews for the paper submitted to AIED 2025 conference (Score: 6)](https://www.reddit.com/r/MachineLearning/comments/1jqevkl/d_anyone_got_reviews_for_the_paper_submitted_to/)**
*   **Summary:** People are checking to see if anyone has received reviews for their AIED 2025 conference submissions.
*   **Emotion:** The emotional tone of the thread is neutral, expressing a state of waiting and anticipation.
*   **Top 3 Points of View:**
    *   Waiting for reviews.
    *   Someone received a review recently.
    *   Someone got their review.

**[[R] Position: Model Collapse Does Not Mean What You Think (Score: 5)](https://arxiv.org/abs/2503.03150)**
*   **Summary:** A discussion about a paper arguing that "model collapse" is not the significant issue some perceive it to be. Some disagree and say that models trained on other model data lead to tool degradation, ignoring engineering principles.
*   **Emotion:** The emotional tone of the thread is neutral, presenting arguments for and against the paper's position.
*   **Top 3 Points of View:**
    *   The paper argues that model collapse is not a significant issue.
    *   Some disagree, stating current models trained on data created by past models prove the paper untrue.
    *   Model collapse is a philosophical academic argument that ignores the reality of real-world engineering.

**[AI tools for ML Research - what am I missing? [D] (Score: 4)](https://www.reddit.com/r/MachineLearning/comments/1jqolkh/ai_tools_for_ml_research_what_am_i_missing_d/)**
*   **Summary:** A thread discussing which AI tools are useful for ML research, with comparisons between OpenAI's and Gemini's research capabilities. Some users share their workflows and preferences for AI tools, while others express privacy concerns.
*   **Emotion:** The emotional tone of the thread is neutral, focusing on practical applications and considerations.
*   **Top 3 Points of View:**
    *   Some researchers use AI tools like ChatGPT and Copilot to help with boilerplate and overviews.
    *   Others don't use AI tools for research or find them necessary.
    *   Gemini 2.5 excels at mathematical reasoning, while Claude 3.5 is better for coding tasks, but is expensive. Privacy issues were also mentioned.

**[[D] Time series models with custom loss (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1jqettp/d_time_series_models_with_custom_loss/)**
*   **Summary:** A thread about time series models with custom loss functions. One comment suggests using Augmented Dynamic Adaptive Models (ADAM). Another comment describes how to create loss functions by using different distributions of the noise term.
*   **Emotion:** The emotional tone of the thread is neutral, focusing on technical aspects and solutions.
*   **Top 3 Points of View:**
    *   Augmented Dynamic Adaptive Models (ADAM) can be used for time series models with custom loss functions.
    *   Loss functions can be created using different distributions of the noise term.
    *   Stationarity conditions depend on the dynamics and the noise distribution.

**[[N] Open-data reasoning model, trained on curated supervised fine-tuning (SFT) dataset, outperforms DeepSeekR1. Big win for the open source community (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1jqqlxc/n_opendata_reasoning_model_trained_on_curated/)**
*   **Summary:** A thread discussing an open-data reasoning model that outperforms DeepSeekR1. A commenter questions if the model surpasses QwQ 32b, the "actual best open reasoning model" of that size, and clarifies that the comparison is with an inferior 32b distill of DeepSeekR1.
*   **Emotion:** The emotional tone of the thread is neutral, balancing enthusiasm for the open-source achievement with critical questioning and clarification.
*   **Top 3 Points of View:**
    *   An open-data reasoning model outperforms DeepSeekR1.
    *   It's misleading to say it outperforms R1, when you mean the inferior 32b distill.
    *   Does it surpass QwQ 32b, the actual best open reasoning model of that size?

**[[D] Interpreting Image Patch and Subpatch Tokens for Latent Diffusion (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1jq9yex/d_interpreting_image_patch_and_subpatch_tokens/)**
*   **Summary:** A thread discussing how images are compressed using CNN-based VAEs for latent diffusion. Patch-based tokenization is usually done as 1x1 or 2x2 if the latent diffusion model is a transformer.
*   **Emotion:** The emotional tone of the thread is positive and helpful, offering explanations and guidance.
*   **Top 3 Points of View:**
    *   Images are commonly compressed using a CNN-based VAE.
    *   Patch-based tokenization is usually done as 1x1 or 2x2.
    *   1x1 is not really a patch anymore, just treat each spatial position as a token.

**[[D] Fine-tuning a fine-tuned YOLO model? (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1jqh8zw/d_finetuning_a_finetuned_yolo_model/)**
*   **Summary:** A thread asking for advice on fine-tuning a YOLO model, with questions about the annotation level of the semi-annotated dataset. The response suggests testing the model on the fully annotated dataset first and considering the overall goals (confidence, precision, accuracy).
*   **Emotion:** The emotional tone of the thread is neutral, seeking and offering advice.
*   **Top 3 Points of View:**
    *   The user is asking how to improve a YOLO model depending on the semi-annotated dataset.
    *   Consider the annotation level of the semi-annotated dataset.
    *   Consider the goal and how to improve it.

**[[P] Privately Hosted LLM (HIPAA Compliant) (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1jqn1xb/p_privately_hosted_llm_hipaa_compliant/)**
*   **Summary:** A thread suggesting to try a PoC project using local compute and HF to test a privately hosted, HIPAA-compliant LLM.
*   **Emotion:** The emotional tone of the thread is neutral, offering a practical suggestion.
*   **Top 3 Points of View:**
    *   Try a PoC project using local compute and HF.

**[[R] For those of you who are familiar with Kolmogorov Arnold Networks and the Meijer-G function, is representing the B-Spline using a Meijer-G function possible? (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1jqp77y/r_for_those_of_you_who_are_familiar_with/)**
*   **Summary:** A thread asking if it's possible to represent the B-Spline using a Meijer-G function. The response suggests taking a linear combination of different basis functions and/or combining them multiplicatively.
*   **Emotion:** The emotional tone of the thread is neutral and informative.
*   **Top 3 Points of View:**
    *   The user is asking if it's possible to represent the B-Spline using a Meijer-G function.
    *   Take a linear combination of different basis functions.
    *   Combine the basis functions multiplicatively.

**[[D] Are there AIs that are trained only on free and open source datasets that are compatible with each other? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1jq3eer/d_are_there_ais_that_are_trained_only_on_free_and/)**
*   **Summary:** A thread asking if there are AIs that are trained only on free and open source datasets. The responses mention HuggingFace's projects and a bill in California that would force developers to post all copyrighted material they used to train the model.
*   **Emotion:** The emotional tone of the thread is neutral, informing about existing resources and legal developments.
*   **Top 3 Points of View:**
    *   The user is asking if there are AIs that are trained only on free and open-source datasets.
    *   HuggingFace has projects related to public domain datasets and open-source code.
    *   There's a California bill that would require disclosure of copyrighted material used in training.

**[[P] Starting a GPU VPS Hosting Service â€“ Need Your Insights on Pricing, Hardware & Features (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1jqcau7/p_starting_a_gpu_vps_hosting_service_need_your/)**
*   **Summary:** A thread discussing insights on pricing, hardware, and features for starting a GPU VPS hosting service. The responses focused on improvements to services for commercial GPUs like A100/H100s.
*   **Emotion:** The emotional tone of the thread is neutral, focusing on practical advice and feedback.
*   **Top 3 Points of View:**
    *   Services should improve host reliability, convenient storage options, and preinstalled packages/dependencies.
    *   The GUI interface can be a helpful factor.
    *   The barrier to entry is high on HPC hardware unless the poster has funding.
