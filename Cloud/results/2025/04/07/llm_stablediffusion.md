---
title: "Stable Diffusion Subreddit"
date: "2025-04-07"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1. [[D] TripoSF: A High-Quality 3D VAE (1024Â³) for Better 3D Assets - Foundation for Future Img-to-3D? (Model + Inference Code Released)](https://i.redd.it/l8qhk9qbzfte1.jpeg) (Score: 72)
    *   The thread discusses TripoSF, a high-quality 3D VAE, and its potential applications in image-to-3D generation.
2.  [How to keep the characters consistent with different emotions and expressions in game using stable diffusion](https://i.redd.it/kuhbker75gte1.png) (Score: 3)
    *   The thread explores techniques for maintaining character consistency in stable diffusion when generating different emotions and expressions for game characters.
3.  [Can these motion controls be trained by Wan2.1?](https://www.reddit.com/r/StableDiffusion/comments/1jtq6sz/can_these_motion_controls_be_trained_by_wan21/) (Score: 1)
    *   The thread discusses the challenges of training motion controls using Wan2.1 and the lack of successful examples.
4.  [Help with Inpainting in ComfyUI](https://www.reddit.com/r/StableDiffusion/comments/1jtsvem/help_with_inpainting_in_comfyui/) (Score: 1)
    *   The thread provides information on using Inpaint Crop and Stitch nodes in ComfyUI for inpainting within a cropped masked area.
5.  [Cute Gnome Kitty Dances to Meow Music! ðŸ˜ºðŸŽ¶](https://youtube.com/shorts/RTIqY4BQ_HA) (Score: 1)
    *   The thread asks whether the creator used open source tools for the video.
6.  [Tiny chef videos?](https://i.redd.it/52zgxnshfgte1.png) (Score: 0)
    *   The thread links to a beginner-friendly guide on making tiny chef videos.
7.  [How to generate different *** expressions and poses while keeping the same face consistent?](https://i.redd.it/jtxe2cr7egte1.jpeg) (Score: 0)
    *   The thread discusses techniques to generate characters with consistent faces but different expressions and poses.
8.  [Wan 2.1 I2V (Also with Davinci 2x Upscaling SuperScale)](https://v.redd.it/fywyvzedvfte1) (Score: 0)
    *   The thread briefly mentions progress using Wan 2.1 I2V with Davinci 2x Upscaling SuperScale.
9.  [Wan.21 - start & end frame](https://v.redd.it/vc2lvok3mgte1) (Score: 0)
    *   The thread shows a video generated with Wan.21 using start and end frames.
10. [Questions about ReActor](https://www.reddit.com/r/StableDiffusion/comments/1jtnzhc/questions_about_reactor/) (Score: 0)
    *   The thread answers questions about ReActor, including its face restore models, checkpoint model impacts, style limitations, image requirements, occlusion options and easy alternatives.
11. [How to use or test the hugging face provided models?](https://www.reddit.com/r/StableDiffusion/comments/1jtpf0l/how_to_use_or_test_the_hugging_face_provided/) (Score: 0)
    *   The thread explains how to use and test models on Hugging Face by clicking on "spaces using this model" and creating an account.
12. [Looking for web tool that allows for reference image to be uploaded, even on Free plan](https://www.reddit.com/r/StableDiffusion/comments/1jtpjwt/looking_for_web_tool_that_allows_for_reference/) (Score: 0)
    *   The thread discusses web tools that allow uploading reference images, even on a free plan.
13. [Best SD Model For Storytelling? (Historical, Fantasy, Characters, etc)](https://www.reddit.com/r/StableDiffusion/comments/1jtqz9l/best_sd_model_for_storytelling_historical_fantasy/) (Score: 0)
    *   The thread recommends Pony or Illustrious SD models for storytelling, emphasizing character focus and style LoRAs from CivitAI.
14. [Can I generate creepypasta thumbnails for youtube without a GPU?](https://www.reddit.com/r/StableDiffusion/comments/1jtra6z/can_i_generate_creepypasta_thumbnails_for_youtube/) (Score: 0)
    *   The thread discusses the possibility of generating creepypasta thumbnails without a GPU.
15. [Help with ComfyUI generating terrible images](https://www.reddit.com/r/StableDiffusion/comments/1jtrtb6/help_with_comfyui_generating_terrible_images/) (Score: 0)
    *   The thread discusses how to fix ComfyUI generating terrible images.
16. [Stable Diffusion Slows at 49% and 97%](https://www.reddit.com/r/StableDiffusion/comments/1jttkk1/stable_diffusion_slows_at_49_and_97/) (Score: 0)
    *   The thread discusses why stable diffusion slows at 49% and 97%.
17. [all models failed at this prompt [the word "hello" but in morse code]](https://www.reddit.com/r/StableDiffusion/comments/1jtu12e/all_models_failed_at_this_prompt_the_word_hello/) (Score: 0)
    *   The thread explains that it is probably impossible for text encoders to parse morse code correctly.
18. [Real Examples Of Photo To Video With Runway ml Gen 4](https://www.youtube.com/watch?v=wZAHEZBkf1g) (Score: 0)
    *   The thread talks about real examples of photo to video with Runway ml Gen 4

# Detailed Analysis by Thread
**[[D] TripoSF: A High-Quality 3D VAE (1024Â³) for Better 3D Assets - Foundation for Future Img-to-3D? (Model + Inference Code Released) (Score: 72)](https://i.redd.it/l8qhk9qbzfte1.jpeg)**
*   **Summary:** This thread discusses the release of TripoSF, a high-quality 3D VAE, and its potential as a foundation for future image-to-3D generation. Users are excited about its potential integration into tools like ComfyUI.
*   **Emotion:** The overall emotional tone is Positive, with users expressing excitement and anticipation for the technology's potential.
*   **Top 3 Points of View:**
    *   TripoSF is a significant advancement in 3D VAE technology.
    *   Users are eager to see TripoSF integrated into ComfyUI.
    *   The technology has the potential to improve image generation and enable higher resolution outputs.

**[How to keep the characters consistent with different emotions and expressions in game using stable diffusion (Score: 3)](https://i.redd.it/kuhbker75gte1.png)**
*   **Summary:**  The thread focuses on methods for maintaining character consistency in Stable Diffusion while varying emotions and expressions. Users discuss the use of LoRAs and ControlNet, as well as custom workflows for character generation.
*   **Emotion:** The overall emotional tone is Neutral, as users mainly share technical advice and workflows.
*   **Top 3 Points of View:**
    *   LoRAs can be used for character consistency, although they can be tricky to get right.
    *   A custom workflow is being developed to create consistent characters, but it currently struggles with clothing.
    *   ControlNet and Flux ControlNet are potential solutions for changing expressions while maintaining consistency.

**[Can these motion controls be trained by Wan2.1? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1jtq6sz/can_these_motion_controls_be_trained_by_wan21/)**
*   **Summary:** The thread explores the possibility of training motion controls using Wan2.1, but a user expresses that training motion controls with Wan did not work well.
*   **Emotion:** The overall emotional tone is Negative because a user said it didn't really work well.
*   **Top 3 Points of View:**
    *   Training motion controls with Wan2.1 may not be effective.
    *   The lack of successful examples on CivitAI suggests challenges in this area.

**[Help with Inpainting in ComfyUI (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1jtsvem/help_with_inpainting_in_comfyui/)**
*   **Summary:** This thread offers guidance on inpainting within ComfyUI using the Inpaint Crop and Stitch nodes. The summary explains the benefits of this method for detailed results, especially when dealing with small masked areas.
*   **Emotion:** Neutral, focuses on technical advice.
*   **Top 3 Points of View:**
    *   Inpaint Crop and Stitch nodes are effective for inpainting within a cropped masked area.
    *   This method automatically composites the result back into the original image.
    *   It can provide better details, especially for small masked areas.

**[Cute Gnome Kitty Dances to Meow Music! ðŸ˜ºðŸŽ¶ (Score: 1)](https://youtube.com/shorts/RTIqY4BQ_HA)**
*   **Summary:** The thread is a simple question asking if open source tools were used to create a video of a cute gnome kitty dancing to meow music.
*   **Emotion:** Neutral, focuses on technical advice.
*   **Top 3 Points of View:**
    *   Inquiry about whether open source tools were used.

**[Tiny chef videos? (Score: 0)](https://i.redd.it/52zgxnshfgte1.png)**
*   **Summary:** The thread shares a link to a beginner-friendly guide for making tiny chef videos.
*   **Emotion:** Neutral, focuses on sharing a helpful resource.
*   **Top 3 Points of View:**
    *   Provide link to guide for tiny chef videos.

**[How to generate different *** expressions and poses while keeping the same face consistent? (Score: 0)](https://i.redd.it/jtxe2cr7egte1.jpeg)**
*   **Summary:** This thread discusses methods for generating images with different *** expressions and poses while keeping the face consistent. Suggested techniques include inpainting/masking, PuLID for SDXL, InfiniteYou for Flux, ChatGPT 4o, ControlNet, and experimenting with Ace++ and text prompts. A YouTube video by mickmumpitz is also recommended.
*   **Emotion:** Neutral, mainly sharing technical suggestions.
*   **Top 3 Points of View:**
    *   Inpainting/masking is a useful method for editing parts of the image.
    *   ChatGPT 4o is good at identity preservation for generating different angles.
    *   ControlNet is a viable tool for maintaining consistency.

**[Wan 2.1 I2V (Also with Davinci 2x Upscaling SuperScale) (Score: 0)](https://v.redd.it/fywyvzedvfte1)**
*   **Summary:** This thread briefly discusses progress with Wan 2.1 I2V and Davinci 2x Upscaling SuperScale. A user also shares their inability to achieve the same quality.
*   **Emotion:** Neutral, focuses on progress and issues with Wan 2.1 I2V.
*   **Top 3 Points of View:**
    *   Progress is being made with Wan 2.1 I2V.
    *   Users are having trouble achieving the same quality.

**[Wan.21 - start & end frame (Score: 0)](https://v.redd.it/vc2lvok3mgte1)**
*   **Summary:** This thread shares a video generated with Wan.21 using start and end frames. The poster provides details on settings and the 8 hour generation time.
*   **Emotion:** Neutral, focuses on details related to Wan.21 and video generation.
*   **Top 3 Points of View:**
    *   Wan.21 was used to generate the video.
    *   The process took 8 hours with no prompt.
    *   The difference between start and end frames may be too large.

**[Questions about ReActor (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1jtnzhc/questions_about_reactor/)**
*   **Summary:** This thread answers questions about ReActor, its face restore models, impact on base models, style limitations, image requirements, occlusion options, and alternative methods.
*   **Emotion:** Neutral, mainly provides answers and information.
*   **Top 3 Points of View:**
    *   ReActor mostly uses the inswapper model with many options for face restore models.
    *   ReActor swaps can be done without a checkpoint model, so thereâ€™s no overlap with base models.
    *   IP-Adapters are recommended for anime, 3D, or other styles to fix occlusion issues.

**[How to use or test the hugging face provided models? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1jtpf0l/how_to_use_or_test_the_hugging_face_provided/)**
*   **Summary:** The thread provides instructions on how to use and test models provided on Hugging Face, directing users to the "spaces using this model" section and emphasizing the need for an account.
*   **Emotion:** Neutral, focuses on providing technical guidance.
*   **Top 3 Points of View:**
    *   Users should click on "spaces using this model".
    *   A demo page will be provided.
    *   An account is required to use the models.

**[Looking for web tool that allows for reference image to be uploaded, even on Free plan (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1jtpjwt/looking_for_web_tool_that_allows_for_reference/)**
*   **Summary:** This thread discusses tools that allow for reference image uploads, even on free plans. One suggestion is Dzine, which supports SVG and PNG, with a discount for .edu email addresses. Tensor art is also suggested.
*   **Emotion:** Neutral, focuses on providing solutions to a specific request.
*   **Top 3 Points of View:**
    *   Dzine supports SVG and PNG and offers a discount for .edu emails.
    *   Tensor art allows uploading images/videos for image 2 image/image 2 video.
    *   GIMP is a free alternative for graphics manipulation.

**[Best SD Model For Storytelling? (Historical, Fantasy, Characters, etc) (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1jtqz9l/best_sd_model_for_storytelling_historical_fantasy/)**
*   **Summary:** This thread recommends Pony or Illustrious SD models for storytelling due to their character focus. The suggestion is to grab the pony base model, find style LoRAs, and explore Pony finetunes on CivitAI.
*   **Emotion:** Neutral, mainly giving recommendation for best models.
*   **Top 3 Points of View:**
    *   Pony and Illustrious models are heavily character-focused.
    *   Use the pony base model with style LoRAs.
    *   Explore Pony finetunes on CivitAI for various styles.

**[Can I generate creepypasta thumbnails for youtube without a GPU? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1jtra6z/can_i_generate_creepypasta_thumbnails_for_youtube/)**
*   **Summary:** The thread discusses whether it is possible to generate creepypasta thumbnails for YouTube without a GPU. Some say that without a GPU you can not run it locally.
*   **Emotion:** Neutral, mainly giving answers to questions.
*   **Top 3 Points of View:**
    *   If you donâ€™t have a GPU, youâ€™re not gonna have one run locally.

**[Help with ComfyUI generating terrible images (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1jtrtb6/help_with_comfyui_generating_terrible_images/)**
*   **Summary:** This thread discusses issues with ComfyUI generating terrible images.
*   **Emotion:** Neutral, mainly giving answers to questions.
*   **Top 3 Points of View:**
    *   Fixed by setting - low vram And forced torch32 in the launching bat file.

**[Stable Diffusion Slows at 49% and 97% (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1jttkk1/stable_diffusion_slows_at_49_and_97/)**
*   **Summary:** This thread discusses why stable diffusion slows down at 49% and 97%. A possible reason is that it may be switching to RAM due to VRAM being maxed out.
*   **Emotion:** Neutral, mainly giving answers to questions.
*   **Top 3 Points of View:**
    *   It may be switching to RAM due to VRAM being maxed out.

**[all models failed at this prompt [the word "hello" but in morse code] (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1jtu12e/all_models_failed_at_this_prompt_the_word_hello/)**
*   **Summary:** This thread explores why all models failed when prompted with the word "hello" in morse code.
*   **Emotion:** Neutral, mainly giving answers to questions.
*   **Top 3 Points of View:**
    *   Probably impossible for the text encoders to parse correctly.

**[Real Examples Of Photo To Video With Runway ml Gen 4 (Score: 0)](https://www.youtube.com/watch?v=wZAHEZBkf1g)**
*   **Summary:** This thread provides information about using Runway ml Gen 4 to turn real photos into videos. Some users suggest it is not very good.
*   **Emotion:** Negative, one user states that it is "not even good".
*   **Top 3 Points of View:**
    *   Wan is another option.
    *   They're not even good, barely any movement.
