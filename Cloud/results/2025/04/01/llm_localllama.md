---
title: "LocalLLaMA Subreddit"
date: "2025-04-01"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["localllama", "LLM", "AI"]
---

# Overall Ranking and Top Discussions
1.  [[D] Just upgraded my RTX 3060 with 192GB of VRAM](https://www.reddit.com/r/LocalLLaMA/comments/1jotzue/just_upgraded_my_rtx_3060_with_192gb_of_vram/) (Score: 261)
    *   People were discussing upgrading their RTX 3060 with a large amount of VRAM, likely as an April Fool's joke.
2.  [You can now check if your Laptop/ Rig can run a GGUF directly from Hugging Face! ðŸ¤—](https://v.redd.it/0bo4dp52p8se1) (Score: 220)
    *   Hugging Face launched a new feature to check if a GGUF model can run on your machine.
3.  [DeepMind will delay sharing research to remain competitive](https://www.reddit.com/r/LocalLLaMA/comments/1jp1555/deepmind_will_delay_sharing_research_to_remain/) (Score: 130)
    *   Discussion on DeepMind delaying sharing research to maintain a competitive edge in the AI field.
4.  [An idea: an LLM trapped in the past](https://www.reddit.com/r/LocalLLaMA/comments/1josy27/an_idea_an_llm_trapped_in_the_past/) (Score: 101)
    *   The thread is about the idea of creating an LLM that is trained on data from the past, potentially before certain events or cultural shifts.
5.  [New GGUF quants of V3-0324](https://huggingface.co/ubergarm/DeepSeek-V3-0324-GGUF) (Score: 66)
    *   Discussion of new GGUF quantization of DeepSeek-V3-0324 model, including performance and memory usage.
6.  [GemmaCoder3-12b: Fine-Tuning Gemma 3 for Code Reasoning](https://huggingface.co/blog/burtenshaw/google-gemma3-gemma-code) (Score: 37)
    *   People are discussing the performance and capabilities of GemmaCoder3-12b, a fine-tuned version of Gemma 3 for code reasoning.
7.  [Tenstorrent's Big Quiet Box of AI](https://m.youtube.com/watch?v=vWw-1bk7k2c) (Score: 20)
    *   Discussion about Tenstorrent's new AI box, focusing on VRAM and performance.
8.  [Smallest model capable of detecting profane/nsfw language?](https://www.reddit.com/r/LocalLLaMA/comments/1jp1sy8/smallest_model_capable_of_detecting_profanensfw/) (Score: 9)
    *   The thread discusses the feasibility of using a small model for detecting profane or NSFW language, with some suggesting simpler solutions like banned word lists.
9.  [Does Kokoro tts have safetensors version?](https://www.reddit.com/r/LocalLLaMA/comments/1jorjva/does_kokoro_tts_have_safetensors_version/) (Score: 5)
    *   A question about whether Kokoro TTS (Text-to-Speech) has a safetensors version.
10. [Best current model for document analysis?](https://www.reddit.com/r/LocalLLaMA/comments/1jouhac/best_current_model_for_document_analysis/) (Score: 5)
    *   Discussion about the best models for document analysis, with a focus on Qwen 2.5 VL 32/72.
11. [how many 3090 can i really connect to a Asus ProArt X670E Creator board?](https://www.reddit.com/r/LocalLLaMA/comments/1jowhxj/how_many_3090_can_i_really_connect_to_a_asus/) (Score: 3)
    *   Users are discussing how to connect multiple 3090 GPUs to an Asus ProArt X670E Creator board.
12. [I dove into MCP and how it can benefit from orchestration frameworks!](https://www.reddit.com/r/LocalLLaMA/comments/1jp31pu/i_dove_into_mcp_and_how_it_can_benefit_from/) (Score: 3)
    *   A discussion on MCP communication and its integration with the Pocket Flow Framework.
13. [notebook LLM local](https://www.reddit.com/r/LocalLLaMA/comments/1joxsx9/notebook_llm_local/) (Score: 2)
    *   A query about running NotebookLM locally, and the components required.
14. [What is the best VLM for fine-tuning](https://www.reddit.com/r/LocalLLaMA/comments/1joxzgf/what_is_the_best_vlm_for_finetuning/) (Score: 2)
    *   A question about the best Vision Language Model (VLM) for fine-tuning.
15. [DeepSeek 3FS: non-RDMA install, faster ecosystem app dev/testing.](https://blog.open3fs.com/2025/04/01/deepseek-3fs-non-rdma-install-faster-ecosystem-app-dev-testing.html) (Score: 0)
    *   A post about DeepSeek 3FS, which offers a faster way to install and develop ecosystem applications.
16. [Dual RTX 5090 Beats $25,000 H100 in Real-World LLM Performance](https://www.hardware-corner.net/dual-rtx-5090-vs-h100-for-llm/) (Score: 0)
    *   Discussion about a comparison between dual RTX 5090s and an H100 for LLM performance.
17. [Best llm for Converting Angular to React](https://www.reddit.com/r/LocalLLaMA/comments/1jp0f8v/best_llm_for_converting_angular_to_react/) (Score: 0)
    *   A discussion about the best LLMs for converting Angular code to React code.
18. [Download Fails (Official Instructions)](https://www.reddit.com/r/LocalLLaMA/comments/1jp1gjy/download_fails_official_instructions/) (Score: 0)
    *   Help about a download issue, with suggestions to download manually from Hugging Face.

# Detailed Analysis by Thread
**[[D] Just upgraded my RTX 3060 with 192GB of VRAM (Score: 261)](https://www.reddit.com/r/LocalLLaMA/comments/1jotzue/just_upgraded_my_rtx_3060_with_192gb_of_vram/)**
*  **Summary:**  The thread discusses upgrading an RTX 3060 with 192GB of VRAM, likely as an April Fool's joke. Commenters share their own supposed hardware upgrades and joke about the absurdity of the original post.
*  **Emotion:** The overall emotional tone of the thread is neutral, with a few negative comments, but mostly reflects humor and amusement, related to the April Fool's theme.
*  **Top 3 Points of View:**
    *   The original poster is jokingly claiming an extreme hardware upgrade.
    *   Other users are playing along with the joke, sharing their exaggerated hardware specs.
    *   Some users are skeptical, pointing out the implausibility of the claim.

**[You can now check if your Laptop/ Rig can run a GGUF directly from Hugging Face! ðŸ¤— (Score: 220)](https://v.redd.it/0bo4dp52p8se1)**
*  **Summary:** Hugging Face launched a new feature that allows users to check if a GGUF model is compatible with their hardware directly from the Hugging Face website. Users are reacting to the announcement, providing feedback, and reporting bugs.
*  **Emotion:** The overall emotional tone is positive, with users expressing excitement and appreciation for the new feature.
*  **Top 3 Points of View:**
    *   Users are enthusiastic about the new Hugging Face feature.
    *   Some users are reporting bugs and requesting additional features.
    *   Some users are discussing the variability of GGUF performance based on context length and KV cache usage.

**[DeepMind will delay sharing research to remain competitive (Score: 130)](https://www.reddit.com/r/LocalLLaMA/comments/1jp1555/deepmind_will_delay_sharing_research_to_remain/)**
*  **Summary:**  The thread discusses DeepMind's decision to delay sharing research to maintain a competitive edge. Commenters express disappointment and debate the ethics of prioritizing profit over open research.
*  **Emotion:** The overall emotional tone is somewhat negative, reflecting disappointment and concern over DeepMind's decision. There's also a sense of resignation and acceptance.
*  **Top 3 Points of View:**
    *   Some users are disappointed that DeepMind is prioritizing profit over open research.
    *   Some users believe a six-month delay is reasonable, considering the amount of research DeepMind publishes.
    *   Some users think delaying research is detrimental to the field's overall progress.

**[An idea: an LLM trapped in the past (Score: 101)](https://www.reddit.com/r/LocalLLaMA/comments/1josy27/an_idea_an_llm_trapped_in_the_past/)**
*  **Summary:** The thread revolves around the idea of creating an LLM trained on data from a specific historical period. The discussion includes the feasibility of such a model, potential applications, and the implications of training an LLM without knowledge of future events.
*  **Emotion:** The emotional tone is mostly neutral and interested, with some users expressing excitement and others considering the practical aspects.
*  **Top 3 Points of View:**
    *   An LLM trapped in the past would be possible to create, especially one up to the 1950s, using readily available public domain books, letters, and newspapers.
    *   Old models will be sought after for the information they contain that hasn't been subjected to cleansing.
    *   An LLM in the past could be used for academic research.

**[New GGUF quants of V3-0324 (Score: 66)](https://huggingface.co/ubergarm/DeepSeek-V3-0324-GGUF)**
*  **Summary:**  This thread discusses the new GGUF quantization of the DeepSeek-V3-0324 model. Users share performance benchmarks, discuss memory usage, and offer tips for running the model. There's also a Rickroll attempt.
*  **Emotion:** The overall emotional tone is neutral and informative, with users focused on technical aspects and performance.
*  **Top 3 Points of View:**
    *   Users are sharing performance benchmarks of the new GGUF quants.
    *   Users are discussing memory usage and how to fit the model into 24GB of VRAM.
    *   Some users are skeptical about the claims being made, possibly due to the date (April 1st).

**[GemmaCoder3-12b: Fine-Tuning Gemma 3 for Code Reasoning (Score: 37)](https://huggingface.co/blog/burtenshaw/google-gemma3-gemma-code)**
*  **Summary:** The thread discusses the GemmaCoder3-12b model, a fine-tuned version of Gemma 3 for code reasoning. Users share benchmarks and compare it to other models.
*  **Emotion:** The emotional tone is mostly positive, with users expressing interest and optimism about the model's capabilities.
*  **Top 3 Points of View:**
    *   Gemma 3 12b is a hidden gem and performs well at coding.
    *   The thread provides model benchmarks.
    *   Model comparison with Qwen.

**[Tenstorrent's Big Quiet Box of AI (Score: 20)](https://m.youtube.com/watch?v=vWw-1bk7k2c)**
*  **Summary:** The thread discusses Tenstorrent's new AI box, focusing on the need for more VRAM on their cards and comparing it to Nvidia GPUs.
*  **Emotion:** The overall emotional tone is neutral and critical, with users pointing out the product's limitations and suggesting improvements.
*  **Top 3 Points of View:**
    *   Tenstorrent needs to put more VRAM on their cards.
    *   The commenter notes the memory bandwidth is similar to Blackwell Pro 6000.
    *   The video shows it running llama 70b running at 5 tps.

**[Smallest model capable of detecting profane/nsfw language? (Score: 9)](https://www.reddit.com/r/LocalLLaMA/comments/1jp1sy8/smallest_model_capable_of_detecting_profanensfw/)**
*  **Summary:** The thread discusses the possibility of using a small LLM for detecting profane/NSFW language. Most replies suggest using simple banned word lists as LLMs would be too heavy for that task.
*  **Emotion:** The emotional tone is neutral, with users giving practical advice and offering alternative solutions.
*  **Top 3 Points of View:**
    *   The user wants to detect profane/nsfw language.
    *   It's suggested that a simple banned word list should be used instead of an LLM.
    *   The user should check for free solutions from ggwp AI, utopiaanalytics since their game is small and has a low chat volume.

**[Does Kokoro tts have safetensors version? (Score: 5)](https://www.reddit.com/r/LocalLLaMA/comments/1jorjva/does_kokoro_tts_have_safetensors_version/)**
*  **Summary:** This thread is a question about whether Kokoro TTS (Text-to-Speech) has a safetensors version, and an answer with suggestion on how to convert manually pytorch weights as safetensors.
*  **Emotion:** The overall emotional tone of the thread is slightly negative.
*  **Top 3 Points of View:**
    *   Does Kokoro tts have safetensors version?
    *   It can be manually saved pytorch weights as safetensors with the package of the same name.

**[Best current model for document analysis? (Score: 5)](https://www.reddit.com/r/LocalLLaMA/comments/1jouhac/best_current_model_for_document_analysis/)**
*  **Summary:** The thread revolves around finding the best model for document analysis.
*  **Emotion:** The emotional tone of the thread is informative.
*  **Top 3 Points of View:**
    *   Qwen 2.5 VL 32/72 is probably the best.
    *   The performance on Apple devices tank the more context you give to it.
    *   Check out the RULER benchmark

**[how many 3090 can i really connect to a Asus ProArt X670E Creator board? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1jowhxj/how_many_3090_can_i_really_connect_to_a_asus/)**
*  **Summary:** Users are discussing how to connect multiple 3090 GPUs to an Asus ProArt X670E Creator board, providing various methods and considerations.
*  **Emotion:** The emotional tone of the thread is mostly neutral and informative, with users trying to assist with a technical question.
*  **Top 3 Points of View:**
    *   The user wants to know the amount of 3090 can be connected to Asus ProArt X670E Creator board.
    *   The motherboard has a bifurcation card, nvme slots, two thunderbolt ports and a powered thunderbolt hub. A total of 14 GPus can be run with this motherboard.
    *   With proper riser cards you can connect 6 in each pcie slots.

**[I dove into MCP and how it can benefit from orchestration frameworks! (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1jp31pu/i_dove_into_mcp_and_how_it_can_benefit_from/)**
*  **Summary:** A discussion on MCP communication and its integration with the Pocket Flow Framework.
*  **Emotion:** Neutral and informative.
*  **Top 3 Points of View:**
    *   A server provides a tool via MCP.
    *   A client calls that tool using the MCP protocol.

**[notebook LLM local (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1joxsx9/notebook_llm_local/)**
*  **Summary:** This thread is a question about how to run NotebookLM locally, and the components required.
*  **Emotion:** The overall emotional tone of the thread is neutral.
*  **Top 3 Points of View:**
    *   The LLM isnâ€™t the only part of notebookLM, thereâ€™s also the document parsing and RAG pipeline.
    *   Excluding those, to answer your original question, maybe QwQ32B?

**[What is the best VLM for fine-tuning (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1joxzgf/what_is_the_best_vlm_for_finetuning/)**
*  **Summary:** This thread is a question about What is the best Vision Language Model (VLM) for fine-tuning.
*  **Emotion:** The overall emotional tone of the thread is neutral.
*  **Top 3 Points of View:**
    *   What is the best VLM for fine-tuning
    *   given your privacy requirements, it would be worthwhile to share what compute you have available, imo that's going to be your initial limiting factor
---
title: "LocalLLaMA Subreddit"
date: "2025-04-01"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["localllama", "LLM", "AI"]

# Overall Ranking and Top Discussions
1.  [[D] Just upgraded my RTX 3060 with 192GB of VRAM](https://www.reddit.com/r/LocalLLaMA/comments/1jotzue/just_upgraded_my_rtx_3060_with_192gb_of_vram/) (Score: 261)
    *   People were discussing upgrading their RTX 3060 with a large amount of VRAM, likely as an April Fool's joke.
2.  [You can now check if your Laptop/ Rig can run a GGUF directly from Hugging Face! ðŸ¤—](https://v.redd.it/0bo4dp52p8se1) (Score: 220)
    *   Hugging Face launched a new feature to check if a GGUF model can run on your machine.
3.  [DeepMind will delay sharing research to remain competitive](https://www.reddit.com/r/LocalLLaMA/comments/1jp1555/deepmind_will_delay_sharing_research_to_remain/) (Score: 130)
    *   Discussion on DeepMind delaying sharing research to maintain a competitive edge in the AI field.
4.  [An idea: an LLM trapped in the past](https://www.reddit.com/r/LocalLLaMA/comments/1josy27/an_idea_an_llm_trapped_in_the_past/) (Score: 101)
    *   The thread is about the idea of creating an LLM that is trained on data from the past, potentially before certain events or cultural shifts.
5.  [New GGUF quants of V3-0324](https://huggingface.co/ubergarm/DeepSeek-V3-0324-GGUF) (Score: 66)
    *   Discussion of new GGUF quantization of DeepSeek-V3-0324 model, including performance and memory usage.
6.  [GemmaCoder3-12b: Fine-Tuning Gemma 3 for Code Reasoning](https://huggingface.co/blog/burtenshaw/google-gemma3-gemma-code) (Score: 37)
    *   People are discussing the performance and capabilities of GemmaCoder3-12b, a fine-tuned version of Gemma 3 for code reasoning.
7.  [Tenstorrent's Big Quiet Box of AI](https://m.youtube.com/watch?v=vWw-1bk7k2c) (Score: 20)
    *   Discussion about Tenstorrent's new AI box, focusing on VRAM and performance.
8.  [Smallest model capable of detecting profane/nsfw language?](https://www.reddit.com/r/LocalLLaMA/comments/1jp1sy8/smallest_model_capable_of_detecting_profanensfw/) (Score: 9)
    *   The thread discusses the feasibility of using a small model for detecting profane or NSFW language, with some suggesting simpler solutions like banned word lists.
9.  [Does Kokoro tts have safetensors version?](https://www.reddit.com/r/LocalLLaMA/comments/1jorjva/does_kokoro_tts_have_safetensors_version/) (Score: 5)
    *   A question about whether Kokoro TTS (Text-to-Speech) has a safetensors version.
10. [Best current model for document analysis?](https://www.reddit.com/r/LocalLLaMA/comments/1jouhac/best_current_model_for_document_analysis/) (Score: 5)
    *   Discussion about the best models for document analysis, with a focus on Qwen 2.5 VL 32/72.
11. [how many 3090 can i really connect to a Asus ProArt X670E Creator board?](https://www.reddit.com/r/LocalLLaMA/comments/1jowhxj/how_many_3090_can_i_really_connect_to_a_asus/) (Score: 3)
    *   Users are discussing how to connect multiple 3090 GPUs to an Asus ProArt X670E Creator board.
12. [I dove into MCP and how it can benefit from orchestration frameworks!](https://www.reddit.com/r/LocalLLaMA/comments/1jp31pu/i_dove_into_mcp_and_how_it_can_benefit_from/) (Score: 3)
    *   A discussion on MCP communication and its integration with the Pocket Flow Framework.
13. [notebook LLM local](https://www.reddit.com/r/LocalLLaMA/comments/1joxsx9/notebook_llm_local/) (Score: 2)
    *   A query about running NotebookLM locally, and the components required.
14. [What is the best VLM for fine-tuning](https://www.reddit.com/r/LocalLLaMA/comments/1joxzgf/what_is_the_best_vlm_for_finetuning/) (Score: 2)
    *   A question about the best Vision Language Model (VLM) for fine-tuning.
15. [DeepSeek 3FS: non-RDMA install, faster ecosystem app dev/testing.](https://blog.open3fs.com/2025/04/01/deepseek-3fs-non-rdma-install-faster-ecosystem-app-dev-testing.html) (Score: 0)
    *   A post about DeepSeek 3FS, which offers a faster way to install and develop ecosystem applications.
16. [Dual RTX 5090 Beats $25,000 H100 in Real-World LLM Performance](https://www.hardware-corner.net/dual-rtx-5090-vs-h100-for-llm/) (Score: 0)
    *   Discussion about a comparison between dual RTX 5090s and an H100 for LLM performance.
17. [Best llm for Converting Angular to React](https://www.reddit.com/r/LocalLLaMA/comments/1jp0f8v/best_llm_for_converting_angular_to_react/) (Score: 0)
    *   A discussion about the best LLMs for converting Angular code to React code.
18. [Download Fails (Official Instructions)](https://www.reddit.com/r/LocalLLaMA/comments/1jp1gjy/download_fails_official_instructions/) (Score: 0)
    *   Help about a download issue, with suggestions to download manually from Hugging Face.

# Detailed Analysis by Thread
**[[D] Just upgraded my RTX 3060 with 192GB of VRAM (Score: 261)](https://www.reddit.com/r/LocalLLaMA/comments/1jotzue/just_upgraded_my_rtx_3060_with_192gb_of_vram/)**
*  **Summary:**  The thread discusses upgrading an RTX 3060 with 192GB of VRAM, likely as an April Fool's joke. Commenters share their own supposed hardware upgrades and joke about the absurdity of the original post.
*  **Emotion:** The overall emotional tone of the thread is neutral, with a few negative comments, but mostly reflects humor and amusement, related to the April Fool's theme.
*  **Top 3 Points of View:**
    *   The original poster is jokingly claiming an extreme hardware upgrade.
    *   Other users are playing along with the joke, sharing their exaggerated hardware specs.
    *   Some users are skeptical, pointing out the implausibility of the claim.

**[You can now check if your Laptop/ Rig can run a GGUF directly from Hugging Face! ðŸ¤— (Score: 220)](https://v.redd.it/0bo4dp52p8se1)**
*  **Summary:** Hugging Face launched a new feature that allows users to check if a GGUF model is compatible with their hardware directly from the Hugging Face website. Users are reacting to the announcement, providing feedback, and reporting bugs.
*  **Emotion:** The overall emotional tone is positive, with users expressing excitement and appreciation for the new feature.
*  **Top 3 Points of View:**
    *   Users are enthusiastic about the new Hugging Face feature.
    *   Some users are reporting bugs and requesting additional features.
    *   Some users are discussing the variability of GGUF performance based on context length and KV cache usage.

**[DeepMind will delay sharing research to remain competitive (Score: 130)](https://www.reddit.com/r/LocalLLaMA/comments/1jp1555/deepmind_will_delay_sharing_research_to_remain/)**
*  **Summary:**  The thread discusses DeepMind's decision to delay sharing research to maintain a competitive edge. Commenters express disappointment and debate the ethics of prioritizing profit over open research.
*  **Emotion:** The overall emotional tone is somewhat negative, reflecting disappointment and concern over DeepMind's decision. There's also a sense of resignation and acceptance.
*  **Top 3 Points of View:**
    *   Some users are disappointed that DeepMind is prioritizing profit over open research.
    *   Some users believe a six-month delay is reasonable, considering the amount of research DeepMind publishes.
    *   Some users think delaying research is detrimental to the field's overall progress.

**[An idea: an LLM trapped in the past (Score: 101)](https://www.reddit.com/r/LocalLLaMA/comments/1josy27/an_idea_an_llm_trapped_in_the_past/)**
*  **Summary:** The thread revolves around the idea of creating an LLM trained on data from a specific historical period. The discussion includes the feasibility of such a model, potential applications, and the implications of training an LLM without knowledge of future events.
*  **Emotion:** The emotional tone is mostly neutral and interested, with some users expressing excitement and others considering the practical aspects.
*  **Top 3 Points of View:**
    *   An LLM trapped in the past would be possible to create, especially one up to the 1950s, using readily available public domain books, letters, and newspapers.
    *   Old models will be sought after for the information they contain that hasn't been subjected to cleansing.
    *   An LLM in the past could be used for academic research.

**[New GGUF quants of V3-0324 (Score: 66)](https://huggingface.co/ubergarm/DeepSeek-V3-0324-GGUF)**
*  **Summary:**  This thread discusses the new GGUF quantization of the DeepSeek-V3-0324 model. Users share performance benchmarks, discuss memory usage, and offer tips for running the model. There's also a Rickroll attempt.
*  **Emotion:** The overall emotional tone is neutral and informative, with users focused on technical aspects and performance.
*  **Top 3 Points of View:**
    *   Users are sharing performance benchmarks of the new GGUF quants.
    *   Users are discussing memory usage and how to fit the model into 24GB of VRAM.
    *   Some users are skeptical about the claims being made, possibly due to the date (April 1st).

**[GemmaCoder3-12b: Fine-Tuning Gemma 3 for Code Reasoning (Score: 37)](https://huggingface.co/blog/burtenshaw/google-gemma3-gemma-code)**
*  **Summary:** The thread discusses the GemmaCoder3-12b model, a fine-tuned version of Gemma 3 for code reasoning. Users share benchmarks and compare it to other models.
*  **Emotion:** The emotional tone is mostly positive, with users expressing interest and optimism about the model's capabilities.
*  **Top 3 Points of View:**
    *   Gemma 3 12b is a hidden gem and performs well at coding.
    *   The thread provides model benchmarks.
    *   Model comparison with Qwen.

**[Tenstorrent's Big Quiet Box of AI (Score: 20)](https://m.youtube.com/watch?v=vWw-1bk7k2c)**
*  **Summary:** The thread discusses Tenstorrent's new AI box, focusing on the need for more VRAM on their cards and comparing it to Nvidia GPUs.
*  **Emotion:** The overall emotional tone is neutral and critical, with users pointing out the product's limitations and suggesting improvements.
*  **Top 3 Points of View:**
    *   Tenstorrent needs to put more VRAM on their cards.
    *   The commenter notes the memory bandwidth is similar to Blackwell Pro 6000.
    *   The video shows it running llama 70b running at 5 tps.

**[Smallest model capable of detecting profane/nsfw language? (Score: 9)](https://www.reddit.com/r/LocalLLaMA/comments/1jp1sy8/smallest_model_capable_of_detecting_profanensfw/)**
*  **Summary:** The thread discusses the possibility of using a small LLM for detecting profane/NSFW language. Most replies suggest using simple banned word lists as LLMs would be too heavy for that task.
*  **Emotion:** The emotional tone is neutral, with users giving practical advice and offering alternative solutions.
*  **Top 3 Points of View:**
    *   The user wants to detect profane/nsfw language.
    *   It's suggested that a simple banned word list should be used instead of an LLM.
    *   The user should check for free solutions from ggwp AI, utopiaanalytics since their game is small and has a low chat volume.

**[Does Kokoro tts have safetensors version? (Score: 5)](https://www.reddit.com/r/LocalLLaMA/comments/1jorjva/does_kokoro_tts_have_safetensors_version/)**
*  **Summary:** This thread is a question about whether Kokoro TTS (Text-to-Speech) has a safetensors version, and an answer with suggestion on how to convert manually pytorch weights as safetensors.
*  **Emotion:** The overall emotional tone of the thread is slightly negative.
*  **Top 3 Points of View:**
    *   Does Kokoro tts have safetensors version?
    *   It can be manually saved pytorch weights as safetensors with the package of the same name.

**[Best current model for document analysis? (Score: 5)](https://www.reddit.com/r/LocalLLaMA/comments/1jouhac/best_current_model_for_document_analysis/)**
*  **Summary:** The thread revolves around finding the best model for document analysis.
*  **Emotion:** The emotional tone of the thread is informative.
*  **Top 3 Points of View:**
    *   Qwen 2.5 VL 32/72 is probably the best.
    *   The performance on Apple devices tank the more context you give to it.
    *   Check out the RULER benchmark

**[how many 3090 can i really connect to a Asus ProArt X670E Creator board? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1jowhxj/how_many_3090_can_i_really_connect_to_a_asus/)**
*  **Summary:** Users are discussing how to connect multiple 3090 GPUs to an Asus ProArt X670E Creator board, providing various methods and considerations.
*  **Emotion:** The emotional tone of the thread is mostly neutral and informative, with users trying to assist with a technical question.
*  **Top 3 Points of View:**
    *   The user wants to know the amount of 3090 can be connected to Asus ProArt X670E Creator board.
    *   The motherboard has a bifurcation card, nvme slots, two thunderbolt ports and a powered thunderbolt hub. A total of 14 GPus can be run with this motherboard.
    *   With proper riser cards you can connect 6 in each pcie slots.

**[I dove into MCP and how it can benefit from orchestration frameworks! (Score: 3)](https://www.reddit.com/r/LocalLLaMA
