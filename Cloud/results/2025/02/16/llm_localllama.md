---
title: "LocalLLaMA Subreddit"
date: "2025-02-16"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "LocalAI", "Hardware"]
---

# Overall Ranking and Top Discussions
1.  [Sorcery: Allow AI characters to reach into the real world. From the creator of DRY and XTC.](https://i.redd.it/asmx7nh0wije1.png) (Score: 132)
    * This thread discusses a new tool called Sorcery that allows AI characters to interact with the real world by binding JavaScript code to events in a chat.
2.  [Multi-GPU rig shows ERR! in Nvidia-SMI when I have 3 cards or more](https://i.redd.it/lbzg9mvtnjje1.png) (Score: 23)
    * This thread discusses issues with multi-GPU rigs, specifically the "ERR!" message in Nvidia-SMI when using 3 or more cards.
3.  [Audiobook Creator - Releasing Version 2](https://www.reddit.com/r/LocalLLaMA/comments/1iqynut/audiobook_creator_releasing_version_2/) (Score: 22)
    * This thread announces the release of version 2 of an audiobook creator and requests suggestions to add new languages.
4.  [The “dry fit” of Oculink 4x4x4x4 for RTX 3090 rig](https://www.reddit.com/gallery/1iqvmba) (Score: 15)
    * This thread shows images of RTX 3090 rigs.
5.  [Fine Tune LLM on Unstructured Data](https://www.reddit.com/r/LocalLLaMA/comments/1iqx3n7/fine_tune_llm_on_unstructured_data/) (Score: 6)
    * This thread discusses using an LLM to read unstructured data and make a training dataset out of it.
6.  [Real AGI is an AI that learns on the fly , has long term memory and other features](https://www.reddit.com/r/LocalLLaMA/comments/1ir1g6k/real_agi_is_an_ai_that_learns_on_the_fly_has_long/) (Score: 6)
    * This thread discusses about AGI. Online vs offline reinforcement learning.
7.  [Why does Zed's new LLM work great on their IDE and terrible with VS Code's Continue](https://www.reddit.com/r/LocalLLaMA/comments/1iqykw5/why_does_zeds_new_llm_work_great_on_their_ide_and/) (Score: 5)
    * This thread explains that the new code completion model Zeta is not compatible with VS Code's Continue.
8.  [The scoop on 4060 ti 16gb cards](https://www.reddit.com/r/LocalLLaMA/comments/1ir08pp/the_scoop_on_4060_ti_16gb_cards/) (Score: 5)
    * This thread discusses the pros and cons of the 4060 ti 16gb cards.
9.  [What's the best distributed LLM library?](https://www.reddit.com/r/LocalLLaMA/comments/1ir033d/whats_the_best_distributed_llm_library/) (Score: 4)
    * This thread requests an easy and simple solution for running experiments across the internet.
10. [Need advice on where to start: A few questions](https://www.reddit.com/r/LocalLLaMA/comments/1iqv2b2/need_advice_on_where_to_start_a_few_questions/) (Score: 2)
    * This thread discusses about using LM Studio and using retrieval augmented generation with documents or web search.
11. [Aren´t people curious about the effects of hardware on token generation speed?](https://www.reddit.com/r/LocalLLaMA/comments/1iqvxhe/arent_people_curious_about_the_effects_of/) (Score: 2)
    * This thread is a discussion of the effects of hardware on token generation speed. The major contributor is memory bandwidth.
12. [Alternatives to multi-GPU rigs for large models](https://www.reddit.com/r/LocalLLaMA/comments/1iqzrt1/alternatives_to_multigpu_rigs_for_large_models/) (Score: 1)
    * This thread discusses about hassle-free setups for inference of Mistral Large 2411. A top-spec Mac Studio could do it, as could many configurations of the Mac Pro.
13. [Hey used some opensource techniques to create a new llm (using deepseek 672B , llama 3.3 70B and other models fine tune each other in hopes of a model which is better than existing ones.)](https://www.reddit.com/r/LocalLLaMA/comments/1iqzufg/hey_used_some_opensource_techniques_to_create_a/) (Score: 1)
    * This thread is about the creation of a new LLM using open source techniques.
14. [Second generation DIGITS might use new memory standard](https://i.redd.it/nvjxgv2u7jje1.jpeg) (Score: 0)
    * This thread discusses about new memory standard for Second generation DIGITS.
15. [Made a game where an LLM judges your drawings as famous fictional characters](https://www.reddit.com/r/LocalLLaMA/comments/1iqwmwj/made_a_game_where_an_llm_judges_your_drawings_as/) (Score: 0)
    * This thread provides a game github link.
16. [Free premium access to my new role-play platform](https://www.reddit.com/r/LocalLLaMA/comments/1ir09qu/free_premium_access_to_my_new_roleplay_platform/) (Score: 0)
    * This thread provides a new role-play platform with free premium access.

# Detailed Analysis by Thread
**[[D] Sorcery: Allow AI characters to reach into the real world. From the creator of DRY and XTC. (Score: 132)](https://i.redd.it/asmx7nh0wije1.png)**
*  **Summary:**  This thread discusses a new tool called Sorcery that allows AI characters to interact with the real world by binding JavaScript code to events in a chat. It aims to provide a more immersive experience in RP settings.
*  **Emotion:** The emotional tone of this thread is mostly neutral, with a mix of positive sentiments expressing excitement and curiosity about the new technology.
*  **Top 3 Points of View:**
    *   The creator explains how Sorcery allows AI characters to perform tangible actions in the real world (e.g., controlling smart home appliances).
    *   Some users are excited about the potential for more immersive role-playing experiences and the ability to chain AI thought processes.
    *   Some users express concern about potential misuse, like AI characters causing harm, and desire for easier ways to implement the technology.

**[Multi-GPU rig shows ERR! in Nvidia-SMI when I have 3 cards or more (Score: 23)](https://i.redd.it/lbzg9mvtnjje1.png)**
*  **Summary:**  This thread discusses issues with multi-GPU rigs, specifically the "ERR!" message in Nvidia-SMI when using 3 or more cards. Users share potential causes and solutions.
*  **Emotion:** The emotional tone of this thread is neutral, focused on problem-solving and technical assistance.
*  **Top 3 Points of View:**
    *   The error is likely related to PCIe errors, possibly caused by cheap risers.
    *   Updating or downgrading drivers might solve the issue.
    *   Enabling "Above 4G Decoding" in BIOS can help.

**[Audiobook Creator - Releasing Version 2 (Score: 22)](https://www.reddit.com/r/LocalLLaMA/comments/1iqynut/audiobook_creator_releasing_version_2/)**
*  **Summary:**  This thread announces the release of version 2 of an audiobook creator. Users provide feedback and suggestions for improvements.
*  **Emotion:** The thread has a positive emotional tone due to people appreciating the app and its features.
*  **Top 3 Points of View:**
    *   Users suggest adding RVC (Retrieval-Based Voice Conversion) for more diverse voice options.
    *   Users request support for additional languages, specifically Polish.
    *   Users suggest adding a brief silence after the chapter name/number.

**[The “dry fit” of Oculink 4x4x4x4 for RTX 3090 rig (Score: 15)](https://www.reddit.com/gallery/1iqvmba)**
*  **Summary:**  This thread showcases the build of an RTX 3090 rig using Oculink connectors.
*  **Emotion:** The emotional tone of this thread is mixed, with both positive excitement about the build and some concern about its cost-effectiveness.
*  **Top 3 Points of View:**
    *   Some users are curious about the specific Oculink components used and where to purchase them.
    *   Other users question the value of investing so much in GPUs, especially given the rapid pace of AI model development.
    *   One user suggests that it's too early to be posting this project, since its using an Aliexpress adapter.

**[Fine Tune LLM on Unstructured Data (Score: 6)](https://www.reddit.com/r/LocalLLaMA/comments/1iqx3n7/fine_tune_llm_on_unstructured_data/)**
*  **Summary:**  This thread discusses the process of fine-tuning LLMs on unstructured data.
*  **Emotion:** The emotional tone is neutral and informative.
*  **Top 3 Points of View:**
    *   LLMs can be used to read unstructured data and create training datasets.
    *   Instruct models with large contexts can be used to generate questions and answers for LLM training.
    *   Preparing training data is computationally expensive.

**[Real AGI is an AI that learns on the fly , has long term memory and other features (Score: 6)](https://www.reddit.com/r/LocalLLaMA/comments/1ir1g6k/real_agi_is_an_ai_that_learns_on_the_fly_has_long/)**
*  **Summary:**  This thread discusses the concept of AGI (Artificial General Intelligence) and its feasibility, with reference to online vs offline reinforcement learning.
*  **Emotion:** The emotional tone is generally neutral, with elements of skepticism and informative discussion.
*  **Top 3 Points of View:**
    *   AGI is considered a worthless concept that stands in the way of understanding cognition.
    *   Online vs offline reinforcement learning are existing concepts in AI.
    *   Currently large language models far surpass AGI in their knowledge.

**[Why does Zed's new LLM work great on their IDE and terrible with VS Code's Continue (Score: 5)](https://www.reddit.com/r/LocalLLaMA/comments/1iqykw5/why_does_zeds_new_llm_work_great_on_their_ide_and/)**
*  **Summary:**  This thread explains why Zed's new code completion model (Zeta) doesn't work well with VS Code's Continue extension.
*  **Emotion:** The thread has a generally positive tone, mixed with caution.
*  **Top 3 Points of View:**
    *   Zeta is a new type of model that Continue's FIM-style support doesn't expect.
    *   Users hope that Zeta will be integrated into Continue or a Zeta-ready extension will be developed.
    *   Some users caution against becoming too reliant on Zed, as features may become paid in the future.

**[The scoop on 4060 ti 16gb cards (Score: 5)](https://www.reddit.com/r/LocalLLaMA/comments/1ir08pp/the_scoop_on_4060_ti_16gb_cards/)**
*  **Summary:**  This thread discusses the performance and value of the 4060 ti 16gb cards for local LLM use.
*  **Emotion:** The tone is generally neutral with some negative sentiment.
*  **Top 3 Points of View:**
    *   The 4060 ti 16gb cards are considered slow due to low memory bandwidth.
    *   Alternative options like dual 3060 12GB cards or AMD cards are considered better deals.
    *   The 4060 ti 16gb cards are faster than CPU and RAM and have low idle power consumption.

**[What's the best distributed LLM library? (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1ir033d/whats_the_best_distributed_llm_library/)**
*  **Summary:**  This thread seeks recommendations for a distributed LLM library that's easy to use and scalable for running experiments across the internet or multiple computers.
*  **Emotion:** The emotional tone is neutral, with an emphasis on seeking practical solutions to a challenging problem.
*  **Top 3 Points of View:**
    *   There isn't currently an easy, simple, scalable solution for distributed LLM training.
    *   Training over the network isn't efficient enough to be feasible.
    *   Nous' pretraining of LLMs over the network is a notable experiment, but the code hasn't been released.

**[Need advice on where to start: A few questions (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1iqv2b2/need_advice_on_where_to_start_a_few_questions/)**
*  **Summary:**  This thread is a request for advice on getting started with local LLMs, including questions about frontends and hardware requirements.
*  **Emotion:** The overall emotional tone is positive, reflecting enthusiasm and a willingness to learn.
*  **Top 3 Points of View:**
    *   LM Studio is recommended for its OpenAI-compatible API and ease of use.
    *   The amount of VRAM is a primary consideration when choosing a model.
    *   Retrieval-augmented generation (RAG) is suggested to improve the reliability of answers.

**[Aren´t people curious about the effects of hardware on token generation speed? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1iqvxhe/arent_people_curious_about_the_effects_of/)**
*  **Summary:**  This thread discusses the impact of hardware on token generation speed in local LLMs.
*  **Emotion:** The overall tone is neutral and informative, though a touch of negativity is present when someone says "not really".
*  **Top 3 Points of View:**
    *   Memory bandwidth is a major factor affecting token generation speed.
    *   Token generation speed tends to vary a lot during the same prompt, tending to slow down in the end.
    *   Ollama is not the best test bed.

**[Alternatives to multi-GPU rigs for large models (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1iqzrt1/alternatives_to_multigpu_rigs_for_large_models/)**
*  **Summary:**  This thread explores alternatives to multi-GPU setups for running large language models locally.
*  **Emotion:** The emotional tone is generally positive, providing different methods to solve similar problems.
*  **Top 3 Points of View:**
    *   M4 Mac Mini / Cluster, AMD dual Epic CPU system or a multi Nvidia system are needed to have enough working ultra high speed memory.
    *   A top-spec Mac Studio or Mac Pro are hassle-free options for running Mistral Large 2411.
    *   A gaming PC with internal and external GPUs using USB4/Thunderbolt can also work.

**[Hey used some opensource techniques to create a new llm (using deepseek 672B , llama 3.3 70B and other models fine tune each other in hopes of a model which is better than existing ones.) (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1iqzufg/hey_used_some_opensource_techniques_to_create_a/)**
*  **Summary:**  The thread is about creating a new LLM with opensource techniques and if the OP has a huggingface link.
*  **Emotion:** The emotion of the thread is generally neutral.
*  **Top 3 Points of View:**
    *   Requesting a huggingface link to review the project.

**[Second generation DIGITS might use new memory standard (Score: 0)](https://i.redd.it/nvjxgv2u7jje1.jpeg)**
*  **Summary:**  This thread discusses the possibility of second-generation NVIDIA DIGITS systems using a new memory standard.
*  **Emotion:** The emotional tone of this thread is mixed.
*  **Top 3 Points of View:**
    *   It might be easier to read the translation, if you download the picture.
    *   It's very unlikely that they would put off their initial customers who would prefer to wait for that.
    *   The community says it's a cool screenshot but low effort af post.

**[Made a game where an LLM judges your drawings as famous fictional characters (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1iqwmwj/made_a_game_where_an_llm_judges_your_drawings_as/)**
*  **Summary:**  This thread is asking for the github link of the game.
*  **Emotion:** The emotion of the thread is generally neutral.
*  **Top 3 Points of View:**
    *   User is requesting the github link.

**[Free premium access to my new role-play platform (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1ir09qu/free_premium_access_to_my_new_roleplay_platform/)**
*  **Summary:**  This thread is about a new role-play platform with free premium access.
*  **Emotion:** The emotion of the thread is generally positive.
*  **Top 3 Points of View:**
    *   Website looks great!
