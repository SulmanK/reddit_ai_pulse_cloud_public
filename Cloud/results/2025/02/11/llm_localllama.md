---
title: "LocalLLaMA Subreddit"
date: "2025-02-11"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["AI", "LocalLLaMA", "Models"]
---

# Overall Ranking and Top Discussions
1.  [4x3090 in a 4U case, don't recommend it](https://www.reddit.com/gallery/1in69s3) (Score: 84)
    * Discussion about building a server with 4x3090 GPUs in a 4U case, including specs, costs, and performance benchmarks.
2.  [ChatGPT 4o feels straight up *** after using o1 and DeepSeek for awhile](https://www.reddit.com/r/LocalLLaMA/comments/1in7nka/chatgpt_4o_feels_straight_up_stupid_after_using/) (Score: 58)
    *  Users comparing ChatGPT 4o to other models like o1 and DeepSeek, with some finding it lacking in logic and reasoning abilities.
3.  [Why AMD or Intel doesn't sell card with huge amount of Vram ?](https://www.reddit.com/r/LocalLLaMA/comments/1in4b81/why_amd_or_intel_doesnt_sell_card_with_huge/) (Score: 47)
    * Discussion on why AMD and Intel don't offer consumer-grade graphics cards with very large amounts of VRAM, covering topics like market segmentation, cost, and memory limitations.
4.  [Chonky Boi has arrived](https://i.imgur.com/kh64WJy.jpeg) (Score: 23)
    * A user showing off a new GPU, likely an AMD card, and answering questions about why they chose it over an Nvidia 4090.
5.  [EU mobilizes $200 billion in AI race against US and China](https://www.theverge.com/news/609930/eu-200-billion-investment-ai-development) (Score: 20)
    * Discussion on the EU's investment in AI development to compete with the US and China.
6.  [In France, we can get Mistral Chat Pro for 1 year for free](https://www.iliad.fr/en/actualites/article/free-france-s-first-telco-to-include-a-premium-ai-assistant-in-its-mobile-plans-le-chat-pro-developed-by-mistral-ai) (Score: 6)
    * A post about a French telecom offering Mistral Chat Pro for free with a purchase of one of their telco products.
7.  [1-Click AI Tools in your browser - free to use with local models](https://www.reddit.com/r/LocalLLaMA/comments/1in3u3y/1click_ai_tools_in_your_browser_free_to_use_with/) (Score: 3)
    * Post about a browser extension for using local AI models. Some users express concerns about the need to create an account.
8.  [Best Open-source models?](https://www.reddit.com/r/LocalLLaMA/comments/1in8hlq/best_opensource_models/) (Score: 2)
    *  Discussion regarding the best open-source AI models.
9.  [Generating Unfeasibly Large Case Statements](https://www.reddit.com/r/LocalLLaMA/comments/1in5fcx/generating_unfeasibly_large_case_statements/) (Score: 2)
    * Abstract discussion about creating complex systems for LLMs and encountering unexpected results.
10. [Will LLM API Pricing Models Break with Long Compute Times and Low Token Output?](https://www.reddit.com/r/LocalLLaMA/comments/1in4nod/will_llm_api_pricing_models_break_with_long/) (Score: 1)
    * Discussion about the sustainability of LLM API pricing models, especially considering long compute times and low token output.
11. [What version of DeepSeek can I run on a 4090?](https://www.reddit.com/r/LocalLLaMA/comments/1in8q1v/what_version_of_deepseek_can_i_run_on_a_4090/) (Score: 1)
    *  Users discuss which versions of DeepSeek AI models can be run on an Nvidia 4090 GPU.
12. [Has someone setup minicpm-o 2.6 with vLLM and CPU](https://www.reddit.com/r/LocalLLaMA/comments/1in86jy/has_someone_setup_minicpmo_26_with_vllm_and_cpu/) (Score: 1)
    * Question about setting up minicpm-o 2.6 with vLLM and CPU, with the acknowledgement that vLLM support for CPU is still experimental.
13. [CPU and Ram](https://www.reddit.com/r/LocalLLaMA/comments/1in91j5/cpu_and_ram/) (Score: 1)
    * Question regarding the speed of the systems and the amount of memory bandwidth that systems need to run correctly.
14. [ChatGPT scammy bevaiour](https://i.redd.it/wrqq3a2a8kie1.png) (Score: 0)
    * Users discuss whether to believe the information that ChatGPT gives and discuss how images are created through it.
15. [I unintentionally obtained information on Phi-5's release date](https://www.reddit.com/r/LocalLLaMA/comments/1in8x65/i_unintentionally_obtained_information_on_phi5s/) (Score: 0)
    * A user claims to have unintentionally obtained information about the release date of Phi-5 and faces questions and skepticism from other users.
16. [How to Train an Anime Video Generation AI (explain it to me as if I’m a 5yo!)](https://www.reddit.com/r/LocalLLaMA/comments/1in982e/how_to_train_an_anime_video_generation_ai_explain/) (Score: 0)
    * Asking how to train an anime video generation AI.

# Detailed Analysis by Thread
**[4x3090 in a 4U case, don't recommend it (Score: 84)](https://www.reddit.com/gallery/1in69s3)**
*  **Summary:** A user shares their experience building a rackmountable server with 4x3090 GPUs in a 4U case, detailing the components, costs, and performance. They highlight the challenges of the build and offer benchmarks using llama3.3-70B.
*  **Emotion:** Mostly Neutral, with some comments expressing positive sentiment about the build and others showing concern about potential issues like airflow.
*  **Top 3 Points of View:**
    * The build is impressive and functional for running large language models.
    * The choice of motherboard and CPU might limit future upgrade options.
    * Adequate airflow is crucial to prevent overheating with such a high-power setup.

**[ChatGPT 4o feels straight up *** after using o1 and DeepSeek for awhile (Score: 58)](https://www.reddit.com/r/LocalLLaMA/comments/1in7nka/chatgpt_4o_feels_straight_up_stupid_after_using/)**
*  **Summary:** Users discuss their experiences with ChatGPT 4o, comparing it unfavorably to other models like o1 and DeepSeek. Some users feel its reasoning abilities have declined, while others find it better for specific tasks like image understanding.
*  **Emotion:** Mixed, with a slightly negative leaning due to disappointment in ChatGPT 4o's performance. Some positive sentiment is present for specific use cases.
*  **Top 3 Points of View:**
    * ChatGPT 4o's reasoning abilities have degraded compared to previous versions or other models.
    * DeepSeek V3 is a superior base model for reasoning and logic tasks.
    * ChatGPT 4o remains strong in image understanding tasks.

**[Why AMD or Intel doesn't sell card with huge amount of Vram ? (Score: 47)](https://www.reddit.com/r/LocalLLaMA/comments/1in4b81/why_amd_or_intel_doesnt_sell_card_with_huge/)**
*  **Summary:** A discussion on the reasons why AMD and Intel do not sell consumer graphics cards with very large amounts of VRAM. The thread explores market segmentation, cost of HBM, the need for wider memory buses, and the profitability of focusing on high-end server GPUs.
*  **Emotion:** Neutral, with factual explanations and reasoned arguments dominating the discussion.
*  **Top 3 Points of View:**
    * Market segmentation: Large VRAM cards are primarily targeted at the professional/datacenter market.
    * Cost and limitations of fast memory chips: HBM is expensive, and there are limitations in memory bus width.
    * Profitability: It's more profitable to sell high-VRAM cards at a premium for AI and professional use.

**[Chonky Boi has arrived (Score: 23)](https://i.imgur.com/kh64WJy.jpeg)**
*  **Summary:** A user showcases a new GPU and answers questions about its capabilities and why they chose it over an Nvidia 4090.
*  **Emotion:** Positive, driven by the excitement of a new hardware acquisition.
*  **Top 3 Points of View:**
    * AMD is becoming a more attractive option due to perceived issues with Nvidia's latest releases (the "total flop 5090 launch").
    * Users are interested in the t/s (tokens per second) speeds that the card can achieve.
    * The AMD W6800 is seen as a good choice by some users.

**[EU mobilizes $200 billion in AI race against US and China (Score: 20)](https://www.theverge.com/news/609930/eu-200-billion-investment-ai-development)**
*  **Summary:** A discussion regarding the EU's $200 billion investment in AI to compete with the US and China.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    * The money will be used to buy the necessary red tape and printer ink for all the paperwork.

**[In France, we can get Mistral Chat Pro for 1 year for free (Score: 6)](https://www.iliad.fr/en/actualites/article/free-france-s-first-telco-to-include-a-premium-ai-assistant-in-its-mobile-plans-le-chat-pro-developed-by-mistral-ai)**
*   **Summary:** A French telecom provider includes Mistral Chat Pro for 1 year in its mobile plans.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    * The product is only free with the purchase of certain Telco products.

**[1-Click AI Tools in your browser - free to use with local models (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1in3u3y/1click_ai_tools_in_your_browser_free_to_use_with/)**
*   **Summary:** A post highlighting the free AI Tools in a browser where it seems you have to make an account to add Ollama.
*   **Emotion:** Positive.
*   **Top 3 Points of View:**
    * There is no need for an account.

**[Best Open-source models? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1in8hlq/best_opensource_models/)**
*   **Summary:** A discussion regarding the best open-source AI models.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    * The best fully open source model is probably Moxin, but it doesn't compete with the open-weights models mentioned.
    * Phi 3.5 (4B) is good at instruction following.
    * Developing a software for weight slip analysis, and the recognition of deepseek is the best, but struggles a lot with understanding the commands. Second best was Mistral.

**[Generating Unfeasibly Large Case Statements (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1in5fcx/generating_unfeasibly_large_case_statements/)**
*   **Summary:** Trying to create the symbolic "ramp" that we jump our LLMs off of, but for now it's less hot wheel tracks and more finger boards.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    * Whenever we try to jump off a ramp we find a loop-de-loop instead.

**[Will LLM API Pricing Models Break with Long Compute Times and Low Token Output? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1in4nod/will_llm_api_pricing_models_break_with_long/)**
*   **Summary:** Suggesting that LLM API pricing models could be broken with long compute times and low token output.
*   **Emotion:** Positive.
*   **Top 3 Points of View:**
    * It is suggested that you can tier this in different endpoints with different settings and pricing: \`-low, -mid, -high, -super, -ultra, -lvl9000, -yacht\`
    * Suggesting building a tool that undercuts a lot of inference providers by basically becoming a inference provider yourself, and spinning up spot inference clusters.
    * Theyre all pretty open that most of their consumer tiers lose money, and that extends to the API as well.

**[Has someone setup minicpm-o 2.6 with vLLM and CPU (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1in86jy/has_someone_setup_minicpmo_26_with_vllm_and_cpu/)**
*   **Summary:** Questioning if anyone has set up minicpm-o 2.6 with vLLM and CPU.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    * vLLM support for CPU is still sketchy

**[What version of DeepSeek can I run on a 4090? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1in8q1v/what_version_of_deepseek_can_i_run_on_a_4090/)**
*   **Summary:** Inquiring as to what version of DeepSeek one can run on a 4090.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    * One can run the 685B version at Q1 with 0,66 tk/s or the 70B Q4 version at comfortable 2,8 tk/s.
    * The Deepseek r1 7B and 8B are running on a 4080 with ollama
    * 32b runs on 3090 no problem, so one should be able to run it too on a 4090.

**[CPU and Ram (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1in91j5/cpu_and_ram/)**
*   **Summary:** Question regarding the speed of the systems and the amount of memory bandwidth that systems need to run correctly.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    * The speed on system like this will depend on memory bandwidth, so one should inquire about how many channels it has, at least.

**[ChatGPT scammy bevaiour (Score: 0)](https://i.redd.it/wrqq3a2a8kie1.png)**
*   **Summary:** Users discuss whether to believe the information that ChatGPT gives and discuss how images are created through it.
*   **Emotion:** Positive.
*   **Top 3 Points of View:**
    * If you say “re create the image” or tell it exactly what you want it to do it will do it.
    * LLMs (non reasoning models) don't have a behind the scenes actions. The llm doesn't even do the drawing, it just shoots a prompt over to dall-e and he does a new art piece based on the description.
    * In general you need to always know the accurate technical info about what's possible. If you ask an AI to write code, it might produce code like it knows everything, but it might be broken code and you have to talk to it about it.

**[I unintentionally obtained information on Phi-5's release date (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1in8x65/i_unintentionally_obtained_information_on_phi5s/)**
*   **Summary:**  A user claims to have unintentionally obtained information about the release date of Phi-5.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    * Ask about information like this on an account where you did not provide personal information like state you live in.
    * Tell everyone the date.
    * The user lied.

**[How to Train an Anime Video Generation AI (explain it to me as if I’m a 5yo!) (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1in982e/how_to_train_an_anime_video_generation_ai_explain/)**
*   **Summary:** Inquiring as to how to train an anime video generation AI.
*   **Emotion:** Positive.
*   **Top 3 Points of View:**
    * This is not really LLM thing, but there are some local "video" generators that can be ran, where it generates an initial photo, and the "next frame".
