---
title: "Machine Learning Subreddit"
date: "2025-02-05"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "datasets"]
---

# Overall Ranking and Top Discussions
1.  [[D] What are current UNPOPULAR research topics in computer vision and language technology? 2025](https://www.reddit.com/r/MachineLearning/comments/1ihzy00/d_what_are_current_unpopular_research_topics_in/) (Score: 62)
    * This thread discusses unpopular research topics in computer vision and language technology.
2.  [R] Transformer-Squared: Self-adaptive LLMs](https://www.reddit.com/r/MachineLearning/comments/1iicsz0/r_transformersquared_selfadaptive_llms/) (Score: 20)
    * This thread discusses Transformer-Squared, a new method for adapting large language models (LLMs).
3.  [[D] How to Scale Your Model: A Systems View of LLMs on TPUs](https://www.reddit.com/r/MachineLearning/comments/1ihx3oq/d_how_to_scale_your_model_a_systems_view_of_llms/) (Score: 15)
    * This thread discusses scaling models, specifically a systems view of LLMs on TPUs.
4.  [Would researchers and data scientists actually use this? I'm building an AI tool to find datasets faster. [D]](https://www.reddit.com/r/MachineLearning/comments/1ii44ba/would_researchers_and_data_scientists_actually/) (Score: 14)
    * This thread discusses the utility of a new AI tool for finding datasets, with mixed opinions on its value.
5.  [[N] How Deepseek trained their R1 models, and how frontier LLMs are trained today.](https://www.reddit.com/r/MachineLearning/comments/1iii013/n_how_deepseek_trained_their_r1_models_and_how/) (Score: 13)
    * This thread discusses how Deepseek trained their R1 models, and how frontier LLMs are trained today.
6.  [[R] Illusory Safety: Redteaming DeepSeek R1 and the Strongest Fine-Tunable Models of OpenAI, Anthropic, and Google](https://www.reddit.com/r/MachineLearning/comments/1iif6qk/r_illusory_safety_redteaming_deepseek_r1_and_the/) (Score: 3)
    * This thread analyzes the "illusory safety" of redteaming DeepSeek R1 and other strong fine-tunable models.
7.  [[D]OCR Models to analyze complex invoices](https://www.reddit.com/r/MachineLearning/comments/1ii9spy/docr_models_to_analyze_complex_invoices/) (Score: 1)
    * This thread discusses OCR models for analyzing complex invoices.
8.  [Does specialization in niche ML subfield (e.g. medical) limit future opportunities in big tech? [D]](https://www.reddit.com/r/MachineLearning/comments/1iij25z/does_specialization_in_niche_ml_subfield_eg/) (Score: 1)
    * This thread discusses whether specializing in a niche ML subfield limits future opportunities in big tech.
9.  [[D] How hard is from scratch training on small datasets, really ?](https://www.reddit.com/r/MachineLearning/comments/1ii1smw/d_how_hard_is_from_scratch_training_on_small/) (Score: 0)
    * This thread discusses the difficulty of training from scratch on small datasets.

# Detailed Analysis by Thread
**[[D] What are current UNPOPULAR research topics in computer vision and language technology? 2025 (Score: 62)](https://www.reddit.com/r/MachineLearning/comments/1ihzy00/d_what_are_current_unpopular_research_topics_in/)**
*  **Summary:** This thread is a discussion about research topics in computer vision and language technology that are currently not receiving much attention.
*  **Emotion:** The overall emotional tone is neutral, with users sharing information and opinions in a calm and informative manner.
*  **Top 3 Points of View:**
    *   Image pyramids, especially a specific paper, deserve more attention.
    *   Reinforcement Learning (RL) is fundamental and will be used in startups focusing on real-world engineering/logistics problems.
    *   Anything related to safety and trustworthiness of AI is under-represented.

**[[R] Transformer-Squared: Self-adaptive LLMs (Score: 20)](https://www.reddit.com/r/MachineLearning/comments/1iicsz0/r_transformersquared_selfadaptive_llms/)**
*  **Summary:** This thread discusses a new paper introducing "Transformer-Squared," a self-adaptive LLM that uses Singular Value Fine-Tuning (SVF) to optimize performance with reduced computational costs.
*  **Emotion:** The thread expresses a positive and interested tone, with some users being cautiously optimistic.
*  **Top 3 Points of View:**
    *   Transformer-Squared introduces a significant innovation in adapting LLMs through SVF.
    *   Targeting fine-tuning to specific experts could solve catastrophic forgetting.
    *   Some users question whether the benchmark improvements are significant enough.

**[[D] How to Scale Your Model: A Systems View of LLMs on TPUs (Score: 15)](https://www.reddit.com/r/MachineLearning/comments/1ihx3oq/d_how_to_scale_your_model_a_systems_view_of_llms/)**
*  **Summary:** This thread is about scaling models, with a focus on a systems view of LLMs on TPUs.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   A user asked for a PyTorch version of the information.

**[Would researchers and data scientists actually use this? I'm building an AI tool to find datasets faster. [D] (Score: 14)](https://www.reddit.com/r/MachineLearning/comments/1ii44ba/would_researchers_and_data_scientists_actually/)**
*  **Summary:** The thread discusses whether researchers and data scientists would use a new AI tool designed to find datasets faster. Opinions are mixed, with some questioning its necessity due to existing tools like Google Dataset Search, while others see potential value, especially for specific dataset needs and features like dataset reputation scores.
*  **Emotion:** The emotional tone is mixed, ranging from positive to negative. There are expressions of enthusiasm and skepticism.
*  **Top 3 Points of View:**
    *   Some believe the tool is unnecessary because existing solutions like Google Dataset Search are sufficient.
    *   Others think it's a useful idea, especially if it includes features like dataset reputation scores and handles licensing/attribution properly.
    *   Some point out that the need to crawl dataset websites will be a majority of the work.

**[[N] How Deepseek trained their R1 models, and how frontier LLMs are trained today. (Score: 13)](https://www.reddit.com/r/MachineLearning/comments/1iii013/n_how_deepseek_trained_their_r1_models_and_how/)**
*  **Summary:** This thread revolves around a discussion on how Deepseek trained their R1 models and the general training methodologies used for frontier LLMs today.
*  **Emotion:** The overall emotional tone is inquisitive and neutral, as users seek to understand the technical details.
*  **Top 3 Points of View:**
    *   Inquiry about the scale of experts and parameters in the model architecture.
    *   Request for explanation regarding the auxiliary loss and its relation to solving MoE issues.

**[[R] Illusory Safety: Redteaming DeepSeek R1 and the Strongest Fine-Tunable Models of OpenAI, Anthropic, and Google (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1iif6qk/r_illusory_safety_redteaming_deepseek_r1_and_the/)**
*  **Summary:** The thread discusses a paper focused on the "illusory safety" of AI models, specifically those from DeepSeek, OpenAI, Anthropic, and Google, highlighting that safety measures added through fine-tuning can be removed by further fine-tuning.
*  **Emotion:** The emotional tone is negative, with concerns about AI safety.
*  **Top 3 Points of View:**
    *   AI safety is doomed to fail, as users are not interested in having their assistant refuse their requests.
    *   Safety training can be removed by fine-tuning.

**[[D]OCR Models to analyze complex invoices (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1ii9spy/docr_models_to_analyze_complex_invoices/)**
*  **Summary:** This thread discusses the use of OCR models for analyzing complex invoices.
*  **Emotion:** The thread has a positive tone.
*  **Top 3 Points of View:**
    *   Suggesting Azure Document Intelligence as an option for labeling samples and retrieving fields.
    *   Suggesting the use of preprocessing techniques to improve OCR performance, as well as fine-tuning pretrained models with correctly labeled data.
    *   Suggesting converting PDFs to text and using an LLM to analyze it.

**[Does specialization in niche ML subfield (e.g. medical) limit future opportunities in big tech? [D] (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1iij25z/does_specialization_in_niche_ml_subfield_eg/)**
*  **Summary:** The thread discusses whether specializing in a niche ML subfield like medicine limits future opportunities in big tech.
*  **Emotion:** The emotional tone is positive.
*  **Top 3 Points of View:**
    *   The skills learned in a niche field are transferrable.
    *   It's one's responsibility to find opportunities aligned with their goals.
    *   No, specialization does not limit opportunities.

**[[D] How hard is from scratch training on small datasets, really ? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ii1smw/d_how_hard_is_from_scratch_training_on_small/)**
*  **Summary:** This thread discusses the challenges of training models from scratch on small datasets.
*  **Emotion:** The thread has a neutral tone.
*  **Top 3 Points of View:**
    *   Training from scratch on small datasets makes each architecture tweak matter more.
    *   Architectural choices that seem arbitrary might have theoretical motivations.
    *   2014 architectures were less stable than modern ones, which makes training harder.
