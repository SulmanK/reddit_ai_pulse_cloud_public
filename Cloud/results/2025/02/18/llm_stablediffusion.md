---
title: "Stable Diffusion Subreddit"
date: "2025-02-18"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [Don't Look Back / UltraReal Fine-Tune V4](https://v.redd.it/nhygy45ppxje1) (Score: 46)
    * Discussing a new image created with Pinokio, Forge, UltraReal Fine-Tune and Realistic Amplifier Lora with music created by Udio and motion (Kling).
2.  [So I tested regional prompting on Krita today, feel like I just levelled up on image gen. Also impressed with the results and Ai varied interpretation of 'jojo pose' and 'energetic pose' prompt. NoobAIxl, with BAstyle lora, no artist put into prompts](https://www.reddit.com/gallery/1isjpy2) (Score: 24)
    *  Sharing results of using regional prompting in Krita with NoobAIxl and BAstyle lora, focusing on AI's interpretation of poses.
3.  [Model-guidance (MG), a replacement to CFG?](https://www.reddit.com/r/StableDiffusion/comments/1isj9a7/modelguidance_mg_a_replacement_to_cfg/) (Score: 12)
    * Exploring Model-guidance (MG) as a potential replacement for CFG (classifier-free guidance) in stable diffusion.
4.  [Knight vs Rogue: Ambush](https://i.redd.it/bdwkidq64yje1.png) (Score: 8)
    *  Showcasing an image of a knight ambushed by a rogue, created using txt2image with regional prompter and adetailer.
5.  [LORA Magic? Comparing Flux Base vs. 4 LORAs](https://i.redd.it/62720cvbmxje1.png) (Score: 4)
    *  Comparing the results of image generation using Flux base model versus using it with four different LoRAs.
6.  [How do you create quick videos 2-10 secs from an image?](https://www.reddit.com/r/StableDiffusion/comments/1isjqvv/how_do_you_create_quick_videos_210_secs_from_an/) (Score: 4)
    *  Asking for methods to create short videos (2-10 seconds) from a single image, with a suggestion of using Klingai.com.
7.  [Training LORA with Flux : what's the matter with "Repeat trains per image" parameter](https://www.reddit.com/r/StableDiffusion/comments/1isiosf/training_lora_with_flux_whats_the_matter_with/) (Score: 3)
    *  Questioning the effects of the "Repeat trains per image" parameter when training a LoRA with Flux.
8.  [Is there any way to avoid horizontal lines in your images with FLUX in ComfyUI when you use resolutions above 1024x1024? I used to go with 1200x1600 on Tensor and Civitai, but I can't get it to work in ComfyUI.](https://i.redd.it/b1fv8cgzdxje1.png) (Score: 1)
    *  Seeking solutions to avoid horizontal lines when generating images with FLUX in ComfyUI at resolutions higher than 1024x1024.
9.  [Any Tips to Avoid Text Distortion When Training LoRA?](https://www.reddit.com/r/StableDiffusion/comments/1ishg8t/any_tips_to_avoid_text_distortion_when_training/) (Score: 1)
    * Asking for tips to avoid text distortion when training LoRAs.
10. [Yolo use in SwarmUI](https://www.reddit.com/r/StableDiffusion/comments/1isifme/yolo_use_in_swarmui/) (Score: 1)
    * Inquiring about the use of Yolo in SwarmUI for automatic segmentation and refining.
11. [Help with AI learning models](https://www.reddit.com/r/StableDiffusion/comments/1ish4jx/help_with_ai_learning_models/) (Score: 0)
    * Asking for help regarding AI learning models.
12. [I have a unique image set of 10,000 grocery store images from an old art project. I’d love to train a model on them but I’m not sure where to start](https://www.reddit.com/r/StableDiffusion/comments/1isiazv/i_have_a_unique_image_set_of_10000_grocery_store/) (Score: 0)
    * Seeking advice on how to train a model using a unique dataset of 10,000 grocery store images.
13. [Official Hunyuan FP8 .pt model makes noise](https://www.reddit.com/r/StableDiffusion/comments/1isibcc/official_hunyuan_fp8_pt_model_makes_noise/) (Score: 0)
    *  Discussion about the official Hunyuan FP8 .pt model.
14. [What will replace Comfyui?](https://www.reddit.com/r/StableDiffusion/comments/1isj7vs/what_will_replace_comfyui/) (Score: 0)
    * Speculating on what might replace ComfyUI in the future.
15. [What on earth am I missing?](https://www.reddit.com/r/StableDiffusion/comments/1isjjy0/what_on_earth_am_i_missing/) (Score: 0)
    *  Seeking advice on why image generation results aren't matching expectations, despite using the same settings as others.

# Detailed Analysis by Thread
**[Don't Look Back / UltraReal Fine-Tune V4 (Score: 46)](https://v.redd.it/nhygy45ppxje1)**
*  **Summary:** The thread is about a user showcasing a video created using Stable Diffusion with specific models and tools like UltraReal Fine-Tune, Realistic Amplifier Lora, Udio, and Kling. Some viewers perceive it as an advertisement.
*  **Emotion:** The overall emotional tone is Neutral, with sentiment scores around 0.6, indicating a factual presentation and mixed reactions.
*  **Top 3 Points of View:**
    *   The user is promoting their creation using specific AI tools.
    *   Some users perceive the post as an advertisement for the tools used.
    *   Some users express disinterest due to the content not being locally relevant.

**[So I tested regional prompting on Krita today, feel like I just levelled up on image gen. Also impressed with the results and Ai varied interpretation of 'jojo pose' and 'energetic pose' prompt. NoobAIxl, with BAstyle lora, no artist put into prompts (Score: 24)](https://www.reddit.com/gallery/1isjpy2)**
*  **Summary:** The thread showcases the results of using regional prompting in Krita for image generation, highlighting the AI's interpretation of poses and the tools used (NoobAIxl, BAstyle lora).
*  **Emotion:** The emotional tone is generally Positive, with comments expressing appreciation for the compositions and poses.
*  **Top 3 Points of View:**
    *   The user is excited about the improved results from using regional prompting.
    *   Other users are impressed with the generated images.
    *   Some users are looking for resources or guides on how to replicate the results, asking for Lora details.

**[Model-guidance (MG), a replacement to CFG? (Score: 12)](https://www.reddit.com/r/StableDiffusion/comments/1isj9a7/modelguidance_mg_a_replacement_to_cfg/)**
*   **Summary:** The thread discusses "Model-guidance" (MG) as a potential alternative to "Classifier-Free Guidance" (CFG) in image generation.
*   **Emotion:** Neutral. The discussion is informational and technical.
*   **Top 2 Points of View:**
    *   MG is being explored as a way to improve or replace CFG.
    *   MG is reminiscent of "Warp Drive" in the reForge-AutomaticCFG extension, which speeds up generation.

**[Knight vs Rogue: Ambush (Score: 8)](https://i.redd.it/bdwkidq64yje1.png)**
*   **Summary:** A user shares an image of a knight being ambushed by a rogue, generated using txt2image with regional prompter and adetailer, and provides the prompt details and CivitAI link for others to use.
*   **Emotion:** Neutral, with a high sentiment score, reflecting the detailed and informative nature of the post.
*   **Top 1 Points of View:**
    *   The user shares their image generation process and provides resources for others to replicate it.

**[LORA Magic? Comparing Flux Base vs. 4 LORAs (Score: 4)](https://i.redd.it/62720cvbmxje1.png)**
*  **Summary:** The thread compares image generation results using the Flux base model versus using it with four different LoRAs, with users discussing the differences and suggesting alternative settings and workflows.
*  **Emotion:** The overall emotional tone is Neutral, with varying opinions on the effectiveness of the LORAs.
*  **Top 3 Points of View:**
    *   Some users find the differences between the base model and the LoRA enhanced images subtle or unappealing.
    *   Others suggest that better settings and prompts, rather than LoRAs, can significantly improve the results.
    *   Some users provide specific LoRA combinations and settings for others to try.

**[How do you create quick videos 2-10 secs from an image? (Score: 4)](https://www.reddit.com/r/StableDiffusion/comments/1isjqvv/how_do_you_create_quick_videos_210_secs_from_an/)**
*  **Summary:**  The thread asks for recommendations on how to create short videos from a single image.
*  **Emotion:**  Negative, due to the suggestion of a tool considered to produce poor results.
*  **Top 1 Points of View:**
    *   The user is seeking advice on creating short videos from images.

**[Training LORA with Flux : what's the matter with "Repeat trains per image" parameter (Score: 3)](https://www.reddit.com/r/StableDiffusion/comments/1isiosf/training_lora_with_flux_whats_the_matter_with/)**
*  **Summary:** The thread questions the impact of the "Repeat trains per image" parameter when training LoRAs using Flux.
*  **Emotion:** Neutral. The discussion is technical and focused on training parameters.
*  **Top 2 Points of View:**
    *   Epochs are only effective for optimizers like Prodigy and do not matter with AdamW, where you can train 1000+ steps, single epoch.
    *   The model needs to learn the same image at different noise levels, which is only possible by multiple epochs.

**[Is there any way to avoid horizontal lines in your images with FLUX in ComfyUI when you use resolutions above 1024x1024? I used to go with 1200x1600 on Tensor and Civitai, but I can't get it to work in ComfyUI. (Score: 1)](https://i.redd.it/b1fv8cgzdxje1.png)**
*  **Summary:** The thread seeks advice on resolving horizontal lines in images generated with FLUX in ComfyUI at high resolutions.
*  **Emotion:** Neutral. The discussion is technical and focused on troubleshooting image generation issues.
*  **Top 3 Points of View:**
    *   Using a different checkpoint, particularly a dedistilled version of flux.
    *   Tiling the image.
    *   Using resolutions divisible by 16.

**[Any Tips to Avoid Text Distortion When Training LoRA? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1ishg8t/any_tips_to_avoid_text_distortion_when_training/)**
*  **Summary:** A user seeks advice on how to prevent text distortion when training LoRAs.
*  **Emotion:** Neutral. The response suggests a workaround rather than a direct solution within the AI generation process.
*  **Top 1 Points of View:**
    *   The diffusion process inherently distorts text, so post-processing in Photoshop or similar software is recommended.

**[Yolo use in SwarmUI (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1isifme/yolo_use_in_swarmui/)**
*  **Summary:** This thread discusses the use of Yolo in SwarmUI for automatic segmentation and refining of images.
*  **Emotion:** Positive, as the commenter reports successful implementation.
*  **Top 1 Points of View:**
    *   Yolo can be successfully implemented in SwarmUI for automatic segmentation and refining, particularly for faces.

**[Help with AI learning models (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1ish4jx/help_with_ai_learning_models/)**
*  **Summary:** A user is asking for help with AI learning models.
*  **Emotion:** Neutral, providing information about faceswap solutions and LoRA training.
*  **Top 1 Points of View:**
    *   Suggests faceswap solutions via apps or Stable Diffusion tools like reActor and LoRA training on CivitAI.

**[I have a unique image set of 10,000 grocery store images from an old art project. I’d love to train a model on them but I’m not sure where to start (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1isiazv/i_have_a_unique_image_set_of_10000_grocery_store/)**
*  **Summary:** A user with a dataset of grocery store images is seeking advice on how to train a model using them.
*  **Emotion:** Neutral. The responses offer technical advice on data preparation and training methods.
*  **Top 2 Points of View:**
    *   Thoroughly captioning the images is necessary.
    *   Fine-tuning the entire model with Dreambooth, rather than a LoRA, is recommended.

**[Official Hunyuan FP8 .pt model makes noise (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1isibcc/official_hunyuan_fp8_pt_model_makes_noise/)**
*  **Summary:**  A thread discussing the Hunyuan FP8 model.
*  **Emotion:**  Neutral.
*  **Top 1 Points of View:**
    *   No viewpoints were found.

**[What will replace Comfyui? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1isj7vs/what_will_replace_comfyui/)**
*  **Summary:**  The thread discusses possible replacements for ComfyUI.
*  **Emotion:**  Neutral, with a mix of opinions on ComfyUI's strengths and weaknesses.
*  **Top 3 Points of View:**
    *   A new UI is being developed, suggesting ComfyUI might be on the way out.
    *   ComfyUI offers control and flexibility.
    *   A more "plug and play" interface is required.

**[What on earth am I missing? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1isjjy0/what_on_earth_am_i_missing/)**
*  **Summary:** A user is seeking advice on why their image generation results don't match expectations, despite using the same settings.
*  **Emotion:** Neutral, with a mix of helpful suggestions and troubleshooting tips.
*  **Top 3 Points of View:**
    *   Hardware limitations can affect the quality of results.
    *   Forge is an better alternative to AUTO1111 and switch to it.
    *   Manually adding LoRA’s and their weights to the end of your prompt is important.
