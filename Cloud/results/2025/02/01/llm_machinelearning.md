---
title: "Machine Learning Subreddit"
date: "2025-02-01"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "research"]
---

# Overall Ranking and Top Discussions
1.  [[R] Molecular Fingerprints Are Strong Models for Peptide Function Prediction](https://www.reddit.com/r/MachineLearning/comments/1if2wlc/r_molecular_fingerprints_are_strong_models_for/) (Score: 42)
    *   A discussion about the effectiveness of molecular fingerprints in predicting peptide function, including questions about the intuition behind this, code availability and a request for statistical testing.
2.  [[2412.20302] EXAdam: The Power of Adaptive Cross-Moments](https://arxiv.org/abs/2412.20302) (Score: 16)
    *   A discussion about the EXAdam algorithm, with questions about its generalization beyond CIFAR and requests for PyTorch implementations.
3.  [[D] Sentence classification and Custom Entity Recognition for Information extraction - Does This Approach Work?](https://www.reddit.com/r/MachineLearning/comments/1if4jn8/d_sentence_classification_and_custom_entity/) (Score: 7)
    *   Users are discussing approaches to sentence classification and custom entity recognition for information extraction, including suggestions of tools like gliNER and beautiful soup.
4.  [[D]What is the best speech recognition model now?](https://www.reddit.com/r/MachineLearning/comments/1ifbd48/dwhat_is_the_best_speech_recognition_model_now/) (Score: 7)
    *  Users are asking for and giving recommendations about different speech recognition models such as vosk and hugging face moonshine.
5.  [ [News] Tulu 3 model performing better than 4o and Deepseek?](https://www.reddit.com/r/MachineLearning/comments/1ifetmm/news_tulu_3_model_performing_better_than_4o_and/) (Score: 6)
    *  A discussion about the Tulu 3 model and its comparison with Deepseek and Llama models.
6.  [[Discussion] Reason for Activation Steering over finetuning?](https://www.reddit.com/r/MachineLearning/comments/1ieygxx/discussion_reason_for_activation_steering_over/) (Score: 5)
    *  A discussion on the benefits of activation steering vs fine-tuning for LLMs, considering aspects like bias reduction, catastrophic forgetting and resource constraints.
7.  [[D] Why not use DeepSeek to reward DeepSeek?](https://wilsoniumite.com/2025/01/30/why-not-use-deepseek-to-reward-deepseek/) (Score: 0)
    *  A short comment on the idea of using DeepSeek to reward DeepSeek.
8.  [[D] The key to ASI according to me](https://www.reddit.com/r/MachineLearning/comments/1ifap01/d_the_key_to_asi_according_to_me/) (Score: 0)
     * A highly critical discussion about a post claiming a new insight to achieve ASI, including questioning the novelty of the idea and recommending alternative subreddits.

# Detailed Analysis by Thread
**[[R] Molecular Fingerprints Are Strong Models for Peptide Function Prediction (Score: 42)](https://www.reddit.com/r/MachineLearning/comments/1if2wlc/r_molecular_fingerprints_are_strong_models_for/)**
*   **Summary:** The thread discusses a research paper on using molecular fingerprints for peptide function prediction. Users are asking questions about the methodology, including the meaning of molecular fingerprint, intuition behind its success and are giving suggestions to improve the paper with added description of the datasets used and a statistical significance test on the smaller datasets.
*   **Emotion:** The emotional tone is mostly positive and neutral, with users expressing interest and appreciation for the work, alongside some analytical curiosity. There are many neutral comments, suggesting an inquisitive mood.
*   **Top 3 Points of View:**
    *   Molecular fingerprints are an effective tool for peptide function prediction.
    *   The paper would benefit from a more detailed description of the datasets.
    *   Statistical significance tests are needed, especially for smaller datasets.

**[[2412.20302] EXAdam: The Power of Adaptive Cross-Moments (Score: 16)](https://arxiv.org/abs/2412.20302)**
*   **Summary:** The thread discusses the EXAdam algorithm, with users inquiring about its generalizability beyond CIFAR datasets and asking for a PyTorch implementation.
*   **Emotion:** The emotional tone is neutral, with users expressing an interest in the algorithm but with skepticism and curiosity.
*   **Top 3 Points of View:**
    *   The EXAdam algorithm seems interesting.
    *   More tests are needed to check generalizability beyond CIFAR.
    *   A PyTorch implementation would be helpful.

**[[D] Sentence classification and Custom Entity Recognition for Information extraction - Does This Approach Work? (Score: 7)](https://www.reddit.com/r/MachineLearning/comments/1if4jn8/d_sentence_classification_and_custom_entity/)**
*   **Summary:** The thread discusses a project on sentence classification and custom entity recognition, users provide helpful suggestions such as the use of gliNER and beautiful soup and one user is interested in knowing if the project will be open sourced.
*   **Emotion:** The emotional tone is positive and neutral, with some users being encouraging and offering suggestions.
*   **Top 3 Points of View:**
    *   The project is interesting and relevant.
    *   gliNER is a good tool for entity recognition.
    *   Beautiful soup can be helpful when dealing with HTML.

**[[D]What is the best speech recognition model now? (Score: 7)](https://www.reddit.com/r/MachineLearning/comments/1ifbd48/dwhat_is_the_best_speech_recognition_model_now/)**
*  **Summary:** The thread is seeking recommendations about different speech recognition models. Users suggested models such as vosk and hugging face moonshine.
*   **Emotion:** The emotional tone is neutral, with users giving direct and helpful answers to the question.
*   **Top 3 Points of View:**
    *   vosk is a good, older option.
    *  Hugging face moonshine is worth checking out.
    *  Nvidia has a speech recognition model as well.

**[[News] Tulu 3 model performing better than 4o and Deepseek? (Score: 6)](https://www.reddit.com/r/MachineLearning/comments/1ifetmm/news_tulu_3_model_performing_better_than_4o_and/)**
*   **Summary:** The thread discusses the performance of the Tulu 3 model compared to models like 4o and Deepseek. Users point out the specific Deepseek model being compared and the need for more accurate comparison to Llama 3.1.
*   **Emotion:** The emotional tone is neutral to positive, with users being curious and analytical.
*   **Top 3 Points of View:**
    *   The comparison should be with Deepseek V3 not R1.
    *   The model is something worth checking out.
    *   There is not much of a difference with Llama 3.1 405B.

**[[Discussion] Reason for Activation Steering over finetuning? (Score: 5)](https://www.reddit.com/r/MachineLearning/comments/1ieygxx/discussion_reason_for_activation_steering_over/)**
*   **Summary:** The thread discusses the advantages of activation steering over fine-tuning, with users citing faster adjustment capabilities, targeted changes, and preventing catastrophic forgetting as key reasons.
*   **Emotion:** The overall emotional tone is neutral to slightly negative, with users expressing caution about fine-tuning and highlighting the benefits of activation steering.
*   **Top 3 Points of View:**
    *   Activation steering allows for faster, real-time adjustments compared to fine-tuning.
    *   Fine-tuning can lead to catastrophic forgetting and is more resource intensive.
    *   SAEs are a new method for steering models, less prone to prompt specifics.

**[[D] Why not use DeepSeek to reward DeepSeek? (Score: 0)](https://wilsoniumite.com/2025/01/30/why-not-use-deepseek-to-reward-deepseek/)**
*   **Summary:** This thread contains a single, humorous comment related to the concept of using DeepSeek to reward DeepSeek.
*   **Emotion:** The emotional tone is neutral, with the comment being a lighthearted reference.
*   **Top 3 Points of View:**
    *   The post is a humorous reference to an existing meme.

**[[D] The key to ASI according to me (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ifap01/d_the_key_to_asi_according_to_me/)**
*   **Summary:** This thread features a post claiming an insight into achieving Artificial Superintelligence (ASI). The majority of the comments are very dismissive, stating that the ideas are not new, lacking originality, and are highly doubtable.
*  **Emotion:** The overall tone is negative with users being skeptical and critical.
*   **Top 3 Points of View:**
    *   The idea presented is not original and already exists in other papers.
    *   The claim is overly simplistic.
    *   The discussion is more suited for the singularity subreddit.
