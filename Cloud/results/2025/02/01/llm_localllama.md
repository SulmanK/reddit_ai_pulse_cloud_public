---
title: "LocalLLaMA Subreddit"
date: "2025-02-01"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["AI", "LLM", "Local Models"]
---

# Overall Ranking and Top Discussions
1.  [[SmolVLM fully open source](https://x.com/andimarafioti/status/1885341684134978035)](https://x.com/andimarafioti/status/1885341684134978035) (Score: 49)
    *   The discussion revolves around the release of SmolVLM, a fully open-source model, with some comments expressing enthusiasm and others commenting on the model's small size.
2.  [What went into training DeepSeek-R1? A technical summary of the training of v3 and R1](https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1) (Score: 5)
    *   This thread discusses the training process of DeepSeek-R1, highlighting the budget and challenges involved in training MoE models.
3.  [what do you use AI for?](https://www.reddit.com/r/LocalLLaMA/comments/1ifdd9r/what_do_you_use_ai_for/) (Score: 4)
    *   This thread is a discussion about various ways people utilize AI, including coding, writing, data analysis, and even personal projects.
4. [Cline - Any usable DeepSeek-R1 finetunes?](https://www.reddit.com/r/LocalLLaMA/comments/1ifes7j/cline_any_usable_deepseekr1_finetunes/) (Score: 2)
    * This thread discusses the usability of DeepSeek-R1 finetunes for tool use.
5.  [Let's build this](https://www.reddit.com/r/LocalLLaMA/comments/1ifdwv3/lets_build_this/) (Score: 2)
    *   This thread is a collection of short comments, including one encouraging to build friendships and some interest in using for STEM.
6.  [Where can I buy cheap full r1](https://www.reddit.com/r/LocalLLaMA/comments/1iff2me/where_can_i_buy_cheap_full_r1/) (Score: 2)
     * The conversation revolves around where to find affordable options for running DeepSeek-R1 models.
7.  [How to get the DS-R1 distill llama and qwen models to properly roleplay?](https://www.reddit.com/r/LocalLLaMA/comments/1ifg1ty/how_to_get_the_dsr1_distill_llama_and_qwen_models/) (Score: 1)
    *   This thread is about the challenges of using certain AI models for roleplaying and hoping that roleplay models will get better with chain of thought.
8.  [Fine Tuning LLM on AMD GPU](https://www.reddit.com/r/LocalLLaMA/comments/1iff26y/fine_tuning_llm_on_amd_gpu/) (Score: 1)
    *  The thread is about fine tuning LLMs on AMD GPUs.
9.  [Anyone managed to run deepseek-r1-32b with chat-ui?](https://www.reddit.com/r/LocalLLaMA/comments/1ifdfmc/anyone_managed_to_run_deepseekr132b_with_chatui/) (Score: 1)
    *  This thread seeks advice on running deepseek-r1-32b with a chat UI, with a recommendation for openwebui.
10. [Where I can find an API that serves hexgrad/Kokoro-82M a TTS model and pay per token ?](https://www.reddit.com/r/LocalLLaMA/comments/1ifdsxl/where_i_can_find_an_api_that_serves/) (Score: 1)
    *  This thread asks for recommendations for an API that serves a specific TTS model, with a suggestion for a free online tool.
11. [Out of the loop with autonomous agents](https://www.reddit.com/r/LocalLLaMA/comments/1ifdlo3/out_of_the_loop_with_autonomous_agents/) (Score: 0)
    *   This thread discusses how to use a browser agent from scripts instead of a user interface.
12.  [We need a different kind of AGI](https://www.reddit.com/r/LocalLLaMA/comments/1ifdrje/we_need_a_different_kind_of_agi/) (Score: 0)
    *  This thread is about what should be the new approach to create a different kind of AGI.

# Detailed Analysis by Thread
**[ SmolVLM fully open source (Score: 49)](https://x.com/andimarafioti/status/1885341684134978035)**
*  **Summary:** The thread discusses the release of SmolVLM, a fully open-source model. Some users express excitement and thank the HF team, while others note its small size and raise questions about the open-source practices for larger models.
*  **Emotion:** The overall emotional tone is positive, with some neutral observations. There's a mix of excitement and curiosity about the new model and its implications.
*  **Top 3 Points of View:**
    *   Enthusiasm for the open-source nature of SmolVLM.
    *   Commentary on the small size and low investment compared to larger models.
    *   Discussion on what constitutes true open source, with a focus on OSI compliance.

**[ What went into training DeepSeek-R1? A technical summary of the training of v3 and R1 (Score: 5)](https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1)**
*   **Summary:** This thread summarizes the training details of DeepSeek-R1, including the training budget and the challenges involved in training MoE models, and refers to a linked post providing even more detail on V3.
*  **Emotion:** The emotional tone is mainly positive, with a focus on the informative aspects of the technical summary.
*  **Top 3 Points of View:**
    *  The training budget of $5M for V3.
    *  The difficulty of training Mixture of Experts models.
    *  The computational cost of going from V3 to R1 is estimated to be $1M.

**[ what do you use AI for? (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1ifdd9r/what_do_you_use_ai_for/)**
*   **Summary:** This thread explores various applications of AI, from coding and writing to strategic planning and personal projects. Users share their specific use cases for local AI models and reasons for choosing this approach.
*  **Emotion:** The emotional tone is mixed, ranging from positive to neutral. There is excitement about the possibilities of AI, balanced with pragmatic discussion about its limitations.
*  **Top 3 Points of View:**
    * AI is used for job related automation and document review.
    * The main uses are coding, content creation, planning and image generation.
    * Local models are preferred for privacy and control over code and prompts.

**[ Cline - Any usable DeepSeek-R1 finetunes? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1ifes7j/cline_any_usable_deepseekr1_finetunes/)**
*   **Summary:**  This thread discusses the potential for fine-tuning DeepSeek-R1 for improved tool usage, with one comment expressing skepticism.
*  **Emotion:** The emotional tone is neutral, with a hint of skepticism.
*  **Top 3 Points of View:**
    *  Doubts about finetunes improving tool use.

**[ Let's build this (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1ifdwv3/lets_build_this/)**
*   **Summary:** This thread contains a very short conversation with users expressing interest in building friendships and explore for STEM.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *  Encouragement to build friendships.
    *  Interest for STEM.
    *  Comment about the post being from "Xitter".

**[ Where can I buy cheap full r1 (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1iff2me/where_can_i_buy_cheap_full_r1/)**
*   **Summary:**  Users are looking for affordable ways to use DeepSeek-R1, with suggestions including Kluster, OpenRouter, Azure, Fireworks, and CentML.
*  **Emotion:** The overall tone is neutral.
*  **Top 3 Points of View:**
    *  Kluster.ai has it for $2/mt but it is kinda slow.
    *  Running full R1 is either slow or expensive and it is recommended to use chatgpt instead.
    *  Openrouter and Azure offer it for free.

**[ How to get the DS-R1 distill llama and qwen models to properly roleplay? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1ifg1ty/how_to_get_the_dsr1_distill_llama_and_qwen_models/)**
*   **Summary:** This thread discusses the difficulty of using certain AI models for roleplaying scenarios, noting that current models are not trained for proper RP.
*  **Emotion:** The emotional tone is generally positive with an undertone of frustration.
*  **Top 3 Points of View:**
    * The lack of proper roleplay capabilities for the current models.
    *  Hope for improvement with chain of thought reasoning.

**[ Fine Tuning LLM on AMD GPU (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1iff26y/fine_tuning_llm_on_amd_gpu/)**
*  **Summary:** This thread discusses the possibility of fine-tuning LLMs on AMD GPUs, and questions the size of the dataset and base model used.
*  **Emotion:** The emotional tone is neutral, with some curiosity.
*  **Top 3 Points of View:**
    * The user had some success fine-tuning with an AMD GPU.
    * The process took 244 steps and 10 minutes while consuming 16.6GB of VRAM.

**[ Anyone managed to run deepseek-r1-32b with chat-ui? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1ifdfmc/anyone_managed_to_run_deepseekr132b_with_chatui/)**
*   **Summary:** This thread is asking for recommendations for a way to run deepseek-r1-32b with chat UI.
*  **Emotion:** The emotional tone is positive, offering a simple solution.
*  **Top 3 Points of View:**
    * The recommendation of using openwebui.

**[ Where I can find an API that serves hexgrad/Kokoro-82M a TTS model and pay per token ? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1ifdsxl/where_i_can_find_an_api_that_serves/)**
*  **Summary:** This thread asks for an API that serves hexgrad/Kokoro-82M a TTS model and pays per token.
*  **Emotion:** The emotional tone is neutral
*  **Top 3 Points of View:**
    *  A free online tool is recommended.

**[ Out of the loop with autonomous agents (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1ifdlo3/out_of_the_loop_with_autonomous_agents/)**
*  **Summary:** This thread discusses how to call a browser-use agent from a script.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    * The use of a browser agent from script is recommended.

**[ We need a different kind of AGI (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1ifdrje/we_need_a_different_kind_of_agi/)**
*   **Summary:** This thread proposes a different approach to achieving AGI, with responses ranging from skepticism and suggestions to start with simple models, and remarks that this approach has already failed.
*  **Emotion:** The emotional tone is mixed, from positive to neutral and skeptical.
*  **Top 3 Points of View:**
     *  The poster proposes an approach to achieve a different kind of AGI.
     *  Suggestions that the poster should start designing and training models to understand the basics before proposing AGI approaches.
     * Remarks that this approach had already failed in the past.
