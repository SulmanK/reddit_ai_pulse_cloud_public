---
title: "Machine Learning Subreddit"
date: "2025-02-12"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "SSMs"]
---

# Overall Ranking and Top Discussions
1.  [[D] What happened to SSMs and linear attentions?](https://www.reddit.com/r/MachineLearning/comments/1in9y30/d_what_happened_to_ssms_and_linear_attentions/) (Score: 70)
    *   This thread discusses the current relevance and usage of State Space Models (SSMs) and linear attention mechanisms in machine learning, particularly in large language models (LLMs).
2.  [Research] Novel Clustering Metric - The Jaccard-Concentration Index](https://www.reddit.com/r/MachineLearning/comments/1indsvi/research_novel_clustering_metric_the/) (Score: 15)
    *   This thread introduces a novel clustering metric called the Jaccard-Concentration Index.
3.  [[R] New Paper: Can frontier models self-explore and discover their own capabilities in an open-ended way?](https://www.reddit.com/r/MachineLearning/comments/1inv1gy/r_new_paper_can_frontier_models_selfexplore_and/) (Score: 8)
    *   This thread discusses a new research paper about frontier models exploring and discovering their capabilities in an open-ended way.
4.  Machine psychology?[R]](https://www.reddit.com/r/MachineLearning/comments/1inh9cy/machine_psychologyr/) (Score: 6)
    *   This thread discusses the intersection of machine learning and psychology.
5.  [[D] Causal inference in irregular time series data?](https://www.reddit.com/r/MachineLearning/comments/1inpgkw/d_causal_inference_in_irregular_time_series_data/) (Score: 3)
    *   This thread is about causal inference methods applicable to irregular time series data.
6.  [[D] Challenges with Real-time Inference at Scale](https://www.reddit.com/r/MachineLearning/comments/1inl6pv/d_challenges_with_realtime_inference_at_scale/) (Score: 2)
    *   This thread focuses on the challenges and solutions for performing real-time inference at scale with machine learning models.
7.  Structured data parsing [D]](https://www.reddit.com/r/MachineLearning/comments/1ins93r/structured_data_parsing_d/) (Score: 2)
    *   This thread discusses methods for parsing structured data, particularly tabular data extraction from PDFs.
8.  [[D] Fine-tuning to replace complicated activations with simpler ones](https://www.reddit.com/r/MachineLearning/comments/1inyd67/d_finetuning_to_replace_complicated_activations/) (Score: 2)
    *   This thread explores the possibility of fine-tuning models to replace complex activation functions with simpler ones.
9.  [[D] Where are ICLR 2025 submissions???](https://www.reddit.com/r/MachineLearning/comments/1inegee/d_where_are_iclr_2025_submissions/) (Score: 0)
    *   This thread discusses the unusual availability of ICLR 2025 submissions, with links to resources for finding them.
10. [[R] I Tested DeepSeek's Censorship Filters—Here's What I Discovered](https://www.reddit.com/r/MachineLearning/comments/1ins943/r_i_tested_deepseeks_censorship_filtersheres_what/) (Score: 0)
    *   This thread discusses the censorship filters of the DeepSeek model, noting their limitations and potential biases.
11. [[P] Optimize leave-one-out cross-validation for lasso regression](https://www.reddit.com/r/MachineLearning/comments/1inw1qh/p_optimize_leaveoneout_crossvalidation_for_lasso/) (Score: 0)
    *   This thread is about optimizing leave-one-out cross-validation for Lasso regression.

# Detailed Analysis by Thread
**[[D] What happened to SSMs and linear attentions? (Score: 70)](https://www.reddit.com/r/MachineLearning/comments/1in9y30/d_what_happened_to_ssms_and_linear_attentions/)**
*  **Summary:** This thread discusses the current relevance and usage of State Space Models (SSMs) and linear attention mechanisms in machine learning, particularly in large language models (LLMs).
*  **Emotion:** The emotional tone of the thread is predominantly neutral, with some expressions of positive sentiment related to specific models and advancements. However, one comment shows a negative sentiment toward SSMs and linear attentions.
*  **Top 3 Points of View:**
    *   SSMs like Mamba are effective for specific applications like DNA foundation models and audio generation.
    *   Linear attention is not very relevant for large models due to compute cost scaling.
    *   The success of models like Claude might be attributed to innovative architectures like SSMs and linear attention.

**[[Research] Novel Clustering Metric - The Jaccard-Concentration Index (Score: 15)](https://www.reddit.com/r/MachineLearning/comments/1indsvi/research_novel_clustering_metric_the/)**
*  **Summary:** This thread introduces a novel clustering metric called the Jaccard-Concentration Index.
*  **Emotion:** The thread has a positive emotional tone, with a user expressing interest in checking out the new metric.
*  **Top 3 Points of View:**
    *   The new clustering metric is interesting.
    *   User will check out the new metric.
    *   Thanks to creator.

**[[R] New Paper: Can frontier models self-explore and discover their own capabilities in an open-ended way? (Score: 8)](https://www.reddit.com/r/MachineLearning/comments/1inv1gy/r_new_paper_can_frontier_models_selfexplore_and/)**
*  **Summary:** This thread discusses a new research paper about frontier models exploring and discovering their capabilities in an open-ended way.
*  **Emotion:** The emotional tone is positive, with expressions of fascination and appreciation for the research.
*  **Top 3 Points of View:**
    *   The choice of LLM as the "Scientist" matters significantly for evaluation.
    *   The graph comparing Llama 3-8b to GPT-4o is valuable for building LLM pipelines.
    *   Understanding the capability space could improve task-specific distillation.

**[Machine psychology?[R] (Score: 6)](https://www.reddit.com/r/MachineLearning/comments/1inh9cy/machine_psychologyr/)**
*  **Summary:** This thread discusses the intersection of machine learning and psychology.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Human-computer interaction and AI-human collaboration fields apply psychology theories to AI development.
    *   User is asking if other user is thinking about a specific paper.
    *   N/A

**[[D] Causal inference in irregular time series data? (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1inpgkw/d_causal_inference_in_irregular_time_series_data/)**
*  **Summary:** This thread is about causal inference methods applicable to irregular time series data.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Looking for suggestions for causal inference on multiple events.
    *   Suggests SSMs or the s4 architecture.
    *   N/A

**[[D] Challenges with Real-time Inference at Scale (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1inl6pv/d_challenges_with_realtime_inference_at_scale/)**
*  **Summary:** This thread focuses on the challenges and solutions for performing real-time inference at scale with machine learning models.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Queries about the LLMs being run and the reason for building custom infrastructure.
    *   Suggestion to scale out to more GPUs with Beam Cloud.
    *   Suggestion to try model quantization.

**[Structured data parsing [D] (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1ins93r/structured_data_parsing_d/)**
*  **Summary:** This thread discusses methods for parsing structured data, particularly tabular data extraction from PDFs.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Looking for ways to parse structured data.
    *   Suggesting using a tabular data extraction method from a pdf.
    *   N/A

**[[D] Fine-tuning to replace complicated activations with simpler ones (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1inyd67/d_finetuning_to_replace_complicated_activations/)**
*  **Summary:** This thread explores the possibility of fine-tuning models to replace complex activation functions with simpler ones.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Importance of layer complexity and the shape of the problem.
    *   Suggestion to try knowledge distillation.
    *   There are many ways to load weights in pytorch.

**[[D] Where are ICLR 2025 submissions??? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1inegee/d_where_are_iclr_2025_submissions/)**
*  **Summary:** This thread discusses the unusual availability of ICLR 2025 submissions, with links to resources for finding them.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Paper copilot can visit them all.
    *   Papers should be on the website.
    *   Very weird that they aren't on the website.

**[[R] I Tested DeepSeek's Censorship Filters—Here's What I Discovered (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ins943/r_i_tested_deepseeks_censorship_filtersheres_what/)**
*  **Summary:** This thread discusses the censorship filters of the DeepSeek model, noting their limitations and potential biases.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Interesting that they have censorship filters.
    *   Trained with western contents, so it must be pretty impossible to reframe a llm afterwards.
    *   N/A

**[[P] Optimize leave-one-out cross-validation for lasso regression (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1inw1qh/p_optimize_leaveoneout_crossvalidation_for_lasso/)**
*  **Summary:** This thread is about optimizing leave-one-out cross-validation for Lasso regression.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Links to GitHub.
    *   Links to nbviewer.
    *   Links to binder.
