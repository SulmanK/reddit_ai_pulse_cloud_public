---
title: "LocalLLaMA Subreddit"
date: "2025-02-02"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["local models", "AI", "LLMs"]
---

# Overall Ranking and Top Discussions
1.  [mistral-small-24b-instruct-2501 is simply the best](https://www.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/) (Score: 399)
    *   The thread discusses the merits of the mistral-small-24b-instruct-2501 model, with some users praising its performance while others share their experiences and ask about setup details.
2.  [DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt.](https://x.com/rohanpaul_ai/status/1886025249273339961?t=Wpp2kGJKVSZtSAOmTJjh0g&s=19) (Score: 138)
    *   This thread revolves around the DeepSeek-R1 model failing safety tests, with many users arguing that this is a positive feature, indicating a lack of censorship rather than a failure.
3.  [Medtator, local Llama research medical helper](https://i.redd.it/g7qcnt3fvrge1.jpeg) (Score: 17)
    *   The thread showcases a new research medical helper called Medtator, with users discussing its potential and raising concerns about its name and availability.
4.  [Mistral small 3 through Openrouter is broken, while it works great with the exact same prompts through other providers and the official API](https://i.redd.it/mwax9fnbprge1.png) (Score: 14)
    *   This thread discusses a user's issue with Mistral small 3 when used through Openrouter, with other users offering troubleshooting suggestions.
5.  [Is AWQ 8-bit quantization a thing?](https://www.reddit.com/r/LocalLLaMA/comments/1ig41d3/is_awq_8bit_quantization_a_thing/) (Score: 3)
    *   Users in this thread are discussing whether AWQ supports 8-bit quantization.
6. [Reading on Distributed AI infra](https://www.reddit.com/r/LocalLLaMA/comments/1ig1q7g/reading_on_distributed_ai_infra/) (Score: 2)
    *  This thread is about learning resources for distributed AI infrastructure, with users sharing links to relevant papers and repositories.
7.  [How much difference does a third 3090 make?](https://www.reddit.com/r/LocalLLaMA/comments/1ig6i3c/how_much_difference_does_a_third_3090_make/) (Score: 1)
    *   The discussion here is about the performance gains of adding a third 3090 GPU to a system, especially for running large language models.
8.  [Dual 3080ti’s or a 3090](https://www.reddit.com/r/LocalLLaMA/comments/1ig1wkn/dual_3080tis_or_a_3090/) (Score: 1)
    *  This thread is about a user deciding between dual 3080ti's or a single 3090 GPU, with some users offering sales and technical advice.
9.  [What’s are the best GUIs for chat?](https://www.reddit.com/r/LocalLLaMA/comments/1ig7fwk/whats_are_the_best_guis_for_chat/) (Score: 1)
    *  This thread asks for recommendations on the best GUIs for interacting with LLMs, with users suggesting different platforms.
10. [Can you run DeepSeek R1 distilled version with web search?](https://www.reddit.com/r/LocalLLaMA/comments/1ig5nsb/can_you_run_deepseek_r1_distilled_version_with/) (Score: 0)
     * Users are seeking information on running the DeepSeek R1 distilled version with web search capabilities.
11. [What are people using Deepseek R1 for?](https://www.reddit.com/r/LocalLLaMA/comments/1ig2qxo/what_are_people_using_deepseek_r1_for/) (Score: 0)
    * This thread asks users what they are using the Deepseek R1 model for, with a variety of use cases being mentioned.
12. [Absolute Best Hardware](https://www.reddit.com/r/LocalLLaMA/comments/1ig36cr/absolute_best_hardware/) (Score: 0)
    *   This thread discusses optimal hardware configurations for running local LLMs, including GPU options and cloud alternatives.
13. [I'm a noob but I don't see the DeepSeek benefits — just hype?](https://www.reddit.com/r/LocalLLaMA/comments/1ig6mbk/im_a_noob_but_i_dont_see_the_deepseek_benefits/) (Score: 0)
    *  A user expresses skepticism about the DeepSeek model, with other users explaining the differences between the full model and its distilled versions.

# Detailed Analysis by Thread
**[mistral-small-24b-instruct-2501 is simply the best (Score: 399)](https://www.reddit.com/r/LocalLLaMA/comments/1ig2cm2/mistralsmall24binstruct2501_is_simply_the_best/)**
*   **Summary:** The thread starts with a strong endorsement of the mistral-small-24b-instruct-2501 model. Users discuss its performance, its potential as a local alternative to models like GPT-4o, and its balance of speed and intelligence. Some users also share their experiences using it for specific tasks, such as role-playing, and ask for setup advice.
*   **Emotion:** The overall emotional tone is positive, with users expressing excitement and satisfaction with the model. There's a mix of enthusiasm, curiosity, and a desire for more information on how to best use the model.
*   **Top 3 Points of View:**
    *   The mistral-small-24b-instruct-2501 model is a top performer, especially for its size, comparable to a "70b light" model.
    *  Some users find it very good and similar to a local gpt-4o mini while some users have had bad experiences for specific use cases, such as role-playing.
    *   Users are interested in optimizing their setups and are seeking advice on specific parameters and software (e.g. Ollama, precision settings, etc.).

**[DeepSeek-R1 fails every safety test. It exhibits a 100% attack success rate, meaning it failed to block a single harmful prompt. (Score: 138)](https://x.com/rohanpaul_ai/status/1886025249273339961?t=Wpp2kGJKVSZtSAOmTJjh0g&s=19)**
*   **Summary:** This thread discusses the DeepSeek-R1 model's inability to pass safety tests, which is viewed by many as a positive trait rather than a failure. Users see this as a lack of censorship, allowing the model to answer user prompts without restrictions. There is also some discussion about the politics surrounding AI safety and censorship.
*  **Emotion:** The dominant emotion is positive, with many users celebrating the model's lack of censorship. There's also a hint of frustration with the concept of safety testing and the idea of censoring AI models.
*   **Top 3 Points of View:**
    *   The model's failure to block harmful prompts is actually a sign of success, as it indicates the model is not censored.
    *   The term "safety" is viewed as a synonym for "censorship," and users criticize the idea of restricting what an AI model can say.
    *  Some users believe the reaction to Deepseek's lack of safety is an overreaction of the "censorship lemmings"

**[Medtator, local Llama research medical helper (Score: 17)](https://i.redd.it/g7qcnt3fvrge1.jpeg)**
*   **Summary:** This thread introduces "Medtator", a local Llama research medical helper. Users are interested in the project and are looking for more information. One user points out that the name is already in use, while another wants to use it for EMR replacement systems.
*   **Emotion:** The general emotion is curiosity and hopefulness about the potential of the application. There is a little bit of concern and skepticism about the naming and availability of the software.
*  **Top 3 Points of View:**
    *   The tool seems promising for medical research purposes.
    *  There are concerns about the project's name being already in use and the lack of a repository on GitHub.
    *   Some users see the tool as a potential component of future Electronic Medical Record replacement systems.

**[Mistral small 3 through Openrouter is broken, while it works great with the exact same prompts through other providers and the official API (Score: 14)](https://i.redd.it/mwax9fnbprge1.png)**
*   **Summary:** A user is facing issues with Mistral small 3 when using Openrouter, but it works well through other providers and the official API. Other users suggest potential causes for this issue, such as high temperature, malformed template, or exceeding the context limit, and they share their working configurations.
*   **Emotion:** The emotional tone is a mix of frustration from the user experiencing the issue, and helpfulness from other users offering possible solutions.
*   **Top 3 Points of View:**
    *   The user is experiencing technical issues with the model, specifically when using Openrouter.
    *   Other users suggest common issues, such as high temperature, malformed templates, or exceeding the context limit.
    *  Some users share that it works well for them with default parameters, indicating that the issue might be specific to the user's setup.

**[Is AWQ 8-bit quantization a thing? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1ig41d3/is_awq_8bit_quantization_a_thing/)**
*  **Summary:** This thread is focused on whether the AutoAWQ library supports 8-bit quantization for models. Users generally agree that 8-bit quantization is not available in AutoAWQ, with 4-bit being the standard.
*   **Emotion:** The emotional tone of the thread is neutral, with a focus on factual information and inquiry.
*   **Top 3 Points of View:**
    *  AutoAWQ does not seem to support 8-bit quantization.
    *  Most users have only seen 4-bit quantization with AutoAWQ.
    *  There is a general consensus that 8-bit quantization is not a standard or widely-used option in this context.

**[Reading on Distributed AI infra (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1ig1q7g/reading_on_distributed_ai_infra/)**
*   **Summary:** This thread is about finding resources on distributed AI infrastructure. Users provide links to papers and repositories related to tensor, pipeline, and sequence parallelism.
*   **Emotion:** The tone of this thread is neutral and informative.
*   **Top 3 Points of View:**
     *  The thread is looking for papers and materials for distributed AI infrastructure
     *  The conversation has papers about Megatron, tensor parallel, pipeline parallel, and sequence parallel.
     *  Users are encouraged to read frontier model tech reports like llama and Deepseek.

**[How much difference does a third 3090 make? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1ig6i3c/how_much_difference_does_a_third_3090_make/)**
*   **Summary:** The thread discusses the benefits of adding a third 3090 GPU, particularly for running large models. The main benefit is being able to run larger models like Mistral Large 123B. The thread also mentions Tabbyapi with EXL2 quants as a good choice and advises avoiding GGUF unless offloading to RAM.
*   **Emotion:** The tone of the thread is neutral and informative with a focus on technical details and advice.
*   **Top 3 Points of View:**
    * Adding a third 3090 enables running larger models like Mistral Large 123B, although at lower quant and without a draft model initially.
    *  Adding a fourth 3090 will provide a huge difference both in terms of performance and quality.
    *  Tabbyapi with EXL2 quants are a good choice for best performance on multi-GPU.

**[Dual 3080ti’s or a 3090 (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1ig1wkn/dual_3080tis_or_a_3090/)**
*   **Summary:** The thread is about choosing between dual 3080ti’s or a single 3090 GPU for AI tasks. One user offers a 3090 for sale, while another comments about the used price of 3090 being too high. A user suggests that running a 3090 alongside a 3080ti would cause the 3090 to be "downgraded" to 3080 speeds.
*   **Emotion:** The tone is a mix of transactional (selling) and practical (advice).
*   **Top 3 Points of View:**
    *   A user is selling a 3090, and is available for meet in person or shipping.
    *  The price of used 3090 might be too expensive
    *   A 3090 alongside a 3080ti may have compatibility issues or performance throttling.

**[What’s are the best GUIs for chat? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1ig7fwk/whats_are_the_best_guis_for_chat/)**
*   **Summary:** This thread is a user asking for the best GUI for chat. Msty is recommended as a user friendly, easy to install option.  A user also links an API that adds web search to Deepseek.
*   **Emotion:** The emotional tone is positive and helpful.
*   **Top 3 Points of View:**
     *  Msty is a good GUI choice for users looking for an easy option.
     *  A user offers an API that adds web search into Deepseek
     *  Users are sharing different GUI options available for local LLM.

**[Can you run DeepSeek R1 distilled version with web search? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1ig5nsb/can_you_run_deepseek_r1_distilled_version_with/)**
*   **Summary:**  The thread is a user asking how to run a distilled DeepSeek R1 with web search. The suggestion is to load the model with ollama and use Open WebUI, which includes native web search.
*   **Emotion:** The tone of this thread is neutral and helpful.
*   **Top 3 Points of View:**
     * The user is looking for ways to integrate web search with a distilled DeepSeek R1 model.
     * The suggestion is to use Open WebUI in combination with ollama to achieve the goal
     * The suggestion implies that Open WebUI has web search capabilities natively.

**[What are people using Deepseek R1 for? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1ig2qxo/what_are_people_using_deepseek_r1_for/)**
*   **Summary:**  This thread asks for real use cases for the Deepseek R1 model. Users are using the model for things like technical writing, documentation, research, coding, D&D solo adventures, grammar/style correction, and even generating limericks. There is some indication that the small local models are not as good as larger ones, and some people are using it to test censorship.
*   **Emotion:**  The emotional tone is mostly positive.
*   **Top 3 Points of View:**
    *  DeepSeek R1 is being used for technical writing, research, and documentation.
    *  The model is good for creative purposes like D&D and writing limericks.
    *  Some users find smaller, local versions of DeepSeek R1 slow and less useful.

**[Absolute Best Hardware (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1ig36cr/absolute_best_hardware/)**
*   **Summary:** This thread discusses the best hardware for running local LLMs. Options mentioned include laptops, cloud computing, GPUs like 3060, used 3090 and the Mac M4. Cloud computing is a great way to test local models before committing.
*  **Emotion:**  The tone is informative and practical.
*  **Top 3 Points of View:**
    *   Laptops have a hard cap on their VRAM, so users are limited to smaller 8b models.
    *   Cloud computing is a viable option and good way to test a lot of models for less.
    *   A used 3090 or Mac M4 is a good option for home use.

**[I'm a noob but I don't see the DeepSeek benefits — just hype? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1ig6mbk/im_a_noob_but_i_dont_see_the_deepseek_benefits/)**
*   **Summary:**  A user expresses skepticism about DeepSeek, with other users clarifying that the hype is centered on the bigger 600B+ models, not the smaller distilled versions. Some users compare DeepSeek to other models such as Qwen and Mistral. Users feel like they need to put a visible banner at the top of the page clarifying this issue.
*   **Emotion:**  The tone is a mix of skepticism, clarification, and slight frustration from users trying to explain this issue.
*   **Top 3 Points of View:**
    *   The hype around DeepSeek is primarily about the large 600B+ parameter models, not the smaller distilled versions.
    *   The smaller distilled DeepSeek models are not significantly better (and can be worse in some cases) than other models like Qwen and Mistral.
    *   Users are frustrated by this widespread misconception and some want a banner at the top of the page about this issue.
