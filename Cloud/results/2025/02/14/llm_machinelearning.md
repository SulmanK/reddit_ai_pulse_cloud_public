---
title: "Machine Learning Subreddit"
date: "2025-02-14"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "deep learning"]
---

# Overall Ranking and Top Discussions
1.  [[D] We built GenAI at Google and Apple, then left to build an open source AI lab, to enable the open community to collaborate and build the next DeepSeek. Ask us anything on Friday, Feb 14 from 9am-12pm PT!](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/) (Score: 83)
    *   People are asking questions about the open source AI lab, including funding, the rationale behind open AI, the possibility of deploying and serving the models, and areas of research.
2.  [[P]GPT-2 in Pure C(and full CUDA worklogs to come)](https://www.reddit.com/r/MachineLearning/comments/1ioybio/pgpt2_in_pure_cand_full_cuda_worklogs_to_come/) (Score: 47)
    *   Users expressed enthusiasm and asked about the development approach and testing workflow for implementing GPT-2 in pure C with CUDA.
3.  [[R] Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach](https://www.reddit.com/r/MachineLearning/comments/1ip8lhf/r_scaling_up_testtime_compute_with_latent/) (Score: 27)
    *   The discussion is around test-time recurrent depth and its potential for scaling. One user pointed out that performance saturates at a depth of 32.
4.  [[P] GNNs for time series anomaly detection](https://www.reddit.com/r/MachineLearning/comments/1ipgk8p/p_gnns_for_time_series_anomaly_detection/) (Score: 26)
    *   People are discussing the implementation of GNNs for time series anomaly detection and the challenges associated with the SWaT dataset.
5.  [[D] Diffusion models and their statistical uncertainty?](https://www.reddit.com/r/MachineLearning/comments/1ip6doi/d_diffusion_models_and_their_statistical/) (Score: 7)
    *   Users are discussing diffusion models and the estimation of error.
6.  [[R] Doing a PhD in Europe+UK](https://www.reddit.com/r/MachineLearning/comments/1ip9vuw/r_doing_a_phd_in_europeuk/) (Score: 6)
    *   People are recommending Inria, Scool team in Lille as the follow-up team to SequeL.
7.  [[D] ML debugging interview for experienced roles](https://www.reddit.com/r/MachineLearning/comments/1ip4ypj/d_ml_debugging_interview_for_experienced_roles/) (Score: 5)
    *   A user recommends the MLE Path YouTube channel for preparing for ML interviews.
8.  [[D]Can you deploy Unsloth's DeepSeek r1 1.58 bit to XNOR logic gates? And calculate them?](https://www.reddit.com/r/MachineLearning/comments/1iov74j/dcan_you_deploy_unsloths_deepseek_r1_158_bit_to/) (Score: 2)
    *   The discussion centers around deploying Unsloth's DeepSeek model to XNOR logic gates and the limitations of using 1.58-bit parameters.
9.  [Thesis choice - Algorithm fairness, explainable and trustworthy AI [D]](https://www.reddit.com/r/MachineLearning/comments/1ipekfw/thesis_choice_algorithm_fairness_explainable_and/) (Score: 2)
    *   A user with a Ph.D. in XAI recommends looking at the GDR Mascotnum as a basis for relevant research.
10. [[D] How to Automate Naming Bulk Audio Samples Based on Their Audio Features?](https://www.reddit.com/r/MachineLearning/comments/1ip0rnt/d_how_to_automate_naming_bulk_audio_samples_based/) (Score: 0)
    *   People are discussing ways to automate the naming of bulk audio samples based on their audio features.
11. [[D] Can you recommend a good serverless GPU provider that supports running WhisperX?](https://www.reddit.com/r/MachineLearning/comments/1ip2oko/d_can_you_recommend_a_good_serverless_gpu/) (Score: 0)
    *   Users are recommending serverless GPU providers for running WhisperX, with Runpod and Google Cloud Run being suggested.
12. [[D] How much should I charge for building a customer service chatbot to replace Intercom?](https://www.reddit.com/r/MachineLearning/comments/1ip802v/d_how_much_should_i_charge_for_building_a/) (Score: 0)
    *   The discussion revolves around pricing strategies for building a customer service chatbot.
13. [[D] Val acc higher than train acc](https://www.reddit.com/r/MachineLearning/comments/1ip88et/d_val_acc_higher_than_train_acc/) (Score: 0)
    *   People are exploring reasons why validation accuracy might be higher than training accuracy in ML models.
14. [[D] Need advice on AI calorie estimation app](https://www.reddit.com/r/MachineLearning/comments/1ipck6l/d_need_advice_on_ai_calorie_estimation_app/) (Score: 0)
    *   The discussion involves the portion size problem and suggests using Gemini Flash 2.0 for building a multimodal pipeline.

# Detailed Analysis by Thread
**[[D] We built GenAI at Google and Apple, then left to build an open source AI lab, to enable the open community to collaborate and build the next DeepSeek. Ask us anything on Friday, Feb 14 from 9am-12pm PT! (Score: 83)](https://www.reddit.com/r/MachineLearning/comments/1ioxatq/d_we_built_genai_at_google_and_apple_then_left_to/)**
*   **Summary:** An "Ask Me Anything" (AMA) thread by individuals who previously worked on GenAI at Google and Apple and have now started an open-source AI lab.
*   **Emotion:** The overall emotional tone of the thread is neutral, as most comments are informational and question-based.
*   **Top 3 Points of View:**
    *   Questions about the rationale behind creating an "open" AI lab and whether it truly offers more accessibility than existing models.
    *   Inquiries about the funding model for the organization, especially considering the high costs associated with training AI models.
    *   Questions regarding their philosophy on deploying and serving the models, including the possibility of local fine-tuning versus a centralized API.

**[[P]GPT-2 in Pure C(and full CUDA worklogs to come) (Score: 47)](https://www.reddit.com/r/MachineLearning/comments/1ioybio/pgpt2_in_pure_cand_full_cuda_worklogs_to_come/)**
*   **Summary:** A post announcing the implementation of GPT-2 in pure C with CUDA, with the promise of worklogs to come.
*   **Emotion:** The overall emotion is positive, with users expressing excitement and appreciation.
*   **Top 3 Points of View:**
    *   Expression of general excitement and appreciation for the project.
    *   Inquiry about the specific development approach and testing workflow used.
    *   Acknowledgement of the author.

**[[R] Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach (Score: 27)](https://www.reddit.com/r/MachineLearning/comments/1ip8lhf/r_scaling_up_testtime_compute_with_latent/)**
*   **Summary:** A discussion centered on a research paper about scaling test-time computation using latent reasoning and a recurrent depth approach.
*   **Emotion:** Neutral, with a focus on technical analysis and critical evaluation.
*   **Top 3 Points of View:**
    *   The observation that test-time recurrent depth doesn't seem to offer open-ended scaling in the demonstrated experiments.
    *   Questioning whether recurrent depth at training time presents a scalable path for improved data efficiency.
    *   Request for the author to share the work logs.

**[[P] GNNs for time series anomaly detection (Score: 26)](https://www.reddit.com/r/MachineLearning/comments/1ipgk8p/p_gnns_for_time_series_anomaly_detection/)**
*   **Summary:** A post sharing a repository for GNNs applied to time series anomaly detection.
*   **Emotion:** The general emotion is neutral.
*   **Top 3 Points of View:**
    *   Suggestion that the author should share papers or docs to better communicate the project.
    *   Request for the author to share paper on the Github.
    *   Discussion about the unreliability of the SWaT dataset for time series anomaly detection.

**[[D] Diffusion models and their statistical uncertainty? (Score: 7)](https://www.reddit.com/r/MachineLearning/comments/1ip6doi/d_diffusion_models_and_their_statistical/)**
*   **Summary:** Discussion regarding diffusion models and the estimation of error and uncertainty.
*   **Emotion:** The overall emotional tone of the thread is neutral.
*   **Top 3 Points of View:**
    *   Diffusion is used to simulate the reversal of degradation into noise because noise is easy to sample from.
    *   Denoising loss can be a good option for estimation of the error of a diffusion model.
    *   Flow matching from a gaussian to your data distribution helps predict x_1 directly.

**[[R] Doing a PhD in Europe+UK (Score: 6)](https://www.reddit.com/r/MachineLearning/comments/1ip9vuw/r_doing_a_phd_in_europeuk/)**
*   **Summary:** Recommending PhD programs in Europe and the UK.
*   **Emotion:** The overall emotional tone of the thread is neutral.
*   **Top 1 Points of View:**
    *   Inria, Scool team in Lille is a good recommendation.

**[[D] ML debugging interview for experienced roles (Score: 5)](https://www.reddit.com/r/MachineLearning/comments/1ip4ypj/d_ml_debugging_interview_for_experienced_roles/)**
*   **Summary:** Advice on preparing for ML debugging interviews for experienced roles.
*   **Emotion:** The overall emotional tone of the thread is neutral.
*   **Top 1 Points of View:**
    *   The MLE Path youtube channel has a lot of videos about ML interviews.

**[[D]Can you deploy Unsloth's DeepSeek r1 1.58 bit to XNOR logic gates? And calculate them? (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1iov74j/dcan_you_deploy_unsloths_deepseek_r1_158_bit_to/)**
*   **Summary:** Discussion about deploying Unsloth's DeepSeek model to XNOR logic gates.
*   **Emotion:** The overall emotional tone of the thread is neutral.
*   **Top 1 Points of View:**
    *   Only the parameters are 1.58 bits, not the activations.

**[Thesis choice - Algorithm fairness, explainable and trustworthy AI [D] (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1ipekfw/thesis_choice_algorithm_fairness_explainable_and/)**
*   **Summary:** Choosing a thesis about Algorithm fairness, explainable and trustworthy AI.
*   **Emotion:** The overall emotional tone of the thread is positive.
*   **Top 1 Points of View:**
    *   Look at the GDR Mascotnum as a basis for relevant research.

**[[D] How to Automate Naming Bulk Audio Samples Based on Their Audio Features? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ip0rnt/d_how_to_automate_naming_bulk_audio_samples_based/)**
*   **Summary:** The discussion revolves around automating the naming of audio samples based on audio features using machine learning techniques.
*   **Emotion:** The overall emotional tone of the thread is neutral.
*   **Top 3 Points of View:**
    *   Using a CNN to perform multi-class classification is an option for static tags.
    *   CLAP suggests zero shot classification, given appropriate text strings.
    *   Python code for renaming audio samples to include instrument and mood.

**[[D] Can you recommend a good serverless GPU provider that supports running WhisperX? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ip2oko/d_can_you_recommend_a_good_serverless_gpu/)**
*   **Summary:** Discussion about serverless GPU providers for running WhisperX.
*   **Emotion:** The overall emotional tone of the thread is neutral.
*   **Top 1 Points of View:**
    *   Runpod and Google Cloud Run have GPU support, and are decent options.

**[[D] How much should I charge for building a customer service chatbot to replace Intercom? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ip802v/d_how_much_should_i_charge_for_building_a/)**
*   **Summary:** Advice on pricing a customer service chatbot project.
*   **Emotion:** The overall emotional tone of the thread is neutral.
*   **Top 3 Points of View:**
    *   Charge as much as they are willing to pay.
    *   Use a flat hourly rate for development + API costs.
    *   Charge per hour for the first customer, keep the code rights, and then change for next customers.

**[[D] Val acc higher than train acc (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ip88et/d_val_acc_higher_than_train_acc/)**
*   **Summary:** The discussion centers around possible reasons for validation accuracy being higher than training accuracy in a machine learning model.
*   **Emotion:** Neutral, with elements of curiosity and problem-solving.
*   **Top 3 Points of View:**
    *   Data leak is a possible reason
    *   Validation typically has less augmentation applied, or validation set might have less noise.
    *   Dropout applied during training and not validation or model not capturing the features needed.

**[[D] Need advice on AI calorie estimation app (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ipck6l/d_need_advice_on_ai_calorie_estimation_app/)**
*   **Summary:** Advice needed on developing an AI calorie estimation app.
*   **Emotion:** The overall emotional tone of the thread is neutral.
*   **Top 1 Points of View:**
    *   Use Gemini Flash 2.0 because it is dirt cheap and you can build a multimodal pipeline for this in pyspur.
