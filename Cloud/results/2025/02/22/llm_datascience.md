---
title: "Data Science Subreddit"
date: "2025-02-22"
description: "Analysis of top discussions and trends in the datascience subreddit"
tags: ["datascience", "AI", "LLM"]
---

# Overall Ranking and Top Discussions
1.  [Was the hype around DeepSeek warranted or unfounded?](https://www.reddit.com/r/datascience/comments/1iv8cbv/was_the_hype_around_deepseek_warranted_or/) (Score: 30)
    * Discusses whether the attention given to DeepSeek, a company known for its AI models, is justified, with various users weighing in on its impact on the AI landscape and its potential to challenge established players.
2.  [Are LLMs good with ML model outputs?](https://www.reddit.com/r/datascience/comments/1ivgrnb/are_llms_good_with_ml_model_outputs/) (Score: 4)
    * Explores the effectiveness of using Large Language Models (LLMs) in conjunction with the outputs of other machine learning models, with users sharing insights on potential applications, limitations, and alternative approaches.

# Detailed Analysis by Thread
**[Was the hype around DeepSeek warranted or unfounded? (Score: 30)](https://www.reddit.com/r/datascience/comments/1iv8cbv/was_the_hype_around_deepseek_warranted_or/)**
*   **Summary:**  The discussion revolves around the justification of the hype surrounding DeepSeek, a company in the AI field. Users discuss its open-source contributions, cost-effectiveness, and impact on the AI industry's competitive landscape. The thread also touches on the techniques used by DeepSeek, such as distillation, reinforcement learning, and mixture of experts.
*   **Emotion:** The overall emotional tone of the thread is neutral. While individual comments have varying sentiment scores, the predominant label is "Neutral", indicating an objective and analytical discussion.
*   **Top 3 Points of View:**
    *   DeepSeek's open weights release has significantly closed the gap between closed source and open source AI models, allowing smaller labs to participate in AI research.
    *   DeepSeek has demonstrated that AI models can be trained and maintained much more cheaply than previously believed, which is a significant business breakthrough.
    *   The techniques used by DeepSeek, while not entirely new, have been scaled up and optimized for hardware usage.

**[Are LLMs good with ML model outputs? (Score: 4)](https://www.reddit.com/r/datascience/comments/1ivgrnb/are_llms_good_with_ml_model_outputs/)**
*  **Summary:** The thread discusses the utility of using Large Language Models (LLMs) to process and interpret the outputs of other machine learning models. The discussion explores potential use cases, limitations, and alternative approaches, particularly in the context of root cause analysis and automated tasks.
*  **Emotion:** The overall emotional tone is slightly negative, with users expressing skepticism about the reliability of LLMs for complex problem-solving tasks, but predominantly neutral.
*  **Top 3 Points of View:**
    *  LLMs are not reliable problem-solving machines, as they are engineered to be language models and are not numerically reliable. Causal inference workflows are recommended instead.
    *  LLMs are best used only when it's not worth the effort to use rule-based approaches, suggesting that simpler solutions may be more effective in some cases.
    *  Using an agentic AI framework with multiple agents and providing functions for them to call APIs can be a valuable approach to the goal, but raw output as is, is not useful.
