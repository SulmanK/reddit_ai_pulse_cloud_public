---
title: "Machine Learning Subreddit"
date: "2025-02-22"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machine learning", "AI", "research"]
---

# Overall Ranking and Top Discussions
1.  [[P] Decensor AI models Qwen/Deepseek by finetuning with non political data](https://www.reddit.com/r/MachineLearning/comments/1iv6ckk/p_decensor_ai_models_qwendeepseek_by_finetuning/) (Score: 23)
    *   Discusses fine-tuning AI models to remove censorship by using non-political data.
2.  [[R] Evaluating LLM Knowledge Across 285 Graduate Disciplines: A Comprehensive Benchmark Using Human-LLM Collaborative Filtering](https://www.reddit.com/r/MachineLearning/comments/1ivd069/r_evaluating_llm_knowledge_across_285_graduate/) (Score: 15)
    *   Shares and corrects details about a benchmark evaluating LLM knowledge across various graduate disciplines.
3.  [[R] Calculating costs of fine tuning an Vision Language Model](https://www.reddit.com/r/MachineLearning/comments/1ivjrwi/r_calculating_costs_of_fine_tuning_an_vision/) (Score: 11)
    *   Discusses calculating the costs associated with fine-tuning a Vision Language Model, providing a breakdown of the estimation process.
4.  [People who finetuned Whisper, please give some feedback! [P]](https://www.reddit.com/r/MachineLearning/comments/1iv3aqx/people_who_finetuned_whisper_please_give_some/) (Score: 10)
    *   Asks for feedback from people who have fine-tuned the Whisper model, leading to discussions on datasets, hardware and optimal settings.
5.  [[D] Elastic/Serverless GPU instances for transformer hyper-parameter search](https://www.reddit.com/r/MachineLearning/comments/1iv6o6n/d_elasticserverless_gpu_instances_for_transformer/) (Score: 7)
    *   Discusses the problem of needing to find Elastic/Serverless GPU instances for transformer hyper-parameter searches and possible solutions.
6.  [[D] Does anyone know what SAM's official web demo uses? I just cannot replicate the results locally with the params.](https://www.reddit.com/r/MachineLearning/comments/1iv8be1/d_does_anyone_know_what_sams_official_web_demo/) (Score: 6)
    *   Asks about the technology behind SAM's official web demo, with suggestions on where to find the relevant code and how to replicate the results.
7.  [[D] ICLR 2025: question, submitted a paper for a workshop, received a review, don't know how to submit a rebuttal.](https://www.reddit.com/r/MachineLearning/comments/1iv3qv2/d_iclr_2025_question_submitted_a_paper_for_a/) (Score: 1)
    *   Asks about the rebuttal process for a workshop paper submitted to ICLR 2025.
8.  [[D] How Can I Level Up as a Software Engineer? Looking for Growth Opportunities Beyond Work!](https://www.reddit.com/r/MachineLearning/comments/1ivodtm/d_how_can_i_level_up_as_a_software_engineer/) (Score: 0)
    *   Inquires on how to level up as a software engineer and find growth opportunities beyond work.

# Detailed Analysis by Thread
**[[P] Decensor AI models Qwen/Deepseek by finetuning with non political data (Score: 23)](https://www.reddit.com/r/MachineLearning/comments/1iv6ckk/p_decensor_ai_models_qwendeepseek_by_finetuning/)**
*  **Summary:** This thread discusses the process of fine-tuning AI models, specifically Qwen/Deepseek, to remove censorship by using non-political data. A link to a blog post about a "decensored reasoning model" is provided.
*  **Emotion:** The emotional tone of the thread is Neutral.
*  **Top 3 Points of View:**
    *   AI models can be fine-tuned to remove censorship.
    *   Using non-political data is key to achieving this.
    *   The linked blog post provides more information on the "Openthinker" model.

**[[R] Evaluating LLM Knowledge Across 285 Graduate Disciplines: A Comprehensive Benchmark Using Human-LLM Collaborative Filtering (Score: 15)](https://www.reddit.com/r/MachineLearning/comments/1ivd069/r_evaluating_llm_knowledge_across_285_graduate/)**
*  **Summary:** This thread shares and corrects details about a benchmark evaluating LLM knowledge across various graduate disciplines. Corrections include the number of questions used, which models were tested, and how the questions were generated.
*  **Emotion:** The emotional tone of the thread is Neutral.
*  **Top 3 Points of View:**
    *   The benchmark evaluates LLM knowledge across 285 graduate disciplines.
    *   Claude 3.5 was tested, not Claude 2.
    *   The questions were collected by expert annotators with LLM assistance.

**[[R] Calculating costs of fine tuning an Vision Language Model (Score: 11)](https://www.reddit.com/r/MachineLearning/comments/1ivjrwi/r_calculating_costs_of_fine_tuning_an_vision/)**
*  **Summary:** This thread discusses calculating the costs associated with fine-tuning a Vision Language Model. It provides a breakdown of the estimation process, including tokenization, throughput benchmarks, and hardware considerations.
*  **Emotion:** The emotional tone of the thread is Neutral.
*  **Top 3 Points of View:**
    *   Estimate the dataset size in token counts.
    *   Find forward throughput benchmarks for different models/hardware.
    *   Factor in wiggle room for experimentation.

**[People who finetuned Whisper, please give some feedback! [P] (Score: 10)](https://www.reddit.com/r/MachineLearning/comments/1iv3aqx/people_who_finetuned_whisper_please_give_some/)**
*  **Summary:** This thread is a request for feedback from people who have fine-tuned the Whisper model. Responses include advice on hardware requirements, dataset considerations, and the importance of monitoring training loss.
*  **Emotion:** The emotional tone of the thread is generally Positive, with expressions of helpfulness and encouragement.
*  **Top 3 Points of View:**
    *   Finetuning Whisper can be rewarding.
    *   The choice of model size (small/medium) depends on available VRAM.
    *   Monitoring training loss and experimenting with learning rates is crucial.

**[[D] Elastic/Serverless GPU instances for transformer hyper-parameter search (Score: 7)](https://www.reddit.com/r/MachineLearning/comments/1iv6o6n/d_elasticserverless_gpu_instances_for_transformer/)**
*  **Summary:** This thread discusses the problem of needing to find Elastic/Serverless GPU instances for transformer hyper-parameter searches. Several solutions are proposed, including cloud services like AWS SageMaker, Google AI Platform, SkyPilot, Shadeform, RunPod and Modal.com.
*  **Emotion:** The overall emotional tone is Neutral, with elements of positive sentiment in recommendations.
*  **Top 3 Points of View:**
    *   Cloud services like AWS SageMaker or Google AI Platform can be used for on-demand GPU instances.
    *   SkyPilot is a tool that scales to many parallel jobs and auto-terminates cloud instances.
    *   RunPod and Modal.com are services that can be used for hyperparameter sweeps.

**[[D] Does anyone know what SAM's official web demo uses? I just cannot replicate the results locally with the params. (Score: 6)](https://www.reddit.com/r/MachineLearning/comments/1iv8be1/d_does_anyone_know_what_sams_official_web_demo/)**
*  **Summary:** This thread inquires about the technology behind SAM's official web demo and the difficulty in replicating the results locally. Users suggest checking the original segment anything model repository for the code and provide a GitHub link.
*  **Emotion:** The emotional tone of the thread is Neutral.
*  **Top 3 Points of View:**
    *   The code for the demo can be found in the original segment anything model repository.
    *   The provided GitHub repository worked well for a past project.
    *   Replicating the results locally can be challenging.

**[[D] ICLR 2025: question, submitted a paper for a workshop, received a review, don't know how to submit a rebuttal. (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1iv3qv2/d_iclr_2025_question_submitted_a_paper_for_a/)**
*  **Summary:** This thread asks about the rebuttal process for a workshop paper submitted to ICLR 2025. The response explains that workshops don't always have rebuttals and advises checking the workshop's page for deadlines or emailing the chairs.
*  **Emotion:** The emotional tone of the thread is Neutral.
*  **Top 3 Points of View:**
    *   Workshops don't always have rebuttals.
    *   Check the workshop's page for rebuttal deadlines.
    *   Email the chairs if the deadline is approaching and unclear.

**[[D] How Can I Level Up as a Software Engineer? Looking for Growth Opportunities Beyond Work! (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ivodtm/d_how_can_i_level_up_as_a_software_engineer/)**
*  **Summary:** This thread inquires on how to level up as a software engineer and find growth opportunities beyond work. A single response advises to spend time outside work on things that are enjoyable.
*  **Emotion:** The emotional tone of the thread is Positive.
*  **Top 3 Points of View:**
    *   Spend your time outside work on things that you enjoy doing.
