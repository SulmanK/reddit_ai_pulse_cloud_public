---
title: "LocalLLaMA Subreddit"
date: "2025-02-03"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["local LLM", "AI", "deep learning"]
---

# Overall Ranking and Top Discussions
1.  [[Project] I trained a tinystories model from scratch for educational purposes, how cooked? (1M-parameters)](https://i.redd.it/mdy6shrszyge1.png) (Score: 32)
    *   Users discussed the results of training a small language model, with many asking for details and tutorials.
2.  [Introducing Deeper Seeker - A simpler and OSS version of OpenAI's latest Deep Research feature.](https://www.reddit.com/r/LocalLLaMA/comments/1igyy0n/introducing_deeper_seeker_a_simpler_and_oss/) (Score: 12)
    *   People discussed the new open-source project, comparing it with OpenAI's deep research feature.
3.  [Imatrix gguf vs regular GGUF? Which one to use? Why?](https://www.reddit.com/r/LocalLLaMA/comments/1igvpu5/imatrix_gguf_vs_regular_gguf_which_one_to_use_why/) (Score: 6)
    *   Users discussed which type of GGUF file format is better for using LLMs.
4.  [Trying to run Unsloth Deepseek R1 1.58](https://www.reddit.com/r/LocalLLaMA/comments/1igxllw/trying_to_run_unsloth_deepseek_r1_158/) (Score: 3)
    *   Users shared experiences and solutions for running a specific model.
5.  [Didn't realize R1 Distilled Qwen 32B was THIS good!](https://x.com/abhinand58/status/1886457265827536998) (Score: 1)
    *   People discussed the positive performance of a particular distilled LLM.
6.  [Can I run LLMs locally with my hardware?](https://www.reddit.com/r/LocalLLaMA/comments/1igv1bg/can_i_run_llms_locally_with_my_hardware/) (Score: 1)
    *   Users provided advice to someone on running LLMs locally with the user's hardware setup.
7.  [M3 Max, 64GB RAM](https://www.reddit.com/r/LocalLLaMA/comments/1igvsc0/m3_max_64gb_ram/) (Score: 1)
    *   Users recommend models that would be useful on this hardware.
8.  [o3-mini-LOW beats medium & high! Model Performance on Med School STEP 1-3 Exams (o1, DeepSeek R1, o3-mini, Claude, Llama 405b, Mistral, etc)](https://www.reddit.com/r/LocalLLaMA/comments/1igwysd/o3minilow_beats_medium_high_model_performance_on/) (Score: 1)
    *   Users debate the validity of an evaluation that suggested a smaller model was outperforming larger ones.
9.  [Parallel interference on multiple GPU](https://www.reddit.com/r/LocalLLaMA/comments/1igz2lj/parallel_interference_on_multiple_gpu/) (Score: 1)
    *   A user asked about running models in parallel on multiple GPUs
10. [Beyond LLMs?](https://www.reddit.com/r/LocalLLaMA/comments/1igu223/beyond_llms/) (Score: 0)
    *  Users are discussing if LLM's are the way to AGI.
11. [Does the DeepSeek "search" button just not work anymore, for anyone?](https://www.reddit.com/r/LocalLLaMA/comments/1igul54/does_the_deepseek_search_button_just_not_work/) (Score: 0)
    *  Users reported issues with the DeepSeek Search feature.
12. [Lex' podcast: DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megacluster...](https://youtube.com/watch?v=_1f-o0nqpEI&si=oPHY39vrkxW2eXKq) (Score: 0)
     *  Users are discussing Lex podcast, where they dislike some of his recent actions but like his technology interviews.
13. [How can I use LLava with Deepseek?](https://www.reddit.com/r/LocalLLaMA/comments/1igwyzc/how_can_i_use_llava_with_deepseek/) (Score: 0)
     * A user is asking how to use LLava with Deepseek model.
14. [Deepseek doing weird answer](https://www.reddit.com/r/LocalLLaMA/comments/1igxf33/deepseek_doing_weird_answer/) (Score: 0)
    *  Users discuss the odd behaviors of the Deepseek model.
15. [Any open source projects similar to open AI's new deep research that I can run locally?](https://www.reddit.com/r/LocalLLaMA/comments/1igzegf/any_open_source_projects_similar_to_open_ais_new/) (Score: 0)
    *  Users are looking for Open Source projects similar to Open AI's deep research feature.

# Detailed Analysis by Thread
**[ [Project] I trained a tinystories model from scratch for educational purposes, how cooked? (1M-parameters) (Score: 32)](https://i.redd.it/mdy6shrszyge1.png)**
*   **Summary:** The user shared a 1M-parameter model they trained. People were generally impressed, noting its surprising coherence for such a small model. They requested more information and tutorials on the training process.
*   **Emotion:** The overall emotional tone is positive, with users expressing curiosity, encouragement, and appreciation for the work. There's a mix of neutral and positive sentiments.
*   **Top 3 Points of View:**
    *   The model's coherence is surprisingly good for its size.
    *   The user should provide a write-up or tutorial on how they trained the model.
    *   There is interest in the hardware specs used for training.

**[Introducing Deeper Seeker - A simpler and OSS version of OpenAI's latest Deep Research feature. (Score: 12)](https://www.reddit.com/r/LocalLLaMA/comments/1igyy0n/introducing_deeper_seeker_a_simpler_and_oss/)**
*   **Summary:**  The user introduced Deeper Seeker, an open-source alternative to OpenAI's Deep Research feature. Users are interested in its performance, licensing, local model support, and whether DeepSeek R1 is used in the project.
*   **Emotion:** The emotional tone of the thread is mostly neutral, with a mix of curiosity and mild enthusiasm. There is a general positive sentiment toward the open-source project.
*   **Top 3 Points of View:**
    *   Users are curious about the performance of Deeper Seeker compared to OpenAI's offering.
    *   There is a desire to see local model support added to the tool.
    *   There are questions about the licensing of the project.

**[Imatrix gguf vs regular GGUF? Which one to use? Why? (Score: 6)](https://www.reddit.com/r/LocalLLaMA/comments/1igvpu5/imatrix_gguf_vs_regular_gguf_which_one_to_use_why/)**
*   **Summary:**  The user asks about the difference between Imatrix GGUF and regular GGUF files for LLMs. Users suggest that the Imatrix version is better, especially with IQ quantization.
*   **Emotion:** The emotional tone is mostly neutral, with a slight positive inclination toward Imatrix GGUF.
*  **Top 3 Points of View:**
    *   Imatrix GGUF is theoretically better than regular GGUF.
    *   Imatrix plus IQ quantization is recommended.


**[Trying to run Unsloth Deepseek R1 1.58 (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1igxllw/trying_to_run_unsloth_deepseek_r1_158/)**
*   **Summary:**  The user is having issues running the Unsloth Deepseek R1 1.58 model. Users offer advice regarding server setups, memory management using swap or disk sectors. They also suggest trying Ollama first to see if the model will use the GPU.
*   **Emotion:** The emotional tone is neutral, with users focusing on providing technical advice and solutions.
*   **Top 3 Points of View:**
    *   Server setups are better for running this model because they have less DDR bandwidth limitations.
    *   Memory management techniques such as using swap or disk sectors might help.
    *   Try running the model with Ollama to see if it can be loaded onto the GPU

**[Didn't realize R1 Distilled Qwen 32B was THIS good! (Score: 1)](https://x.com/abhinand58/status/1886457265827536998)**
*   **Summary:**  The user was impressed by the performance of the R1 Distilled Qwen 32B model. Users discussed their experiences, UI, and reasoning capabilities of the model.
*   **Emotion:** The general emotional tone is positive, with people sharing their experiences with the model. However, there are also some negative sentiments related to the model's consistency and math abilities.
*  **Top 3 Points of View:**
    *   The model is considered really good and beneficial for development.
    *   Some users experience inconsistent outputs and randomness in the model's responses.
    *   There is skepticism about the model's ability to handle math accurately.
    *   The model is considered the best local LLM for some users.

**[Can I run LLMs locally with my hardware? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1igv1bg/can_i_run_llms_locally_with_my_hardware/)**
*   **Summary:** The user asked if they could run LLMs locally with their hardware. Users provided multiple recommendations on what models to try and what software to use.
*   **Emotion:** The general tone is positive and supportive, with users offering helpful and encouraging advice.
*   **Top 3 Points of View:**
    *  There are many small models that can be run locally on the provided hardware.
    *  LM Studio is a good starting point for beginners to try out local LLMs.
    *  Llama.cpp with CUDA and GGUF from hugging face can be used.
    *  A rule of thumb is to multiply the model's size (GB) by 1.5 to get the required RAM

**[M3 Max, 64GB RAM (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1igvsc0/m3_max_64gb_ram/)**
*   **Summary:** The user asked for model recommendations given their M3 Max with 64 GB RAM. Users provided specific models suitable for this setup.
*  **Emotion:** The thread has a positive and helpful tone, with users offering recommendations.
*   **Top 3 Points of View:**
    *   Qwen2.5-7B-Instruct-1M-Q8\_0 is good for long context
    *   Phi-4 model by Microsoft is a tiny beast for its size.
    *   Llama 3.3 70B and Mistral Small 3 are recommended to try.

**[o3-mini-LOW beats medium & high! Model Performance on Med School STEP 1-3 Exams (o1, DeepSeek R1, o3-mini, Claude, Llama 405b, Mistral, etc) (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1igwysd/o3minilow_beats_medium_high_model_performance_on/)**
*   **Summary:**  The user claimed that a small model was outperforming larger models on med school exams. Users challenged the validity of the evaluation due to potential data contamination.
*   **Emotion:** The overall tone is critical and skeptical. The thread expresses a negative sentiment toward the accuracy of the evaluation.
*   **Top 3 Points of View:**
    *   The evaluation is likely flawed as the questions have been publicly available and discussed online.
    *   The results might not be statistically significant and needs more testing.
    *  Models may have been overfit on this specific type of test.

**[Parallel interference on multiple GPU (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1igz2lj/parallel_interference_on_multiple_gpu/)**
*   **Summary:**  A user asked if it was possible to use multiple GPUs for LLM inference. A user stated that mlc-llm can do this, but llama-cpp can't.
*  **Emotion:** The overall tone of the thread is positive and informative.
*   **Top 3 Points of View:**
    *  Yes, multiple GPU inference is possible.
    *  mlc-llm can be used for multiple GPU inference.
    *  llama-cpp will only use 1 GPU at a time.

**[Beyond LLMs? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1igu223/beyond_llms/)**
*   **Summary:** The post discusses if LLM's are the way to achieve AGI. The user believes LLMs are not just dumb autocomplete but rewriting engines.
*  **Emotion:** The tone of the thread is generally positive and informative.
*   **Top 3 Points of View:**
    *   LLMs are not the way to AGI.
    *  LLMs are not dumb autocomplete.
    *  LLMs are rewriting engines.

**[Does the DeepSeek "search" button just not work anymore, for anyone? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1igul54/does_the_deepseek_search_button_just_not_work/)**
*   **Summary:** Users are reporting that the DeepSeek "search" button is not working. Some users are experiencing an error message.
*   **Emotion:** The tone of the discussion is neutral and informative.
*   **Top 3 Points of View:**
    *   The search service is temporarily unavailable due to technical issues.
    *   The search function is not working for multiple users.
    *   The issue is unrelated to local LLMs

**[Lex' podcast: DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megacluster... (Score: 0)](https://youtube.com/watch?v=_1f-o0nqpEI&si=oPHY39vrkxW2eXKq)**
*   **Summary:** Users are commenting on Lex's podcast. Many find Lex to be a bad host and dislike some of the actions he has done recently, but his pure technology interviews are better.
*   **Emotion:** The thread contains mixed emotions, some positive (enjoying the content) and negative (disliking the host).
*  **Top 3 Points of View:**
    *   The podcast offers an insightful deep-dive into deep seek architecture and neural nets.
    *   Some people dislike the host because of his recent actions.
    *  The pure technology interviews might be better than his other work.

**[How can I use LLava with Deepseek? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1igwyzc/how_can_i_use_llava_with_deepseek/)**
*   **Summary:** A user is asking how to use LLava with the Deepseek model. A user responded stating that the full Deepseek doesn't have great function calling currently.
*   **Emotion:** The thread has a neutral tone.
*   **Top 3 Points of View:**
    *   It is important to specify if you are using the full Deepseek or a distill version.
    *   The full Deepseek model doesn't have great function calling.
    *   Using the right system prompts could help.

**[Deepseek doing weird answer (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1igxf33/deepseek_doing_weird_answer/)**
*   **Summary:** A user is experiencing the Deepseek model giving strange answers. Users advise that 8B models are too small, and a larger model such as 70B or the full 671B should be used.
*   **Emotion:** The overall emotion of the thread is neutral and informative.
*   **Top 3 Points of View:**
    *  8B Deepseek model is too small and should be upgraded to at least 70B.
    *  Models such as Deepseek (Distill) overthink answers.
    *  The models weird answers are normal.

**[Any open source projects similar to open AI's new deep research that I can run locally? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1igzegf/any_open_source_projects_similar_to_open_ais_new/)**
*   **Summary:** A user asked about open-source projects that are similar to Open AI's deep research tool that can be run locally. Users provided links to various open-source projects.
*   **Emotion:** The thread's tone is generally neutral and informative.
*  **Top 3 Points of View:**
    *   Deeper Seeker is an open-source project similar to Open AI's deep research tool.
    *   open-deep-research is another open-source project similar to Open AI's deep research tool.
    *  Co-Storm is also similar to Open AI's deep research tool.
