---
title: "Stable Diffusion Subreddit"
date: "2025-02-23"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [Can stuff like this be done in ComfyUI, where you take cuts from different images and blend them together to a single image?](https://www.reddit.com/gallery/1iwhrjm) (Score: 84)
    *   Discusses methods and tools within ComfyUI for blending and compositing images.
2.  [Veo 2 now supports image to video, incredible results ðŸ¤¯](https://v.redd.it/ubmwdou2cyke1) (Score: 3)
    *   A video showcasing the image-to-video capabilities of Veo 2, with discussion about its accessibility and VRAM requirements.
3.  [MEMO-AVATAR LipSync: the Best Open:Source Lip-Sync Software to Date](https://v.redd.it/al1j90wb4yke1) (Score: 2)
    *   Showcases MEMO-AVATAR LipSync, with discussions around its features, modifications for Windows, and VRAM requirements.
4.  [Mac Users what are your average time to run SDXL or Flux models ? and what you did for performance ?](https://www.reddit.com/r/StableDiffusion/comments/1iwi79d/mac_users_what_are_your_average_time_to_run_sdxl/) (Score: 2)
    *   Asks about SDXL and Flux model performance on Macs, along with optimization tips.
5.  [Why am I getting a NansException all of a sudden?](https://www.reddit.com/r/StableDiffusion/comments/1iwjzii/why_am_i_getting_a_nansexception_all_of_a_sudden/) (Score: 2)
    *   Asks for help with NansException and possible fixes.
6.  [Anyone have a workflow for 360 img or videos?](https://www.reddit.com/r/StableDiffusion/comments/1iwhjy9/anyone_have_a_workflow_for_360_img_or_videos/) (Score: 1)
    *   Asks for workflows for creating 360 images and videos.
7.  [Store that builds custom PCs for Stable Diffusion?](https://www.reddit.com/r/StableDiffusion/comments/1iwhvy7/store_that_builds_custom_pcs_for_stable_diffusion/) (Score: 1)
    *   Seeks recommendations for stores specializing in building PCs for Stable Diffusion.
8.  [Would a LORA trained on Diffusion Pipe w/ FP8 hunyuan work when inferenced with the FP16 version of the model?](https://www.reddit.com/r/StableDiffusion/comments/1iwi04u/would_a_lora_trained_on_diffusion_pipe_w_fp8/) (Score: 1)
    *   Inquires about the compatibility of LoRAs trained with FP8 with FP16 models.
9.  [flux1-dev-bnb-nf4-v2.safetensors](https://www.reddit.com/r/StableDiffusion/comments/1iwjek2/flux1devbnbnf4v2safetensors/) (Score: 1)
    *   Discusses sampler/scheduler combinations for Flux models.
10. [Should gender be tagged for SDXL/Pony Lora training?](https://www.reddit.com/r/StableDiffusion/comments/1iwjjhl/should_gender_be_tagged_for_sdxlpony_lora_training/) (Score: 1)
    *   Asks whether gender should be tagged for SDXL/Pony LoRA training.
11. [Fundamental Question (LoRA?)](https://www.reddit.com/r/StableDiffusion/comments/1iwk0aq/fundamental_question_lora/) (Score: 1)
    *   Discussion about making a LoRA.
12. [Illustrious Anime Flat Model](https://www.reddit.com/r/StableDiffusion/comments/1iwk956/illustrious_anime_flat_model/) (Score: 1)
    *   Asks about creating an Illustrious Anime Flat Model.
13. [Coming to life...](https://www.reddit.com/gallery/1iwkc8l) (Score: 0)
    *   Shares a ComfyUI workflow for generating anime-to-realistic images.

# Detailed Analysis by Thread
**[Can stuff like this be done in ComfyUI, where you take cuts from different images and blend them together to a single image? (Score: 84)](https://www.reddit.com/gallery/1iwhrjm)**
*  **Summary:** The original poster asks about creating blended images from different image cuts within ComfyUI. Commenters suggest tools, workflows, and nodes such as Krita with the ComfyUI plugin, IC-Light, DiffDiff, invokeAI, and low-denoise img2img. Some point to specific nodes like Regional-Prompting-FLUX and ComfyUI-BrushNet.
*  **Emotion:** The overall emotional tone is Neutral, as the comments primarily consist of suggestions and factual information. A small subset are positive offering words of encouragement.
*  **Top 3 Points of View:**
    *   Using Krita with the ComfyUI plugin is a viable solution.
    *   Specific nodes like Regional-Prompting-FLUX or ComfyUI-BrushNet can help.
    *   A low denoise img-img generation technique combined with Faceswap can achieve the desired result.

**[Veo 2 now supports image to video, incredible results ðŸ¤¯ (Score: 3)](https://v.redd.it/ubmwdou2cyke1)**
*  **Summary:** A user shares a video showcasing Veo 2's image-to-video capabilities. The comments focus on VRAM requirements, frustration about limited access to Veo 2, and a user noting the "bear looks" at the character in the video. There's also confusion about whether Veo 2 is open source.
*  **Emotion:** Predominantly Neutral, but contains an element of Negative stemming from frustration over limited access.
*  **Top 3 Points of View:**
    *   Users are interested in the VRAM requirements of Veo 2.
    *   Some are frustrated that Veo 2 isn't publicly available.
    *   One user humorously comments on a scene within the video.

**[MEMO-AVATAR LipSync: the Best Open:Source Lip-Sync Software to Date (Score: 2)](https://v.redd.it/al1j90wb4yke1)**
*  **Summary:** A user shares information about MEMO-AVATAR LipSync. Comments include a user modifying the repository for Windows, along with questions about VRAM requirements, inference time, and video-to-video capabilities.
*  **Emotion:** The general emotion is Neutral, with commenters mainly asking technical questions and sharing modifications.
*  **Top 3 Points of View:**
    *   A user has modified the software for Windows and will upload a tutorial.
    *   Users are concerned about VRAM requirements and inference time.
    *   There is interest in whether the software supports video-to-video lip-syncing.

**[Mac Users what are your average time to run SDXL or Flux models ? and what you did for performance ? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1iwi79d/mac_users_what_are_your_average_time_to_run_sdxl/)**
*  **Summary:** Users are discussing the performance of SDXL and Flux models on Macs, particularly focusing on generation times and performance optimization strategies. They share their experiences with different Mac models and software (like Draw Things and DiffusionBee) and various settings affecting performance.
*  **Emotion:** The overall emotional tone is Neutral, primarily focused on sharing technical information and seeking advice.
*  **Top 3 Points of View:**
    *   Mac M1 users are experiencing SDXL image generation in approximately 54 seconds.
    *   Draw Things has its own benchmarks validated by ArgMax team for their work on MLX
    *   DiffusionBee is a fast, free app for image generation.

**[Why am I getting a NansException all of a sudden? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1iwjzii/why_am_i_getting_a_nansexception_all_of_a_sudden/)**
*  **Summary:** A user is encountering a NansException error and seeks assistance.
*  **Emotion:** The overall emotion is Neutral, as the response provides a technical explanation and solution.
*  **Top 3 Points of View:**
    *   The NansException is likely due to issues with VAE precision.
    *   Adding "--no-half-vae" to command line arguments might fix it.
    *   Using the fp16 version of VAE could resolve the problem.

**[Anyone have a workflow for 360 img or videos? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1iwhjy9/anyone_have_a_workflow_for_360_img_or_videos/)**
*  **Summary:** The user seeks workflows for creating 360-degree images or videos.
*  **Emotion:** Neutral as the comments provide resources and links.
*  **Top 3 Points of View:**
    *   LoRA is a viable resource to create 360 degree images.
    *   Provides links to resources for creating panoramic images.

**[Store that builds custom PCs for Stable Diffusion? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1iwhvy7/store_that_builds_custom_pcs_for_stable_diffusion/)**
*  **Summary:** The user is asking for recommendation to find custom PCs for Stable Diffusion.
*  **Emotion:** Mixed between Neutral and Positive.
*  **Top 3 Points of View:**
    *   You just need machine with enough VRAM to run Stable Diffusion
    *   Any machine with nvidia gpu 8+ GB VRAM and 16 gb RAM if you want go cheap and easy use.

**[Would a LORA trained on Diffusion Pipe w/ FP8 hunyuan work when inferenced with the FP16 version of the model? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1iwi04u/would_a_lora_trained_on_diffusion_pipe_w_fp8/)**
*  **Summary:** The user asks if a LORA trained on Diffusion Pipe w/ FP8 hunyuan work when inferenced with the FP16 version of the model.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   The answer is yes.
    *   Most of my trainings were done at bf16.

**[flux1-dev-bnb-nf4-v2.safetensors (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1iwjek2/flux1devbnbnf4v2safetensors/)**
*  **Summary:** The user is asking about the flux model.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   Suggests DPM++ 2M with SGM Uniform and Euler with Beta sampler/schedulers.
    *   Ancestral, Karras and SDE are non-functional for DiT models like Flux and SD3.5

**[Should gender be tagged for SDXL/Pony Lora training? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1iwjjhl/should_gender_be_tagged_for_sdxlpony_lora_training/)**
*  **Summary:** The user is asking if they should tag the gender for SDXL/Pony Lora training.
*  **Emotion:** Positive
*  **Top 3 Points of View:**
    *   You should tag the gender since it will help you with better results during the actual use
    *   It's not necessary, but it does help model to figure out which gender the character is in relation to the trigger word.

**[Fundamental Question (LoRA?) (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1iwk0aq/fundamental_question_lora/)**
*  **Summary:** The user is asking about making a LoRA
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Making a LoRA is probably your best bet.

**[Illustrious Anime Flat Model (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1iwk956/illustrious_anime_flat_model/)**
*  **Summary:** The user is asking about Illustrious Anime Flat Model
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   If you need a more classical anime style, then try "anime screencap" or "anime coloring" in prompt

**[Coming to life... (Score: 0)](https://www.reddit.com/gallery/1iwkc8l)**
*  **Summary:** The user shares their ComfyUI workflow for generating anime-to-realistic images and provides a link to the workflow.
*  **Emotion:** Positive
*  **Top 3 Points of View:**
    *   Shares a ComfyUI workflow for generating anime-to-realistic images.
