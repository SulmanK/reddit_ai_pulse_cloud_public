---
title: "LocalLLaMA Subreddit"
date: "2025-03-29"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "LocalAI", "Models"]
---

# Overall Ranking and Top Discussions
1.  [First time testing: Qwen2.5:72b -> Ollama Mac + open-webUI -> M3 Ultra 512 gb](https://www.reddit.com/gallery/1jmqqxz) (Score: 65)
    *   A user shares their initial experiences testing the Qwen2.5:72b model on an M3 Ultra Mac using Ollama and open-webUI, leading to discussions about performance, alternative software like mlx and LMStudio, and suggestions for further testing scenarios, such as running larger models and evaluating real-world performance.
2.  [4x3090](https://i.redd.it/zi8ghi2ifore1.jpeg) (Score: 57)
    *   A user showcases their 4x3090 setup, prompting discussions about running larger models, using tensor parallelism, comparing performance with different operating systems, and optimizing configurations with tools like vllm's marlin AWQ engine and SGlang.
3.  [Seen a lot of setups but I had to laugh at this one. Price isn't terrible but with how it looks to be maintained I'd be worried about springing a leak.](https://i.redd.it/rvhj7wnchore1.png) (Score: 20)
    *   Users react to a quirky-looking local LLM setup, debating its practicality, aesthetics, and potential risks, with some appreciating its unique style while others express concerns about maintenance and potential leaks.
4.  [I Made a simple online tokenizer for any Hugging Face model](https://www.reddit.com/r/LocalLLaMA/comments/1jmqlii/i_made_a_simple_online_tokenizer_for_any_hugging/) (Score: 16)
    *   A user shares a simple online tokenizer tool for Hugging Face models, receiving positive feedback for its convenience and sparking discussions about the concept of tokenization and alternative tools.
5.  [[Build] A Beautiful Contradiction](https://v.redd.it/irhtg3os1ore1) (Score: 14)
    *   A user showcases a build inside a 2019 Mac Pro case with modern components, leading to discussions about the choice of components, cooling solutions, and the rationale behind using two PSUs.
6.  [SOTA 3d?](https://huggingface.co/spaces/VAST-AI/TripoSG) (Score: 8)
    *   A user asks if a tool is SOTA 3D, followed by another user sharing an image generated with it.
7.  [Cloud GPU suggestions for a privacy-conscious network engineer?](https://www.reddit.com/r/LocalLLaMA/comments/1jmoak6/cloud_gpu_suggestions_for_a_privacyconscious/) (Score: 6)
    *   A user requests cloud GPU suggestions for a privacy-conscious setup, prompting discussions about different cloud providers like Akashnet, RunPod, and vast.ai, with considerations for privacy, cost, model size, and enterprise solutions.
8.  [How The ChatGPT Voice and Video Mode Perform So Well](https://www.reddit.com/r/LocalLLaMA/comments/1jmraxr/how_the_chatgpt_voice_and_video_mode_perform_so/) (Score: 3)
    *   Discussion on ChatGPT voice and video mode. The API is great for English, but less so for other languages.
9.  [Benchmark for no comment editing](https://www.reddit.com/r/LocalLLaMA/comments/1jmqzu2/benchmark_for_no_comment_editing/) (Score: 2)
    *   Users discuss how some models will generate excessive comments, but then delete their own comments when prompted to fix a bug.
10. [Fastest LLM platform for Qwen/Deepseek/LLama?](https://www.reddit.com/r/LocalLLaMA/comments/1jmprik/fastest_llm_platform_for_qwendeepseekllama/) (Score: 1)
    *   A user asks about the fastest LLM platform. The answer is Groq.
11. [Open Router models](https://www.reddit.com/r/LocalLLaMA/comments/1jmmokm/open_router_models/) (Score: 0)
    *   Users discuss issues with Open Router models only accepting 1 image at a time.
12. [Best way to run Local LLMs on a single NVIDIA GPU?](https://www.reddit.com/r/LocalLLaMA/comments/1jmn40y/best_way_to_run_local_llms_on_a_single_nvidia_gpu/) (Score: 0)
    *   Users discuss the best ways to run local LLMs on a single NVIDIA GPU, recommending tools like Ollama, LM Studio, and llama.cpp, with considerations for ease of use, performance, and hardware requirements.
13. [Getting started. Thinking about GPT4ALL](https://www.reddit.com/r/LocalLLaMA/comments/1jmp6ty/getting_started_thinking_about_gpt4all/) (Score: 0)
    *   A user getting started with local LLMs is thinking about GPT4ALL, resulting in discussions about GPT4ALL vs LMStudio, as well as a Github project to mimic the chat environment.
14. [Is there an desktop app to connect ollama with?](https://www.reddit.com/r/LocalLLaMA/comments/1jmpjxf/is_there_an_desktop_app_to_connect_ollama_with/) (Score: 0)
    *   Users are looking for desktop apps to connect with Ollama, with users recommending apps like Open-WebUI, Chatbox, MarOs AI Chat, Enchanted-LLM, and Msty.
15. [What is the performance difference between 9070XT and 5070Ti when running LLMs?](https://www.reddit.com/r/LocalLLaMA/comments/1jmq0xe/what_is_the_performance_difference_between_9070xt/) (Score: 0)
    *   Users discuss the performance difference between RX 9070XT and 5070Ti when running LLMs.
16. [What model would you recommend for Image Generation with an M4 Mac Mini (16GB)](https://www.reddit.com/r/LocalLLaMA/comments/1jmq3sl/what_model_would_you_recommend_for_image/) (Score: 0)
    *   Users discuss which model to use for image generation with an M4 Mac Mini (16GB), and FLUX.1-schnell is recommended.
17. [I want to run AI models locally on my computer without policy restrictions](https://www.reddit.com/r/LocalLLaMA/comments/1jmq4q2/i_want_to_run_ai_models_locally_on_my_computer/) (Score: 0)
    *   A user wants to run AI models locally without policy restrictions, prompting discussions about which models to use as well as LM Studio vs Ollama.
18. [Which open source LLM to run locally on MacOS to generate Ghiblis style of images?](https://www.reddit.com/r/LocalLLaMA/comments/1jmupsd/which_open_source_llm_to_run_locally_on_macos_to/) (Score: 0)
    *   A user asks which open source LLM to run locally on MacOS to generate Ghiblis style images, and Flux is recommended.

# Detailed Analysis by Thread
**[First time testing: Qwen2.5:72b -> Ollama Mac + open-webUI -> M3 Ultra 512 gb (Score: 65)](https://www.reddit.com/gallery/1jmqqxz)**
*  **Summary:** A user shares their initial experiences testing the Qwen2.5:72b model on an M3 Ultra Mac using Ollama and open-webUI. They include a picture of their dog looking concerned.
*  **Emotion:** The overall emotional tone of the thread is positive and neutral. There's excitement about running a large model locally, coupled with neutral technical discussion and observational comments.
*  **Top 3 Points of View:**
    *   The M3 Ultra Mac can run large language models effectively.
    *   mlx is a better alternative to Ollama.
    *   There is a request for the user to make a YT video and run large models and see what the real world performance is.

**[4x3090 (Score: 57)](https://i.redd.it/zi8ghi2ifore1.jpeg)**
*  **Summary:** A user shows off their 4x3090 setup, prompting discussion about running larger models.
*  **Emotion:** The overall emotional tone is neutral, with a hint of positivity, as users admire the setup and offer technical advice.
*  **Top 3 Points of View:**
    *   With this setup, the user should be running 70b models minimum with big context.
    *   Consider model parallelism to split the model into chunks.
    *   The user should try SGlang instead of vllm.

**[Seen a lot of setups but I had to laugh at this one. Price isn't terrible but with how it looks to be maintained I'd be worried about springing a leak. (Score: 20)](https://i.redd.it/rvhj7wnchore1.png)**
*  **Summary:** Users react to a picture of a quirky-looking local LLM setup, with visible water cooling and exposed components.
*  **Emotion:** The emotional tone is mixed. Some users are negative, while others are positive. Some users are neutral, pointing out the practical aspects.
*  **Top 3 Points of View:**
    *   The setup looks cool and like something from an Alien movie.
    *   The sheer extent of embarrassment people would put themselves through rather than build a real server.
    *   Concerns about the setup's maintenance and the potential for leaks are expressed.

**[I Made a simple online tokenizer for any Hugging Face model (Score: 16)](https://www.reddit.com/r/LocalLLaMA/comments/1jmqlii/i_made_a_simple_online_tokenizer_for_any_hugging/)**
*  **Summary:** A user shares a simple online tokenizer tool for Hugging Face models.
*  **Emotion:** The emotional tone of the thread is generally positive. Users express appreciation for the tool's convenience.
*  **Top 3 Points of View:**
    *   The online tokenizer is convenient, saving users from having to open a Jupyter Notebook.
    *   There is a question about turning this into a library and whether the source code is available.
    *   An alternative tokenizer tool is provided.

**[[Build] A Beautiful Contradiction (Score: 14)](https://v.redd.it/irhtg3os1ore1)**
*  **Summary:** A user showcases a build inside a 2019 Mac Pro case with modern components.
*  **Emotion:** The overall emotional tone is positive and neutral, with users showing interest and asking questions about the build.
*  **Top 3 Points of View:**
    *   The build is impressive and well-executed.
    *   There are questions about why the builder chose to use two PSUs instead of one.
    *   There is a suggestion to stack four mac studios per side, interconnect with thunderbolt 5 and have an absolute absolute monster.

**[SOTA 3d? (Score: 8)](https://huggingface.co/spaces/VAST-AI/TripoSG)**
*  **Summary:** A user asks if a tool is SOTA 3D, followed by another user sharing an image generated with it.
*  **Emotion:** The emotional tone of the thread is neutral.
*  **Top 3 Points of View:**
    *   Is the tool SOTA 3D?
    *   The 3d model looks like Pickle Rick.

**[Cloud GPU suggestions for a privacy-conscious network engineer? (Score: 6)](https://www.reddit.com/r/LocalLLaMA/comments/1jmoak6/cloud_gpu_suggestions_for_a_privacy-conscious/)**
*  **Summary:** A user requests cloud GPU suggestions for a privacy-conscious setup.
*  **Emotion:** The overall emotional tone of the thread is neutral and helpful, with users providing suggestions and advice.
*  **Top 3 Points of View:**
    *   Akashnet is recommended, but may be too expensive.
    *   RunPod and vast.ai are recommended.
    *   AWS Bedrock is recommended because it is secure.

**[How The ChatGPT Voice and Video Mode Perform So Well (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1jmraxr/how_the_chatgpt_voice_and_video_mode_perform_so/)**
*  **Summary:** Discussion on ChatGPT voice and video mode.
*  **Emotion:** The emotional tone of the thread is neutral.
*  **Top 3 Points of View:**
    *   The realtime API is great for English.
    *   For less frequent languages it is not good enough.

**[Benchmark for no comment editing (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1jmqzu2/benchmark_for_no_comment_editing/)**
*  **Summary:** Users discuss how some models will generate excessive comments, but then delete their own comments when prompted to fix a bug.
*  **Emotion:** The emotional tone of the thread is neutral and funny.
*  **Top 3 Points of View:**
    *   The user should just learn how to prompt.
    *   It's funny that some models will generate excessive "obvious" comments.

**[Fastest LLM platform for Qwen/Deepseek/LLama? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1jmprik/fastest_llm_platform_for_qwendeepseekllama/)**
*  **Summary:** A user asks about the fastest LLM platform.
*  **Emotion:** The emotional tone of the thread is neutral.
*  **Top 3 Points of View:**
    *   Groq is by far the fastest for Llama. Nothing comes close.

**[Open Router models (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1jmmokm/open_router_models/)**
*  **Summary:** Users discuss issues with Open Router models only accepting 1 image at a time.
*  **Emotion:** The emotional tone of the thread is neutral.
*  **Top 3 Points of View:**
    *   Some models only accept 1 image at a time.

**[Best way to run Local LLMs on a single NVIDIA GPU? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1jmn40y/best_way_to_run_local_llms_on_a_single_nvidia_gpu/)**
*  **Summary:** Users discuss the best ways to run local LLMs on a single NVIDIA GPU.
*  **Emotion:** The overall emotional tone is neutral and helpful.
*  **Top 3 Points of View:**
    *   Ollama is a good option, but the user will have to set up their models manually.
    *   LM Studio is another good option.
    *   llama.cpp is also an option.

**[Getting started. Thinking about GPT4ALL (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1jmp6ty/getting_started_thinking_about_gpt4all/)**
*  **Summary:** A user getting started with local LLMs is thinking about GPT4ALL.
*  **Emotion:** The overall emotional tone is positive and neutral.
*  **Top 3 Points of View:**
    *   LMstudio is a good tool for playing around with AI.
    *    GPT4ALL is good for connection to documents but falls down a bit with the selection of models.
    *   Seek step-by-step guides, it's only complicated until you get used to it a bit.

**[Is there an desktop app to connect ollama with? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1jmpjxf/is_there_an_desktop_app_to_connect_ollama_with/)**
*  **Summary:** Users are looking for desktop apps to connect with Ollama.
*  **Emotion:** The overall emotional tone is positive and neutral.
*  **Top 3 Points of View:**
    *   OpenWebUI is good.
    *   Chatbox is good.
    *   MarOs AI Chat is good.

**[What is the performance difference between 9070XT and 5070Ti when running LLMs? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1jmq0xe/what_is_the_performance_difference_between_9070xt/)**
*  **Summary:** Users discuss the performance difference between RX 9070XT and 5070Ti when running LLMs.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Nvidia+CUDA is most efficient.
    *   AMD is about 1.5 times slower at least.
    *   Memory bandwidth can be used to determine the token generation speed.

**[What model would you recommend for Image Generation with an M4 Mac Mini (16GB) (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1jmq3sl/what_model_would_you_recommend_for_image/)**
*  **Summary:** Users discuss which model to use for image generation with an M4 Mac Mini (16GB).
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   FLUX.1-schnell is recommended.
    *   The user can use stable diffusion.
    *   A Q8 or Q6 should be used for Flux

**[I want to run AI models locally on my computer without policy restrictions (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1jmq4q2/i_want_to_run_ai_models_locally_on_my_computer/)**
*  **Summary:** A user wants to run AI models locally without policy restrictions.
*  **Emotion:** The overall emotional tone is positive and neutral.
*  **Top 3 Points of View:**
    *   LM Studio is the easiest way to install something and go. It works even with AMD.
    *   If you're looking for function/tool calling then Qwen2.5 7B.
    *   Yeah, good luck with that.

**[Which open source LLM to run locally on MacOS to generate Ghiblis style of images? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1jmupsd/which_open_source_llm_to_run_locally_on_macos_to/)**
*  **Summary:** A user asks which open source LLM to run locally on MacOS to generate Ghiblis style images.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   could try comfy with flux dev, but thats not licensed for selling, you can use flux schnell though afaik
