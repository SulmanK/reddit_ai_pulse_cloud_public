---
title: "Stable Diffusion Subreddit"
date: "2025-03-11"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [[D] Wan I2V 720p - can do anime motion fairly well (within reason)](https://v.redd.it/4vcjktvnf3oe1) - Score: 128
    *   The post discusses the capabilities of Kijai's Wan I2V workflow in creating anime motion, specifically testing its performance with a 720p screencap.

2.  [Wan2.1 8 bit Q Version RTX 4060ti 16GB 30 Min Video Gen Time - Quality is insane.](https://v.redd.it/ldf3l099j3oe1) - Score: 18
    *   This post highlights the video generation quality achieved with Wan2.1 on an RTX 4060ti, noting a 30-minute generation time.

3.  [Psychedelic Illusions Flux [dev] LoRA](https://www.reddit.com/gallery/1j8zlbu) - Score: 6
    *   A post about a super experimental Psychedelic Illusions Flux LoRA

4.  [SDXL Face Transfer](https://i.redd.it/b2gi4ug4d3oe1.png) - Score: 1
    *   The post discusses an SDXL Face Transfer using the ID transfer technique.

5.  [Regional prompting for videos? (or, two different character LORAs in one image)](https://www.reddit.com/r/StableDiffusion/comments/1j8urin/regional_prompting_for_videos_or_two_different/) - Score: 1
    *   A question about regional prompting for videos, or two different character LoRAs in one image

6.  [Any local ai tools to change how i look and sound?](https://www.reddit.com/r/StableDiffusion/comments/1j8z7rr/any_local_ai_tools_to_change_how_i_look_and_sound/) - Score: 1
    *   A question about local AI tools to change how someone looks and sounds

7.  [Happy Shanka from The First Law (credit to /u/Grubulon for the pic)](https://v.redd.it/oojky8px73oe1) - Score: 0
    *   A post showcasing a video creation of a "Happy Shanka" from The First Law using Wan2.1 I2V.

8.  [Flux.Dev FP8 take 40 Minutes to Generate One Image](https://www.reddit.com/r/StableDiffusion/comments/1j8wkaa/fluxdev_fp8_take_40_minutes_to_generate_one_image/) - Score: 0
    *   This post discusses slow image generation times with Flux.Dev FP8.

9.  [Wan Image2Video - RTX 5080 workflow?](https://www.reddit.com/r/StableDiffusion/comments/1j8x13x/wan_image2video_rtx_5080_workflow/) - Score: 0
    *   A question about Wan Image2Video and a workflow for RTX 5080

# Detailed Analysis by Thread
**[[D] Wan I2V 720p - can do anime motion fairly well (within reason)](https://v.redd.it/4vcjktvnf3oe1) (Score: 128)**
*  **Summary:** The thread discusses the capabilities of Kijai's Wan I2V workflow in creating anime motion, specifically testing its performance with a 720p screencap. The original poster details their workflow and observations on the model's ability to handle various actions and complexities in anime-style animation.
*  **Emotion:** The overall emotional tone is neutral, with pockets of positive sentiment. The initial post is largely informative and analytical, while some comments express excitement and appreciation.
*  **Top 3 Points of View:**
    *   Wan I2V is capable of handling simpler anime actions effectively, but struggles with more complex scenarios.
    *   The model's performance is impressive, considering the training data primarily consists of horizontal scenes, while anime is often vertical.
    *   While already practically useful, the model could benefit from features like motion brushes and mid-/end-frame conditioning for introducing new content within a scene.

**[Wan2.1 8 bit Q Version RTX 4060ti 16GB 30 Min Video Gen Time - Quality is insane.](https://v.redd.it/ldf3l099j3oe1) (Score: 18)**
*  **Summary:** This thread showcases the quality of video generation achieved using Wan2.1 on an RTX 4060ti 16GB, with a generation time of 30 minutes. The comments include requests for the prompt and sampler parameters used in the generation.
*  **Emotion:** The emotional tone is primarily neutral, with some comments expressing excitement ("crazy").
*  **Top 3 Points of View:**
    *   Wan2.1 produces high-quality video.
    *   Details regarding prompts and parameters are requested.
    *   The user shares the prompt that was used to create the macro shot of snowflakes.

**[Psychedelic Illusions Flux [dev] LoRA](https://www.reddit.com/gallery/1j8zlbu) (Score: 6)**
*  **Summary:** A thread to showcase an experimental Psychedelic Illusions Flux LoRA and provide links to try it for free or download the LoRA.
*  **Emotion:** The emotional tone is primarily neutral.
*  **Top 3 Points of View:**
    *   The poster created a super experimental LoRA for Psychedelic Illusions.
    *   User is able to try it for free or download.
    *   The poster asked how one would prompt these.

**[SDXL Face Transfer](https://i.redd.it/b2gi4ug4d3oe1.png) (Score: 1)**
*  **Summary:** The thread is about testing an ID transfer technique in diffusers SDXL pipeline combining controlnet with inpaint.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   The poster is testing an ID transfer technique in diffusers SDXL pipeline combining controlnet with inpaint.
    *   The user asked if it is possible to push further like change the style like make it color line art.
    *   The image used in the post does not have a different pose.

**[Regional prompting for videos? (or, two different character LORAs in one image)](https://www.reddit.com/r/StableDiffusion/comments/1j8urin/regional_prompting_for_videos_or_two_different/) (Score: 1)**
*  **Summary:** A question about regional prompting for videos, or two different character LoRAs in one image.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Regional prompting with multiple LoRAs even with images is a headache.
    *   Regional prompting is the same as applying different text conditioning on different parts of the image.
    *   Something like an inpainting would work better.

**[Any local ai tools to change how i look and sound?](https://www.reddit.com/r/StableDiffusion/comments/1j8z7rr/any_local_ai_tools_to_change_how_i_look_and_sound/) (Score: 1)**
*  **Summary:** A question about local AI tools to change how someone looks and sounds
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Yes there are many tools for that.
    *   Real time ai voice changers and avatars are available
    *   Reddit is full recommendations for these already, just gotta look for it yourself.

**[Happy Shanka from The First Law (credit to /u/Grubulon for the pic)](https://v.redd.it/oojky8px73oe1) (Score: 0)**
*  **Summary:** The poster shares an AI-generated video of a character (Shanka) from "The First Law" series using Wan2.1 I2V. The post includes the specific prompt used for the generation.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   A video was created using Wan2.1 I2V 480pt
    *   The prompt used to create the video was: An *** humanoid neanderthal creature with crooked sharp teeth and drool turns directly towards the camera to look at the viewer with both eyes, and smiles wide menancingly. It has pointed ears, small dark eyes and is balding with a prominent forehead. It has mutton chop beard and a hairy body.
    *   The user gave credit to Grubulon for the picture.

**[Flux.Dev FP8 take 40 Minutes to Generate One Image](https://www.reddit.com/r/StableDiffusion/comments/1j8wkaa/fluxdev_fp8_take_40_minutes_to_generate_one_image/) (Score: 0)**
*  **Summary:** The thread discusses the issue of extremely slow image generation times (40 minutes) when using Flux.Dev FP8. Several users offer suggestions and comparisons to their own experiences with different hardware configurations.
*  **Emotion:** The emotional tone is mixed, with neutral comments providing advice and slightly negative comments expressing disbelief or suggesting the user is doing something wrong.
*  **Top 3 Points of View:**
    *   40 minutes for image generation with Flux.Dev FP8 is not normal and indicates a problem.
    *   Possible causes include VRAM offloading to system RAM, incorrect quantization or precision settings, or generating an image that is too large or with too many steps.
    *   Users share their own generation times with different GPUs and settings for comparison.

**[Wan Image2Video - RTX 5080 workflow?](https://www.reddit.com/r/StableDiffusion/comments/1j8x13x/wan_image2video_rtx_5080_workflow/) (Score: 0)**
*  **Summary:** This thread is about how to set up Wan Image2Video with RTX 5080.
*  **Emotion:** The emotional tone is primarily positive and neutral, with users offering helpful advice and resources.
*  **Top 3 Points of View:**
    *   Download Pinokio
    *   User may need a version of Comfy that is compatible with Blackwell.
    *   Check out the blog to get the video model native support.
