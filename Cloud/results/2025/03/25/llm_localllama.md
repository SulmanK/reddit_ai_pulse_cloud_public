---
title: "LocalLLaMA Subreddit"
date: "2025-03-25"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "AI", "DeepSeek"]
---

# Overall Ranking and Top Discussions
1.  [Deepseek v3](https://i.redd.it/xaic503gbqqe1.jpeg) (Score: 356)
    *   This thread discusses the Deepseek v3 model and its potential impact on OpenAI.
2.  [New DeepSeek benchmark scores](https://i.redd.it/smu0dyp3rpqe1.jpeg) (Score: 322)
    *   This thread is about the new benchmark scores of DeepSeek.
3.  [Change log of DeepSeek-V3-0324](https://www.reddit.com/r/LocalLLaMA/comments/1jj9d5c/change_log_of_deepseekv30324/) (Score: 69)
    *   This thread discusses the change log of DeepSeek-V3-0324
4.  [Deep seek V3 03 24 TESTED. Beats Sonnet & Open AI 4-o](https://www.reddit.com/r/LocalLLaMA/comments/1jj2yqt/deep_seek_v3_03_24_tested_beats_sonnet_open_ai_4o/) (Score: 65)
    *   This thread is about Deep seek V3 beating Sonnet & Open AI 4-o
5.  [Deepseek-v3-0324 on Aider](https://i.redd.it/ssol9q8ecrqe1.png) (Score: 49)
    *   This thread discusses Deepseek-v3-0324 usage on Aider and its cost compared to other models.
6.  [Gemma 3 x P102-100  squad.](https://i.redd.it/a5argvrwdqqe1.jpeg) (Score: 12)
    *   This thread discusses the compatibility of Gemma 3 with P102-100 GPUs.
7.  [One shot website (DeepSeek V3.1)](https://www.reddit.com/r/LocalLLaMA/comments/1jjaall/one_shot_website_deepseek_v31/) (Score: 9)
    *   This thread discusses a one-shot website using DeepSeek V3.1
8.  [Gemma3 vision in llama.cpp](https://www.reddit.com/r/LocalLLaMA/comments/1jj2a2d/gemma3_vision_in_llamacpp/) (Score: 6)
    *   This thread discusses Gemma3 vision being used in llama.cpp
9.  [DeepSeek dethroned on MMLU-Pro leaderboard](https://www.reddit.com/r/LocalLLaMA/comments/1jj54k5/deepseek_dethroned_on_mmlupro_leaderboard/) (Score: 6)
    *   This thread is about DeepSeek being dethroned on the MMLU-Pro leaderboard.
10. [Best AI for summarizing technical or scientific papers?](https://www.reddit.com/r/LocalLLaMA/comments/1jj2zs5/best_ai_for_summarizing_technical_or_scientific/) (Score: 5)
    *   This thread discusses the best AI model for summarizing technical or scientific papers.
11. [Is Image input possible on android?](https://www.reddit.com/r/LocalLLaMA/comments/1jj3dyq/is_image_input_possible_on_android/) (Score: 3)
    *   This thread asks if image input is possible on Android.
12. [Searching for Good Audio Tokenizer](https://www.reddit.com/r/LocalLLaMA/comments/1jj3hjx/searching_for_good_audio_tokenizer/) (Score: 3)
    *   This thread is about searching for a good audio tokenizer.
13. [what finetuning tool/library do you recommend](https://www.reddit.com/r/LocalLLaMA/comments/1jj3txz/what_finetuning_toollibrary_do_you_recommend/) (Score: 2)
    *   This thread asks for recommendations on finetuning tools/libraries.
14. [Am I missing something when trying to run a vision model?](https://www.reddit.com/r/LocalLLaMA/comments/1jj476v/am_i_missing_something_when_trying_to_run_a/) (Score: 1)
    *   This thread discusses problems running a vision model
15. [An Open Source Phone Use Agent with OmniParser and Qwen2.5 VL](https://youtu.be/6qlNYhquk3g) (Score: 1)
    *   This thread discusses a phone use agent that lets you control an Android phone using natural language commands.
16. [Looking for someone to make a tool - or a website where i can explain what i need and request someone to do it.](https://www.reddit.com/r/LocalLLaMA/comments/1jj2bet/looking_for_someone_to_make_a_tool_or_a_website/) (Score: 0)
    *   This thread is about looking for someone to make a tool
17. [How to keep a model in memory?](https://www.reddit.com/r/LocalLLaMA/comments/1jj63ur/how_to_keep_a_model_in_memory/) (Score: 0)
    *   This thread is about keeping a model in memory.
18. [2-step deepseek v3 endpoint](https://www.reddit.com/r/LocalLLaMA/comments/1jjagt2/2step_deepseek_v3_endpoint/) (Score: 0)
    *   This thread discusses a 2-step deepseek v3 endpoint
19. [Implications for local LLM scene if Trump does a full Nvidia ban in China](https://www.reddit.com/r/LocalLLaMA/comments/1jjblbt/implications_for_local_llm_scene_if_trump_does_a/) (Score: 0)
    *   This thread discusses the potential implications of a full Nvidia ban in China on the local LLM scene.

# Detailed Analysis by Thread
**[Deepseek v3 (Score: 356)](https://i.redd.it/xaic503gbqqe1.jpeg)**
*  **Summary:** This thread discusses the Deepseek v3 model, its potential impact on OpenAI, and the feasibility of running LLMs locally. Some users express excitement about the model, while others discuss hardware requirements and costs.
*  **Emotion:** The emotional tone is mostly neutral with some positive and negative comments.
*  **Top 3 Points of View:**
    *   Deepseek v3 poses a potential threat to OpenAI, especially if it becomes multimodal.
    *   The cost and hardware requirements (e.g., needing a $10k Mac) are a barrier to local use.
    *   The pricing on platforms like Fireworks and DeepInfra makes Deepseek v3 attractive.

**[New DeepSeek benchmark scores (Score: 322)](https://i.redd.it/smu0dyp3rpqe1.jpeg)**
*  **Summary:** This thread focuses on the new benchmark scores of DeepSeek, with comparisons to models like Sonnet. Users discuss the implications of the scores, express excitement about future versions (R1), and criticize the benchmark methodology.
*  **Emotion:** The emotional tone is mostly neutral with some positive and negative comments.
*  **Top 3 Points of View:**
    *   DeepSeek's performance is impressive, especially compared to Sonnet 3.7.
    *   There is anticipation for the release of R1 as a state-of-the-art coder.
    *   The benchmark methodology (only 4 problems) is questionable.

**[Change log of DeepSeek-V3-0324 (Score: 69)](https://www.reddit.com/r/LocalLLaMA/comments/1jj9d5c/change_log_of_deepseekv30324/)**
*  **Summary:** This thread analyzes the change log of DeepSeek-V3-0324, focusing on enhanced reasoning abilities and performance jumps. Users question the definition of "reasoning" in the context of the model and discuss the limitations of the AIME test as a benchmark.
*  **Emotion:** The emotional tone is predominantly neutral, with elements of positive sentiment regarding the advancements in open weight models.
*  **Top 3 Points of View:**
    *   The progress of open weight models is encouraging, even if local usage is limited for many.
    *   The term "enhanced reasoning abilities" needs clarification, as it may not refer to traditional Chain-of-Thought reasoning.
    *   The AIME test's limited sample size and potential for answer memorization should be considered when evaluating the model's performance.

**[Deep seek V3 03 24 TESTED. Beats Sonnet & Open AI 4-o (Score: 65)](https://www.reddit.com/r/LocalLLaMA/comments/1jj2yqt/deep_seek_v3_03_24_tested_beats_sonnet_open_ai_4o/)**
*  **Summary:** This thread discusses the testing of Deepseek V3, with claims that it outperforms Sonnet and OpenAI 4-o. Users request benchmark results and highlight the advantage of V3 being a non-SaaS option.
*  **Emotion:** Positive with request
*  **Top 3 Points of View:**
    *   Deepseek V3 beats Sonnet.
    *   Users want the raw data
    *   Deepseek is non SaaS

**[Deepseek-v3-0324 on Aider (Score: 49)](https://i.redd.it/ssol9q8ecrqe1.png)**
*  **Summary:** This thread discusses the performance and cost of using Deepseek-v3-0324 on Aider. The cost is a recurring point.
*  **Emotion:** Positive to Neutral.
*  **Top 3 Points of View:**
    *   Users are impressed with the performance.
    *   Cost is a major factor.
    *   Some find the presented information difficult to read.

**[Gemma 3 x P102-100 squad. (Score: 12)](https://i.redd.it/a5argvrwdqqe1.jpeg)**
*  **Summary:** This thread discusses the use of Gemma 3 with P102-100 GPUs.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   Gemma 3 works fine with llama.cpp.
    *   vLLM can be tricky because of poor FP16 performance.
    *   Model loading times can be a problem.

**[One shot website (DeepSeek V3.1) (Score: 9)](https://www.reddit.com/r/LocalLLaMA/comments/1jjaall/one_shot_website_deepseek_v31/)**
*  **Summary:** This thread discusses a "one-shot" website potentially using DeepSeek V3.1. There's excitement about DeepSeek's progress and the potential of R2.
*  **Emotion:** Primarily Neutral
*  **Top 3 Points of View:**
    *   Deepseek is cooking.
    *   It is unknown where you can try V3 for free.
    *   DeepSeek is making up names.

**[Gemma3 vision in llama.cpp (Score: 6)](https://www.reddit.com/r/LocalLLaMA/comments/1jj2a2d/gemma3_vision_in_llamacpp/)**
*  **Summary:** This thread discusses the implementation of Gemma3 vision within the llama.cpp framework. Users share their experiences and compatible models.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   Gemma3 vision works in Koboldcpp.
    *   The Bartowski models are still split into an mmproj.
    *   Ollama also supports Gemma-3 multimodal.

**[DeepSeek dethroned on MMLU-Pro leaderboard (Score: 6)](https://www.reddit.com/r/LocalLLaMA/comments/1jj54k5/deepseek_dethroned_on_mmlupro_leaderboard/)**
*  **Summary:** This thread discusses the dethroning of DeepSeek on the MMLU-Pro leaderboard. Users make comments about its coding abilities.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   Hunyuan-T1 is not as good as R1 in coding.

**[Best AI for summarizing technical or scientific papers? (Score: 5)](https://www.reddit.com/r/LocalLLaMA/comments/1jj2zs5/best_ai_for_summarizing_technical_or_scientific/)**
*  **Summary:** The thread discusses the best AI model for summarizing technical or scientific papers, with a suggestion to use llm-pdf.py for testing and mentions of O1-pro as a good option.
*  **Emotion:** Mostly Neutral
*  **Top 3 Points of View:**
    *   O1-pro is recommended due to efficient context usage.
    *   llm-pdf.py can be used for testing.

**[Is Image input possible on android? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1jj3dyq/is_image_input_possible_on_android/)**
*  **Summary:** This thread discusses the possibility of using image input on Android devices.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Qwen2-vl and Moondream are good models.

**[Searching for Good Audio Tokenizer (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1jj3hjx/searching_for_good_audio_tokenizer/)**
*  **Summary:** The thread is about finding a good audio tokenizer, with a suggestion to try wavtokenizer.
*  **Emotion:** Neutral to Positive
*  **Top 3 Points of View:**
    *   Wavtokenizer is a viable option and can be trained in 48k.
    *   Scratch on MacOS is not recommended.

**[what finetuning tool/library do you recommend (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1jj3txz/what_finetuning_toollibrary_do_you_recommend/)**
*  **Summary:** This thread asks for recommendations on finetuning tools/libraries.
*  **Emotion:** Neutral to Positive.
*  **Top 3 Points of View:**
    *   HF, Unsloth, and MLX are recommended.
    *   llama-factory is really cool.
    *   qlora-pipe is recommended.

**[Am I missing something when trying to run a vision model? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1jj476v/am_i_missing_something_when_trying_to_run_a/)**
*  **Summary:** The thread discusses potential issues when running a vision model, specifically related to GGUF file metadata.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   The user may need to use llama-gemma3-cli, not llama-llava-cli.
    *   There may be an issue with the GGUF file format or the version of llama.cpp.

**[An Open Source Phone Use Agent with OmniParser and Qwen2.5 VL (Score: 1)](https://youtu.be/6qlNYhquk3g)**
*  **Summary:** Phone Use Agent lets you control an Android phone using natural language commands.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   The agent uses Microsoft's OmniParser, Qwen2.5VL 3B/vLLM and ADB to navigate around the phone and input text depending on what prompt you give it.

**[Looking for someone to make a tool - or a website where i can explain what i need and request someone to do it. (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1jj2bet/looking_for_someone_to_make_a_tool_or_a_website/)**
*  **Summary:** The thread is about someone looking for a tool or website to explain their needs and request someone to build it.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   Context issues may arise with a large number of pages.
    *   smallDocling can be used to OCR the PDFs.
    *   Make a job listing.

**[How to keep a model in memory? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1jj63ur/how_to_keep_a_model_in_memory/)**
*  **Summary:** The thread discusses how to keep a model in memory using Ollama, suggesting the use of the OLLAMA_KEEP_ALIVE variable.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   Set the OLLAMA_KEEP_ALIVE variable to "-1" for infinite keep-alive.
    *   Set the OLLAMA_KEEP_ALIVE variable to 360 in seconds.
    *   If using Linux, set persistence mode on the GPU.

**[2-step deepseek v3 endpoint (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1jjagt2/2step_deepseek_v3_endpoint/)**
*  **Summary:** The thread involves a discussion about a 2-step deepseek v3 endpoint.
*  **Emotion:** Negative
*  **Top 3 Points of View:**
    *   The endpoint isn't perfect.

**[Implications for local LLM scene if Trump does a full Nvidia ban in China (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1jjblbt/implications_for_local_llm_scene_if_trump_does_a/)**
*  **Summary:** The thread discusses the potential implications of a full Nvidia ban in China on the local LLM scene.
*  **Emotion:** Neutral to Positive
*  **Top 3 Points of View:**
    *   China is capable of innovating without Nvidia.
    *   The current Blackwell generation of GPUs is not ideal.
    *   The world should not hinder the development of AI.
