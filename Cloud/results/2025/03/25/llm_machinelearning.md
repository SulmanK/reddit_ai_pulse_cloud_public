---
title: "Machine Learning Subreddit"
date: "2025-03-25"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "NLP"]
---

# Overall Ranking and Top Discussions
1.  [[D] "Topological" Deep Learning - Promising or Hype?](https://www.reddit.com/r/MachineLearning/comments/1ji6xlv/d_topological_deep_learning_promising_or_hype/) (Score: 82)
    * The discussion revolves around the potential of topological deep learning and whether it's truly a promising direction or just another hype in the field, particularly in comparison to GNNs and geometric deep learning.
2.  [[D] ICML 2025 review discussion](https://www.reddit.com/r/MachineLearning/comments/1jiuqib/d_icml_2025_review_discussion/) (Score: 76)
    * This thread is about the ICML 2025 review process, with people sharing their anxieties and hopes regarding their submissions and the reviewer process.
3.  [[D] Relationship between loss and lr schedule](https://www.reddit.com/gallery/1jilo1l) (Score: 55)
    * The thread discusses the relationship between the loss function and the learning rate schedule during neural network training.
4.  [[D] Reviewed several ACL papers on data resources and feel that LLMs are undermining this field](https://www.reddit.com/r/MachineLearning/comments/1jihs98/d_reviewed_several_acl_papers_on_data_resources/) (Score: 41)
    * Discussion centers around concerns that LLMs might be negatively impacting the quality and integrity of data resource papers submitted to ACL, especially in the context of low-resource languages.
5.  [[P] Local AI Voice Assistant with Ollama + gTTS](https://www.reddit.com/r/MachineLearning/comments/1jidrkx/p_local_ai_voice_assistant_with_ollama_gtts/) (Score: 20)
    * A user shares a local AI voice assistant project built with Ollama and gTTS.
6.  [[D] Conformal Prediction in Industry](https://www.reddit.com/r/MachineLearning/comments/1ji1s82/d_conformal_prediction_in_industry/) (Score: 7)
    * The thread discusses the use of conformal prediction in industry and seeks resources and experiences related to its practical application.
7.  [[R] How can I dynamically estimate parameters A and B in this equation: DeltaP[t+1] = A*DeltaP[t] + B*Qp ?](https://www.reddit.com/r/MachineLearning/comments/1jifxob/r_how_can_i_dynamically_estimate_parameters_a_and/) (Score: 6)
    * A user asks for advice on how to dynamically estimate parameters in a given equation, and others suggest statistical inference and specific algorithms.
8.  [[D] How are you handling reproducibility in your ML work?](https://www.reddit.com/r/MachineLearning/comments/1ji57cm/d_how_are_you_handling_reproducibility_in_your_ml/) (Score: 5)
    * The thread discusses different approaches to handling reproducibility in machine learning projects, including version control, DVC pipelines, and AWS SageMaker pipelines.
9.  [[D] What exactly counts as “uncertainty quantification”?](https://www.reddit.com/r/MachineLearning/comments/1jj1249/d_what_exactly_counts_as_uncertainty/) (Score: 4)
    * This thread asks what counts as "uncertainty quantification" and relates it to acquisition functions.
10. [[P] Why do the NaN inputs increase the model output? Does this SHAP plot look concerning?](https://www.reddit.com/r/MachineLearning/comments/1jhwz0h/p_why_do_the_nan_inputs_increase_the_model_output/) (Score: 2)
    * A user is trying to understand why NaN inputs increase the model output and asks if a SHAP plot looks concerning.
11. [[R] Best Loss for RDH Task](https://www.reddit.com/r/MachineLearning/comments/1ji1x2t/r_best_loss_for_rdh_task/) (Score: 1)
    * This thread discusses the best loss functions for a Reversible Data Hiding task.
12. [[P] and [D] Country Recognition Model???](https://www.reddit.com/r/MachineLearning/comments/1jifoxl/p_and_d_country_recognition_model/) (Score: 1)
    * User is asking for suggestions to approach the building of a country recognition model.
13. [[P] I Built a FAANG Job Board for ML Engineers – Only Jobs Scraped in the Last 24h](https://www.reddit.com/r/MachineLearning/comments/1jhxknz/p_i_built_a_faang_job_board_for_ml_engineers_only/) (Score: 0)
    * A user shares a job board specifically for ML engineers at FAANG companies.
14. [[D] Is MCP really a solution… or just another layer we don’t need?](https://www.reddit.com/r/MachineLearning/comments/1ji7cx3/d_is_mcp_really_a_solution_or_just_another_layer/) (Score: 0)
    * The discussion questions the value and necessity of using MCP (presumably a model serving framework) and whether it introduces unnecessary complexity.
15. [[Discussion] What Does GPU On-Demand Pricing Mean and How Can I Optimize Server Run-Time?](https://www.reddit.com/r/MachineLearning/comments/1jicpo2/discussion_what_does_gpu_ondemand_pricing_mean/) (Score: 0)
    * The thread discusses the meaning of on-demand GPU pricing and strategies for optimizing server run-time to reduce costs.
16. [[D] Is the term "interference" used?](https://www.reddit.com/r/MachineLearning/comments/1jils6h/d_is_the_term_interference_used/) (Score: 0)
    * A user asks if the term "interference" is used in the context of ML, specifically regarding a request reaching the model.
17. [[D] Seeking PhD Supervisor in ML/NLP/Explainable AI (Europe-Based) – Recommendations?](https://www.reddit.com/r/MachineLearning/comments/1jj4l8g/d_seeking_phd_supervisor_in_mlnlpexplainable_ai/) (Score: 0)
    * A user asks for recommendations for PhD supervisors in Europe specializing in ML, NLP, and Explainable AI.

# Detailed Analysis by Thread

**[[D] "Topological" Deep Learning - Promising or Hype? (Score: 82)](https://www.reddit.com/r/MachineLearning/comments/1ji6xlv/d_topological_deep_learning_promising_or_hype/)**
*  **Summary:** This thread explores the potential and practicality of Topological Deep Learning (TDL), questioning whether it offers genuine advancements or is simply overhyped. Contributors discuss its relationship to Geometric Deep Learning (GDL) and Graph Neural Networks (GNNs), touching on applications, computational costs, and current research limitations.
*  **Emotion:** The overall emotional tone is Neutral, with discussions focusing on technical aspects and reasoned arguments.
*  **Top 3 Points of View:**
    * TDL might just be adding extra complexity to what GNNs can already handle, and real benefits are yet to be seen.
    * TDL is a good complementary tool to GDL. TDL looks at the substructures of the data and GDL the overall structures of the data.
    * TDL tends to be GNN with n-node /n-edge interaction, so it's the same just a focus on higher order interactions, but modeling higher orders is more expensive.

**[[D] ICML 2025 review discussion (Score: 76)](https://www.reddit.com/r/MachineLearning/comments/1jiuqib/d_icml_2025_review_discussion/)**
*  **Summary:** A thread where researchers share their feelings and concerns regarding the ICML 2025 review process. Topics include the anxiety around reviews, the nature of rebuttal processes, and the general pressure to get accepted into top-tier conferences.
*  **Emotion:** The emotional tone is mixed, with Positive sentiments like "Good luck" balanced by Negative sentiments like "scared" and "stress".
*  **Top 3 Points of View:**
    * People wish each other luck with the ICML review process.
    * There is stress and anxiety associated with waiting for reviews.
    * There is a desire for a real-time discussion similar to ICLR.

**[[D] Relationship between loss and lr schedule (Score: 55)](https://www.reddit.com/gallery/1jilo1l)**
*  **Summary:** The thread investigates the observed relationship between the loss function and the learning rate (LR) schedule during neural network training. Contributors analyze periodic loss patterns, the effect of LR decay, and the role of batch size in gradient descent.
*  **Emotion:** The emotional tone is largely Neutral, characterized by objective observations and analytical explanations.
*  **Top 3 Points of View:**
    * Loss decreases quickly at the start, then slows down, but once the lr is dropped the loss starts improving faster again until the cycle repeats.
    * Larger batches are better at approximating gradient descent because they introduce less noise into gradients.
    * Lr introduces some discretization error proportional to lr. Gradient descent moves you better toward x0 until numerical issues take over again.

**[[D] Reviewed several ACL papers on data resources and feel that LLMs are undermining this field (Score: 41)](https://www.reddit.com/r/MachineLearning/comments/1jihs98/d_reviewed_several_acl_papers_on_data_resources/)**
*  **Summary:** The thread reflects on a reviewer's experience with ACL papers, expressing concern that the ease of generating synthetic data with LLMs is leading to a decline in the quality and originality of data resource papers, particularly for low-resource languages. The discussion questions the value of new datasets created using LLMs.
*  **Emotion:** The emotional tone is primarily Negative due to concerns about declining data quality, though also neutral in other areas.
*  **Top 3 Points of View:**
    * EMNLP papers with bad data quality are getting accepted for research in low-resource languages.
    * The quality of data created by manually created datasets is often questionable.
    * The ease-of-use of LLMs has opened the floodgates for more submission attempts to create new datasets.

**[[P] Local AI Voice Assistant with Ollama + gTTS (Score: 20)](https://www.reddit.com/r/MachineLearning/comments/1jidrkx/p_local_ai_voice_assistant_with_ollama_gtts/)**
*  **Summary:** A user shares a local AI voice assistant project built with Ollama and gTTS.
*  **Emotion:** The emotional tone is generally Positive, with a user expressing thanks and need for the project.
*  **Top 3 Points of View:**
    * User needed this and was about to build this with sesame 1b model.

**[[D] Conformal Prediction in Industry (Score: 7)](https://www.reddit.com/r/MachineLearning/comments/1ji1s82/d_conformal_prediction_in_industry/)**
*  **Summary:** The thread explores the application of conformal prediction in industry settings, with users sharing relevant resources and experiences.
*  **Emotion:** The emotional tone is Neutral overall, with users providing information and sharing anecdotes.
*  **Top 3 Points of View:**
    * Seeking a short intro to conformal prediction.
    * The author of this book is a senior guy at Wells Fargo so I’m assuming they are using it.
    * A blog post from an industry lab talking about how conformal prediction could actually be used in practice.

**[[R] How can I dynamically estimate parameters A and B in this equation: DeltaP[t+1] = A*DeltaP[t] + B*Qp ? (Score: 6)](https://www.reddit.com/r/MachineLearning/comments/1jifxob/r_how_can_i_dynamically_estimate_parameters_a_and/)**
*  **Summary:** A user seeks advice on dynamically estimating parameters in a specified equation.
*  **Emotion:** The thread's tone is Neutral and helpful, focusing on providing solutions.
*  **Top 3 Points of View:**
    * Statistical inference on switching linear dynamical systems.
    * Algorithm 1 in Section 2.1.2 of [this dissertation](https://guptagaurav.me/docs/dissertation.pdf#page34) uses EM algorithm
    * A suggestion to check StackExchange websites.

**[[D] How are you handling reproducibility in your ML work? (Score: 5)](https://www.reddit.com/r/MachineLearning/comments/1ji57cm/d_how_are_you_handling_reproducibility_in_your_ml/)**
*  **Summary:** The thread discusses various methods and tools used to ensure reproducibility in machine learning workflows.
*  **Emotion:** The overall emotional tone is Neutral, focusing on practical solutions and technical details.
*  **Top 3 Points of View:**
    * Using same seed and making cudnn deterministic.
    * AWS SageMager pipelines can track all the metadata and artifacts required to reproduce results, especially if your company is already on AWS.
    * mlflow and good tooling that makes sure everything is tracked and logged.

**[[D] What exactly counts as “uncertainty quantification”? (Score: 4)](https://www.reddit.com/r/MachineLearning/comments/1jj1249/d_what_exactly_counts_as_uncertainty/)**
*  **Summary:** The thread tries to define what "uncertainty quantification" means in ML and how it related to acquisition functions.
*  **Emotion:** The emotional tone is neutral with analytical explanations.
*  **Top 3 Points of View:**
    * Uncertainty quantification as a process to describe how information (or lack of) is transferred from data and model to the output.
    * Acquisition functions measure the utility to acquire new data.
    * They are related because typical acquisition functions want some measure of uncertainty.

**[[P] Why do the NaN inputs increase the model output? Does this SHAP plot look concerning? (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1jhwz0h/p_why_do_the_nan_inputs_increase_the_model_output/)**
*  **Summary:** The thread discusses the impact of NaN inputs on a model's output, specifically concerning a SHAP plot.
*  **Emotion:** The overall emotional tone is Neutral, reflecting analytical inquiry and problem-solving.
*  **Top 3 Points of View:**
    * It is worth imputing NaN inputs with another value as the document for lightgbm is unclear to me.
    * If NaNs are random, the ratios should be roughly the same. If they aren't, it might be worth looking into the random process that yields NaNs.

**[[R] Best Loss for RDH Task (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1ji1x2t/r_best_loss_for_rdh_task/)**
*  **Summary:** This thread is focused on finding the best loss function for a Reversible Data Hiding (RDH) task.
*  **Emotion:** The emotional tone is Neutral as it's primarily a technical discussion.
*  **Top 3 Points of View:**
    * Consider MSE or MAE for pixel-level accuracy.
    * Try SSIM (structural similarity index measure) as a loss function.

**[[P] and [D] Country Recognition Model??? (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1jifoxl/p_and_d_country_recognition_model/)**
*  **Summary:** This thread is focused on building a model for recognizing countries based on some data.
*  **Emotion:** The emotional tone is neutral with focus on solutions.
*  **Top 3 Points of View:**
    * Use embeddings and a classifier on top for each country.
    * Sounds like a named entity recognition (NER) task

**[[P] I Built a FAANG Job Board for ML Engineers – Only Jobs Scraped in the Last 24h (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1jhxknz/p_i_built_a_faang_job_board_for_ml_engineers_only/)**
*  **Summary:** This thread is about a job board for ML Engineers.
*  **Emotion:** The emotional tone is generally Neutral.
*  **Top 3 Points of View:**
    * The job board is paywalled and requires a sign up.
    * Megali's work scrapes hackernews, reddit and remoteok daily and send you an email with the jobs that will match your search query.

**[[D] Is MCP really a solution… or just another layer we don’t need? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ji7cx3/d_is_mcp_really_a_solution_or_just_another_layer/)**
*  **Summary:** The thread explores the necessity and value of MCP (presumably a model serving framework).
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    * MCP is not needed for local setup.
    * MCP layer is dumb and over engineered. It requires custom optimisation and custom logic to skip steps for speed optimisation.
    * The fact that you are asking is an answer.

**[[Discussion] What Does GPU On-Demand Pricing Mean and How Can I Optimize Server Run-Time? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1jicpo2/discussion_what_does_gpu_ondemand_pricing_mean/)**
*  **Summary:** This thread discusses GPU on-demand pricing and methods for optimizing server run-time to save costs.
*  **Emotion:** The emotional tone is neutral and helpful.
*  **Top 3 Points of View:**
    * Consider paying for reserved, they can go cheaper depending on cloud provider.
    * Develop on CPU or rather weak GPU instances then move to more powerful GPU or GPU clusters
    * The boto3 library can be used to programmatically start and stop ec2 instances.

**[[D] Is the term "interference" used? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1jils6h/d_is_the_term_interference_used/)**
*  **Summary:** This thread discusses whether the term "interference" is used in ML.
*  **Emotion:** The emotional tone is generally Negative.
*  **Top 3 Points of View:**
    * First time hearing about it.
    * The prompt does not have to reach the model, in AI/ML theory.
    * What a weird question.

**[[D] Seeking PhD Supervisor in ML/NLP/Explainable AI (Europe-Based) – Recommendations? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1jj4l8g/d_seeking_phd_supervisor_in_mlnlpexplainable_ai/)**
*  **Summary:** This thread is focused on finding PhD supervisors for ML/NLP.
*  **Emotion:** The emotional tone is Neutral.
*  **Top 3 Points of View:**
    * Kristian Kersting https://ml-research.github.io/people/kkersting/index.html
Lux De Raedt https://wms.cs.kuleuven.be/people/lucderaedt
And all associated collaborators come to mind
    * Check out [Stephan Bauer's](https://mcml.ai/research/groups/bauer_s/) group
