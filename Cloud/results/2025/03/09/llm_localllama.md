---
title: "LocalLLaMA Subreddit"
date: "2025-03-09"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "Local AI", "Open Source"]
---

# Overall Ranking and Top Discussions
1.  [I've made Deepseek R1 think in Spanish](https://i.redd.it/fx6kdf0w2one1.png) (Score: 79)
    *   This thread discusses how to make the Deepseek R1 model think in Spanish and other languages.
2.  [Dumb question - I use Claude 3.5 A LOT, what setup would I need to create a comparable local solution?](https://www.reddit.com/r/LocalLLaMA/comments/1j75xpm/dumb_question_i_use_claude_35_a_lot_what_setup/) (Score: 53)
    *   This thread explores what kind of local setup would be needed to create a comparable local solution with Claude 3.5.
3.  [QWQ low score in Leaderboard, what happened?](https://i.redd.it/77rco6vfipne1.png) (Score: 30)
    *   This thread discusses why the QWQ model has a low score in the leaderboard.
4.  [Local Deep Research Update - I worked on your requested features and got also help from you](https://www.reddit.com/r/LocalLLaMA/comments/1j79obx/local_deep_research_update_i_worked_on_your/) (Score: 30)
    *   This thread shares an update on a local deep research project.
5.  [Which major open source model will be next? Llama, Mistral, Hermes, Nemotron, Qwen or Grok2?](https://www.reddit.com/r/LocalLLaMA/comments/1j76z48/which_major_open_source_model_will_be_next_llama/) (Score: 28)
    *   This thread is a discussion about which major open-source model will be the next big thing.
6.  [Open WebUi + Tailscale = Beauty](https://www.reddit.com/r/LocalLLaMA/comments/1j79ev6/open_webui_tailscale_beauty/) (Score: 28)
    *   This thread explores the advantages of using Open WebUI and Tailscale together.
7.  [Why ate we not seeing much desktop apps developed with local AI integration,by smaller developers?](https://www.reddit.com/r/LocalLLaMA/comments/1j79oma/why_ate_we_not_seeing_much_desktop_apps_developed/) (Score: 23)
    *   This thread delves into the reasons why there aren't many desktop apps developed with local AI integration by smaller developers.
8.  [What GPU do you use for 32B/70B models, and what speed do you get?](https://www.reddit.com/r/LocalLLaMA/comments/1j7baw1/what_gpu_do_you_use_for_32b70b_models_and_what/) (Score: 7)
    *   This thread discusses what GPU is used for 32B/70B models and the speeds users are getting.
9.  [llama.cpp RPC is great!  the network is not the bottleneck](https://www.reddit.com/r/LocalLLaMA/comments/1j7dedc/llamacpp_rpc_is_great_the_network_is_not_the/) (Score: 5)
    *   This thread is about the performance of llama.cpp RPC.
10. [What is the best framework for running llms locally?](https://www.reddit.com/r/LocalLLaMA/comments/1j7ckg0/what_is_the_best_framework_for_running_llms/) (Score: 4)
    *   This thread discusses the best frameworks for running LLMs locally.
11. [Huawei GPU ????](https://www.reddit.com/r/LocalLLaMA/comments/1j78xnk/huawei_gpu/) (Score: 3)
    *   This thread discusses the capabilities of Huawei GPUs.
12. [When will Llama 4, Gemma 3, or Qwen 3 be released?](https://www.reddit.com/r/LocalLLaMA/comments/1j7g30z/when_will_llama_4_gemma_3_or_qwen_3_be_released/) (Score: 3)
    *   This thread discusses the release dates for Llama 4, Gemma 3, or Qwen 3.
13. [12V-2x6 Power Connector Cooks At Over 150°C With A "Water-Cooled" NVIDIA GeForce RTX 5090 -- For Those Thinking About Buying One or More For LLM Usage](https://wccftech.com/12v-2x6-power-connector-cooks-at-over-150c-with-a-water-cooled-nvidia-geforce-rtx-5090/) (Score: 2)
    *   This thread is about the reported issues with the 12V-2x6 power connector on the NVIDIA GeForce RTX 5090.
14. [Is there any LLM even remotely close to Claude 3.7 Sonnet when it comes to long-form creative writing output?](https://www.reddit.com/r/LocalLLaMA/comments/1j76bo0/is_there_any_llm_even_remotely_close_to_claude_37/) (Score: 2)
    *   This thread is a discussion on the effectiveness of LLMs when it comes to long-form creative writing.
15. [Amount of ram Qwen 2.5-7B-1M takes?](https://www.reddit.com/r/LocalLLaMA/comments/1j79o3l/amount_of_ram_qwen_257b1m_takes/) (Score: 2)
    *   This thread discusses the amount of RAM the Qwen 2.5-7B-1M model takes.
16. [Newest mini is it for local llama](https://www.reddit.com/r/LocalLLaMA/comments/1j7d7n4/newest_mini_is_it_for_local_llama/) (Score: 2)
    *   This thread explores whether the newest mini PC is suitable for running local LLMs.
17. [Which backend is best for dual RTX 3090s with both text and multimodal models?](https://www.reddit.com/r/LocalLLaMA/comments/1j7703h/which_backend_is_best_for_dual_rtx_3090s_with/) (Score: 1)
    *   This thread discusses what the best backend is for dual RTX 3090s for both text and multimodal models.
18. [Models Runnable for New MacBook Air M4 16GB RAM ?](https://www.reddit.com/r/LocalLLaMA/comments/1j79goc/models_runnable_for_new_macbook_air_m4_16gb_ram/) (Score: 1)
    *   This thread seeks to find out what models are runnable for the new Macbook Air M4 with 16GB of RAM.
19. [Should I provide structure in prompt for structured output?](https://www.reddit.com/r/LocalLLaMA/comments/1j7c4ir/should_i_provide_structure_in_prompt_for/) (Score: 1)
    *   This thread is asking if structure should be provided in the prompt for structured output.
20. [is anyone else getting extremely nerfed results for qwq?](https://www.reddit.com/r/LocalLLaMA/comments/1j7fviw/is_anyone_else_getting_extremely_nerfed_results/) (Score: 1)
    *   This thread discusses whether users are getting extremely nerfed results for QwQ.

# Detailed Analysis by Thread
**[ I've made Deepseek R1 think in Spanish (Score: 79)](https://i.redd.it/fx6kdf0w2one1.png)**
*  **Summary:** The thread is about making the Deepseek R1 model think in Spanish. Users are sharing their experiences and code snippets for achieving this, and also testing it in different languages, such as Arabic and Toki Pona. There's a discussion about the purpose of forcing the model to think in a specific language and whether it limits the model's efficiency.
*  **Emotion:** The overall emotional tone of the thread is neutral, with some positive sentiment expressed. Users are generally interested and impressed by the ability of the model to think in different languages.
*  **Top 3 Points of View:**
    *   It's impressive that Deepseek R1 can think in different languages.
    *   Forcing the model to think in a specific language might limit its performance.
    *   Benchmarking the model in Spanish would be fun to see how far generalization can go.

**[ Dumb question - I use Claude 3.5 A LOT, what setup would I need to create a comparable local solution? (Score: 53)](https://www.reddit.com/r/LocalLLaMA/comments/1j75xpm/dumb_question_i_use_claude_35_a_lot_what_setup/)**
*  **Summary:** The thread discusses what setup is needed to create a comparable local solution to Claude 3.5. Users are recommending different models like QwQ 32B and suggesting the use of agentic coding to enhance performance. They also suggest exploring cloud servers and considering the rapidly changing nature of the technology before investing heavily in hardware.
*  **Emotion:** The overall emotional tone is neutral to positive, with helpful and informative responses being provided. There are some notes of negativity regarding the inability to fully match Claude's performance.
*  **Top 3 Points of View:**
    *   QwQ 32B with some agentic coding could be a viable local solution, but it won't match Claude 3.5's level.
    *   Setting up a cloud server with different hardware specs and LLM models is a good way to evaluate performance and needs.
    *   Consider the rapidly changing tech landscape before investing too much in hardware, as solutions may become more efficient and cheaper in the near future.

**[ QWQ low score in Leaderboard, what happened? (Score: 30)](https://i.redd.it/77rco6vfipne1.png)**
*  **Summary:** This thread is about why the QWQ model has a low score on the leaderboard. Users discuss potential reasons such as broken leaderboards, contaminated models, issues with the `<think>` token in the template, and the model's need for a large number of tokens to generate a good answer. Some users believe the benchmarks are flawed and don't accurately reflect the model's capabilities.
*  **Emotion:** The emotional tone is mixed, with some frustration and skepticism towards the leaderboards, as well as some positive sentiment about the QWQ model's actual performance.
*  **Top 3 Points of View:**
    *   The leaderboard is broken and unreliable, and its scores shouldn't be taken as an accurate representation of model performance.
    *   The `<think>` token in the template might be messing up the benchmarks.
    *   QwQ is a difficult model to serve because it needs an enormous amount of time/tokens to make a good answer, which most benchmarks don't allow for.

**[ Local Deep Research Update - I worked on your requested features and got also help from you (Score: 30)](https://www.reddit.com/r/LocalLLaMA/comments/1j79obx/local_deep_research_update_i_worked_on_your/)**
*  **Summary:** The thread is about an update to a local deep research project. Users are discussing the features, asking about compatibility with llama.cpp and OpenAI API endpoints, and suggesting improvements to the prompting strategy.
*  **Emotion:** The overall tone is neutral, with a mix of curiosity, interest, and helpful suggestions.
*  **Top 3 Points of View:**
    *   It's important to provide a conclusion first in the prompt to make the reasoning that follows a rationalization of the snap conclusion.
    *   It should be compatible with a local llama.cpp server.
    *   There's a need for Deep Research integration into Open-WebUI.

**[Which major open source model will be next? Llama, Mistral, Hermes, Nemotron, Qwen or Grok2? (Score: 28)](https://www.reddit.com/r/LocalLLaMA/comments/1j76z48/which_major_open_source_model_will_be_next_llama/)**
*  **Summary:** The thread discusses predictions for the next major open-source model, considering options like Llama 4, Mistral Large, Deepseek R2, and Qwen. Users debate the potential and likely release timelines of each model.
*  **Emotion:** The emotional tone is mostly neutral, with some excitement and anticipation for upcoming releases.
*  **Top 3 Points of View:**
    *   Llama 4 and Qwen are the most likely candidates to be the next major open-source models.
    *   Deepseek R2 is expected to be a very good model.
    *   Mistral models are considered the most usable and cost-effective.

**[Open WebUi + Tailscale = Beauty (Score: 28)](https://www.reddit.com/r/LocalLLaMA/comments/1j79ev6/open_webui_tailscale_beauty/)**
*  **Summary:** The thread celebrates the combination of Open WebUI and Tailscale for accessing LLMs remotely. Users discuss the benefits, such as ease of use, security, and the ability to use LLMs on the desktop with GPU from a laptop while traveling. Alternatives like ZeroTier and Cloudflare tunnels are also mentioned.
*  **Emotion:** The overall emotional tone is positive, with users expressing satisfaction and appreciation for the Open WebUI and Tailscale setup.
*  **Top 3 Points of View:**
    *   Open WebUI + Tailscale provides a great experience for using LLMs remotely.
    *   Tailscale is a convenient and secure solution for accessing local LLMs.
    *   ZeroTier is a great alternative to Tailscale for remote access to models.

**[Why ate we not seeing much desktop apps developed with local AI integration,by smaller developers? (Score: 23)](https://www.reddit.com/r/LocalLLaMA/comments/1j79oma/why_ate_we_not_seeing_much_desktop_apps_developed/)**
*  **Summary:** This thread explores the reasons behind the lack of desktop apps with local AI integration from smaller developers. Discussions revolve around factors like hardware requirements, the preference for SaaS models, deployment challenges, and the difficulty of monetizing local AI solutions.
*  **Emotion:** The overall tone is neutral with a slight negative undertone, as users discuss the challenges and limitations of developing and deploying local AI apps.
*  **Top 3 Points of View:**
    *   Limited hardware availability and high system requirements make it difficult to target a wide audience with local AI apps.
    *   SaaS models are more appealing for developers due to easier deployment, monetization, and the ability to provide better features.
    *   Deploying AI solutions for desktop apps is challenging due to the reliance on Python and the complexity of explaining system requirements to non-technical users.

**[What GPU do you use for 32B/70B models, and what speed do you get? (Score: 7)](https://www.reddit.com/r/LocalLLaMA/comments/1j7baw1/what_gpu_do_you_use_for_32b70b_models_and_what/)**
*  **Summary:** Users are sharing their GPU setups and the speeds they achieve when running 32B/70B models. Discussions include various GPUs like RTX 3090, dual 4090, and M3-Max, along with token generation speeds and configurations.
*  **Emotion:** The tone is primarily neutral, with users sharing technical information and asking questions. There's some positive sentiment related to achieving good speeds.
*  **Top 3 Points of View:**
    *   Dual 4090 setup can achieve around 17-20 tps for a 70B model.
    *   3x 3090s can be used for 72B models with varying token generation speeds depending on quantization and draft models.
    *   M3-Max 64GB is also being used, with speed tests available in linked Reddit posts.

**[llama.cpp RPC is great! the network is not the bottleneck (Score: 5)](https://www.reddit.com/r/LocalLLaMA/comments/1j7dedc/llamacpp_rpc_is_great_the_network_is_not_the/)**
*  **Summary:** The thread discusses the performance of llama.cpp RPC. Users are sharing their experiences and performance data. The discussion also touches upon potential flaws in the test setup, the impact of multiple connections, and the use of mismatched GPUs.
*  **Emotion:** The overall emotional tone is mixed, with some excitement about the potential of llama.cpp RPC, but also skepticism and criticism regarding the test setup and observed performance issues.
*  **Top 3 Points of View:**
    *   The test setup is flawed for prompt processing speed due to the short prompt length.
    *   The more connections, the slower it gets, suggesting a fundamental issue in llama.cpp.
    *   Running mismatched GPUs in the same chassis works well.

**[What is the best framework for running llms locally? (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1j7ckg0/what_is_the_best_framework_for_running_llms/)**
*  **Summary:** This thread is a discussion about the best frameworks for running LLMs locally. Users recommend options such as Ollama, LM Studio, vLLM, llama.cpp, and jan.ai, highlighting their ease of use, performance, and features like OpenAI API support.
*  **Emotion:** The overall emotional tone is neutral, with users offering helpful suggestions and sharing their preferences.
*  **Top 3 Points of View:**
    *   Ollama is easy to use and offers OpenAI API support.
    *   LM Studio is super easy, especially for adjusting context size and other parameters.
    *   Llama.cpp is lightweight and has everything, including top performance, a built-in web UI, and OpenAI compatible API.

**[Huawei GPU ???? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1j78xnk/huawei_gpu/)**
*  **Summary:** The thread explores the use of Huawei GPUs for LLMs. Users share information about Huawei GPU support on vLLM or Aphrodite engine, benchmark comparisons with NVIDIA cards, and potential implications of export bans on AI chips.
*  **Emotion:** The emotional tone is mixed, ranging from curiosity and interest to skepticism about the performance and availability of Huawei GPUs.
*  **Top 3 Points of View:**
    *   Huawei GPUs are significantly slower compared to NVIDIA cards like the 3090.
    *   Export bans on AI chips could increase demand for Huawei GPUs, potentially driving improvements in their drivers.
    *   It is important to consider the lack of support in the LLM ecosystem before buying Huawei GPUs

**[When will Llama 4, Gemma 3, or Qwen 3 be released? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1j7g30z/when_will_llama_4_gemma_3_or_qwen_3_be_released/)**
*  **Summary:** The thread is about the potential release dates of Llama 4, Gemma 3, and Qwen 3. A user speculates that Llama 4 will be released at the end of April, Gemma 3 is already in prototype, and Qwen 3 will be released after July.
*  **Emotion:** The emotional tone is neutral, with users speculating and sharing information.
*  **Top 3 Points of View:**
    *   Llama 4 is expected to be released at the end of April.
    *   Gemma 3 is currently in the prototype phase.
    *   Qwen 3 is not expected to be released until after July.

**[12V-2x6 Power Connector Cooks At Over 150°C With A "Water-Cooled" NVIDIA GeForce RTX 5090 -- For Those Thinking About Buying One or More For LLM Usage (Score: 2)](https://wccftech.com/12v-2x6-power-connector-cooks-at-over-150c-with-a-water-cooled-nvidia-geforce-rtx-5090/)**
*  **Summary:** The thread discusses a news article about the 12V-2x6 power connector overheating on the NVIDIA GeForce RTX 5090, even with water cooling. Users express concern about the 50 series launch, the reliance on CUDA for training, and potential solutions like power limiting the card.
*  **Emotion:** The overall emotional tone is negative, with users expressing disappointment and frustration over the reported issues with the RTX 5090.
*  **Top 3 Points of View:**
    *   The 50 series launch has been awful due to the power connector issue.
    *   It would be great if CUDA magically worked on AMD cards.
    *   Power limiting the card to 70% may mitigate overheating risks while still providing access to the 32GB VRAM for LLMs.

**[Is there any LLM even remotely close to Claude 3.7 Sonnet when it comes to long-form creative writing output? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1j76bo0/is_there_any_llm_even_remotely_close_to_claude_37/)**
*  **Summary:** The thread discusses whether any local LLMs can match Claude 3.7 Sonnet's performance in long-form creative writing. Users suggest writing in short strides and gluing pieces together, keeping notes on character profiles, and using agents.
*  **Emotion:** The overall emotional tone is neutral, with users sharing their experiences and offering suggestions.
*  **Top 3 Points of View:**
    *   No local LLM is remotely close to Claude 3.7 Sonnet for long-form creative writing.
    *   It's better to write with LLMs in short strides (500-1000 words) and then glue pieces together.
    *   Using a small agent to assist with the writing process can be helpful.

**[Amount of ram Qwen 2.5-7B-1M takes? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1j79o3l/amount_of_ram_qwen_257b1m_takes/)**
*  **Summary:** This thread discusses the amount of RAM required to run the Qwen 2.5-7B-1M model. Users provide estimations of VRAM usage and discuss the trade-offs between context length and quality. They also suggest using custom VLLM forks to hit 1M context, and different quantization methods to squeeze it onto a 24GB card.
*  **Emotion:** The overall tone is neutral, as users exchange technical information and discuss the model's resource requirements.
*  **Top 3 Points of View:**
    *   The model requires a significant amount of RAM, with estimations varying depending on the quantization and context length.
    *   Quality deteriorates with longer context lengths.
    *   Custom VLLM forks and specific quantization methods can help to fit the model onto a 24GB card.

**[Newest mini is it for local llama (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1j7d7n4/newest_mini_is_it_for_local_llama/)**
*  **Summary:** This thread discusses whether the newest mini PC is suitable for running local LLMs. Users point out that DDR5 memory is too slow and that the NPU won't help if the memory is slow. They suggest that a Framework mini PC with a better iGPU and unified memory would be a better option.
*  **Emotion:** The overall emotional tone is neutral, with users sharing technical assessments of the mini PC's suitability for local LLMs.
*  **Top 3 Points of View:**
    *   DDR5 memory is too slow for running local LLMs effectively.
    *   The NPU won't compensate for slow memory.
    *   The Framework mini PC with a better iGPU and unified memory would be a better option.

**[Which backend is best for dual RTX 3090s with both text and multimodal models? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1j7703h/which_backend_is_best_for_dual_rtx_3090s_with/)**
*  **Summary:** The thread discusses which backend is best for dual RTX 3090s with both text and multimodal models. Users suggest EXL2/TabbyAPI and vLLM.
*  **Emotion:** The overall emotional tone is neutral, with users providing brief recommendations.
*  **Top 3 Points of View:**
    *   EXL2/TabbyAPI is a good option.
    *   vLLM is a good option.

**[Models Runnable for New MacBook Air M4 16GB RAM ? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1j79goc/models_runnable_for_new_macbook_air_m4_16gb_ram/)**
*  **Summary:** This thread discusses what models can be run on the new MacBook Air M4 with 16GB of RAM. Users suggest that it can run toy LLMs like Phi-Mini, Qwen2.5-3b, and Llama 3.2, and that Qwen2.5-VL:7b q8 can be run in LM Studio for OCR.
*  **Emotion:** The overall emotional tone is neutral, with users sharing technical assessments of the MacBook Air's suitability for local LLMs.
*  **Top 3 Points of View:**
    *   16GB of RAM is miniscule for running LLMs.
    *   It can run toy LLMs like Phi-Mini, Qwen2.5-3b, and Llama 3.2.
    *    Qwen2.5-VL:7b q8 can be run in LM Studio for OCR.

**[Should I provide structure in prompt for structured output? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1j7c4ir/should_i_provide_structure_in_prompt_for/)**
*  **Summary:** This thread is about providing structure in the prompt for structured output. Users are sharing suggestions about using libraries like LangGraph/Langchain.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Providing structure in the prompt can be done using libraries like LangGraph/Langchain.
    *   It can be done directly in the prompt.
    *   Providing structure in the prompt can lead to more reliable output, especially on smaller models.

**[is anyone else getting extremely nerfed results for qwq? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1j7fviw/is_anyone_else_getting_extremely_nerfed_results/)**
*  **Summary:** The thread discusses whether users are getting extremely nerfed results for QwQ. A user suggests that the prompt template and parameters on Ollama are wrong, and recommends using the Unsloth GGUF model and GPT4All as a backend.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   The prompt template and parameters on Ollama are wrong.
    *   The prompt template doesn't have the thinking tag and temp 0.6 is set, but there are more parameters you have to set.
    *   Using the Unsloth GGUF model and GPT4All as a backend can lead to high quality results.
