---
title: "LocalLLaMA Subreddit"
date: "2025-03-12"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "LocalAI", "Gemma"]
---

# Overall Ranking and Top Discussions
1.  [So Gemma 4b on cell phone!](https://v.redd.it/i0bqxnig0boe1) (Score: 62)
    *   Discussing the possibility of running the Gemma 4b model on a cell phone, specifically the S24 Ultra and seeking advice for setting it up on an iPhone using CNVRS.
2.  [Letâ€™s make Gemma 3 think! Here's a notebook to do GRPO on Gemma3 to make it reason.](https://www.reddit.com/r/LocalLLaMA/comments/1j9nioo/lets_make_gemma_3_think_heres_a_notebook_to_do/) (Score: 42)
    *   Exploring methods to improve the reasoning capabilities of the Gemma 3 model using GRPO (Gradient Ratio Policy Optimization), with discussions on the number of steps required and potential applications.
3.  [LM Studio updated with Gemma 3 GGUF support!](https://www.reddit.com/r/LocalLLaMA/comments/1j9reim/lm_studio_updated_with_gemma_3_gguf_support/) (Score: 20)
    *   Announcing and discussing the updated LM Studio with Gemma 3 GGUF support, with initial impressions being positive, particularly regarding the 12b model and its vision understanding.
4.  [Gemma3 makes too many mistakes to be usable](https://www.reddit.com/r/LocalLLaMA/comments/1j9qvem/gemma3_makes_too_many_mistakes_to_be_usable/) (Score: 12)
    *   Discussing the usability of Gemma 3, with some users finding it makes too many mistakes, particularly in visual understanding, while others are impressed with its technical writing abilities, but note it's not a coding model.
5.  [Anyone using a rack mount case for >2 GPU's](https://i.redd.it/bltyuht1xaoe1.jpeg) (Score: 8)
    *   Seeking recommendations for rack mount cases to accommodate more than two GPUs, considering factors like space, cooling, and alternatives like open frame setups.
6.  [ðŸš€ VPTQ Now Supports Deepseek R1 (671B) Inference on 4Ã—A100 GPUs!](https://www.reddit.com/r/LocalLLaMA/comments/1j9poij/vptq_now_supports_deepseek_r1_671b_inference_on/) (Score: 7)
    *   Announcing VPTQ support for Deepseek R1 inference and requesting guidance on quantization examples for Gemma3 and 72B models.
7.  [JSON makes llms dumber?](https://i.redd.it/lq1hr2400boe1.jpeg) (Score: 5)
    *   Exploring the idea that JSON format might make LLMs perform worse compared to other formats like YAML, discussing the possible reasons and whether it depends on the model's training data.
8.  [How to adapt the new OpenAI Agents SDK to work with local Ollama models along with an example agent.](https://www.reddit.com/r/LocalLLaMA/comments/1j9oj4q/how_to_adapt_the_new_openai_agents_sdk_to_work/) (Score: 4)
    *   Discussing the adaptation of the new OpenAI Agents SDK to work with local Ollama models, specifically inquiring about support for the Responses API.
9.  [I need help configuring an LLM 'therapist' to help me process trauma from tumors](https://www.reddit.com/r/LocalLLaMA/comments/1j9qqo3/i_need_help_configuring_an_llm_therapist_to_help/) (Score: 4)
    *   Seeking help configuring an LLM as a therapist to process trauma, discussing hardware requirements, model limitations, the potential need for fine-tuning, and the lack of evidence-based assessment in the field of LLM self-therapy, also explaining why professional therapists weren't helpful.
10. [Methods of doing RAG with ollama and pageassist?](https://www.reddit.com/r/LocalLLaMA/comments/1j9qo15/methods_of_doing_rag_with_ollama_and_pageassist/) (Score: 1)
    *   Inquiring about methods for implementing Retrieval-Augmented Generation (RAG) with Ollama and pageassist.
11. [Nvidia Quadro RTX 8000](https://www.reddit.com/r/LocalLLaMA/comments/1j9qvuj/nvidia_quadro_rtx_8000/) (Score: 1)
    *   Discussing the Nvidia Quadro RTX 8000, evaluating its performance and cost-effectiveness compared to other GPUs, particularly for local LLM use, and considering its VRAM density and support for certain features.
12. [macbook's favorite model change: Mistral Small 3 -> QWQ 32B](https://www.reddit.com/r/LocalLLaMA/comments/1j9r5an/macbooks_favorite_model_change_mistral_small_3/) (Score: 1)
    *   Sharing a user's experience of switching from Mistral Small 3 to QWQ 32B on a Macbook and inquiring about comparison with the new reka flash 3 model.
13. [Best llm to run with 32gb VRAM](https://www.reddit.com/r/LocalLLaMA/comments/1j9ngt5/best_llm_to_run_with_32gb_vram/) (Score: 0)
    *   Asking for recommendations for the best LLM to run with 32GB VRAM, and defining what POC means in this context.
14. [I want to learn the basics of Ai](https://www.reddit.com/r/LocalLLaMA/comments/1j9oa4s/i_want_to_learn_the_basics_of_ai/) (Score: 0)
    *   Seeking resources and advice for learning the basics of AI, with suggestions to use AI itself for learning and to explore resources like Karpathy's Deep Dive into LLMs.
15. [Which inference settings have been used by an authors of LLM?](https://www.reddit.com/r/LocalLLaMA/comments/1j9oa4u/which_inference_settings_have_been_used_by_an/) (Score: 0)
    *   Questioning the use of inference settings and clarifying that they vary the sampling on the inference side, but are not used during training at all.
16. [Which agent to automate checking 1000 company websites whether they advertise specific product/service?](https://www.reddit.com/r/LocalLLaMA/comments/1j9pzko/which_agent_to_automate_checking_1000_company/) (Score: 0)
    *   Seeking advice on which agent to use for automating the process of checking a large number of company websites for specific product/service advertisements.
17. [Trying to Win Over My Team to Use Local LLM - Need Advice!](https://www.reddit.com/r/LocalLLaMA/comments/1j9q0d8/trying_to_win_over_my_team_to_use_local_llm_need/) (Score: 0)
    *   Requesting advice on convincing a team to adopt local LLMs, addressing concerns about data privacy, workflow complexity, and the need for structured outputs.

# Detailed Analysis by Thread
**[So Gemma 4b on cell phone! (Score: 62)](https://v.redd.it/i0bqxnig0boe1)**
*  **Summary:**  The thread discusses the successful implementation of the Gemma 4b model on a cell phone (S24 Ultra) and explores challenges and solutions for running it on iOS devices.
*  **Emotion:** The overall emotional tone is Neutral, with users mainly providing information and asking questions.
*  **Top 3 Points of View:**
    *   Gemma 4b can run on an Android phone (S24 Ultra).
    *   Users are trying to get it working on iPhones using CNVRS but are encountering issues.
    *   There is interest in knowing if the model uses only the keyboard for input.

**[Letâ€™s make Gemma 3 think! Here's a notebook to do GRPO on Gemma3 to make it reason. (Score: 42)](https://www.reddit.com/r/LocalLLaMA/comments/1j9nioo/lets_make_gemma_3_think_heres_a_notebook_to_do/)**
*  **Summary:**  This thread explores the application of Gradient Ratio Policy Optimization (GRPO) to enhance the reasoning abilities of the Gemma 3 model.
*  **Emotion:** The overall emotional tone is Neutral, with a hint of Positive sentiment due to excitement about the potential of improving the model.
*  **Top 3 Points of View:**
    *   GRPO can potentially improve the reasoning capabilities of Gemma 3.
    *   The number of steps needed to achieve a breakthrough "aha" moment is a key question.
    *   There's a desire to use GRPO to enable Gemma with vision to perform computer use tasks.

**[LM Studio updated with Gemma 3 GGUF support! (Score: 20)](https://www.reddit.com/r/LocalLLaMA/comments/1j9reim/lm_studio_updated_with_gemma_3_gguf_support/)**
*  **Summary:**  The thread celebrates the update of LM Studio with Gemma 3 GGUF support, with a user sharing positive initial impressions, particularly for the 12b model, but notes a minor text generation issue.
*  **Emotion:** The emotional tone is Positive, driven by excitement and satisfaction with the new update and model performance.
*  **Top 3 Points of View:**
    *   Gemma 3 12b is considered a very good model.
    *   Vision understanding of the model is impressive.
    *   There might be some minor bugs that need fixing.

**[Gemma3 makes too many mistakes to be usable (Score: 12)](https://www.reddit.com/r/LocalLLaMA/comments/1j9qvem/gemma3_makes_too_many_mistakes_to_be_usable/)**
*  **Summary:**  This thread discusses the mixed experiences with Gemma 3, with some users criticizing its visual understanding and instruction following, while others praise its technical writing capabilities.
*  **Emotion:** The emotional tone is Neutral, with some Negative sentiment expressing disappointment with the model's performance, contrasted by more positive observations regarding other aspects.
*  **Top 3 Points of View:**
    *   Gemma 3's visual understanding is poor.
    *   Gemma 3 is good at technical writing.
    *   Gemma models are not designed for coding tasks, but more for linguistic tasks.

**[Anyone using a rack mount case for >2 GPU's (Score: 8)](https://i.redd.it/bltyuht1xaoe1.jpeg)**
*  **Summary:**  The thread solicits recommendations and experiences with using rack mount cases for systems with more than two GPUs.
*  **Emotion:** The overall emotional tone is Neutral, as users share information and ask questions related to hardware setups.
*  **Top 3 Points of View:**
    *   Open frame configurations are a good alternative if space/privacy is not an issue.
    *   Specific models like Silverstone Rm52 and rm600 can fit 3 GPUs.
    *   Rack mount servers can have loud fans which can be an issue for home use.

**[ðŸš€ VPTQ Now Supports Deepseek R1 (671B) Inference on 4Ã—A100 GPUs! (Score: 7)](https://www.reddit.com/r/LocalLLaMA/comments/1j9poij/vptq_now_supports_deepseek_r1_671b_inference_on/)**
*  **Summary:**  The thread announces the new VPTQ support for Deepseek R1 and requests guidance on how to use VPTQ for quantization with Gemma3 or 72B models.
*  **Emotion:** The emotional tone is Positive, reflecting excitement about the new capabilities and a desire to test them.
*  **Top 3 Points of View:**
    *   VPTQ now supports Deepseek R1.
    *   Users are eager to test VPTQ on their local hardware.
    *   Guidance is needed for using VPTQ with Gemma3 and other models.

**[JSON makes llms dumber? (Score: 5)](https://i.redd.it/lq1hr2400boe1.jpeg)**
*  **Summary:**  The thread explores the idea that LLMs might perform worse with JSON compared to other formats like YAML due to training data and the verbose nature of JSON.
*  **Emotion:** The overall emotional tone is Neutral, as users discuss the potential reasons and offer explanations.
*  **Top 3 Points of View:**
    *   LLMs might be more competent with YAML due to training data.
    *   JSON's structure and separators can be perceived as noise.
    *   Formatting data into human-readable snippets is preferable to structured formats like JSON for some tasks.

**[How to adapt the new OpenAI Agents SDK to work with local Ollama models along with an example agent. (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1j9oj4q/how_to_adapt_the_new_openai_agents_sdk_to_work/)**
*  **Summary:**  The thread discusses how to adapt the new OpenAI Agents SDK to work with local Ollama models.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   The adaption of OpenAI agents SDK with Ollama models.
    *   The user is asking if the Responses API is supported.

**[I need help configuring an LLM 'therapist' to help me process trauma from tumors (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1j9qqo3/i_need_help_configuring_an_llm_therapist_to_help/)**
*  **Summary:**  The thread discusses the challenges and limitations of using local LLMs for self-therapy, particularly with limited hardware, and explores alternative approaches and resources.
*  **Emotion:** The emotional tone is Neutral, with a mix of support and realism about the limitations of current LLMs.
*  **Top 3 Points of View:**
    *   Local models might not be powerful enough for effective therapy.
    *   Fine-tuning is a possibility but requires effort and resources.
    *   LLMs may be helpful for creating tools that guide a user through different steps, such as cooking or panic attack excercises.

**[Methods of doing RAG with ollama and pageassist? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1j9qo15/methods_of_doing_rag_with_ollama_and_pageassist/)**
*  **Summary:**  The thread is a simple question asking about methods for RAG with ollama and pageassist.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   RAG system using .NET.

**[Nvidia Quadro RTX 8000 (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1j9qvuj/nvidia_quadro_rtx_8000/)**
*  **Summary:**  The thread evaluates the Nvidia Quadro RTX 8000 for local LLM use, discussing its pros and cons compared to other options.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   The RTX 8000 is the cheapest two slot nvidia card with 48gb.
    *   The rtx 8000 is a terrible deal IMO, can get 2 3090 for the price.
    *   The Quadro is still decent with modern gaming too, so not a one trick pony.

**[macbook's favorite model change: Mistral Small 3 -> QWQ 32B (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1j9r5an/macbooks_favorite_model_change_mistral_small_3/)**
*  **Summary:**  The thread is a simple sharing of experience with a model change on a Macbook.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   There's a question of comparison with new reka flash 3 model.

**[Best llm to run with 32gb VRAM (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1j9ngt5/best_llm_to_run_with_32gb_vram/)**
*  **Summary:**  The thread is a simple question asking for LLM recommendations based on VRAM.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   Asking for the type of task.
    *   Defining POC in the context.

**[I want to learn the basics of Ai (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1j9oa4s/i_want_to_learn_the_basics_of_ai/)**
*  **Summary:**  The thread is a request for learning resources.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   Suggesting to ask AI for learning.
    *   Suggesting Karpathy's videos.
    *   Suggesting some links to learn the basics of AI.

**[Which inference settings have been used by an authors of LLM? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1j9oa4u/which_inference_settings_have_been_used_by_an/)**
*  **Summary:**  The thread is asking a question about inference settings.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   Parameters vary the sampling on the inference side, but are not used during training at all.

**[Which agent to automate checking 1000 company websites whether they advertise specific product/service? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1j9pzko/which_agent_to_automate_checking_1000_company/)**
*  **Summary:**  The thread is asking a question about automating web site checking.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   Suggesting to use puppeteer to scrape the websites then feed to an AI.

**[Trying to Win Over My Team to Use Local LLM - Need Advice! (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1j9q0d8/trying_to_win_over_my_team_to_use_local_llm_need/)**
*  **Summary:**  The thread asks for advice on convincing a team to use local LLMs.
*  **Emotion:** The overall emotional tone is Positive.
*  **Top 3 Points of View:**
    *   Execs are very worried about exposing customer data.
    *   Local sized models are not going to get very far, big dog models have JSON mode/structured outputs.
    *   Why the workflows are taking forever.

