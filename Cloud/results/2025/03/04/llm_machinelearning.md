---
title: "Machine Learning Subreddit"
date: "2025-03-04"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machine learning", "AI", "research"]
---

# Overall Ranking and Top Discussions
1.  [[R] Cautious Optimizers: Improving Training with One Line of Code](https://arxiv.org/pdf/2411.16085) (Score: 105)
    *   Discussion about a new optimizer and its potential impact on training and convergence.
2.  [[D] ICLR 2025 first timers here? Share what got you accepted](https://www.reddit.com/r/MachineLearning/comments/1j35gpt/d_iclr_2025_first_timers_here_share_what_got_you/) (Score: 24)
    *   A thread for first-time attendees of ICLR 2025 to share their accepted papers.
3.  [[P] Advice, or guidance on how to create an instruction dataset](https://www.reddit.com/r/MachineLearning/comments/1j320ns/p_advice_or_guidance_on_how_to_create_an/) (Score: 7)
    *   Seeking advice on creating an instruction dataset, with suggestions to use LLMs for preparation.
4.  [[D] Benefits of Purged CV in Time Series?](https://www.reddit.com/r/MachineLearning/comments/1j392dd/d_benefits_of_purged_cv_in_time_series/) (Score: 5)
    *   Discussion about the benefits of using Purged Cross-Validation in time series analysis to prevent data leakage.
5.  [[P] classification and detection of leukemia](https://www.reddit.com/r/MachineLearning/comments/1j3de29/p_classification_and_detection_of_leukemia/) (Score: 2)
    *   A user asking about classification and detection of leukemia.
6.  [[D] LF Data annotators for machine learning](https://www.reddit.com/r/MachineLearning/comments/1j37zg9/d_lf_data_annotators_for_machine_learning/) (Score: 1)
    *   Discussion regarding data annotators for machine learning.
7.  [[D] Looking for Insights on Long-Term AI Memory & Context Retention](https://www.reddit.com/r/MachineLearning/comments/1j3j81a/d_looking_for_insights_on_longterm_ai_memory/) (Score: 1)
    *   Seeking insights on long-term AI memory and context retention.
8.  [[D] Confusion matrix . Confusion](https://www.reddit.com/r/MachineLearning/comments/1j3jp8p/d_confusion_matrix_confusion/) (Score: 1)
    *   Discussion of confusion matrix and its components.
9.  [[D] Which PhD thesis should I pick? (Multimodal , XAI, Meta learning, MTL..)](https://www.reddit.com/r/MachineLearning/comments/1j3egx3/d_which_phd_thesis_should_i_pick_multimodal_xai/) (Score: 0)
    *   Seeking advice on choosing a PhD thesis topic among multimodal learning, XAI, meta-learning, and MTL.
10. [[D] Why do LLM's produce different answers with same input?](https://www.reddit.com/r/MachineLearning/comments/1j3erqf/d_why_do_llms_produce_different_answers_with_same/) (Score: 0)
    *   Discussion on why Large Language Models produce different answers with the same input, focusing on temperature and randomness.
11. [[D] Question: Does anyone want to build in AI voice but can't because of price? I'm considering exposing a $1/hr API](https://www.reddit.com/r/MachineLearning/comments/1j3fs8r/d_question_does_anyone_want_to_build_in_ai_voice/) (Score: 0)
    *   A user is questioning whether anyone is interested in utilizing an AI voice building platform priced at $1/hr.

# Detailed Analysis by Thread
**[[R] Cautious Optimizers: Improving Training with One Line of Code (Score: 105)](https://arxiv.org/pdf/2411.16085)**
*  **Summary:** The discussion revolves around a research paper introducing a new optimizer called "Cautious Optimizers." Users are discussing its potential impact on training, convergence, and its relationship to existing optimizers like Rprop and Adam. Some users are also sharing a link to the code implementation on GitHub.
*  **Emotion:** The overall emotional tone is mixed. The discussion includes some negative sentiments related to the difficulty in theoretically reasoning about the tweak, but also positive sentiments related to the naming of the optimizer. The dominant emotions appear to be Neutral, driven by discussions of implementation and comparisons to other optimizers.
*  **Top 3 Points of View:**
    *   The new optimizer might impact global convergence proofs.
    *   The optimizer has similarities to existing optimizers like Rprop.
    *   The tweak is hard for theorists to reason about.

**[[D] ICLR 2025 first timers here? Share what got you accepted (Score: 24)](https://www.reddit.com/r/MachineLearning/comments/1j35gpt/d_iclr_2025_first_timers_here_share_what_got_you/)**
*  **Summary:** This thread is a place for first-time attendees of the ICLR 2025 conference to share their experiences and the work that led to their acceptance. Users are congratulating each other and expressing excitement about the conference.
*  **Emotion:** The overall emotional tone is positive, with users congratulating each other and expressing excitement.
*  **Top 3 Points of View:**
    *   Congratulations and excitement for paper acceptance at ICLR.
    *   Suggestion to consider robotics conferences for similar work.
    *   Excitement about meeting and learning from others.

**[[P] Advice, or guidance on how to create an instruction dataset (Score: 7)](https://www.reddit.com/r/MachineLearning/comments/1j320ns/p_advice_or_guidance_on_how_to_create_an/)**
*  **Summary:** A user is seeking advice on how to create an instruction dataset.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Use another LLM to prepare the data.

**[[D] Benefits of Purged CV in Time Series? (Score: 5)](https://www.reddit.com/r/MachineLearning/comments/1j392dd/d_benefits_of_purged_cv_in_time_series/)**
*  **Summary:** This thread discusses the benefits of using Purged Cross-Validation (CV) in time series analysis. The main focus is on preventing data leakage due to autocorrelation.
*  **Emotion:** The overall emotional tone is neutral, focusing on technical explanations.
*  **Top 3 Points of View:**
    *   Purged CV prevents data leakage in time series.
    *   Time series data has autocorrelation, so information might leak.
    *   Embargoing and purging are used to prevent information leakage

**[[P] classification and detection of leukemia (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1j3de29/p_classification_and_detection_of_leukemia/)**
*  **Summary:** A user asking about classification and detection of leukemia.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Look it up on Kaggle.

**[[D] LF Data annotators for machine learning (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1j37zg9/d_lf_data_annotators_for_machine_learning/)**
*  **Summary:** Discussion regarding data annotators for machine learning.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Recommendation to use "Label Your Data" for data annotation.

**[[D] Looking for Insights on Long-Term AI Memory & Context Retention (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1j3j81a/d_looking_for_insights_on_longterm_ai_memory/)**
*  **Summary:** Seeking insights on long-term AI memory and context retention.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Suggesting a research paper regarding AI memory.

**[[D] Confusion matrix . Confusion (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1j3jp8p/d_confusion_matrix_confusion/)**
*  **Summary:** Discussion of confusion matrix and its components.
*  **Emotion:** The overall emotional tone is positive.
*  **Top 3 Points of View:**
    *   Belief that there is mistake in the table labeling in a confusion matrix

**[[D] Which PhD thesis should I pick? (Multimodal , XAI, Meta learning, MTL..) (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1j3egx3/d_which_phd_thesis_should_i_pick_multimodal_xai/)**
*  **Summary:** Seeking advice on choosing a PhD thesis topic among multimodal learning, XAI, meta-learning, and MTL. Users discuss the importance of data sources, supervisors, and collaboration.
*  **Emotion:** The overall emotional tone is neutral, offering advice and considerations.
*  **Top 3 Points of View:**
    *   External factors like the supervisor are more important.
    *   Each modality will take weeks-months to fully understand.
    *   Get a solid grounding on the technical matters before getting into medical data.

**[[D] Why do LLM's produce different answers with same input? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1j3erqf/d_why_do_llms_produce_different_answers_with_same/)**
*  **Summary:** Discussion on why Large Language Models produce different answers with the same input, focusing on temperature and randomness.
*  **Emotion:** The overall emotional tone is neutral, explaining the technical reasons behind the phenomenon.
*  **Top 3 Points of View:**
    *   LLMs predict a probability distribution over tokens and draw from that distribution.
    *   The "temperature" setting controls the randomness.
    *   GPU parallelism and floating point inaccuracies can also contribute.

**[[D] Question: Does anyone want to build in AI voice but can't because of price? I'm considering exposing a $1/hr API (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1j3fs8r/d_question_does_anyone_want_to_build_in_ai_voice/)**
*  **Summary:** A user is questioning whether anyone is interested in utilizing an AI voice building platform priced at $1/hr.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   AI voices can be cloned on github for free.
    *   A user is offering a realtime voice API for closer to $0.25/h.
