---
title: "Stable Diffusion Subreddit"
date: "2025-03-04"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [Elden Ring According To AI (Lots of Wan i2v awesomeness)](https://v.redd.it/hzwtp6gzrpme1) (Score: 150)
    *   People are discussing a video generated by AI that depicts Elden Ring. The quality and potential of AI in creating such content is a key point.
2.  [Channel Wan Local Weather](https://v.redd.it/zpu43hfj8qme1) (Score: 46)
    *   The thread is about AI-generated local weather forecasts using Channel Wan. Users are exploring possibilities and making jokes related to the content.
3.  [Torturing my poor RTX 3090 with the highest resolution on I2V Wan.](https://v.redd.it/7pll867uxpme1) (Score: 12)
    *   Users are sharing experiences and tips on running I2V Wan at high resolutions, including hardware configurations, rendering times, and optimization techniques.
4.  [Avat3r: Avatars via Guassian Reconstruction](https://www.reddit.com/r/StableDiffusion/comments/1j3krgq/avat3r_avatars_via_guassian_reconstruction/) (Score: 4)
    *   This thread is discussing avatars created using Gaussian reconstruction and the drawbacks of this technique.
5.  [Can I run WAN with 16gb 4060ti?](https://www.reddit.com/r/StableDiffusion/comments/1j3hyi9/can_i_run_wan_with_16gb_4060ti/) (Score: 3)
    *   Users are discussing whether a 16GB 4060ti can run WAN, sharing their experiences with different configurations and settings.
6.  [Why do VRAM requirements scale with number of frames in WAN2.1?](https://www.reddit.com/r/StableDiffusion/comments/1j3isjg/why_do_vram_requirements_scale_with_number_of/) (Score: 3)
    *   The thread discusses why VRAM requirements increase with the number of frames in WAN2.1 and potential workarounds for memory limitations.
7.  [RuntimeError: CUDA error: no kernel image is available HELP Please](https://www.reddit.com/r/StableDiffusion/comments/1j3ix0m/runtimeerror_cuda_error_no_kernel_image_is/) (Score: 3)
    *   Users are troubleshooting a CUDA error related to kernel image availability, with suggestions on installing the correct PyTorch version.
8.  [What workflows can do jersey swaps in videos?](https://v.redd.it/nnxx4pddxpme1) (Score: 2)
    *   This thread briefly mentions the interesting potential of workflows for doing jersey swaps in videos.
9.  [Hunyuan Video Extension](https://www.reddit.com/r/StableDiffusion/comments/1j3gxft/hunyuan_video_extension/) (Score: 2)
    *   The thread discusses the Hunyuan Video Extension and potential ways to improve video generation using I2V with specific schedulers or frame-by-frame adherence scheduling.
10. [Stable Diffusion WebUi](https://www.reddit.com/r/StableDiffusion/comments/1j3h3td/stable_diffusion_webui/) (Score: 2)
    *   New users are looking for resources to get started with Stable Diffusion WebUI, with links to helpful websites and YouTube playlists provided.
11. [Tip for dataset pruning: "head out of frame"](https://www.reddit.com/r/StableDiffusion/comments/1j3hd7d/tip_for_dataset_pruning_head_out_of_frame/) (Score: 2)
    *   The thread discusses a technique for dataset pruning ("head out of frame") and the idea of generating full text captions from tags.
12. [Why my OpenPose ControlNet Doesn't Work with Flux? Any Advice?](https://www.reddit.com/r/StableDiffusion/comments/1j3jbxz/why_my_openpose_controlnet_doesnt_work_with_flux/) (Score: 2)
    *   Users are seeking advice on why OpenPose ControlNet isn't working with Flux, with suggestions on using the correct versions and potential issues with the mode select node.
13. [Wan 2.1 ComfyUI (can you help me ?)](https://www.reddit.com/r/StableDiffusion/comments/1j3iatb/wan_21_comfyui_can_you_help_me/) (Score: 1)
    *   A user is asking for help with Wan 2.1 in ComfyUI, with advice given to start with basic workflows and avoid overcomplicating things.
14. [Anyone tried with success recreating a Naruto character lora ?](https://www.reddit.com/r/StableDiffusion/comments/1j3j3tp/anyone_tried_with_success_recreating_a_naruto/) (Score: 1)
    *   The thread discusses the possibility of recreating Naruto character LoRAs, with advice on using proper booru tags and having a good dataset.
15. [Newbie question, If I want to make a finger or toe longer for example.. how should I do it?](https://www.reddit.com/r/StableDiffusion/comments/1j3jm6a/newbie_question_if_i_want_to_make_a_finger_or_toe/) (Score: 1)
    *   A newbie is asking how to lengthen a finger or toe in Stable Diffusion, with suggestions to use ControlNet and a modified hand image generated with Design Doll.
16. [What Is The Best Prompt Adherence Image Model](https://www.reddit.com/r/StableDiffusion/comments/1j3gq0b/what_is_the_best_prompt_adherence_image_model/) (Score: 0)
    *   The thread discusses the best image models for prompt adherence, noting that it depends on the use case and that SDXL is good for anime-style images.
17. [Is there a online video generator that is like promptchan but pay as you go rather than monthly payment?](https://www.reddit.com/r/StableDiffusion/comments/1j3j37t/is_there_a_online_video_generator_that_is_like/) (Score: 0)
    *   A user is looking for an online video generator with a pay-as-you-go model, with a suggestion to use Runpod with ComfyUI and WAN.

# Detailed Analysis by Thread
**[Elden Ring According To AI (Lots of Wan i2v awesomeness) (Score: 150)](https://v.redd.it/hzwtp6gzrpme1)**
*  **Summary:**  Users are discussing a video generated by AI that depicts Elden Ring. They comment on the inconsistency in quality, but acknowledge the impressive beginning. They also speculate about the future potential of AI in creating such content.
*  **Emotion:** The overall emotional tone is positive, with users expressing excitement and amazement at the video's quality and the future potential of AI. There are also neutral comments with questions.
*  **Top 3 Points of View:**
    *   The beginning of the video is very impressive, despite some inconsistencies in quality.
    *   The future of AI video generation is promising.
    *   The creator is asked about the workflow and tools used to generate the video.

**[Channel Wan Local Weather (Score: 46)](https://v.redd.it/zpu43hfj8qme1)**
*  **Summary:** The thread revolves around an AI-generated local weather forecast using Channel Wan. Users are commenting on the possibilities this opens up and are also sharing jokes and observations related to the content. The technical aspects of the video creation (software, hardware) are also discussed.
*  **Emotion:** The overall emotional tone is neutral, with a hint of amusement. Users are making observations and jokes, but the sentiment remains largely neutral.
*  **Top 3 Points of View:**
    *   This technology opens up many possibilities for AI video generation.
    *   There's humor to be found in the AI's interpretation of weather forecasts.
    *   The video was generated using Wan 14B 720p fp8 and other specified parameters.

**[Torturing my poor RTX 3090 with the highest resolution on I2V Wan. (Score: 12)](https://v.redd.it/7pll867uxpme1)**
*  **Summary:** Users share experiences and tips about running I2V Wan at high resolutions. The conversation includes hardware setups, render times, and optimization strategies for faster performance. They also discuss software and techniques, like AdaptiveGuider, sageatten, and tea cache, and the benefits of different resolutions.
*  **Emotion:** The emotional tone is mostly neutral, with a hint of positive sentiment as users share helpful tips and express amazement.
*  **Top 3 Points of View:**
    *   High resolution video generation with I2V Wan is resource-intensive.
    *   Optimization techniques like AdaptiveGuider and block swapping are crucial for performance.
    *   The optimal resolution for I2V Wan is 720x720.

**[Avat3r: Avatars via Guassian Reconstruction (Score: 4)](https://www.reddit.com/r/StableDiffusion/comments/1j3krgq/avat3r_avatars_via_guassian_reconstruction/)**
*  **Summary:** This thread discusses avatars created using Gaussian reconstruction. The discussion is focused around Gaussian splats make, as well as some drawbacks to the technique, especially compared to traditional triangle-based 3D modeling for things like hair.
*  **Emotion:** The emotional tone is slightly negative, as the commenter expresses disappointment with the Gaussian splat method due to glitching and difficulty in converting them to triangles.
*  **Top 3 Points of View:**
    *   Gaussian splats are cool but have limitations, such as difficulty in conversion to triangles.
    *   Hair is difficult to model in both Gaussian splats and traditional 3D polygon/triangle workflows.
    *   Gaussian splats are easier to use for blurry parts of a person.

**[Can I run WAN with 16gb 4060ti? (Score: 3)](https://www.reddit.com/r/StableDiffusion/comments/1j3hyi9/can_i_run_wan_with_16gb_4060ti/)**
*  **Summary:** Users discuss whether a 16GB 4060ti can run WAN. They share their experiences with different configurations, settings, and optimization techniques, including the use of negative prompts and Tea Cache.
*  **Emotion:** The emotional tone is mostly neutral. Users share practical information and test results. There is a hint of frustration in one comment encouraging experimentation.
*  **Top 3 Points of View:**
    *   Yes, WAN can run on a 16GB 4060ti.
    *   Negative prompts are essential for good results.
    *   Experimentation is encouraged to find optimal settings.

**[Why do VRAM requirements scale with number of frames in WAN2.1? (Score: 3)](https://www.reddit.com/r/StableDiffusion/comments/1j3isjg/why_do_vram_requirements_scale_with_number_of/)**
*  **Summary:** This thread explores the reasons behind the increased VRAM requirements in WAN2.1 as the number of frames increases. Users provide explanations such as temporal attention and the lack of a tiled diffusion equivalent, which leads to the entire video being processed at once.
*  **Emotion:** The emotional tone is neutral, focusing on technical explanations.
*  **Top 3 Points of View:**
    *   VRAM scales due to temporal attention mechanisms.
    *   There's no tiled diffusion equivalent for video processing.
    *   A possible workaround is to render X frames and then continue with I2V, but errors can accumulate over time.

**[RuntimeError: CUDA error: no kernel image is available HELP Please (Score: 3)](https://www.reddit.com/r/StableDiffusion/comments/1j3ix0m/runtimeerror_cuda_error_no_kernel_image_is/)**
*  **Summary:** Users are troubleshooting a CUDA error. They discuss potential causes, such as incorrect PyTorch versions, and offer solutions, including specific steps to uninstall and reinstall PyTorch with the appropriate CUDA compatibility.
*  **Emotion:** The overall emotional tone is neutral, with a hint of hopefulness as users try to help each other solve the error.
*  **Top 3 Points of View:**
    *   The error is likely due to an incompatible PyTorch version for the installed CUDA.
    *   Uninstalling and reinstalling PyTorch with the correct version is a solution.
    *   Consulting AI tools like ChatGPT can provide assistance.

**[What workflows can do jersey swaps in videos? (Score: 2)](https://v.redd.it/nnxx4pddxpme1)**
*  **Summary:** This thread briefly expresses interest in workflows for doing jersey swaps in videos.
*  **Emotion:** The emotional tone is positive due to it being found interesting.
*  **Top 3 Points of View:**
    *   This workflow is interesting.

**[Hunyuan Video Extension (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1j3gxft/hunyuan_video_extension/)**
*  **Summary:** The thread discusses the Hunyuan Video Extension and potential ways to improve video generation using I2V. Users suggest using schedulers specifically made for this use case or frame-by-frame overlapping adherence scheduling.
*  **Emotion:** The emotional tone is neutral, with some users sharing technical ideas.
*  **Top 3 Points of View:**
    *   I2V may be improved with a scheduler specifically made for this kind of use case.
    *   Context from preceding frames could be leveraged for better results.
    *   Frame-by-frame overlapping adherence scheduling could be a solution.

**[Stable Diffusion WebUi (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1j3h3td/stable_diffusion_webui/)**
*  **Summary:** The thread is about new users looking for resources to get started with Stable Diffusion WebUI. Links to helpful websites and YouTube playlists are provided.
*  **Emotion:** The emotional tone is neutral and helpful.
*  **Top 3 Points of View:**
    *   Resources are needed for new users to learn Stable Diffusion WebUI.
    *   Links to tutorials are provided as a helpful starting point.

**[Tip for dataset pruning: "head out of frame" (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1j3hd7d/tip_for_dataset_pruning_head_out_of_frame/)**
*  **Summary:** The thread discusses a technique for dataset pruning ("head out of frame") and the idea of generating full text captions from tags generated via WD14 tagger results.
*  **Emotion:** The emotional tone is slightly positive, reflecting the interesting take of the pruning technique.
*  **Top 3 Points of View:**
    *   Generating "full text" captions from tags could be effective and fast.

**[Why my OpenPose ControlNet Doesn't Work with Flux? Any Advice? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1j3jbxz/why_my_openpose_controlnet_doesnt_work_with_flux/)**
*  **Summary:** Users are seeking advice on why OpenPose ControlNet isn't working with Flux. Suggestions include using the correct SDXL/SD1.5 versions and acknowledging potential issues with the Shakkerlabs mode select node.
*  **Emotion:** The emotional tone is negative and a bit helpless.
*  **Top 3 Points of View:**
    *   You need to use the correct SDXL/SD1.5 CNet versions.
    *   Shakkerlabs mode select node could be the problem.
    *   It is hard to get decent results with openpose and flux

**[Wan 2.1 ComfyUI (can you help me ?) (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1j3iatb/wan_21_comfyui_can_you_help_me/)**
*  **Summary:** The thread is about a user asking for help with Wan 2.1 in ComfyUI. Advice is given to start with the basic developer workflows provided by Wan and avoid overcomplicating things.
*  **Emotion:** The emotional tone is neutral, with an emphasis on providing helpful guidance.
*  **Top 3 Points of View:**
    *   Beginners should start with basic workflows.
    *   Avoid overly complex workflows with unnecessary nodes.
    *   Understand each node's function before adding more.

**[Anyone tried with success recreating a Naruto character lora ? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1j3j3tp/anyone_tried_with_success_recreating_a_naruto/)**
*  **Summary:** The thread discusses the feasibility of recreating Naruto character LoRAs. The key advice is to use proper booru tags and have a good dataset.
*  **Emotion:** The emotional tone is positive, suggesting that it is possible to create such LoRAs.
*  **Top 3 Points of View:**
    *   Recreating Naruto character LoRAs is doable.
    *   Using proper booru tags and a good dataset is important.
    *   It involves a lot of trial and error.

**[Newbie question, If I want to make a finger or toe longer for example.. how should I do it? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1j3jm6a/newbie_question_if_i_want_to_make_a_finger_or_toe/)**
*  **Summary:** A newbie is asking how to lengthen a finger or toe in Stable Diffusion. The suggestion is to use ControlNet and feed it an image of the modified hand generated with Design Doll.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Use ControlNet to modify body part lengths.
    *   Generate a modified hand image with Design Doll.
    *   Models may not understand instructions such as "really long toes" out of the box.

**[What Is The Best Prompt Adherence Image Model (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1j3gq0b/what_is_the_best_prompt_adherence_image_model/)**
*  **Summary:** The thread discusses the best image models for prompt adherence, noting that it depends on the use case. SDXL is considered the best option for anime-style images. LoRAs that claim to improve prompt adherence or image quality are typically snake oil.
*  **Emotion:** The emotional tone is neutral, with some negative sentiment towards LoRAs claiming to improve prompt adherence.
*  **Top 3 Points of View:**
    *   The best model for prompt adherence depends on the use case.
    *   SDXL is a good option for anime-style images.
    *   LoRAs that claim to improve prompt adherence or image quality are often ineffective.

**[Is there a online video generator that is like promptchan but pay as you go rather than monthly payment? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1j3j37t/is_there_a_online_video_generator_that_is_like/)**
*  **Summary:** A user is looking for an online video generator with a pay-as-you-go model. They receive the suggestion to use Runpod with ComfyUI and WAN.
*  **Emotion:** The emotional tone is neutral and helpful.
*  **Top 3 Points of View:**
    *   Use Runpod with ComfyUI and WAN for a pay-as-you-go solution.
