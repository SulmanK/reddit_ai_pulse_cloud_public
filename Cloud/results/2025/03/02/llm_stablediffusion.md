---
title: "Stable Diffusion Subreddit"
date: "2025-03-02"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [TeaCache, TorchCompile, SageAttention and SDPA at 30 steps (up to ~70% faster on Wan I2V 480p)](https://v.redd.it/sjojb28gabme1) (Score: 96)
    *   This thread discusses optimizations like TeaCache, TorchCompile, SageAttention, and SDPA to significantly speed up video generation in Stable Diffusion, specifically for the Wan I2V 480p model.
2.  [Wan2.1 GP: generate a 8s WAN 480P video (14B model non quantized) with only 12 GB of VRAM](https://www.reddit.com/r/StableDiffusion/comments/1j1x611/wan21_gp_generate_a_8s_wan_480p_video_14b_model/) (Score: 57)
    *   This thread focuses on generating 8-second WAN 480P videos using the Wan2.1 GP model with only 12 GB of VRAM, opening up possibilities for users with less powerful hardware.
3.  [ComfyUI Wan2.1 14B Image to Video example workflow](https://www.reddit.com/r/StableDiffusion/comments/1j209oq/comfyui_wan21_14b_image_to_video_example_workflow/) (Score: 15)
    *   Users discuss a ComfyUI workflow for image-to-video generation using the Wan2.1 14B model, highlighting its performance on a laptop with a 4070 mobile GPU and 8GB of VRAM.
4.  [Stability Ai should release some custom checkpoints for SD 3.5 large/medium and loras. To keep users interested. Shouldn't be too hard](https://www.reddit.com/r/StableDiffusion/comments/1j20b8s/stability_ai_should_release_some_custom/) (Score: 5)
    *   This thread discusses the perceived lack of interest in SD 3.5 and suggests that Stability AI should release custom checkpoints and LoRAs to revitalize user engagement.
5.  [This took me an hour and a half to make. Wan I2V :((https://v.redd.it/bzxlb7pa9cme1](https://v.redd.it/bzxlb7pa9cme1) (Score: 4)
    *   The user shares a video created using Wan I2V and expresses frustration about the long processing time.
6.  [Buying a prebuilt for StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/1j1wdrb/buying_a_prebuilt_for_stablediffusion/) (Score: 4)
    *   Users discuss the pros and cons of buying prebuilt computers specifically for Stable Diffusion, focusing on price, cooling, upgradability, and the current state of consumer graphics cards.
7.  [Is it possible to get this result?](https://i.redd.it/o9k8crehbcme1.png) (Score: 3)
    *   A user seeks advice on how to replicate a specific image style, sparking a discussion about style transfer techniques in AI image generation.
8.  [What is the best way to compose a complex scene?](https://www.reddit.com/r/StableDiffusion/comments/1j1worv/what_is_the_best_way_to_compose_a_complex_scene/) (Score: 2)
    *   This thread explores different methods for composing complex scenes in Stable Diffusion, including using Blender, ControlNet, inpainting, and regional prompting.
9.  [Segment multiple characters workflow](https://www.reddit.com/r/StableDiffusion/comments/1j1x1bl/segment_multiple_characters_workflow/) (Score: 2)
    *   The thread discusses using YOLO, Impact and Impact-Subpack to segment multiple characters in a Stable Diffusion workflow.
10. [One more wan vid on your feed](https://v.redd.it/n1tvzr0c2cme1) (Score: 0)
    * A user shares their WAN video and expresses their amazement at the quality achievable on personal hardware.
11. [Most cost effective AWS instance ?](https://www.reddit.com/r/StableDiffusion/comments/1j1ymrd/most_cost_effective_aws_instance/) (Score: 0)
    * Users are asking what is the most cost effective AWS instance to use for stable diffusion.
12. [Which models should I use to produce Fritz Lang-style imagery?](https://www.reddit.com/r/StableDiffusion/comments/1j204w0/which_models_should_i_use_to_produce_fritz/) (Score: 0)
    * The user is asking what models to use for Fritz Lang style imagery.

# Detailed Analysis by Thread
**[[D] TeaCache, TorchCompile, SageAttention and SDPA at 30 steps (up to ~70% faster on Wan I2V 480p)](https://v.redd.it/sjojb28gabme1) (Score: 96)**
*   **Summary:** The thread discusses optimizations like TeaCache, TorchCompile, SageAttention, and SDPA to significantly speed up video generation in Stable Diffusion, specifically for the Wan I2V 480p model. Users share their experiences with these optimizations on various hardware configurations and workflows.
*   **Emotion:** The overall emotional tone is neutral, with users providing technical information and asking specific questions. There are some negative emotions expressed by users who are having trouble getting the optimizations to work, and positive emotions from those who are seeing significant speed improvements.
*   **Top 3 Points of View:**
    *   These optimizations can greatly improve the speed of video generation in Stable Diffusion.
    *   The effectiveness of these optimizations depends on the user's hardware and workflow.
    *   Some users are experiencing difficulties setting up and using these optimizations.

**[[D] Wan2.1 GP: generate a 8s WAN 480P video (14B model non quantized) with only 12 GB of VRAM](https://www.reddit.com/r/StableDiffusion/comments/1j1x611/wan21_gp_generate_a_8s_wan_480p_video_14b_model/) (Score: 57)**
*   **Summary:** This thread focuses on generating 8-second WAN 480P videos using the Wan2.1 GP model with only 12 GB of VRAM, opening up possibilities for users with less powerful hardware. Users are requesting tutorials and sharing their excitement about the potential for lower VRAM requirements.
*   **Emotion:** The dominant emotion is positive, driven by the excitement of users who have been limited by their VRAM and see this as a potential solution. There's also a sense of anticipation and hope.
*   **Top 3 Points of View:**
    *   The reduced VRAM requirement is a significant advancement for users with less powerful GPUs.
    *   Users are eager to learn how to implement this optimization and are requesting tutorials.
    *   There are questions about the speed and integration of this optimization into existing workflows like ComfyUI.

**[[D] ComfyUI Wan2.1 14B Image to Video example workflow](https://www.reddit.com/r/StableDiffusion/comments/1j209oq/comfyui_wan21_14b_image_to_video_example_workflow/) (Score: 15)**
*   **Summary:** Users discuss a ComfyUI workflow for image-to-video generation using the Wan2.1 14B model, highlighting its performance on a laptop with a 4070 mobile GPU and 8GB of VRAM. Users share their own experiences and wait to test the new workflow.
*   **Emotion:** The overall emotional tone is positive and neutral. The positive sentiment stems from the fact that the workflow can run in a laptop with 8GB of VRAM.
*   **Top 3 Points of View:**
    *   Excitement about running video models on lower-end hardware.
    *   Eagerness to test the provided ComfyUI workflow.
    *   Questions about expected generation times.

**[Stability Ai should release some custom checkpoints for SD 3.5 large/medium and loras. To keep users interested. Shouldn't be too hard](https://www.reddit.com/r/StableDiffusion/comments/1j20b8s/stability_ai_should_release_some_custom/) (Score: 5)**
*   **Summary:** This thread discusses the perceived lack of interest in SD 3.5 and suggests that Stability AI should release custom checkpoints and LoRAs to revitalize user engagement. Users cite issues with anatomy, lack of fine-tunes, and the release of competing models as reasons for the decline in interest.
*   **Emotion:** The overall tone is negative and neutral, with users expressing disappointment and disillusionment with SD 3.5. Some users are holding out hope for future improvements, but many believe that Stability AI has missed an opportunity.
*   **Top 3 Points of View:**
    *   SD 3.5 has failed to capture user interest due to issues with anatomy and lack of customization options.
    *   Stability AI should release custom checkpoints and LoRAs to address these issues and revive the model's popularity.
    *   The release of competing models and Stability AI's own handling of the SD3 release have further contributed to the decline in interest.

**[This took me an hour and a half to make. Wan I2V :((https://v.redd.it/bzxlb7pa9cme1](https://v.redd.it/bzxlb7pa9cme1) (Score: 4)**
*   **Summary:** The user shares a video created using Wan I2V and expresses frustration about the long processing time. Other users offer suggestions to speed up the process, such as reducing the number of frames and using interpolation.
*   **Emotion:** The overall emotional tone is negative due to the user's frustration with the long processing time, but there are also elements of helpfulness and advice from other users.
*   **Top 3 Points of View:**
    *   Wan I2V can be very time-consuming.
    *   Optimizing the frame rate and using interpolation can help reduce processing time.
    *   Hardware specifications play a significant role in processing speed.

**[Buying a prebuilt for StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/1j1wdrb/buying_a_prebuilt_for_stablediffusion/) (Score: 4)**
*   **Summary:** Users discuss the pros and cons of buying prebuilt computers specifically for Stable Diffusion. Considerations include price, cooling, upgradability, and the value proposition of current high-end graphics cards.
*   **Emotion:** The emotional tone is mostly neutral, with some elements of negativity due to concerns about pricing, hardware limitations, and potential buyer's remorse.
*   **Top 3 Points of View:**
    *   Prebuilt PCs can be convenient but may have limitations in terms of cooling and upgradability.
    *   The current generation of high-end graphics cards may not be worth the investment due to perceived limitations.
    *   Waiting for the next generation of GPUs with larger VRAM may be a better long-term strategy.

**[Is it possible to get this result?](https://i.redd.it/o9k8crehbcme1.png) (Score: 3)**
*   **Summary:** A user seeks advice on how to replicate a specific image style. Users discuss the need for style transfer techniques and explore potential methods.
*   **Emotion:** The overall emotional tone is neutral, with a hint of curiosity and interest in finding a solution.
*   **Top 2 Points of View:**
    *   Achieving the desired style requires style transfer techniques.
    *   There is uncertainty about the best method to use.

**[What is the best way to compose a complex scene?](https://www.reddit.com/r/StableDiffusion/comments/1j1worv/what_is_the_best_way_to_compose_a_complex_scene/) (Score: 2)**
*   **Summary:** This thread explores different methods for composing complex scenes in Stable Diffusion, including using Blender, ControlNet, inpainting, and regional prompting. Users offer specific advice and workflow suggestions.
*   **Emotion:** The overall emotional tone is neutral to positive, with users offering helpful advice and sharing their experiences.
*   **Top 3 Points of View:**
    *   Blender and ControlNet can be used to create a detailed scene and guide the AI generation.
    *   Inpainting allows for precise control over individual elements within the scene.
    *   Regional prompting can be used to control the style and content of different areas of the image.

**[Segment multiple characters workflow](https://www.reddit.com/r/StableDiffusion/comments/1j1x1bl/segment_multiple_characters_workflow/) (Score: 2)**
*   **Summary:** The thread discusses using YOLO, Impact and Impact-Subpack to segment multiple characters in a Stable Diffusion workflow.
*   **Emotion:** The overall emotional tone is neutral, with users providing technical information.
*   **Top 1 Points of View:**
    *   YOLO, Impact and Impact-Subpack can be used to segment multiple characters in a Stable Diffusion workflow.

**[One more wan vid on your feed](https://v.redd.it/n1tvzr0c2cme1) (Score: 0)**
*   **Summary:** The fact that he can produce high-quality media entirely on his own PC is incredible. He used WAN 14B 720p Q5KM, FP8 text encoder and upscaling the video.
*   **Emotion:** The overall emotional tone is positive.
*   **Top 1 Points of View:**
    *   It is amazing to produce high-quality media on a personal computer.

**[Most cost effective AWS instance ?](https://www.reddit.com/r/StableDiffusion/comments/1j1ymrd/most_cost_effective_aws_instance/) (Score: 0)**
*   **Summary:** Probably the G6e is the best for your use case. But even the 48 GB VRAM are probably overkill. And then compare the costs with RunPod or [Vast.ai](http://Vast.ai) (a 4090 is most likely all you need for Comfy).
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 2 Points of View:**
    *   Probably the G6e is the best for your use case. But even the 48 GB VRAM are probably overkill.
    *   Compare the costs with RunPod or [Vast.ai](http://Vast.ai) (a 4090 is most likely all you need for Comfy).

**[Which models should I use to produce Fritz Lang-style imagery?](https://www.reddit.com/r/StableDiffusion/comments/1j204w0/which_models_should_i_use_to_produce_fritz/) (Score: 0)**
*   **Summary:** Train your own LORA?
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 1 Points of View:**
    *   Train your own LORA?
