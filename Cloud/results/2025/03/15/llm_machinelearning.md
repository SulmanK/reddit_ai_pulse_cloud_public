---
title: "Machine Learning Subreddit"
date: "2025-03-15"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machine learning", "AI", "MLOps"]
---

# Overall Ranking and Top Discussions
1.  [[R] Transformers without Normalization (FAIR Meta, New York University, MIT, Princeton University)](https://www.reddit.com/r/MachineLearning/comments/1jbs7xg/r_transformers_without_normalization_fair_meta/) (Score: 129)
    * Discussing a new paper on Transformers without Normalization from FAIR Meta, New York University, MIT, and Princeton University.
2.  [[D] The Cultural Divide between Mathematics and AI](https://sugaku.net/content/understanding-the-cultural-divide-between-mathematics-and-ai/) (Score: 7)
    * A discussion about the cultural differences between the fields of mathematics and artificial intelligence.
3.  [[D] 10 Fallacies of MLOps](https://www.reddit.com/r/MachineLearning/comments/1jbovy1/d_10_fallacies_of_mlops/) (Score: 6)
    * Discussing and highlighting 10 common fallacies or misconceptions in the field of MLOps (Machine Learning Operations).
4.  [[R] Recent advances in recurrent neural networks---any sleepers?](https://www.reddit.com/r/MachineLearning/comments/1jbzcoc/r_recent_advances_in_recurrent_neural_networksany/) (Score: 5)
    *  Inquiring about recent, potentially overlooked, advancements in recurrent neural networks (RNNs).
5.  [[P] Help with Audio Denoising Model (offline)](https://www.reddit.com/r/MachineLearning/comments/1jbes50/p_help_with_audio_denoising_model_offline/) (Score: 4)
    * Seeking assistance and feedback on an offline audio denoising model.
6.  [[D] Looking for feedback on a build](https://www.reddit.com/r/MachineLearning/comments/1jbrjw2/d_looking_for_feedback_on_a_build/) (Score: 3)
    * Requesting feedback on a computer hardware build, presumably for machine learning purposes.
7.  [[P] finance dataset](https://www.reddit.com/r/MachineLearning/comments/1jbuz8h/p_finance_dataset/) (Score: 3)
    * Discussing potential sources and methods for acquiring financial datasets suitable for machine learning projects.
8.  [[D] Kernel functions: How Support Vector Machines transform ghostly ðŸ‘» and pumpkin ðŸŽƒ data! Linear, RBF, Polynomial, and Sigmoid kernels show different ways machine learning algorithms can slice through complex datasets, creating unique decision boundaries that separate the pumpkins from the ghosts.](https://i.redd.it/o53susicnvoe1.png) (Score: 0)
    * A discussion about Kernel functions and Support Vector Machines.
9.  [[P] Finance dataset](https://www.reddit.com/r/MachineLearning/comments/1jbv0hq/p_finance_dataset/) (Score: 0)
    * Discussing potential sources and methods for acquiring financial datasets suitable for machine learning projects.
10. [[D] Which field in AI should I pick for PhD?](https://www.reddit.com/r/MachineLearning/comments/1jbx2zf/d_which_field_in_ai_should_i_pick_for_phd/) (Score: 0)
    * Seeking advice on which specific area of Artificial Intelligence to focus on for a PhD.
11. [[D] Using gRPC in ML systems](https://www.reddit.com/r/MachineLearning/comments/1jbzh5n/d_using_grpc_in_ml_systems/) (Score: 0)
    * Discussing the utilization of gRPC (a high-performance, open-source universal RPC framework) in machine learning systems.

# Detailed Analysis by Thread
**[[R] Transformers without Normalization (FAIR Meta, New York University, MIT, Princeton University) (Score: 129)](https://www.reddit.com/r/MachineLearning/comments/1jbs7xg/r_transformers_without_normalization_fair_meta/)**
*  **Summary:** Discusses a paper from FAIR Meta, New York University, MIT, and Princeton University on transformers without normalization, exploring the use of element-wise tanh and learnable parameters instead of standard normalization techniques.
*  **Emotion:** The overall emotional tone is neutral, with some comments expressing both positive interest and negative concerns about the practicality and sensitivity of the proposed method.
*  **Top 3 Points of View:**
    *   The new method might make the model more sensitive.
    *   The approach separates feature transformation from feature aggregation, similar to depthwise separable convolutions in CNNs.
    *   Replacing normalization with a pointwise operation is beneficial for fusion.

**[[D] The Cultural Divide between Mathematics and AI (Score: 7)](https://sugaku.net/content/understanding-the-cultural-divide-between-mathematics-and-ai/)**
*  **Summary:** A mathematician shares an article about the cultural divide between mathematics and AI, finding it to be a good read.
*  **Emotion:** Predominantly positive, expressing appreciation for the shared article.
*  **Top 3 Points of View:**
    *   The article is a good read for mathematicians.

**[[D] 10 Fallacies of MLOps (Score: 6)](https://www.reddit.com/r/MachineLearning/comments/1jbovy1/d_10_fallacies_of_mlops/)**
*  **Summary:** A user finds a post about 10 fallacies of MLOps useful and agrees with its content.
*  **Emotion:** Strongly positive, with the user expressing agreement and usefulness.
*  **Top 3 Points of View:**
    *   The post is useful and the user agrees with it.

**[[R] Recent advances in recurrent neural networks---any sleepers? (Score: 5)](https://www.reddit.com/r/MachineLearning/comments/1jbzcoc/r_recent_advances_in_recurrent_neural_networksany/)**
*  **Summary:** Discussion on recent advances in RNNs, focusing on alternatives to transformers like Mamba and variants of gated deltanet. The consensus is that transformers are dominant due to massive investment and computational power.
*  **Emotion:** Mostly neutral, providing information and discussing the current state of RNN research in comparison to transformers.
*  **Top 3 Points of View:**
    *   Mamba is the most notable alternative to transformers.
    *   Transformers are unlikely to be surpassed due to the vast resources being invested in them.
    *   Gated deltanet variants with data-dependent state decay and update LR are noteworthy.

**[[P] Help with Audio Denoising Model (offline) (Score: 4)](https://www.reddit.com/r/MachineLearning/comments/1jbes50/p_help_with_audio_denoising_model_offline/)**
*  **Summary:** Users provide suggestions for troubleshooting an audio denoising model, including starting with a simple linear model and checking the sampling rate.
*  **Emotion:** Neutral, offering practical advice and technical insights.
*  **Top 3 Points of View:**
    *   Start with a simple linear model to verify the pipeline.
    *   Consider predicting either the noise or the speech and subtracting it from the input.
    *   Check the sampling rate to rule out aliasing noise and foldback distortion.

**[[D] Looking for feedback on a build (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1jbrjw2/d_looking_for_feedback_on_a_build/)**
*  **Summary:**  Users are discussing the optimal hardware build for machine learning, suggesting Ryzen CPUs for AVX-512 and DDR5 support and noting the high cost of newer Nvidia GPUs.
*  **Emotion:** Neutral, providing advice and discussing hardware options.
*  **Top 3 Points of View:**
    *   A 7000/9000 series AMD Ryzen is a good choice if you're on a budget.
    *   The Nvidia 3090 has been the gold standard, but is now expensive.
    *   Even 16GB of VRAM is a significant step up.

**[[P] finance dataset (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1jbuz8h/p_finance_dataset/)**
*  **Summary:**  Discusses the difficulty of finding high-quality, free financial datasets, suggesting resources like Stooq (potentially illegal) and IBKR's API (requires a paid account).  A user also shares a link to a resource on financial datasets.
*  **Emotion:** Mixed, with some positive suggestions and realistic warnings about the availability of free data.
*  **Top 3 Points of View:**
    *   High-quality financial data is generally not free.
    *   Stooq might be an option, but it's potentially illegal.
    *   IBKR's API is a good resource, but requires a paid account.

**[[D] Kernel functions: How Support Vector Machines transform ghostly ðŸ‘» and pumpkin ðŸŽƒ data! Linear, RBF, Polynomial, and Sigmoid kernels show different ways machine learning algorithms can slice through complex datasets, creating unique decision boundaries that separate the pumpkins from the ghosts. (Score: 0)](https://i.redd.it/o53susicnvoe1.png)**
*  **Summary:** The discussion revolves around the appropriate categorization of the post and whether it's suitable as a "Discussion" topic.
*  **Emotion:** Neutral, with some mild frustration.
*  **Top 3 Points of View:**
    *   The post is basic material
    *   The post shouldn't be tagged as "Discussion."
    *   The user is questioning the nature of discussion.

**[[P] Finance dataset (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1jbv0hq/p_finance_dataset/)**
*  **Summary:** The user expects that finding a finance dataset requires creating it yourself.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    * You need to create it yourself

**[[D] Which field in AI should I pick for PhD? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1jbx2zf/d_which_field_in_ai_should_i_pick_for_phd/)**
*  **Summary:** Advice for choosing a field for a PhD in AI, covering the importance of personal interest, emerging trends like RL and embodied AI, and potential long-term hardware revolutions.
*  **Emotion:** Neutral, providing a balanced perspective and diverse recommendations.
*  **Top 3 Points of View:**
    *   Choose a field you are genuinely interested in.
    *   RL is important for generative AI in the short term.
    *   Embodied AI/robotics will be very hot in the slightly longer term.

**[[D] Using gRPC in ML systems (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1jbzh5n/d_using_grpc_in_ml_systems/)**
*  **Summary:**  Discussing the use of gRPC in ML systems, with comments highlighting its utility for data transfer between devices and linking to a Hugging Face example.
*  **Emotion:** Neutral, providing practical examples and use cases for gRPC.
*  **Top 3 Points of View:**
    *   gRPC is great for sending data between different devices.
    *   HF Text Generation Inference uses gRPC in the LLM space.
    *   The question is too abstract to provide a reasonable answer without specific problem details.
