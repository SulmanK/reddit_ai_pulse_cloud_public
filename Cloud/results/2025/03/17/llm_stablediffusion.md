---
title: "Stable Diffusion Subreddit"
date: "2025-03-17"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [LTX Flow Edit - Animation to Live Action (What If..? Doctor Strange) Low Vram 8gb](https://v.redd.it/omdw78xeq9pe1) (Score: 163)
    *  A user shares their animation-to-live-action edit using LTX Flow, depicting a "What If...?" scenario featuring Doctor Strange. Viewers are requesting workflows and sharing related links and resources.
2.  [Automatic installation of Pytorch 2.8 (Nightly), Triton & SageAttention 2 into a new Portable or Cloned Comfy with your existing Cuda (v12.4/6/8) get increased speed: v4.2](https://www.reddit.com/r/StableDiffusion/comments/1jdfs6e/automatic_installation_of_pytorch_28_nightly/) (Score: 58)
    *  A user provides an automatic installation script for Pytorch 2.8, Triton, and SageAttention 2 to improve speed in ComfyUI. Users are discussing compatibility, installation issues, and performance improvements.
3.  Adding soon voice cloning to AAFactory repository](https://v.redd.it/ixtvgvwlz9pe1) (Score: 18)
    * The user is integrating Zonos for voice cloning to the AAFactory repository.
4.  [SD1.5 + A1111 till the wheels fall off.](https://www.reddit.com/gallery/1jdjlzx) (Score: 17)
    *  A user is showcasing images generated using SD1.5 and A1111, sparking discussion about the longevity and relevance of older models in the face of newer AI advancements.
5.  [Is there a way to generate accurate text using wan 2.1 ?](https://v.redd.it/j89e01xoq9pe1) (Score: 8)
    *  A user is asking for solutions to generate accurate text using WAN 2.1.
6.  Creating my first videos with Wan 2.1 fp8 using images I've generated in the past](https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/e5e91591-b5da-4baf-805e-25c287395f40/original=true,quality=90/ComfyUI_00002_.jpeg) (Score: 5)
    * A user is creating their first videos with Wan 2.1 fp8 using images they've generated in the past.
7.  [How Do You Guys Use The ComfyUI-to-Python-Extension ?](https://www.reddit.com/r/StableDiffusion/comments/1jdjtal/how_do_you_guys_use_the_comfyuitopythonextension/) (Score: 2)
    * A user is asking for tips on how to use the ComfyUI-to-Python-Extension.
8.  [What are the best face swap techniques for achieving anime movie-level detail?](https://www.reddit.com/r/StableDiffusion/comments/1jdihjf/what_are_the_best_face_swap_techniques_for/) (Score: 1)
    *  A user is requesting information on face-swapping techniques to achieve anime movie-level detail.
9.  [What are the options for enterprise cards with performance comparable to the 3000 series?](https://www.reddit.com/r/StableDiffusion/comments/1jdjp76/what_are_the_options_for_enterprise_cards_with/) (Score: 1)
    *  A user is inquiring about enterprise-level graphics cards that offer performance similar to the 3000 series.
10. [Prompting "Halfway between side view and front view"](https://www.reddit.com/r/StableDiffusion/comments/1jdjpju/prompting_halfway_between_side_view_and_front_view/) (Score: 1)
    *  A user is asking for tips for prompting halfway between a side view and front view.
11. [What are some txt2vid or img2vid models that work on 16gb vram?](https://www.reddit.com/r/StableDiffusion/comments/1jdjvyy/what_are_some_txt2vid_or_img2vid_models_that_work/) (Score: 1)
    *  A user is seeking recommendations for text-to-video (txt2vid) or image-to-video (img2vid) models that can run on a system with 16GB of VRAM.
12. [3D background render to “anime” style?](https://www.reddit.com/r/StableDiffusion/comments/1jdk762/3d_background_render_to_anime_style/) (Score: 1)
    *  A user is asking for advice on converting 3D background renders into an anime style.
13. [Bevel help](https://i.redd.it/ne9jeccizape1.jpeg) (Score: 0)
    *  A user is asking for help with creating bevels on text.
14. [Multiple GPU - WAN](https://www.reddit.com/r/StableDiffusion/comments/1jdf874/multiple_gpu_wan/) (Score: 0)
    *  A user is discussing using multiple GPUs with WAN.
15. [What ADetailer Model (Hi-res Fix) does Civitai use?](https://www.reddit.com/r/StableDiffusion/comments/1jdhjdi/what_adetailer_model_hires_fix_does_civitai_use/) (Score: 0)
    *  A user is inquiring about the ADetailer model used by Civitai for Hi-res Fix, seeking to replicate the same parameters on other platforms.

# Detailed Analysis by Thread
**[[D] LTX Flow Edit - Animation to Live Action (What If..? Doctor Strange) Low Vram 8gb](https://v.redd.it/omdw78xeq9pe1) (Score: 163)**
*  **Summary:** A user showcases an animation-to-live-action edit using LTX Flow, depicting a "What If...?" scenario featuring Doctor Strange. Viewers are requesting workflows and sharing related links and resources.
*  **Emotion:** The overall emotional tone is positive, with users expressing appreciation for the work and interest in the techniques used.
*  **Top 3 Points of View:**
    *  The video is well-received, with users praising the concept and execution.
    *  Viewers are interested in learning the workflow used to create the video.
    *  Users are sharing related resources, such as YouTube tutorials and Google Drive links.

**[Automatic installation of Pytorch 2.8 (Nightly), Triton & SageAttention 2 into a new Portable or Cloned Comfy with your existing Cuda (v12.4/6/8) get increased speed: v4.2](https://www.reddit.com/r/StableDiffusion/comments/1jdfs6e/automatic_installation_of_pytorch_28_nightly/) (Score: 58)**
*  **Summary:** A user provides an automatic installation script for Pytorch 2.8, Triton, and SageAttention 2 to improve speed in ComfyUI. Users are discussing compatibility, installation issues, and performance improvements.
*  **Emotion:** The overall emotional tone is neutral, with users focused on technical aspects and potential issues.
*  **Top 3 Points of View:**
    *  Users are interested in the potential speed improvements offered by the script.
    *  There are concerns about compatibility with different GPUs and Python versions.
    *  Users are sharing their experiences with the installation process and reporting any issues they encounter.

**[Adding soon voice cloning to AAFactory repository](https://v.redd.it/ixtvgvwlz9pe1) (Score: 18)**
*  **Summary:** The user is integrating Zonos for voice cloning to the AAFactory repository.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *  The user is sharing repository and website links.
    *  Integrating Zonos for voice cloning.
    *  Providing links to repository and website.

**[SD1.5 + A1111 till the wheels fall off.](https://www.reddit.com/gallery/1jdjlzx) (Score: 17)**
*  **Summary:** A user is showcasing images generated using SD1.5 and A1111, sparking discussion about the longevity and relevance of older models in the face of newer AI advancements.
*  **Emotion:** The overall emotional tone is mixed, ranging from nostalgic to critical, with some users praising the images while others suggest upgrading to newer models.
*  **Top 3 Points of View:**
    *  Some users are committed to using SD1.5 and A1111 until they are no longer viable.
    *  Others believe that newer AI models offer significant improvements in image quality and performance.
    *  Some users are still finding practical uses for SD1.5 in their workflows, particularly for product visualization.

**[Is there a way to generate accurate text using wan 2.1 ?](https://v.redd.it/j89e01xoq9pe1) (Score: 8)**
*  **Summary:** A user is asking for solutions to generate accurate text using WAN 2.1.
*  **Emotion:** The overall emotional tone is neutral
*  **Top 3 Points of View:**
    *  Trying the 14b or 1.3b model. If its the 14b try using the fp16 t5xxl clip model.
    *  Use WAN i2v to make the logo shrink down / disappear, then reverse the video.
    *  Add the text with after effects for more control.

**[Creating my first videos with Wan 2.1 fp8 using images I've generated in the past](https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/e5e91591-b5da-4baf-805e-25c287395f40/original=true,quality=90/ComfyUI_00002_.jpeg) (Score: 5)**
*  **Summary:** A user is creating their first videos with Wan 2.1 fp8 using images they've generated in the past.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *  The user is creating videos with Wan 2.1.
    *  Other users want to see the videos.
    *  Sharing links to images.

**[How Do You Guys Use The ComfyUI-to-Python-Extension ?](https://www.reddit.com/r/StableDiffusion/comments/1jdjtal/how_do_you_guys_use_the_comfyuitopythonextension/) (Score: 2)**
*  **Summary:** A user is asking for tips on how to use the ComfyUI-to-Python-Extension.
*  **Emotion:** The overall emotional tone is positive.
*  **Top 3 Points of View:**
    *  The user is looking for help with the extension.
    *  Shiro is comfy and has nothing to do with the issue.
    *  No other points of view were extracted.

**[What are the best face swap techniques for achieving anime movie-level detail?](https://www.reddit.com/r/StableDiffusion/comments/1jdihjf/what_are_the_best_face_swap_techniques_for/) (Score: 1)**
*  **Summary:** A user is requesting information on face-swapping techniques to achieve anime movie-level detail.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *  Use img2img pass on the original image using a checkpoint/lora of choice.
    *  Ensure identifiable features of the face are in the prompt.
    *  Use ControlNet for identifiable features of the face are in the prompt.

**[What are the options for enterprise cards with performance comparable to the 3000 series?](https://www.reddit.com/r/StableDiffusion/comments/1jdjp76/what_are_the_options_for_enterprise_cards_with/) (Score: 1)**
*  **Summary:** A user is inquiring about enterprise-level graphics cards that offer performance similar to the 3000 series.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *  The user wants the options for enterprise cards with performance comparable to the 3000 series.
    *  A6000 is a possible solution.
    *  No other points of view were extracted.

**[Prompting "Halfway between side view and front view"](https://www.reddit.com/r/StableDiffusion/comments/1jdjpju/prompting_halfway_between_side_view_and_front_view/) (Score: 1)**
*  **Summary:** A user is asking for tips for prompting halfway between a side view and front view.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *  Try three-quarter view.
    *  No other points of view were extracted.
    *  No other points of view were extracted.

**[What are some txt2vid or img2vid models that work on 16gb vram?](https://www.reddit.com/r/StableDiffusion/comments/1jdjvyy/what_are_some_txt2vid_or_img2vid_models_that_work/) (Score: 1)**
*  **Summary:** A user is seeking recommendations for text-to-video (txt2vid) or image-to-video (img2vid) models that can run on a system with 16GB of VRAM.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *  Wan can produce short videos at lower resolutions.
    *  The wan2.1-i2v-14b-480p-Q4_K_S.gguf model works well.
    *  All with gguf

**[3D background render to “anime” style?](https://www.reddit.com/r/StableDiffusion/comments/1jdk762/3d_background_render_to_anime_style/) (Score: 1)**
*  **Summary:** A user is asking for advice on converting 3D background renders into an anime style.
*  **Emotion:** The overall emotional tone is mixed, with some positive suggestions.
*  **Top 3 Points of View:**
    *  Use traditional workflows to stylize the image with color.
    *  ControlNet Depth + ControlNet Canny, and img2img with a good denoise amount.
    *  Kuwahara filter helps to denoise the image and adds some haze.

**[Bevel help](https://i.redd.it/ne9jeccizape1.jpeg) (Score: 0)**
*  **Summary:** A user is asking for help with creating bevels on text.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *  Traditional editing software is more suitable.
    *  Use GIMP to create text effects.
    *  Use Photoshop to create bevels with layer effects, then img2img with a low denoising level.

**[Multiple GPU - WAN](https://www.reddit.com/r/StableDiffusion/comments/1jdf874/multiple_gpu_wan/) (Score: 0)**
*  **Summary:** A user is discussing using multiple GPUs with WAN.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *  Use multi-gpu nodes to load clip into a separate GPU.
    *  CLIP is around 8-9GB, WAN 14B is around 20GB.
    *  Having Multiple GPU running, you won't need heating in the room.

**[What ADetailer Model (Hi-res Fix) does Civitai use?](https://www.reddit.com/r/StableDiffusion/comments/1jdhjdi/what_adetailer_model_hires_fix_does_civitai_use/) (Score: 0)**
*  **Summary:** A user is inquiring about the ADetailer model used by Civitai for Hi-res Fix, seeking to replicate the same parameters on other platforms.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *  Hi-res Fix is not an adetailer (yolo) detection model.
    *  They are looking for an upscaler model.
    *  https://openmodeldb.info contains more upscaler models.
