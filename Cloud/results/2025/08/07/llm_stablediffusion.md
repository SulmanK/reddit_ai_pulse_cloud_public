---
title: "Stable Diffusion Subreddit"
date: "2025-08-07"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [[D] ComfyUI now has "Subgraph" and "Partial Execution"](https://www.reddit.com/gallery/1mk5lk2) (Score: 32)
    *   Users are discussing the new Subgraph and Partial Execution features in ComfyUI, with excitement about the abstraction possibilities for visual programming and concerns about the one-way nature of subgraphs.
2.  [Upscaling Pixel Art with SeedVR2](https://www.reddit.com/gallery/1mk44xt) (Score: 23)
    *   The thread is about upscaling pixel art with SeedVR2, with many users expressing that the upscaling degrades the original pixel art's aesthetic.
3.  [I am proud to share my Wan 2.2 T2I creations. These beauties took me about 2 hours in total. (Help?)](https://www.reddit.com/gallery/1mk6art) (Score: 20)
    *   A user shares their Wan 2.2 Text-to-Image creations and asks for help. Other users provide advice on sampler settings, potential VRAM issues, and suggest workflows.
4.  [Drawing the art for a LoRA training set. Best advice for a complete set?](https://www.reddit.com/gallery/1mk6985) (Score: 19)
    *   A user is creating art for a LoRA training set and seeks advice. Other users commend their work and suggest refining the dataset over time.
5.  [And if we use in Wan2.2 the models I2V in HIGH noise and T2V in LOW noise!!??](https://v.redd.it/alkaep5g4nhf1) (Score: 16)
    *   Users discuss the results of using the I2V model in high noise and T2V model in low noise in Wan 2.2. Some find it gives better prompt following and video motion, while others report issues with detail loss.
6.  [Wan 2.2 longer than 5 seconds?](https://www.reddit.com/r/StableDiffusion/comments/1mk365n/wan_22_longer_than_5_seconds/) (Score: 9)
    *   Users are discussing the limitations of Wan 2.2 regarding video length and ways to work around the 5-second limit. They mention stitching videos, using frame interpolation, and substituting nodes.
7.  [Advanced Voice Cloning AI](https://v.redd.it/9obeaobummhf1) (Score: 8)
    *   The thread discusses advanced voice cloning AI, with users speculating on the methods used and mentioning various AI demos they have tried.
8.  [Memory array problems](https://www.reddit.com/r/StableDiffusion/comments/1mk3m7m/memory_array_problems/) (Score: 2)
    *   Users are discussing tiled upscaling options and standalone upscalers to address memory array problems in stable diffusion.
9.  [Is 5060 ti decent for sdxl, IL, wan 2.2, and Lora training?](https://www.reddit.com/r/StableDiffusion/comments/1mk7bd2/is_5060_ti_decent_for_sdxl_il_wan_22_and_lora/) (Score: 2)
    *   The discussion is about the suitability of the 5060 Ti GPU for running SDXL, IL, Wan 2.2, and Lora training.
10. [Unable to use LivePortrait 2025](https://www.reddit.com/r/StableDiffusion/comments/1mk7fmo/unable_to_use_liveportrait_2025/) (Score: 2)
    *   Users discuss using LivePortrait 2025 and mention a Windows zip file available for offline use with a Gradio interface.
11. [Any wan 2.2 I2V workflow for 12gb vram with lightx2v + loras?](https://www.reddit.com/r/StableDiffusion/comments/1mk89b8/any_wan_22_i2v_workflow_for_12gb_vram_with/) (Score: 2)
    *   Users discuss sharing and using Wan 2.2 I2V workflows for 12GB VRAM with lightx2v and LoRAs.
12. [V2V tests on Wan 2.2 split models vs just Low noise?](https://www.reddit.com/r/StableDiffusion/comments/1mk8dci/v2v_tests_on_wan_22_split_models_vs_just_low_noise/) (Score: 1)
    *   Users are discussing V2V tests on Wan 2.2, comparing split models to just low noise, and finding that resolution matters more than step count.
13. [5060 Ti or USED 3090?](https://www.reddit.com/r/StableDiffusion/comments/1mk8td2/5060_ti_or_used_3090/) (Score: 1)
    *   Users are debating whether to get a 5060 Ti or a used 3090 for stable diffusion tasks.
14. [Apple Chips for Ai](https://www.reddit.com/r/StableDiffusion/comments/1mk32kj/apple_chips_for_ai/) (Score: 0)
    *   Users are discussing the performance of Apple chips for AI tasks, particularly compared to NVIDIA GPUs.
15. [Struggling to make a logo without design experience - any tips?](https://www.reddit.com/r/StableDiffusion/comments/1mk40p7/struggling_to_make_a_logo_without_design/) (Score: 0)
    *   Users discuss the challenges of creating a logo without design experience, emphasizing the importance of understanding brand identity and suggesting professional help.
16. [Docker for All of these image/video generation frameworks](https://www.reddit.com/r/StableDiffusion/comments/1mk5nea/docker_for_all_of_these_imagevideo_generation/) (Score: 0)
    *   Users are discussing the use of Docker containers for image and video generation frameworks.
17. [Which app in pinokio is better to turn a drawing into a photo ?](https://www.reddit.com/r/StableDiffusion/comments/1mk8r4t/which_app_in_pinokio_is_better_to_turn_a_drawing/) (Score: 0)
    *   Users are discussing which app in Pinokio is better for turning a drawing into a photo.

# Detailed Analysis by Thread
**[[D] ComfyUI now has "Subgraph" and "Partial Execution" (Score: 32)](https://www.reddit.com/gallery/1mk5lk2)**
*   **Summary:** Users are discussing the new Subgraph and Partial Execution features in ComfyUI, highlighting the potential for abstraction in visual programming. There is also a warning about the current one-way nature of subgraphs and a feature request link to enable subgraph expansion.
*   **Emotion:** The overall emotional tone is positive, with excitement about the new features.
*   **Top 3 Points of View:**
    *   The new features are a crucial addition for abstracting visual programming, which can become complex.
    *   The inability to undo subgraphs is a significant limitation that users should be aware of.
    *   Subgraphs are compared to similar features in other node-based programs like TouchDesigner, emphasizing their utility.

**[Upscaling Pixel Art with SeedVR2 (Score: 23)](https://www.reddit.com/gallery/1mk44xt)**
*   **Summary:** The thread discusses upscaling pixel art with SeedVR2. Many users express negative sentiments towards the results, stating that the upscaling process degrades the original aesthetic and ruins the specific details of the pixel art.
*   **Emotion:** The overall emotional tone is neutral, with a mix of neutral and positive comments.
*   **Top 3 Points of View:**
    *   Upscaling pixel art defeats the purpose of the original art style, which is appreciated for its detailed pixel-perfect aesthetic.
    *   The upscaling process makes the pixel art "worse" by removing details.
    *   The upscaling turns pixel art into something less aesthetically pleasing.

**[I am proud to share my Wan 2.2 T2I creations. These beauties took me about 2 hours in total. (Help?) (Score: 20)](https://www.reddit.com/gallery/1mk6art)**
*   **Summary:** A user shares their Wan 2.2 Text-to-Image creations and requests help with the long generation time. Other users provide suggestions on sampler settings, potential VRAM issues, workflow links, and comparisons to other models.
*   **Emotion:** The overall emotional tone is neutral, with a mix of positive and neutral comments.
*   **Top 3 Points of View:**
    *   The user's creations are visually appealing and a refreshing change from typical posts.
    *   The 2-hour generation time is unusually long, indicating potential issues with settings or hardware.
    *   Proper sampler settings, like adjusting the start step of the second sampler, are essential.

**[Drawing the art for a LoRA training set. Best advice for a complete set? (Score: 19)](https://www.reddit.com/gallery/1mk6985)**
*   **Summary:** A user is creating art for a LoRA training set and asks for advice. Other users compliment the original artwork and suggest iterative refinement of the training dataset.
*   **Emotion:** The overall emotional tone is positive, with much appreciation for the user's artwork and encouragement.
*   **Top 3 Points of View:**
    *   The user's artwork is impressive and unique.
    *   Iteratively refining the dataset is a key aspect of LoRA training.
    *   Manually creating reference images for specific details that don't train well can be beneficial.

**[And if we use in Wan2.2 the models I2V in HIGH noise and T2V in LOW noise!!?? (Score: 16)](https://v.redd.it/alkaep5g4nhf1)**
*   **Summary:** Users are experimenting with and discussing the effects of using the I2V model in high noise and the T2V model in low noise settings within Wan 2.2. Some are reporting positive results with prompt following and motion, while others note issues with the I2V model losing detail.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 3 Points of View:**
    *   Using I2V in high noise and T2V in low noise can improve prompt following and video motion.
    *   The Wan2.2 I2V models tend to lose details and become blown out.
    *   T2V gives good variations of the image you input.

**[Wan 2.2 longer than 5 seconds? (Score: 9)](https://www.reddit.com/r/StableDiffusion/comments/1mk365n/wan_22_longer_than_5_seconds/)**
*   **Summary:** This thread explores the limitations of Wan 2.2 in generating videos longer than 5 seconds. Users share various methods to overcome this limitation, including stitching videos, using frame interpolation, and substituting nodes.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 3 Points of View:**
    *   Wan 2.2 is primarily trained for 5-second video clips, causing issues when generating longer videos.
    *   Techniques like stitching videos together with the last frame as input for the next can extend the video length.
    *   Using frame interpolation can extend the video length but might result in slow-motion outputs.

**[Advanced Voice Cloning AI (Score: 8)](https://v.redd.it/9obeaobummhf1)**
*   **Summary:** The discussion revolves around advanced voice cloning AI, with users speculating on the techniques used in a demo. Users also mention various AI voice cloning tools they've tried.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 2 Points of View:**
    *   The demo likely utilized audio-to-audio conversion to maintain cadence and emotion.
    *   Several AI voice cloning tools are available, including Zonos, Dia, Higghs Audio V2, and IndexTTS v2.

**[Memory array problems (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1mk3m7m/memory_array_problems/)**
*   **Summary:** Users are suggesting options to resolve Memory Array problems.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 2 Points of View:**
    *   Using Tiled upscaling is a viable option to upscale images by dividing them into sections.
    *   Standalone upscalers can be used.

**[Is 5060 ti decent for sdxl, IL, wan 2.2, and Lora training? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1mk7bd2/is_5060_ti_decent_for_sdxl_il_wan_22_and_lora/)**
*   **Summary:** The discussion revolves around whether a 5060 Ti is a decent GPU for SDXL, IL, wan 2.2, and Lora training.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 1 Point of View:**
    *   A 16GB version of the 5060 Ti card is suitable for running SDXL, IL, and wan 2.2, but Lora training hasn't been tested.

**[Unable to use LivePortrait 2025 (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1mk7fmo/unable_to_use_liveportrait_2025/)**
*   **Summary:** Users share information about LivePortrait 2025.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 1 Point of View:**
    *   A Windows zip file with a Gradio interface is available for offline use.

**[Any wan 2.2 I2V workflow for 12gb vram with lightx2v + loras? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1mk89b8/any_wan_22_i2v_workflow_for_12gb_vram_with/)**
*   **Summary:** Users share information about Wan 2.2 workflows.
*   **Emotion:** The overall emotional tone is positive.
*   **Top 1 Point of View:**
    *   A user shares their workflow via a Google Drive link.

**[V2V tests on Wan 2.2 split models vs just Low noise? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1mk8dci/v2v_tests_on_wan_22_split_models_vs_just_low_noise/)**
*   **Summary:** Users discuss tests on Wan 2.2 split models.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 1 Point of View:**
    *   Resolution seems to matter more than step count in V2V denoising.

**[5060 Ti or USED 3090? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1mk8td2/5060_ti_or_used_3090/)**
*   **Summary:** Users debate whether to buy a 5060 Ti or a used 3090.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 3 Points of View:**
    *   3090 is good luck.
    *   4080 (equivalent to 5070 Ti) is faster than the 3090 in most cases, except when using FLUX with ControlNet and IP Adapter.
    *   3090 is better 100%.

**[Apple Chips for Ai (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1mk32kj/apple_chips_for_ai/)**
*   **Summary:** The discussion compares Apple chips to NVIDIA GPUs for AI tasks.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 2 Points of View:**
    *   Apple chips have longer render times compared to NVIDIA cards.
    *   NVIDIA GPUs are better suited for AI generation tasks and have better support.

**[Struggling to make a logo without design experience - any tips? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1mk40p7/struggling_to_make_a_logo_without_design/)**
*   **Summary:** Users discuss the challenges of creating a logo without design experience.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 3 Points of View:**
    *   Logos are more than just pictures, they represent brand identity.
    *   Stable diffusion can't capture subtle meanings or abstract identities.
    *   Hiring a professional designer is advisable.

**[Docker for All of these image/video generation frameworks (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1mk5nea/docker_for_all_of_these_imagevideo_generation/)**
*   **Summary:** Users are discussing the use of Docker containers for AI tasks.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 2 Points of View:**
    *   Downloading the workflow yourself is similar to using a Docker container.
    *   Existing Docker containers for AI tasks are readily available.

**[Which app in pinokio is better to turn a drawing into a photo ? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1mk8r4t/which_app_in_pinokio_is_better_to_turn_a_drawing/)**
*   **Summary:** Users discuss Pinokio for converting drawings into photos.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 2 Points of View:**
    *   There are no differences in ControlNet across Pinokio's UIs.
    *   ComfyUI and Flux Kontext are recommended for this task.
