---
title: "LocalLLaMA Subreddit"
date: "2025-08-07"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "AI", "Local Models"]
---

# Overall Ranking and Top Discussions
1.  [caught in 4K](https://www.reddit.com/gallery/1mk8bh1) (Score: 61)
    * This thread discusses something that was "caught in 4k", and users are discussing the math logic and cherry picking of test sets related to it.
2.  [Fixed the SWE-bench graph:](https://www.reddit.com/gallery/1mk9qxe) (Score: 29)
    *  Users are discussing a fixed SWE-bench graph, with comments pointing out that the score is not as improved as it appears due to cherry-picked questions.
3.  [Polymarket](https://i.redd.it/puuand3c6nhf1.jpeg) (Score: 15)
    * The thread discusses Polymarket predictions and who decides which model is better.
4.  [gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF 繚 Hugging Face](https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF) (Score: 12)
    * The thread is about a Hugging Face model called Huihui-gpt-oss-20b-BF16-abliterated-GGUF, with users sharing links, asking about quantization, and commenting on its uncensored nature.
5.  [How can I actually learn and try LLM pretraining? (or post training a large LLM )](https://www.reddit.com/r/LocalLLaMA/comments/1mk8oll/how_can_i_actually_learn_and_try_llm_pretraining/) (Score: 9)
    * The thread discusses how to learn and try LLM pretraining, with users providing links and suggestions for training on limited compute.
6.  [Polymarket prediction for best AI model by 2025](https://i.redd.it/jua3bszgenhf1.png) (Score: 7)
    *  The thread is about Polymarket predictions for the best AI model by 2025, with users questioning the absence of Claude and how "best" is measured.
7.  [Qwen3-8b-2508 anyone?  Where are you?  Are you coming?](https://www.reddit.com/r/LocalLLaMA/comments/1mk95w6/qwen38b2508_anyone_where_are_you_are_you_coming/) (Score: 7)
    * Users are anticipating the release of Qwen3-8b-2508 and also expressing interest in the 32b version.
8.  [How i feel about gpt-oss...](https://i.redd.it/shxc8spf7nhf1.gif) (Score: 5)
    *  The thread is a reaction to GPT-OSS, with a user stating that tool calling doesn't work.
9.  [I had gpt 4.5 look up the news that it'll be sunsetted forever and it is SO fascinating. Here is its reply on Open-AI deciding not to just quantize the old models and release them.](https://www.reddit.com/gallery/1mka052) (Score: 2)
    * The thread discusses a GPT 4.5 response about being sunsetted, with users mentioning hallucinations, leading prompts, and the model referencing personal identifiable info.
10. [LM Studio and multiple model loading...is this NEW? How does it work?](https://www.reddit.com/r/LocalLLaMA/comments/1mk9eu2/lm_studio_and_multiple_model_loadingis_this_new/) (Score: 2)
    * Users discuss LM Studio and the ability to load multiple models, clarifying that it's possible with enough memory.
11. [It seems that GPT5 has 3 levels of thinking in common with GPT-OSS](https://www.reddit.com/r/LocalLLaMA/comments/1mk9lu4/it_seems_that_gpt5_has_3_levels_of_thinking_in/) (Score: 2)
    * Users are discussing the levels of thinking in GPT5 and GPT-OSS, with one user testing GPT5 to fix a bug in their app.
12. [Twisted math test for LLMs](https://i.redd.it/ihugrx1wanhf1.jpeg) (Score: 1)
    *  The thread features a twisted math test for LLMs, with users discussing its purpose and relation to unbalanced training data.
13. [Coral Protocol Outperforms Microsoft by 34% With Top GAIA Benchmark for AI Mini-Model !!](https://www.reddit.com/r/LocalLLaMA/comments/1mk8e0f/coral_protocol_outperforms_microsoft_by_34_with/) (Score: 1)
    * This thread shares a report about Coral Protocol outperforming Microsoft in an AI benchmark.
14. [10.48 tok/sec - GPT-OSS-120B on RTX 5090 32 VRAM + 96 RAM in LM Studio (default settings + FlashAttention + Guardrails: OFF)](https://www.reddit.com/r/LocalLLaMA/comments/1mk9c1u/1048_toksec_gptoss120b_on_rtx_5090_32_vram_96_ram/) (Score: 0)
    *  A user shares their GPT-OSS-120B speed on RTX 5090, and another user suggests improvements using llama.cpp and Unsloth.

# Detailed Analysis by Thread
**[caught in 4K (Score: 61)](https://www.reddit.com/gallery/1mk8bh1)**
*  **Summary:** The discussion revolves around a situation where something was "caught in 4K," specifically concerning the math logic and the potential cherry-picking of test sets related to a model, possibly GPT-5.
*  **Emotion:** The overall emotional tone is Neutral, with sentiment scores indicating objectivity in the comments.
*  **Top 3 Points of View:**
    *  Concern about the validity of math logic in some Tweets.
    *  Observation that cherry-picking test sets might be occurring.
    *  Inquiry about the meaning of "validated" in the context.

**[Fixed the SWE-bench graph: (Score: 29)](https://www.reddit.com/gallery/1mk9qxe)**
*  **Summary:** Users are discussing a corrected SWE-bench graph, with comments indicating concerns about the methodology used to achieve the "highest score," specifically pointing out the removal of certain questions from the benchmark.
*  **Emotion:** The emotional tone is predominantly Neutral, with one comment expressing a Negative sentiment regarding the questionable improvements.
*  **Top 3 Points of View:**
    *  The SWE-bench score is inflated due to the removal of questions.
    *  The actual improvement over previous versions is minimal.
    *  There was a selection of particular questions to give the best possible score.

**[Polymarket (Score: 15)](https://i.redd.it/puuand3c6nhf1.jpeg)**
*  **Summary:** The thread is centered on a Polymarket prediction related to AI models. Users are questioning the criteria for determining the "best" model.
*  **Emotion:** The overall emotional tone is Neutral, with sentiment scores indicating objectivity in the comments.
*  **Top 3 Points of View:**
    *  Questioning who will decide which model is better.
    *  Acknowledging that Google is a contender in the prediction.
    *  Uncertainty around the criteria for judging the "best" model.

**[gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF 繚 Hugging Face (Score: 12)](https://huggingface.co/gabriellarson/Huihui-gpt-oss-20b-BF16-abliterated-GGUF)**
*  **Summary:** The thread is about the release of a new language model on Hugging Face. The discussion includes questions about its technical aspects, its performance in LM Studio, and its uncensored nature. There is also a mention of what the name "huihui" means in Russian.
*  **Emotion:** The overall emotional tone is mixed, with both Positive (thanks for sharing) and Neutral comments.
*  **Top 3 Points of View:**
    *  A user expressed thanks for sharing the model.
    *  Inquiry about why mxfp4 was not used as the original model.
    *  Concern that the model being uncensored in lower quants makes it unusable for some.

**[How can I actually learn and try LLM pretraining? (or post training a large LLM ) (Score: 9)](https://www.reddit.com/r/LocalLLaMA/comments/1mk8oll/how_can_i_actually_learn_and_try_llm_pretraining/)**
*  **Summary:** A user is asking for guidance on learning and trying LLM pretraining and post-training. Other users are providing resources, links, and advice.
*  **Emotion:** The overall emotional tone is Neutral, as the comments are mostly informative.
*  **Top 3 Points of View:**
    *  Suggestion to use Tinystories, DataDecide, Textbooks are All You Need, and LIMA for training on limited compute.
    *  Recommending Andrej Karpathy's "Build an LLM from scratch" video and Towards Data Science blog as learning resources.
    *  An upvote as a sign of interest.

**[Polymarket prediction for best AI model by 2025 (Score: 7)](https://i.redd.it/jua3bszgenhf1.png)**
*  **Summary:** This thread discusses a Polymarket prediction for the best AI model by 2025. Users are questioning the absence of Claude in the predictions and the methodology for determining which model is the "best".
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *  Questioning the absence of Claude in the Polymarket prediction.
    *  Request for the source of the information.
    *  Questioning how "best" is actually measured.

**[Qwen3-8b-2508 anyone?  Where are you?  Are you coming? (Score: 7)](https://www.reddit.com/r/LocalLLaMA/comments/1mk95w6/qwen38b2508_anyone_where_are_you_are_you_coming/)**
*  **Summary:** The thread is a request for the release of Qwen3-8b-2508 model.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *  Anticipation and hope for the release of Qwen3-8b-2508.
    *  Request to be tagged when it's out.
    *  Interest in the 32b version of the model.

**[How i feel about gpt-oss... (Score: 5)](https://i.redd.it/shxc8spf7nhf1.gif)**
*  **Summary:** The thread expresses a feeling about GPT-OSS through a GIF, with a comment pointing out that tool calling doesn't work.
*  **Emotion:** The emotional tone is Negative, expressing disappointment.
*  **Top 3 Points of View:**
    *  Tool calling in GPT-OSS is not working.

**[I had gpt 4.5 look up the news that it'll be sunsetted forever and it is SO fascinating. Here is its reply on Open-AI deciding not to just quantize the old models and release them. (Score: 2)](https://www.reddit.com/gallery/1mka052)**
*  **Summary:** A user shares their experience of having GPT 4.5 analyze its own sunsetting. Discussions include concerns about hallucinations, leading prompts, potential existential crises in the model, and the model referencing personal identifiable information.
*  **Emotion:** The emotional tone is predominantly Neutral.
*  **Top 3 Points of View:**
    *  The model's response may be influenced by a leading prompt.
    *  The model may have exhibited an existential crisis and referenced personal identifiable information.
    *  Release weights? just hallucination before CUDA out of memory

**[LM Studio and multiple model loading...is this NEW? How does it work? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1mk9eu2/lm_studio_and_multiple_model_loadingis_this_new/)**
*  **Summary:** Users are discussing whether the ability to load multiple models in LM Studio is a new feature.
*  **Emotion:** The emotional tone is Neutral.
*  **Top 3 Points of View:**
    *  Multiple models can be loaded as long as there's enough memory.

**[It seems that GPT5 has 3 levels of thinking in common with GPT-OSS (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1mk9lu4/it_seems_that_gpt5_has_3_levels_of_thinking_in/)**
*  **Summary:** Users are discussing GPT5 and GPT-OSS, with a user testing GPT5's ability to fix bugs in their app.
*  **Emotion:** The emotional tone is Neutral.
*  **Top 3 Points of View:**
    *  GPT5 has 3 levels of thinking in common with GPT-OSS.
    *  GPT5 "high" takes a long time to "think".
    *  GPT5 can't fix the bug in the app, neither can Claude 4 Sonnet.

**[Twisted math test for LLMs (Score: 1)](https://i.redd.it/ihugrx1wanhf1.jpeg)**
*  **Summary:** This thread features a twisted math test for LLMs. Users are relating it to unbalanced training data.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *  The test is a useful continuation of the "banana plate test."
    *  Tests like these might uncover unbalanced training data.

**[Coral Protocol Outperforms Microsoft by 34% With Top GAIA Benchmark for AI Mini-Model !! (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1mk8e0f/coral_protocol_outperforms_microsoft_by_34_with/)**
*  **Summary:** The thread is about Coral Protocol outperforming Microsoft in a GAIA benchmark for AI Mini-Models.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *  Sharing a link to the Coral GAIA report.

**[10.48 tok/sec - GPT-OSS-120B on RTX 5090 32 VRAM + 96 RAM in LM Studio (default settings + FlashAttention + Guardrails: OFF) (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1mk9c1u/1048_toksec_gptoss120b_on_rtx_5090_32_vram_96_ram/)**
*  **Summary:** This thread reports a user's tok/sec speed for GPT-OSS-120B on an RTX 5090. Another user suggests improvements using llama.cpp and Unsloth.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *  Reporting GPT-OSS-120B performance on specific hardware.
    *  Suggesting the use of llama.cpp for performance improvement.
