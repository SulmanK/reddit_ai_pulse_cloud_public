---
title: "Machine Learning Subreddit"
date: "2025-08-01"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "LLM"]
---

# Overall Ranking and Top Discussions
1.  [[D] The AAAI website is Awful and organization feels clumsy :/(https://www.reddit.com/r/MachineLearning/comments/1meh32e/d_the_aaai_website_is_awful_and_organization/) (Score: 47)
    *   Users discussed the issues with the AAAI website, including vague instructions for supplementary materials and problems with LaTeX updates.
2.  [[D] Weight Tying in LLM Seems to Force the Last MLP to Become the True Unembedding](https://www.reddit.com/r/MachineLearning/comments/1meggd2/d_weight_tying_in_llm_seems_to_force_the_last_mlp/) (Score: 13)
    *   The thread discusses the implications of weight tying in Large Language Models (LLMs), particularly its effect on the last MLP layer.
3.  [[R] I’ve read the ASI‑Arch paper — AI discovered 106 novel neural architectures. What do you think?](https://www.reddit.com/r/MachineLearning/comments/1mexyvt/r_ive_read_the_asiarch_paper_ai_discovered_106/) (Score: 13)
    *   The main topic of this discussion is a critique of the ASI-Arch paper. Users questioned its claims, experimental setup, and the scale of its findings.
4.  [[P] Tri-70B-preview-SFT: Open 70B Parameter LLM for Alignment Research (No RLHF) | Trillion Labs](https://www.reddit.com/r/MachineLearning/comments/1mejly0/p_tri70bpreviewsft_open_70b_parameter_llm_for/) (Score: 11)
    *   This thread is about a new 70B parameter LLM for alignment research.
5.  [[D] Simple Questions Thread](https://www.reddit.com/r/MachineLearning/comments/1meysr1/d_simple_questions_thread/) (Score: 2)
    *   This is a general thread for simple questions related to machine learning, with users asking for advice on learning AI and discussing specific research questions.
6.  [[D] Non theoretical paper for ICLR](https://www.reddit.com/r/MachineLearning/comments/1mehjcj/d_non_theoretical_paper_for_iclr/) (Score: 1)
    *   Users discussed requirements for submitting non-theoretical papers to ICLR, including empirical validation and statistical testing.
7.  [[D] Does Suno truly solve one of the most challenging problem in machine learning ?](https://www.reddit.com/r/MachineLearning/comments/1meqp6c/d_does_suno_truly_solve_one_of_the_most/) (Score: 0)
    *   The discussion revolves around whether Suno has solved a challenging problem in ML, and whether the original post used ChatGPT for writing.

# Detailed Analysis by Thread
**[ [D] The AAAI website is Awful and organization feels clumsy :( (Score: 47)](https://www.reddit.com/r/MachineLearning/comments/1meh32e/d_the_aaai_website_is_awful_and_organization/)**
*  **Summary:**  Users discussed the issues with the AAAI website, including vague instructions for supplementary materials and problems with LaTeX updates. Some share workaround solutions like using Git to manage author kits.
*  **Emotion:** The overall emotional tone is neutral, with a mix of frustration and helpfulness.
*  **Top 3 Points of View:**
    *   The AAAI website has unclear instructions for supplementary materials.
    *   LaTeX updates from conference sites can break workflows.
    *   Using Git for author kits can help manage changes and reduce stress.

**[ [D] Weight Tying in LLM Seems to Force the Last MLP to Become the True Unembedding (Score: 13)](https://www.reddit.com/r/MachineLearning/comments/1meggd2/d_weight_tying_in_llm_seems_to_force_the_last_mlp/)**
*  **Summary:** The thread discusses the implications of weight tying in Large Language Models (LLMs), particularly its effect on the last MLP layer. One perspective compares it to an autoencoder.
*  **Emotion:** The emotional tone is generally positive and interested.
*  **Top 3 Points of View:**
    *   Weight tying may seem to "lose one layer of MLP" computation, but additional layers can be stacked.
    *   Weight tying saves parameters and computation.
    *   Weight tying can be compared to an autoencoder with restricted inputs.

**[ [R] I’ve read the ASI‑Arch paper — AI discovered 106 novel neural architectures. What do you think? (Score: 13)](https://www.reddit.com/r/MachineLearning/comments/1mexyvt/r_ive_read_the_asiarch_paper_ai_discovered_106/)**
*  **Summary:** The main topic of this discussion is a critique of the ASI-Arch paper. Users questioned its claims, experimental setup, and the scale of its findings. They found the language to be hyperbolic.
*  **Emotion:** The emotional tone is mostly neutral and negative, with users expressing skepticism and criticism.
*  **Top 3 Points of View:**
    *   The ASI-Arch paper uses overly hyped language and its claims are not fully supported by the experimental work.
    *   The scale of the models (20M parameters) is too small to claim ASI.
    *   The paper's exploration may be vulnerable to "seed fishing."

**[ [P] Tri-70B-preview-SFT: Open 70B Parameter LLM for Alignment Research (No RLHF) | Trillion Labs (Score: 11)](https://www.reddit.com/r/MachineLearning/comments/1mejly0/p_tri70bpreviewsft_open_70b_parameter_llm_for/)**
*  **Summary:** This thread is about a new 70B parameter LLM for alignment research.
*  **Emotion:** The emotional tone is positive.
*  **Top 3 Points of View:**
    *   The "Tri-70B-preview-SFT" model shows promising performance.

**[ [D] Simple Questions Thread (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1meysr1/d_simple_questions_thread/)**
*  **Summary:** This is a general thread for simple questions related to machine learning, with users asking for advice on learning AI and discussing specific research questions.
*  **Emotion:** The emotional tone is positive and neutral.
*  **Top 3 Points of View:**
    *   One user is seeking guidance on how to start learning AI without a programming background.
    *   Another user is asking ML researchers what they believe is the most important development in the last 6 months.
    *   A newbie is seeking feedback on an idea to improve machine learning models.

**[ [D] Non theoretical paper for ICLR (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1mehjcj/d_non_theoretical_paper_for_iclr/)**
*  **Summary:** Users discussed requirements for submitting non-theoretical papers to ICLR, including empirical validation and statistical testing.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Empirical papers are welcome but need motivation, ablation, and analysis.
    *   Statistical tests are important to show results aren't due to random chance.
    *   Benchmarks, ablation information, and big tables are helpful.

**[ [D] Does Suno truly solve one of the most challenging problem in machine learning ? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1meqp6c/d_does_suno_truly_solve_one_of_the_most/)**
*  **Summary:** The discussion revolves around whether Suno has solved a challenging problem in ML, and whether the original post used ChatGPT for writing.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Audio is not very high dimensional.
    *   Suno is impressive but not a necessary algorithmic improvement.
    *   The original post used ChatGPT for writing.
