---
title: "LocalLLaMA Subreddit"
date: "2025-08-09"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "AI", "Local Models"]
---

# Overall Ranking and Top Discussions
1.  [Can we finally agree that creative writing benchmarks like EQBench are totally useless?](https://www.reddit.com/r/LocalLLaMA/comments/1mlsos9/can_we_finally_agree_that_creative_writing/) (Score: 52)
    *   A discussion about the usefulness and flaws of creative writing benchmarks like EQBench, with some users defending its value and others criticizing its methodology.
2.  [axolotl vs unsloth [performance and everything]](https://www.reddit.com/r/LocalLLaMA/comments/1mltobj/axolotl_vs_unsloth_performance_and_everything/) (Score: 23)
    *   A comparison of the Axolotl and Unsloth libraries for fine-tuning LLMs, discussing their ease of use, performance, and suitability for different use cases (hobbyist vs. professional).
3.  [Why aren't people using small llms to train on their own local datasets?](https://www.reddit.com/r/LocalLLaMA/comments/1mltedd/why_arent_people_using_small_llms_to_train_on/) (Score: 16)
    *   An exploration of the reasons why more people aren't fine-tuning small LLMs on local datasets, considering the complexity, cost, and the effectiveness of alternative approaches like RAG.
4.  [Choose your models](https://www.reddit.com/r/LocalLLaMA/comments/1mlxg6h/choose_your_models/) (Score: 9)
    *   Users share their preferred LLMs, discussing the strengths and weaknesses of different models like Qwen, GLM, and Claude.
5.  [When exactly  "Qwen3-235B-A22B-2507" started generating flow charts?](https://i.redd.it/hw6j18x8u1if1.jpeg) (Score: 6)
    *   A user inquires about the LLM generated flow charts.
6.  [A native iOS client that lets you use your own api key/local models with on-device RAG & Agents on the roadmap? Has anyone tried "Privacy AI"?](https://www.reddit.com/r/LocalLLaMA/comments/1mlsurg/a_native_ios_client_that_lets_you_use_your_own/) (Score: 2)
    *   Discussion about an iOS client and Privacy AI for using local LLMs with RAG and agents.
7.  [XandAI-extension - Allow you to chat with your browser using ollama.](https://www.reddit.com/r/LocalLLaMA/comments/1mlszpz/xandaiextension_allow_you_to_chat_with_your/) (Score: 2)
    *   An announcement of a browser extension for chatting with Ollama models, along with a request for a Firefox version.
8.  [Fastest way to LoRA exported ChatGPT and Claude chats?](https://www.reddit.com/r/LocalLLaMA/comments/1mlvhb6/fastest_way_to_lora_exported_chatgpt_and_claude/) (Score: 2)
    *   A user asks about the fastest way to use LoRA on exported ChatGPT and Claude chats.
9.  [Run GPT-OSS with MLX or GGUF in your CLI using 1 line of code](https://www.reddit.com/r/LocalLLaMA/comments/1mlwaj7/run_gptoss_with_mlx_or_gguf_in_your_cli_using_1/) (Score: 2)
    *   A user inquires about the license of a GitHub project.
10. [Day 2 – Building an AI-Powered Cloud OS](https://www.reddit.com/r/LocalLLaMA/comments/1mly2td/day_2_building_an_aipowered_cloud_os/) (Score: 2)
    *   A user shares their progress on building an AI-powered cloud OS, receiving feedback on the problem it solves and its classification as an operating system.
11. [llama 4 supera a gpt-5 en iq](https://www.reddit.com/r/LocalLLaMA/comments/1mlttjs/llama_4_supera_a_gpt5_en_iq/) (Score: 0)
    *   Discussion about Llama 4 surpassing GPT-5 in IQ.
12. [Which one do you think is the best for agentic coding with Claude Code - Qwen 3 Coder, GLM 4.5, or Kimi K2?](https://www.reddit.com/r/LocalLLaMA/comments/1mluu4a/which_one_do_you_think_is_the_best_for_agentic/) (Score: 0)
    *   A query about the best LLM for agentic coding with Claude Code, considering Qwen 3 Coder, GLM 4.5, and Kimi K2.
13. [GPT-OSS-120B MXFP4 crashing out of llama.cpp when it hits the context limit. Is there a workaround?](https://www.reddit.com/r/LocalLLaMA/comments/1mlvzkj/gptoss120b_mxfp4_crashing_out_of_llamacpp_when_it/) (Score: 0)
    *   A question regarding GPT-OSS-120B MXFP4 crashing out of llama.cpp.
14. [vLLM can not split model across multiple GPUs with different VRAM amount?](https://www.reddit.com/r/LocalLLaMA/comments/1mlxcco/vllm_can_not_split_model_across_multiple_gpus/) (Score: 0)
    *   Inquiry about splitting a model across multiple GPUs with different VRAM amounts in vLLM.

# Detailed Analysis by Thread
**[[D] Can we finally agree that creative writing benchmarks like EQBench are totally useless? (Score: 52)](https://www.reddit.com/r/LocalLLaMA/comments/1mlsos9/can_we_finally_agree_that_creative_writing/)**
*   **Summary:** This thread debates the value of creative writing benchmarks like EQBench. Some users find them useful for measuring prompt adherence and overall writing quality, while others criticize their subjective nature and the potential for models to overfit to the benchmark's evaluation criteria. There's a discussion on the role of LLMs as judges, the importance of human evaluation, and the need for methodological improvements.
*   **Emotion:** The emotional tone is mixed, with both positive and negative sentiments expressed. While some users express appreciation and find value in the benchmarks (positive), others express frustration and skepticism about their accuracy and methodology (negative). Neutral comments also discuss the technical aspects of the benchmark.
*   **Top 3 Points of View:**
    *   EQBench and similar benchmarks are useful for gauging overall writing quality and prompt adherence, even if they have flaws.
    *   The subjective nature of creative writing makes it difficult to create accurate and unbiased benchmarks, especially when using LLMs as evaluators. Human evaluators should be considered despite difficulties to setup.
    *   Benchmarks like EQBench are better than nothing because they offer a large corpus of work to analyze and provide insights, even if the numerical scores should be viewed cautiously.

**[[Discussion] axolotl vs unsloth [performance and everything] (Score: 23)](https://www.reddit.com/r/LocalLLaMA/comments/1mltobj/axolotl_vs_unsloth_performance_and_everything/)**
*   **Summary:** This thread compares Axolotl and Unsloth, two popular libraries for fine-tuning LLMs. Users discuss the ease of use of each library, their memory efficiency, and their suitability for different levels of experience. Axolotl is praised for its config-based approach and production-readiness, while Unsloth is recommended for beginners due to its user-friendly notebooks and accessibility.
*   **Emotion:** The overall emotional tone is positive and neutral, with users sharing their positive experiences with both Axolotl and Unsloth. The discussion focuses on the practical aspects of using these tools.
*   **Top 3 Points of View:**
    *   Axolotl is better suited for production environments due to its configuration-driven approach and ease of replication.
    *   Unsloth is more accessible for beginners due to its user-friendly notebooks and lower hardware requirements.
    *   Both libraries have their strengths and weaknesses, and the best choice depends on the user's experience level, hardware resources, and specific needs.

**[[Discussion] Why aren't people using small llms to train on their own local datasets? (Score: 16)](https://www.reddit.com/r/LocalLLaMA/comments/1mltedd/why_arent_people_using_small_llms_to_train_on/)**
*   **Summary:** This thread explores why more people aren't using small LLMs to train on their own local datasets. Users cite the difficulty and complexity of creating datasets, the cost of training, and the potential for unexpected behavior. They also highlight the effectiveness of RAG (Retrieval-Augmented Generation) as a simpler and faster alternative for encoding new information.
*   **Emotion:** The overall emotional tone is neutral. Users are sharing their experiences and explaining the trade-offs involved in fine-tuning versus using RAG.
*   **Top 3 Points of View:**
    *   Fine-tuning is a complex and time-consuming process that may not always yield the desired results.
    *   RAG is a more efficient and flexible alternative for many common use cases, allowing for faster knowledge updates and model switching.
    *   Fine-tuning can be effective in combination with RAG, and for specific tasks like role-playing, or when dealing with private data.

**[Choose your models (Score: 9)](https://www.reddit.com/r/LocalLLaMA/comments/1mlxg6h/choose_your_models/)**
*   **Summary:** In this thread, users are listing their favorite LLMs, with mentions of Qwen3, GLM Air, GPT 5, Claude Sonnet, and Gemini.
*   **Emotion:** The overall emotional tone is neutral, with users simply stating their preferences.
*   **Top 3 Points of View:**
    *   Qwen models are a popular choice.
    *   GLM Air is another favored option.
    *   Claude Sonnet is liked, especially older versions.

**[When exactly  "Qwen3-235B-A22B-2507" started generating flow charts? (Score: 6)](https://i.redd.it/hw6j18x8u1if1.jpeg)**
*   **Summary:** A user asks when "Qwen3-235B-A22B-2507" started generating flow charts, and another user replies that they are mermaid charts generated by the LLM code.
*   **Emotion:** The emotional tone is positive.
*   **Top 3 Points of View:**
    *   LLMs can generate mermaid charts.

**[A native iOS client that lets you use your own api key/local models with on-device RAG & Agents on the roadmap? Has anyone tried "Privacy AI"? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1mlsurg/a_native_ios_client_that_lets_you_use_your_own/)**
*   **Summary:** This thread discusses a native iOS client for local LLMs with RAG and agent capabilities.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   Some users find the pricing of such clients too high.

**[XandAI-extension - Allow you to chat with your browser using ollama. (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1mlszpz/xandaiextension_allow_you_to_chat_with_your/)**
*   **Summary:** A user introduces XandAI, a browser extension for chatting using Ollama, and another user asks for a Firefox version.
*   **Emotion:** The emotional tone is positive.
*   **Top 3 Points of View:**
    *   Users are interested in using the extension on Firefox.

**[Fastest way to LoRA exported ChatGPT and Claude chats? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1mlvhb6/fastest_way_to_lora_exported_chatgpt_and_claude/)**
*   **Summary:** This thread is about finding the fastest way to perform LoRA on exported ChatGPT and Claude chats.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   Qwen 3 0.6B might be suitable for the task.

**[Run GPT-OSS with MLX or GGUF in your CLI using 1 line of code (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1mlwaj7/run_gptoss_with_mlx_or_gguf_in_your_cli_using_1/)**
*   **Summary:** The discussion revolves around the lack of a license on a GitHub project.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   It's unclear what license applies to the project.

**[Day 2 – Building an AI-Powered Cloud OS (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1mly2td/day_2_building_an_aipowered_cloud_os/)**
*   **Summary:** A user presents their AI-powered cloud OS project.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   The project's purpose and AI functionality are questioned.
    *   Its classification as an operating system is debated.

**[llama 4 supera a gpt-5 en iq (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1mlttjs/llama_4_supera_a_gpt5_en_iq/)**
*   **Summary:** Claims Llama 4 has a higher IQ than GPT-5.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   The claim is met with skepticism and a user states that they can create a benchmark where even a small model can outperform GPT-5.

**[Which one do you think is the best for agentic coding with Claude Code - Qwen 3 Coder, GLM 4.5, or Kimi K2? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1mluu4a/which_one_do_you_think_is_the_best_for_agentic/)**
*   **Summary:** This thread seeks advice on the best LLM for agentic coding in conjunction with Claude Code.
*   **Emotion:** The emotional tone is positive.
*   **Top 3 Points of View:**
    *   GLM 4.5 is great for tool calling.
    *   Context management can be an issue with GLM 4.5.
    *   Qwen 3 Coder and Kimi K2 are also mentioned, with a user noting that Kimi is a larger model.

**[GPT-OSS-120B MXFP4 crashing out of llama.cpp when it hits the context limit. Is there a workaround? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1mlvzkj/gptoss120b_mxfp4_crashing_out_of_llamacpp_when_it/)**
*   **Summary:** The poster is experiencing crashes with GPT-OSS-120B in llama.cpp.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   Unsloth may have addressed related issues.
    *   Switching to Qwen3-30B-A3B-Instruct-2507 with its larger context may be a solution.

**[vLLM can not split model across multiple GPUs with different VRAM amount? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1mlxcco/vllm_can_not_split_model_across_multiple_gpus/)**
*   **Summary:** The question is whether vLLM can split a model across GPUs with different VRAM.
*   **Emotion:** The emotional tone is mixed.
*   **Top 3 Points of View:**
    *   vLLM can not split the model across multiple GPUs with different VRAM amounts.
    *   Exllama supports multiple GPUs with different VRAM amounts.
    *   llama.cpp is an alternative.
