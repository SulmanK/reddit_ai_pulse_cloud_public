---
title: "Machine Learning Subreddit"
date: "2025-08-08"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machine learning", "LLMs", "NeurIPS"]
---

# Overall Ranking and Top Discussions
1.  [[D] Can LLMs Have Accurate World Models?](https://www.reddit.com/r/MachineLearning/comments/1mkelg5/d_can_llms_have_accurate_world_models/) (Score: 31)
    *   The thread discusses the capability of Large Language Models (LLMs) to possess accurate world models.
2.  [[D] Neurips rebuttal score change](https://www.reddit.com/r/MachineLearning/comments/1mkqbkh/d_neurips_rebuttal_score_change/) (Score: 18)
    *   The thread discusses the changes in scores after the rebuttal phase in NeurIPS submissions.
3.  [[D] Disentanglement using Flow matching](https://www.reddit.com/r/MachineLearning/comments/1mkny59/d_disentanglement_using_flow_matching/) (Score: 11)
    *   The thread focuses on the topic of disentanglement using flow matching techniques in machine learning models.
4.  [[D] In 2025, what is a sufficient methodology to analyze document summaries generated by LLMs? BERTScore, G-Eval, Rogue, etc](https://www.reddit.com/r/MachineLearning/comments/1mkge00/d_in_2025_what_is_a_sufficient_methodology_to/) (Score: 6)
    *   The thread discusses methodologies for analyzing document summaries generated by Large Language Models (LLMs), considering metrics like BERTScore, G-Eval, and Rogue.
5.  [[D] Looking for convex-constrained ML problems for benchmarks](https://www.reddit.com/r/MachineLearning/comments/1mkxewf/d_looking_for_convexconstrained_ml_problems_for/) (Score: 3)
    *   The thread is seeking convex-constrained machine learning problems to be used as benchmarks.
6.  [[D] Looking for ideas for a ML initiative](https://www.reddit.com/r/MachineLearning/comments/1mkr9wy/d_looking_for_ideas_for_a_ml_initiative/) (Score: 0)
    *   The thread is looking for ideas for an ML initiative and mentions Hyperpod AI.
7.  [[R] Live coding benchmark: GPT-5, Claude Sonnet 4, Gemini 2.5 Pro, GLM45 — same prompt, varying difficulty](https://www.reddit.com/r/MachineLearning/comments/1mkw2z1/r_live_coding_benchmark_gpt5_claude_sonnet_4/) (Score: 0)
    *   The thread presents a live coding benchmark for several language models including GPT-5, Claude Sonnet 4, Gemini 2.5 Pro, and GLM45.

# Detailed Analysis by Thread
**[[D] Can LLMs Have Accurate World Models? (Score: 31)](https://www.reddit.com/r/MachineLearning/comments/1mkelg5/d_can_llms_have_accurate_world_models/)**
*  **Summary:** The discussion revolves around whether Large Language Models (LLMs) can develop accurate models of the world. Various arguments are presented, with some suggesting LLMs possess internal models capable of answering complex questions, while others argue they lack true understanding due to hallucinations and poor out-of-domain generalization. Research papers are referenced, and comparisons are drawn between LLM "world models" and human/animal world models.
*  **Emotion:** The emotional tone of the thread is predominantly Neutral, with most comments conveying information or arguments in a factual manner. One comment expresses a negative sentiment, stating "Approximate yes, accurate no."
*  **Top 3 Points of View:**
    *   LLMs can answer complex questions suggesting they possess an internal world model.
    *   LLMs hallucinate and don't generalize well, indicating a lack of an accurate world model.
    *   LLMs have internal models of abstract concepts but struggle with causality and real-world physics.

**[[D] Neurips rebuttal score change (Score: 18)](https://www.reddit.com/r/MachineLearning/comments/1mkqbkh/d_neurips_rebuttal_score_change/)**
*  **Summary:** This thread discusses experiences with NeurIPS rebuttal score changes. Some users report that reviewers didn't change their scores despite addressed concerns, while others share instances of score increases or withdrawals. The thread also touches on the likelihood of score bumps for borderline-positive versus high-scoring papers, and the impact of reviewer engagement.
*  **Emotion:** The overall emotional tone is Neutral, primarily consisting of factual reports and observations about the review process.
*  **Top 3 Points of View:**
    *   Reviewers sometimes don't change scores even after concerns are addressed in rebuttals.
    *   Borderline-positive papers are more likely to receive a score increase than already high-scoring papers.
    *   Withdrawals and score changes are common, but the extent of change can vary.

**[[D] Disentanglement using Flow matching (Score: 11)](https://www.reddit.com/r/MachineLearning/comments/1mkny59/d_disentanglement_using_flow_matching/)**
*  **Summary:** The thread discusses the use of flow matching for disentanglement in machine learning. It is noted that flow-matching alone might not be sufficient and could lead to the model "cheating" without proper constraints. Instability issues with flow matching when learning a moving target distribution are also mentioned, along with ideas for reparameterization and loss balancing.
*  **Emotion:** The emotional tone of the thread is Neutral, with a focus on technical discussion and experimental results.
*  **Top 3 Points of View:**
    *   Flow-matching alone might not be sufficient for disentanglement.
    *   The model can "cheat" without fixed priors or capacity constraints.
    *   Learning a moving target distribution with flow matching can be unstable.

**[[D] In 2025, what is a sufficient methodology to analyze document summaries generated by LLMs? BERTScore, G-Eval, Rogue, etc (Score: 6)](https://www.reddit.com/r/MachineLearning/comments/1mkge00/d_in_2025_what_is_a_sufficient_methodology_to/)**
*  **Summary:** The discussion is centered around methodologies for evaluating document summaries generated by LLMs. Suggestions include using LLMs to generate counterfactual questions and then evaluating the summary's ability to answer them using metrics like G-Eval and RAGAS.
*  **Emotion:** The emotional tone is Neutral, as the comments primarily provide suggestions and discuss evaluation methods.
*  **Top 3 Points of View:**
    *   Use LLMs to generate counterfactual questions answerable from the original document.
    *   Evaluate if summaries can answer the generated questions using G-Eval/RAGAS.
    *   Consider using LLMLingua for document compression.

**[[D] Looking for convex-constrained ML problems for benchmarks (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1mkxewf/d_looking_for_convexconstrained_ml_problems_for/)**
*  **Summary:** The thread is a request for convex-constrained ML problems that can be used for benchmarking. The poster is looking for problems where the projection step is costly. Suggestions include optimal transport and SVM training.
*  **Emotion:** The emotional tone is Neutral, with the discussion focused on providing suggestions.
*  **Top 3 Points of View:**
    *   Consider using benchmark suites of well-tested optimization problems.
    *   Optimal transport problems often use doubly stochastic matrices.
    *   SVM training can be optimized using the dual form.

**[[D] Looking for ideas for a ML initiative (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1mkr9wy/d_looking_for_ideas_for_a_ml_initiative/)**
*  **Summary:** The thread starter is looking for ideas for an ML initiative. One comment promotes Hyperpod AI as a tool for quick prototyping and releasing.
*  **Emotion:** The emotional tone is Neutral, with the comment providing a tool suggestion.
*  **Top 3 Points of View:**
    *   Use Hyperpod AI for quick prototyping and releasing of AI models.

**[[R] Live coding benchmark: GPT-5, Claude Sonnet 4, Gemini 2.5 Pro, GLM45 — same prompt, varying difficulty (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1mkw2z1/r_live_coding_benchmark_gpt5_claude_sonnet_4/)**
*  **Summary:** The thread announces a live coding benchmark comparing GPT-5, Claude Sonnet 4, Gemini 2.5 Pro, and GLM45. Comments include requests for the prompt used, skepticism, and suggestions of other models to include.
*  **Emotion:** The emotional tone is Neutral, with a mix of curiosity, skepticism, and suggestions.
*  **Top 3 Points of View:**
    *   Requests for the prompt used in the benchmark.
    *   Skepticism about the value of the "research."
    *   Suggestion to include Qwen for coding benchmarks.
