---
title: "Stable Diffusion Subreddit"
date: "2025-10-01"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [wan2.2 animate is great](https://v.redd.it/uhyw14q1mjsf1) (Score: 208)
    * Users are discussing and reacting to a video created using wan2.2 animate, with some comparing it to a birth and others suggesting it be shared on a specific subreddit.
2.  [Qwen Edit MultiGen (V2)](https://www.reddit.com/gallery/1nvdq8h) (Score: 78)
    * This thread is about Qwen Edit MultiGen (V2) and users are asking about the workflow, GPU requirements, and reporting issues with Reddit.
3.  [Testing workflows to swap faces on images with Qwen (2509)](https://www.reddit.com/r/StableDiffusion/comments/1nvbdxl/testing_workflows_to_swap_faces_on_images_with/) (Score: 34)
    * This thread discusses and tests workflows for swapping faces on images using Qwen. Users are sharing suggestions and improvements to the workflow.
4.  [Wan 2.2  Insight + WanVideoContextOptions Test ~1min](https://v.redd.it/de1xj6h8jjsf1) (Score: 17)
    * Users are discussing the quality and performance of Wan 2.2 with Insight and WanVideoContextOptions, noting glitchiness and random speed changes.
5.  [KaniTTS-370M Released: Multilingual Support + More English Voices](https://huggingface.co/nineninesix/kani-tts-370m) (Score: 12)
    * This thread is about the release of KaniTTS-370M, a multilingual text-to-speech model, and a user is asking about voice cloning capabilities.
6.  [Dual GPU for wan lora training (musubi) and wan image gen (comfyui)?](https://www.reddit.com/r/StableDiffusion/comments/1nvdd8n/dual_gpu_for_wan_lora_training_musubi_and_wan/) (Score: 5)
    * This thread discusses the use of dual GPUs for training Wan LoRA using Musubi and generating images with ComfyUI, with one user sharing their experience with dual GPUs and raylight.
7.  [Yeah so I started using  Qwen Image Edit as main model without input images and I think it works better than the base model.](https://www.reddit.com/gallery/1nvhba3) (Score: 3)
    * A user shares their experience using Qwen Image Edit as the main model without input images. Others discuss LoRAs and workflow experiments.
8.  [Wan 2.2 5B Mac can’t enable i2v](https://www.reddit.com/r/StableDiffusion/comments/1nvdm5f/wan_22_5b_mac_cant_enable_i2v/) (Score: 1)
    * Users are discussing issues with enabling i2v in Wan 2.2 on Macs, including problems with KSampler and general performance.
9.  [Can someone tell me which model will produce these videos ? Sora/grok/veo all give me guardrails](https://v.redd.it/aj23obswgjsf1) (Score: 0)
    * A user is asking which model can produce certain types of videos, noting that Sora, Grok, and Veo have guardrails. Other users suggest Wan 2.2 s2v and prompting carefully on Sora.
10. [Farewell, summer. From the series — Queen Jedi on vacation.](https://v.redd.it/xo0eiulzbjsf1) (Score: 0)
    * A user shares a video and another user asks about the graphics card and program used to create it.
11. [New computer - one RTX 6000 or dual RTX 5000?](https://www.reddit.com/r/StableDiffusion/comments/1nvf1vt/new_computer_one_rtx_6000_or_dual_rtx_5000/) (Score: 0)
    * Users are debating whether to get one RTX 6000 or dual RTX 5000 for a new computer, with suggestions for alternative setups and considerations for RAM and processor.
12. [How are people making these ultra-realistic AI model reels?](https://www.reddit.com/r/StableDiffusion/comments/1nvfeb0/how_are_people_making_these_ultrarealistic_ai/) (Score: 0)
    * This thread discusses how people are creating ultra-realistic AI model reels, with suggestions including Wan image 2 video and proprietary sources like Google Veo.
13. [Qwen Image Edit 2509: Crashes at "Requested to load WanVAE"](https://www.reddit.com/r/StableDiffusion/comments/1nvfvkc/qwen_image_edit_2509_crashes_at_requested_to_load/) (Score: 0)
    * A user is experiencing crashes with Qwen Image Edit 2509 and others are offering help by suggesting to share a screenshot of workflow and asking about precision settings.
14. [Anybody here using Diffsynth for inference Wan animate or Qwen?](https://www.reddit.com/r/StableDiffusion/comments/1nvg2ol/anybody_here_using_diffsynth_for_inference_wan/) (Score: 0)
    * A user asks about using Diffsynth for inference with Wan animate or Qwen.

# Detailed Analysis by Thread
**[wan2.2 animate is great (Score: 208)](https://v.redd.it/uhyw14q1mjsf1)**
*  **Summary:** Users are discussing and reacting to a video created using wan2.2 animate, with some comparing it to a birth and others suggesting it be shared on a specific subreddit.
*  **Emotion:** The overall emotional tone is Neutral, with some instances of Positive sentiment due to expressions of enjoyment and humor.
*  **Top 3 Points of View:**
    * The video is impressive and well-made.
    * The video is somewhat disturbing or graphic.
    * The video is suitable for sharing on r/IASIP.

**[Qwen Edit MultiGen (V2) (Score: 78)](https://www.reddit.com/gallery/1nvdq8h)**
*  **Summary:** This thread is about Qwen Edit MultiGen (V2) and users are asking about the workflow, GPU requirements, and reporting issues with Reddit.
*  **Emotion:** The overall emotional tone is Neutral, with some instances of Positive sentiment due to expressions of excitement. A comment expressing an apology, has negative sentiment.
*  **Top 3 Points of View:**
    * The Qwen Edit MultiGen (V2) results are impressive.
    * Information about the workflow is needed.
    * There are some problems with Reddit.

**[Testing workflows to swap faces on images with Qwen (2509) (Score: 34)](https://www.reddit.com/r/StableDiffusion/comments/1nvbdxl/testing_workflows_to_swap_faces_on_images_with/)**
*  **Summary:** This thread discusses and tests workflows for swapping faces on images using Qwen. Users are sharing suggestions and improvements to the workflow.
*  **Emotion:** The overall emotional tone is Positive, with users expressing enthusiasm and offering helpful suggestions.
*  **Top 3 Points of View:**
    * The presented workflow is a good starting point for face swapping.
    * Masking and color matching can improve the results.
    * Using the official BF16 model and following Owen's principles on image edit instruction can improve results.

**[Wan 2.2  Insight + WanVideoContextOptions Test ~1min (Score: 17)](https://v.redd.it/de1xj6h8jjsf1)**
*  **Summary:** Users are discussing the quality and performance of Wan 2.2 with Insight and WanVideoContextOptions, noting glitchiness and random speed changes.
*  **Emotion:** The overall emotional tone is Neutral, with some Negative sentiment expressed due to concerns about glitchiness and speed changes.
*  **Top 3 Points of View:**
    * The video has noticeable glitches.
    * The random speed changes are disturbing.
    * There are questions about memory requirements and node availability.

**[KaniTTS-370M Released: Multilingual Support + More English Voices (Score: 12)](https://huggingface.co/nineninesix/kani-tts-370m)**
*  **Summary:** This thread is about the release of KaniTTS-370M, a multilingual text-to-speech model, and a user is asking about voice cloning capabilities.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 1 Points of View:**
    * Is voice cloning possible with this?

**[Dual GPU for wan lora training (musubi) and wan image gen (comfyui)? (Score: 5)](https://www.reddit.com/r/StableDiffusion/comments/1nvdd8n/dual_gpu_for_wan_lora_training_musubi_and_wan/)**
*  **Summary:** This thread discusses the use of dual GPUs for training Wan LoRA using Musubi and generating images with ComfyUI, with one user sharing their experience with dual GPUs and raylight.
*  **Emotion:** The overall emotional tone is Positive, with a user sharing their experience with diffusion-pipe for LoRA training, and with raylight custom nodes plugin for ComfyUI enabling FSDP.
*  **Top 1 Points of View:**
    * The model is absolutely worth having multi-GPUs.

**[Yeah so I started using  Qwen Image Edit as main model without input images and I think it works better than the base model. (Score: 3)](https://www.reddit.com/gallery/1nvhba3)**
*  **Summary:** A user shares their experience using Qwen Image Edit as the main model without input images. Others discuss LoRAs and workflow experiments.
*  **Emotion:** The overall emotional tone is Positive, with the comments being very supportive and helpful.
*  **Top 3 Points of View:**
    * Qwen Image Edit works better than the base model.
    * The v2 8 step Loras seem to be more natural.
    * Everyone in the images looks the same.

**[Wan 2.2 5B Mac can’t enable i2v (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1nvdm5f/wan_22_5b_mac_cant_enable_i2v/)**
*  **Summary:** Users are discussing issues with enabling i2v in Wan 2.2 on Macs, including problems with KSampler and general performance.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 2 Points of View:**
    * On Mac, use Command (⌘) instead of CTRL.
    * KSampler is not working properly on Macs.

**[Can someone tell me which model will produce these videos ? Sora/grok/veo all give me guardrails (Score: 0)](https://v.redd.it/aj23obswgjsf1)**
*  **Summary:** A user is asking which model can produce certain types of videos, noting that Sora, Grok, and Veo have guardrails. Other users suggest Wan 2.2 s2v and prompting carefully on Sora.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 2 Points of View:**
    * Use Wan 2.2 s2v.
    * Prompt Sora carefully.

**[Farewell, summer. From the series — Queen Jedi on vacation. (Score: 0)](https://v.redd.it/xo0eiulzbjsf1)**
*  **Summary:** A user shares a video and another user asks about the graphics card and program used to create it.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 1 Points of View:**
    * Can I ask what graphics card you have and what program you use to create those videos?

**[New computer - one RTX 6000 or dual RTX 5000? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1nvf1vt/new_computer_one_rtx_6000_or_dual_rtx_5000/)**
*  **Summary:** Users are debating whether to get one RTX 6000 or dual RTX 5000 for a new computer, with suggestions for alternative setups and considerations for RAM and processor.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    * Get yourself a virtual machine.
    * The use case you're describing can be done much cheaper.
    * One RTX 6000 Pro would be better than dual RTX 5000.

**[How are people making these ultra-realistic AI model reels? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1nvfeb0/how_are_people_making_these_ultrarealistic_ai/)**
*  **Summary:** This thread discusses how people are creating ultra-realistic AI model reels, with suggestions including Wan image 2 video and proprietary sources like Google Veo.
*  **Emotion:** The overall emotional tone is Neutral, with a slight positive sentiment.
*  **Top 3 Points of View:**
    * Any photo or image can be sent into wan image 2 video and produce pretty amazing stuff
    * Most of the meme ones are being made with proprietary sources, like google Veo.
    * The content being shared is NSFW.

**[Qwen Image Edit 2509: Crashes at "Requested to load WanVAE" (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1nvfvkc/qwen_image_edit_2509_crashes_at_requested_to_load/)**
*  **Summary:** A user is experiencing crashes with Qwen Image Edit 2509 and others are offering help by suggesting to share a screenshot of workflow and asking about precision settings.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 2 Points of View:**
    * Share a screenshot of the workflow.
    * What precision do you use?

**[Anybody here using Diffsynth for inference Wan animate or Qwen? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1nvg2ol/anybody_here_using_diffsynth_for_inference_wan/)**
*  **Summary:** A user asks about using Diffsynth for inference with Wan animate or Qwen.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 1 Points of View:**
    * The repo link: [https://github.com/modelscope/DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio)
