---
title: "LocalLLaMA Subreddit"
date: "2025-10-01"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["localllama", "AI", "Local LLM"]
---

# Overall Ranking and Top Discussions
1.  [I've built Jarvis completely on-device in the browser](https://v.redd.it/hge6ipzncjsf1) (Score: 44)
    * The post discusses a user's project, Jarvis, which is built completely on-device in the browser.
2.  [NVIDIA DGX Spark expected to become available in October 2025](https://www.reddit.com/r/LocalLLaMA/comments/1nvdyiy/nvidia_dgx_spark_expected_to_become_available_in/) (Score: 31)
    * The discussion revolves around the NVIDIA DGX Spark and its potential uses for local LLM development.
3.  [Hunyuan Image 3.0 vs HunyuanImage 2.1](https://i.redd.it/5mci5v1vijsf1.png) (Score: 7)
    * This thread compares the performance of Hunyuan Image 3.0 against Hunyuan Image 2.1.
4.  [How to use mmproj files + Looking for uncensored model for sorting images.](https://www.reddit.com/r/LocalLLaMA/comments/1nvdz7g/how_to_use_mmproj_files_looking_for_uncensored/) (Score: 6)
    * The thread seeks information on using mmproj files and finding an uncensored model for image sorting.
5.  [Qwen 235B on 2x3090's vs 3x MI50](https://www.reddit.com/r/LocalLLaMA/comments/1nvgnv4/qwen_235b_on_2x3090s_vs_3x_mi50/) (Score: 6)
    * This discussion compares the performance of the Qwen 235B model on different hardware configurations (2x3090s vs 3x MI50).
6.  [Connecting 6 AMD AI Max 395+ for QWen3-235B-A22B. Is this really that much faster than just 1 server ?](https://b23.tv/TO5oW7j) (Score: 4)
    * The post questions whether connecting 6 AMD AI Max 395+ is faster than using a single server for QWen3-235B-A22B.
7.  [Anyone here gone from custom RAG builds to an actual product?](https://www.reddit.com/r/LocalLLaMA/comments/1nvg83w/anyone_here_gone_from_custom_rag_builds_to_an/) (Score: 4)
    * The thread asks if anyone has successfully transitioned their custom RAG builds into a commercial product.
8.  [Anyone try this one yet? Can it run quantized?](https://www.reddit.com/r/LocalLLaMA/comments/1nvgo0e/anyone_try_this_one_yet_can_it_run_quantized/) (Score: 3)
    * User asks about a model from a screenshot if it can be quantized.
9.  [Translating text within an image (outputting an image)](https://www.reddit.com/r/LocalLLaMA/comments/1nvcwqb/translating_text_within_an_image_outputting_an/) (Score: 3)
    * A discussion about methods for translating text within an image and outputting a translated image.
10. [Help me with my product research?](https://forms.gle/9KESTQwgfa2VgYe9A) (Score: 2)
    * User is doing product research and asking for feedback.
11. [My Journey with RAG, OpenSearch & LLMs (Local LLM)](https://i.redd.it/r3wpsst0sjsf1.jpeg) (Score: 1)
    * The post discusses a user's experience with RAG, OpenSearch, and LLMs.
12. [Finetunning and RL](https://www.reddit.com/r/LocalLLaMA/comments/1nvi36j/finetunning_and_rl/) (Score: 1)
    * User needs help with finetunning and RL.
13. [What's your hope we still get to see GLM 4.6 Air?](https://www.reddit.com/r/LocalLLaMA/comments/1nvdy0u/whats_your_hope_we_still_get_to_see_glm_46_air/) (Score: 1)
    * The post expresses hope for the release of GLM 4.6 Air.
14. [Hi guys, im a newbie in this app, is there any way i can use plugins maybe to make the model gen tokens faster? and maybe make it accept images?](https://www.reddit.com/r/LocalLLaMA/comments/1nvctpx/hi_guys_im_a_newbie_in_this_app_is_there_any_way/) (Score: 0)
    * A newbie asks for ways to make the model generate tokens faster and accept images.
15. [the last edge device. live on the bleeding edge. the edge ai you have been looking for.](https://www.reddit.com/r/LocalLLaMA/comments/1nvguck/the_last_edge_device_live_on_the_bleeding_edge/) (Score: 0)
    * The post advertises an edge AI device.

# Detailed Analysis by Thread
**[I've built Jarvis completely on-device in the browser (Score: 44)](https://v.redd.it/hge6ipzncjsf1)**
*   **Summary:** The creator of Jarvis, an on-device browser-based assistant, is showcasing its functionality and receiving feedback from the community. Users are expressing their admiration, inquiring about the technology behind it, and requesting features and access to the repository.
*   **Emotion:** The overall emotional tone is largely Positive, with many comments expressing excitement and appreciation for the project. Neutral comments are related to people asking for more information.
*   **Top 3 Points of View:**
    *   The project is impressive and innovative.
    *   More details about the technology stack and a repository are desired.
    *   The project has potential and is worth trying out.

**[NVIDIA DGX Spark expected to become available in October 2025 (Score: 31)](https://www.reddit.com/r/LocalLLaMA/comments/1nvdyiy/nvidia_dgx_spark_expected_to_become_available_in/)**
*   **Summary:** The discussion centers on the NVIDIA DGX Spark, expected to be released in October 2025. Users are debating its value proposition, particularly its specifications, price, and potential use cases in the context of local LLM development.
*   **Emotion:** The overall emotional tone is Neutral, with people mostly giving their technical opinions.
*   **Top 3 Points of View:**
    *   The current specs and price make the DGX Spark unattractive compared to alternatives.
    *   It is not for local LLM inference, but rather for mimicking a DGX rack for development.
    *   AMD cards might be faster than NVIDIA cards in llama.cpp.

**[Hunyuan Image 3.0 vs HunyuanImage 2.1 (Score: 7)](https://i.redd.it/5mci5v1vijsf1.png)**
*   **Summary:** A user shared an image comparing Hunyuan Image 3.0 to Hunyuan Image 2.1, and another user commented that the graphs are not showing the same thing.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   The graph are not showing the same thing.

**[How to use mmproj files + Looking for uncensored model for sorting images. (Score: 6)](https://www.reddit.com/r/LocalLLaMA/comments/1nvdz7g/how_to_use_mmproj_files_looking_for_uncensored/)**
*   **Summary:** The user is asking for help on how to use mmproj files, and looking for an uncensored model for sorting images.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   Joycaption and WD-Tagger models can do nsfw content.
    *   https://simonwillison.net/2025/May/10/llama-cpp-vision/
    *   https://huggingface.co/mradermacher/llama-joycaption-beta-one-hf-llava-GGUF

**[Qwen 235B on 2x3090's vs 3x MI50 (Score: 6)](https://www.reddit.com/r/LocalLLaMA/comments/1nvgnv4/qwen_235b_on_2x3090s_vs_3x_mi50/)**
*   **Summary:** The thread discusses performance comparisons of running the Qwen 235B model on 2x 3090 GPUs versus 3x AMD MI50 GPUs.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   RTX 3090s are preferable to AMD MI50s.
    *   There are easier ways to improve speed.
    *   Adding another 3090 is a good option.

**[Connecting 6 AMD AI Max 395+ for QWen3-235B-A22B. Is this really that much faster than just 1 server ? (Score: 4)](https://b23.tv/TO5oW7j)**
*   **Summary:** This thread explores whether connecting 6 AMD AI Max 395+ GPUs to run QWen3-235B-A22B is faster than using a single server.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   Connecting 6 AMD AI Max is most definitely slower than just 1 server
    *   The presenter said they also tested this with Qwen 3 480B and DeepSeek 671B without problem.
    *   Just get a rtx pro for that money

**[Anyone here gone from custom RAG builds to an actual product? (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1nvg83w/anyone_here_gone_from_custom_rag_builds_to_an/)**
*   **Summary:** Users share their experiences of moving from custom RAG builds to a commercial product.
*   **Emotion:** The overall emotional tone is mixed with both positive and negative sentiments.
*   **Top 3 Points of View:**
    *   One user has a custom RAG product that they use in their products.
    *   One user's attempt failed miserably.

**[Anyone try this one yet? Can it run quantized? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1nvgo0e/anyone_try_this_one_yet_can_it_run_quantized/)**
*   **Summary:** User asks about a model from a screenshot if it can be quantized. People say the post is low-effort because the user did not give any context.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   It's rude when people do not include all the information up front.
    *   The model should run fine with 6GB vram.

**[Translating text within an image (outputting an image) (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1nvcwqb/translating_text_within_an_image_outputting_an/)**
*   **Summary:** The user asks about translating text within an image.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   User suggests trying "qwen image edit" or "nano banana".

**[Help me with my product research? (Score: 2)](https://forms.gle/9KESTQwgfa2VgYe9A)**
*   **Summary:** The user is looking for feedback on product research.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   I'm curious to know what you guys aim to do differently

**[My Journey with RAG, OpenSearch & LLMs (Local LLM) (Score: 1)](https://i.redd.it/r3wpsst0sjsf1.jpeg)**
*   **Summary:** The user shares their journey with RAG, OpenSearch, and LLMs.
*   **Emotion:** The overall emotional tone is Positive.
*   **Top 3 Points of View:**
    *   User was inspired to build it.

**[Finetunning and RL (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1nvi36j/finetunning_and_rl/)**
*   **Summary:** User needs help with finetunning and RL.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   The 72b document extraction model sometimes makes up plausible-looking order numbers instead of reading what's actually there.
    *   This is a RAG job, not a fine-tuning job.

**[What's your hope we still get to see GLM 4.6 Air? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1nvdy0u/whats_your_hope_we_still_get_to_see_glm_46_air/)**
*   **Summary:** The user expresses hope of still seeing GLM 4.6 Air.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   There will be one. Probably in two weeks.
    *   Well it's called 4.6 not 5, so I think we will see the "lighter" model but it won't be 4.6, but something else, later
    *   just buy 5 blackwell 6000s and join the heavyweight division

**[Hi guys, im a newbie in this app, is there any way i can use plugins maybe to make the model gen tokens faster? and maybe make it accept images? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1nvctpx/hi_guys_im_a_newbie_in_this_app_is_there_any_way/)**
*   **Summary:** A newbie asks for ways to make the model generate tokens faster and accept images.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   Newer Mistral Small models accept images.
    *   Which app?
    *   If you want it to generate faster, either have better specs orquantize it, about images i dont think this is possible to my knowledge, just use different model

**[the last edge device. live on the bleeding edge. the edge ai you have been looking for. (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1nvguck/the_last_edge_device_live_on_the_bleeding_edge/)**
*   **Summary:** The post advertises an edge AI device.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   so much edge, i'm about to blow
