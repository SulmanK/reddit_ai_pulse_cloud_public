---
title: "Stable Diffusion Subreddit"
date: "2025-10-19"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "video generation"]
---

# Overall Ranking and Top Discussions

1.  [Wan 2.2 Realism, Motion and Emotion.](https://v.redd.it/w22sv96g02wf1) (Score: 762)
    *   Users are discussing the realistic video generated using Wan 2.2, praising its quality, motion, and emotion. Some are seeking information on the workflow and LORAs used to achieve this level of realism.
2.  I’m making an open-sourced comfyui-integrated video editor, and I want to know if you’d find it useful](https://v.redd.it/hkva3wfbo2wf1) (Score: 182)
    *  A user is asking the community if they would find an open-source, ComfyUI-integrated video editor useful. The responses are generally positive, with users offering suggestions for features and integration with existing video editing software.
3.  [Introducing InSubject 0.5, a QwenEdit LoRA trained for creating highly consistent characters/objects w/ just a single reference - samples attached, link + dataset below](https://www.reddit.com/gallery/1oayez0) (Score: 24)
    *   A user is introducing InSubject 0.5, a QwenEdit LoRA, and sharing samples and a dataset. Other users are interested in how to use it and whether it can maintain consistency for characters not specifically trained with the LoRA.
4.  [I built a (opensource) UI for Stable Diffusion focused on workflow and ease of use - Meet PrismXL!](https://www.reddit.com/r/StableDiffusion/comments/1oawx5t/i_built_a_opensource_ui_for_stable_diffusion/) (Score: 9)
    *   A user is showcasing PrismXL, an open-source UI for Stable Diffusion focused on workflow and ease of use.
5.  [WAN 2.2. I always get "grainy" look on objects like hair or fire. Here is an image of my workflow. What might be done better?](https://www.reddit.com/r/StableDiffusion/comments/1oar7on/wan_22_i_always_get_grainy_look_on_objects_like/) (Score: 2)
    *   A user is asking for help with a grainy look on objects when using WAN 2.2, sharing their workflow and asking for suggestions.
6.  [Where can I find LoRas for wan2.2 5b?](https://www.reddit.com/r/StableDiffusion/comments/1oauqy4/where_can_i_find_loras_for_wan22_5b/) (Score: 2)
    *   A user is asking where to find LoRAs for wan2.2 5b.
7.  [Noob Question: Wan Video 2.2 I2V-A14B](https://www.reddit.com/r/StableDiffusion/comments/1oapsh7/noob_question_wan_video_22_i2va14b/) (Score: 1)
    *   A user is asking a noob question about Wan Video 2.2 I2V-A14B and how to download the model and Lora.
8.  [Are there video depth maps? And can I adjust how closely the gen follows the movement in V2V?](https://www.reddit.com/r/StableDiffusion/comments/1oapyrm/are_there_video_depth_maps_and_can_i_adjust_how/) (Score: 1)
    *   A user is asking about video depth maps and the ability to adjust how closely the generation follows the movement in V2V (video-to-video).
9.  [Forge/Automatic11111 Issue](https://www.reddit.com/r/StableDiffusion/comments/1oat5q5/forgeautomatic11111_issue/) (Score: 1)
    *   A user is reporting an issue with Forge/Automatic11111 and asking for help.
10. [Lenovo 16" Legion Pro 7i 5090 for image and video gen](https://www.reddit.com/r/StableDiffusion/comments/1oax9vx/lenovo_16_legion_pro_7i_5090_for_image_and_video/) (Score: 1)
    *   A user is asking about Lenovo 16" Legion Pro 7i 5090 for image and video generation.
11. [How can I run RVC on Google Cloud since my computer won't handle it?](https://www.reddit.com/r/StableDiffusion/comments/1oazi5u/how_can_i_run_rvc_on_google_cloud_since_my/) (Score: 1)
    *   A user is asking for help on how to run RVC on Google Cloud since their computer won't handle it.
12. [Snoo The Alien - Does this look obviously ai generated?](https://i.redd.it/odid9fwny1wf1.png) (Score: 0)
    *   A user is asking if a Snoo the Alien image looks obviously AI generated.
13. [Wan 2.2 14B GGUF Generates solid colors](https://i.redd.it/ionzhpta22wf1.png) (Score: 0)
    *   A user is reporting that Wan 2.2 14B GGUF generates solid colors and asking for help.
14. [Wan Lora plus text encoder training?](https://www.reddit.com/r/StableDiffusion/comments/1oard0i/wan_lora_plus_text_encoder_training/) (Score: 0)
    *   A user is asking about Wan Lora plus text encoder training.
15. [Anyone use Text to Video LTX or WAN on rtx2080??](https://www.reddit.com/r/StableDiffusion/comments/1oau2w5/anyone_use_text_to_video_ltx_or_wan_on_rtx2080/) (Score: 0)
    *   A user is asking if anyone uses Text to Video LTX or WAN on rtx2080.
16. [Do you guys know what kind of AI does some creators use to make AI videos for these anime characters that looks like in a studio recording set?](https://i.redd.it/00aj0980v3wf1.jpeg) (Score: 0)
    *   A user is asking about what kind of AI is used to make AI videos for anime characters that looks like in a studio recording set.
17. [ANNA — a deeply emotional short film created with AI (4K)](https://youtu.be/s6sQzgEZ-n0) (Score: 0)
    *   A user shared a video called ANNA — a deeply emotional short film created with AI (4K).
18. [Trouble with Comfy Linux install](https://www.reddit.com/gallery/1oattq7) (Score: 0)
    *   A user is asking for help with a Comfy Linux install.

# Detailed Analysis by Thread

**[Wan 2.2 Realism, Motion and Emotion. (Score: 762)](https://v.redd.it/w22sv96g02wf1)**
*   **Summary:** This thread revolves around a highly realistic video generated using Wan 2.2. Users are impressed with the quality, motion, and emotional depth of the video. There's a strong interest in the technical aspects, with users requesting information on the LORAs used and the workflow involved in creating the video.
*   **Emotion:** The overall emotional tone is highly Positive. Comments express amazement, admiration, and a desire to learn how to replicate the results.
*   **Top 3 Points of View:**
    *   The video represents the pinnacle of realistic AI video generation with WAN 2.2.
    *   Users are eager to understand the specific techniques and tools used, particularly the LORAs.
    *   The quality is so high that it raises expectations for future AI-generated content, such as anticipating similar realism in GTA6.

**[I’m making an open-sourced comfyui-integrated video editor, and I want to know if you’d find it useful (Score: 182)](https://v.redd.it/hkva3wfbo2wf1)**
*   **Summary:** The thread discusses a user's project to create an open-source video editor integrated with ComfyUI. The creator seeks feedback on its potential usefulness. The community shows interest and provides suggestions.
*   **Emotion:** The emotional tone is mixed, with generally Neutral sentiment and occasional Positive sentiment. Users express interest, but also offer critical feedback and suggestions.
*   **Top 3 Points of View:**
    *   An open-source ComfyUI-integrated video editor would be useful to many users.
    *   The editor should focus on simplifying editing functions and allow exporting to more advanced video editing programs.
    *   Some users think it's better to have dedicated software instead of softwares that cram too much functionnalities and try to be everything.

**[Introducing InSubject 0.5, a QwenEdit LoRA trained for creating highly consistent characters/objects w/ just a single reference - samples attached, link + dataset below (Score: 24)](https://www.reddit.com/gallery/1oayez0)**
*   **Summary:** A user introduces a new LoRA (InSubject 0.5) for creating consistent characters/objects with a single reference image, and shares the associated dataset. The community expresses interest and asks questions about its usage and capabilities.
*   **Emotion:** The overall emotional tone is Positive, with expressions of amazement and appreciation for the shared resource.
*   **Top 3 Points of View:**
    *   The LoRA is a cool and useful tool for achieving character consistency.
    *   Users are seeking clear instructions on how to implement and use the LoRA.
    *   There is curiosity about whether the LoRA can maintain consistency for characters not specifically trained with it.

**[I built a (opensource) UI for Stable Diffusion focused on workflow and ease of use - Meet PrismXL! (Score: 9)](https://www.reddit.com/r/StableDiffusion/comments/1oawx5t/i_built_a_opensource_ui_for_stable_diffusion/)**
*   **Summary:** A user is introducing PrismXL, an open-source UI for Stable Diffusion focused on workflow and ease of use.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   A user suggests to add the ability to add 1 LoRA at a time by clicking the plus sign.

**[WAN 2.2. I always get "grainy" look on objects like hair or fire. Here is an image of my workflow. What might be done better? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1oar7on/wan_22_i_always_get_grainy_look_on_objects_like/)**
*   **Summary:** A user is asking for help with a grainy look on objects when using WAN 2.2, sharing their workflow and asking for suggestions.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   The user uses res\_2s sampler, beta57 scheduler to fix it.
    *   The resolution of 640x640 is low.
    *   The lightx2v loras are a bit old and other newer ones are out.

**[Where can I find LoRas for wan2.2 5b? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1oauqy4/where_can_i_find_loras_for_wan22_5b/)**
*   **Summary:** A user is asking where to find LoRAs for wan2.2 5b.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   User responds with "forget it".

**[Noob Question: Wan Video 2.2 I2V-A14B (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1oapsh7/noob_question_wan_video_22_i2va14b/)**
*   **Summary:** A user is asking a noob question about Wan Video 2.2 I2V-A14B and how to download the model and Lora.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   Choose a model according to your VRAM size. If you have 16GB, you cannot use a 20GB model.
    *   High and Low noise models help the model determine consistent output quickly.
    *   The format of the model should not affect the ability to load a lora.

**[Are there video depth maps? And can I adjust how closely the gen follows the movement in V2V? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1oapyrm/are_there_video_depth_maps_and_can_i_adjust_how/)**
*   **Summary:** A user is asking about video depth maps and the ability to adjust how closely the generation follows the movement in V2V (video-to-video).
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   Video Depth Maps, DepthAnything V2 and Marigold are great for video depth estimation and MiDaS is for frame-by-frame processing.
    *   V2V movement control is controlled by: Denoise Strength, CFG Scale, ControlNet Strength and conditioning scale.
    *   You can use wan 2.2 fun control or wan 2.2 fun vace, or an older model to set the strength of the control video.

**[Forge/Automatic11111 Issue (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1oat5q5/forgeautomatic11111_issue/)**
*   **Summary:** A user is reporting an issue with Forge/Automatic11111 and asking for help.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   Stop Forge. Delete the venv folder and restart Forge it will rebuilt it.
    *   both a1111 and forge are abandoned. i'd move on to better/current tools like swarm or comfyui
    *   try portable version of latest automatic clone.

**[Lenovo 16" Legion Pro 7i 5090 for image and video gen (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1oax9vx/lenovo_16_legion_pro_7i_5090_for_image_and_video/)**
*   **Summary:** A user is asking about Lenovo 16" Legion Pro 7i 5090 for image and video generation.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   A mobile workstation isn't something you want to carry around.
    *   LoRA training blocks his machine for too long and the temperatures can't be healthy, neither for machnine nor for him when using the keyboard.
    *   Consider using a very light weight laptop and just rent the GPU in the cloud on a per use basis.

**[How can I run RVC on Google Cloud since my computer won't handle it? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1oazi5u/how_can_i_run_rvc_on_google_cloud_since_my/)**
*   **Summary:** A user is asking for help on how to run RVC on Google Cloud since their computer won't handle it.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   It is recommended to run it on Runpod.io or Lambda Labs or another online service.

**[Snoo The Alien - Does this look obviously ai generated? (Score: 0)](https://i.redd.it/odid9fwny1wf1.png)**
*   **Summary:** A user is asking if a Snoo the Alien image looks obviously AI generated.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   Count the toes 😉 maybe it's a racial feature of that particular alien.
    *   I wouldn't say it's definitely AI generated, but it certainly looks CGI/rendered.
    *   The user created it using Krita and the AI Diffusion plugin.

**[Wan 2.2 14B GGUF Generates solid colors (Score: 0)](https://i.redd.it/ionzhpta22wf1.png)**
*   **Summary:** A user is reporting that Wan 2.2 14B GGUF generates solid colors and asking for help.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   Post the workflow.
    *   This is either Noise disabled on the Low noise KSampler, or a strange Sampler/Scheduler combo.
    *   Make sure the lightning models are appropriately matched to their respective high noise and low noise paths to the ksampler.

**[Wan Lora plus text encoder training? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1oard0i/wan_lora_plus_text_encoder_training/)**
*   **Summary:** A user is asking about Wan Lora plus text encoder training.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   It's not really recommended to train text encoders even in SD models, unless you know what you're doing.

**[Anyone use Text to Video LTX or WAN on rtx2080?? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1oau2w5/anyone_use_text_to_video_ltx_or_wan_on_rtx2080/)**
*   **Summary:** A user is asking if anyone uses Text to Video LTX or WAN on rtx2080.
*   **Emotion:** The emotional tone is Positive.
*   **Top 3 Points of View:**
    *   Don't buy a 2080 laptop.
    *   Get a 4060 or 5050.
    *   If you don't need a laptop for portability, you can build your own equivalent or better PC

**[Do you guys know what kind of AI does some creators use to make AI videos for these anime characters that looks like in a studio recording set? (Score: 0)](https://i.redd.it/00aj0980v3wf1.jpeg)**
*   **Summary:** A user is asking about what kind of AI is used to make AI videos for anime characters that looks like in a studio recording set.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   Pretty sure that's just Sora 2.
    *   perhaps a combination of Qwen plus some character or realism LORAs and also SeedVR2 for upscaling could pull them off.

**[ANNA — a deeply emotional short film created with AI (4K) (Score: 0)](https://youtu.be/s6sQzgEZ-n0)**
*   **Summary:** A user shared a video called ANNA — a deeply emotional short film created with AI (4K).
*   **Emotion:** The emotional tone is Positive.
*   **Top 3 Points of View:**
    *   The music was nice, too. The biggest issue is that the actress just has dead eyes and no emotion in between the moments where you obviously prompt for her to experience them.
    *   You have demonstrated that this technology has the potential to allow a single artist to create in ways not possible without it.
    *   The video looks almost exactly like every other AI slop. Very discordant, zero emotions (despite your tagline).

**[Trouble with Comfy Linux install (Score: 0)](https://www.reddit.com/gallery/1oattq7)**
*   **Summary:** A user is asking for help with a Comfy Linux install.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   install torch torchvision and ROCm for AMD
    *   you installed the cuda version of torch, that's for nvidia cards, uninstall torch and install the amd rocm version,
    *   The first thing you should get installed in your venv is the right version of torch. Then you can run the pip install -r requirements.
