---
title: "LocalLLaMA Subreddit"
date: "2025-10-19"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["localllama", "AI", "models"]
---

# Overall Ranking and Top Discussions
1.  [I built a 1B CAD generator model](https://v.redd.it/pn0yo3o2v3wf1) (Score: 23)
    *   A user shares their work on building a 1B CAD generator model and seeks feedback.
2.  [Quantized some MoE models with MXFP4](https://www.reddit.com/r/LocalLLaMA/comments/1oav3r1/quantized_some_moe_models_with_mxfp4/) (Score: 17)
    *   A user has quantized Mixture of Experts (MoE) models with MXFP4 and the community discusses its advantages and potential use cases.
3.  [Two new Google models, "lithiumflow" and "orionmist", have been added to LMArena. This is Google's naming scheme and "orion" has been used internally with Gemini 3 codenames, so these are likely Gemini 3 models](https://www.reddit.com/r/LocalLLaMA/comments/1oav4hi/two_new_google_models_lithiumflow_and_orionmist/) (Score: 17)
    *   Two new Google models, lithiumflow and orionmist, have been added to LMArena. This post speculates whether those models are from Google Gemini 3.
4.  [lazylms - TUI for LM Studio](https://www.reddit.com/gallery/1oauxgg) (Score: 8)
    *   A user introduces lazylms, a TUI (Text User Interface) for LM Studio, a tool for running local language models.
5.  [If the bubble really pops how can that affect local AI models?](https://www.reddit.com/r/LocalLLaMA/comments/1oatip1/if_the_bubble_really_pops_how_can_that_affect/) (Score: 6)
    *   Users discuss the potential impact of an AI bubble burst on the local AI model landscape, debating whether it would hinder progress or simply lead to cheaper resources.
6.  [Reverse Engineering and Tracing internal thoughts of LLM](https://www.reddit.com/r/LocalLLaMA/comments/1oavy6f/reverse_engineering_and_tracing_internal_thoughts/) (Score: 6)
    *   A user shares their work on reverse engineering and tracing the internal thoughts of LLMs, and other users share their thoughts.
7.  [Free API Key for GLM 4.6](https://www.reddit.com/r/LocalLLaMA/comments/1oayl4j/free_api_key_for_glm_46/) (Score: 5)
    *   A user shares a free API key for GLM 4.6 and invites others to test it.
8.  [Best Current Model for Programming?](https://www.reddit.com/r/LocalLLaMA/comments/1oat6fh/best_current_model_for_programming/) (Score: 3)
    *   Users discuss which current models are best for programming tasks.
9.  [Energy Based Adapter Help](https://www.reddit.com/r/LocalLLaMA/comments/1oaumqx/energy_based_adapter_help/) (Score: 2)
    *   A user is asking for help with an Energy Based Adapter project.
10. [How can I determine OCR confidence level when using a VLM?](https://www.reddit.com/r/LocalLLaMA/comments/1oaum1r/how_can_i_determine_ocr_confidence_level_when/) (Score: 0)
    *   A user asks how to determine OCR confidence level when using a VLM (Visual Language Model).
11. [Got new 5070ti gpu, have access to 16gb vram. What things can I do with it for AI?](https://www.reddit.com/r/LocalLLaMA/comments/1oayrh9/got_new_5070ti_gpu_have_access_to_16gb_vram_what/) (Score: 0)
    *   A user asks for suggestions on what AI-related tasks they can perform with their new 5070ti GPU and 16GB of VRAM.
12. [N00b looking to get initial hardware to play with](https://www.reddit.com/r/LocalLLaMA/comments/1oatzjf/n00b_looking_to_get_initial_hardware_to_play_with/) (Score: 0)
    *   A newbie asks for advice on initial hardware to play with local LLMs.
13. [The next breakthrough is high computer low memory , not MOE](https://www.reddit.com/r/LocalLLaMA/comments/1oatd53/the_next_breakthrough_is_high_computer_low_memory/) (Score: 0)
    *   The post claims that the next breakthrough is high compute low memory, not MOE (Mixture of Experts)
14. [I came from the future and in the future we all laugh at MoEs and "Thinkers" ðŸ¤£](https://www.reddit.com/r/LocalLLaMA/comments/1oasmnx/i_came_from_the_future_and_in_the_future_we_all/) (Score: 0)
    *   The poster claims that in the future, MoEs and "Thinkers" are laughed at.
15. [Its Impossible, Change My Mind](https://www.reddit.com/gallery/1oau144) (Score: 0)
    *   The post claims that you can not determine whether the model is benchmaxed, if they perform very very good in benchmarks but dont use massive amount of reasoning tokens.
16. [Confused about GLM 4.6 running locally.](https://www.reddit.com/r/LocalLLaMA/comments/1oavg1z/confused_about_glm_46_running_locally/) (Score: 0)
    *   The user is confused about why GLM 4.6 is not working as expected when running it locally with ollama.
17. [Hot take: Recursive reasoning might be the actual path to AGI, not scaling to 1T parameters](https://www.reddit.com/r/LocalLLaMA/comments/1oavtxb/hot_take_recursive_reasoning_might_be_the_actual/) (Score: 0)
    *   The poster suggests that recursive reasoning might be the actual path to AGI instead of scaling to 1T parameters.
18. [Best Ollama model for coding?](https://www.reddit.com/r/LocalLLaMA/comments/1oaxowb/best_ollama_model_for_coding/) (Score: 0)
    *   A user asks which is the best Ollama model for coding.

# Detailed Analysis by Thread
**[[P] I built a 1B CAD generator model (Score: 23)](https://v.redd.it/pn0yo3o2v3wf1)**
*  **Summary:** A user showcases their creation of a 1B CAD generator model and invites feedback and discussion.
*  **Emotion:** The emotion is generally neutral.
*  **Top 3 Points of View:**
    *   The user is seeking advice on potential use cases for text-to-3D data.
    *   Another user recommends exploring the "objectverse dataset."
    *   One user jokingly remarks about the CAD model triggering their OCD.

**[[D] Quantized some MoE models with MXFP4 (Score: 17)](https://www.reddit.com/r/LocalLLaMA/comments/1oav3r1/quantized_some_moe_models_with_mxfp4/)**
*  **Summary:** The thread discusses the quantization of Mixture of Experts (MoE) models using the MXFP4 format, with users sharing their experiences, opinions, and technical questions.
*  **Emotion:** The overall tone of the thread is positive, with users expressing gratitude, sharing positive experiences, and showing interest in the topic.
*  **Top 3 Points of View:**
    *   MXFP4 is a great format for MoE GGUFs with the lowest quality degradation.
    *   Some believe traditional 4-bit formats are just as good.
    *   Users are requesting quantization for specific models (e.g., glm 4.5 air, qwen 235B).

**[[D] Two new Google models, "lithiumflow" and "orionmist", have been added to LMArena. This is Google's naming scheme and "orion" has been used internally with Gemini 3 codenames, so these are likely Gemini 3 models (Score: 17)](https://www.reddit.com/r/LocalLLaMA/comments/1oav4hi/two_new_google_models_lithiumflow_and_orionmist/)**
*  **Summary:**  Discussion regarding two new Google models added to LMArena and speculation about whether they are related to Gemini 3 or Gemma.
*  **Emotion:** The emotion is mostly neutral and positive.
*  **Top 3 Points of View:**
    *   Speculation that the models are Gemini 3 models.
    *   Suggestion that the models could be Gemma.
    *   One user hopes they can turn off the "thinking" in Gemini.

**[[P] lazylms - TUI for LM Studio (Score: 8)](https://www.reddit.com/gallery/1oauxgg)**
*  **Summary:** A user is introducing their project, lazylms, which is a TUI for LM Studio.
*  **Emotion:** The emotion is mostly neutral.
*  **Top 3 Points of View:**
    *   The user is sharing their new project.
    *   Some users are facing issues loading models.
    *   Some users are asking why it was made for LM Studio and not llama.cpp.

**[[Discussion] If the bubble really pops how can that affect local AI models? (Score: 6)](https://www.reddit.com/r/LocalLLaMA/comments/1oatip1/if_the_bubble_really_pops_how_can_that_affect/)**
*  **Summary:** The thread explores the potential consequences of an AI bubble burst on the local AI model community, including hardware costs, research funding, and the availability of open-source models.
*  **Emotion:** The emotion is mostly positive and neutral.
*  **Top 3 Points of View:**
    *   A burst could lead to cheaper compute and GPUs.
    *   Companies may release fewer new models to the public.
    *   It could resemble the dotcom bubble burst, with a temporary slowdown but eventual resurgence.

**[[Discussion] Reverse Engineering and Tracing internal thoughts of LLM (Score: 6)](https://www.reddit.com/r/LocalLLaMA/comments/1oavy6f/reverse_engineering_and_tracing_internal_thoughts/)**
*  **Summary:** A user shares their work on reverse engineering and tracing the internal thoughts of LLMs.
*  **Emotion:** The emotion is mostly positive and neutral.
*  **Top 3 Points of View:**
    *   A user finds the documentation helpful.
    *   The AI doesn't know that Paris is France's capital.
    *   It pays most attention to the start of the sentence and only 5.6% to the word "France".

**[[Request] Free API Key for GLM 4.6 (Score: 5)](https://www.reddit.com/r/LocalLLaMA/comments/1oayl4j/free_api_key_for_glm_46/)**
*  **Summary:**  A user shares a free API key for GLM 4.6 and invites others to test it.
*  **Emotion:** The emotion is neutral.
*  **Top 3 Points of View:**
    *   The key has a low max token output limit.
    *   User offering the key wants to know if there will be rate limiting and data collecting.
    *   Users are willing to test the key.

**[[Question] Best Current Model for Programming? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1oat6fh/best_current_model_for_programming/)**
*  **Summary:**  Users are discussing which current models are best for programming tasks.
*  **Emotion:** The emotion is neutral.
*  **Top 3 Points of View:**
    *   GLM 4.5-Air, Seed-oss-36b, gpt-oss-120b
    *   Qwen 3 coder a3b. Gom 4.5 air.
    *   Some users agree with the questions.

**[[Help] Energy Based Adapter Help (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1oaumqx/energy_based_adapter_help/)**
*  **Summary:**  A user is asking for help with an Energy Based Adapter project, and the repetition in the text generated.
*  **Emotion:** The emotion is positive.
*  **Top 3 Points of View:**
    *   The repetition is likely due to the non-autoregressive setup.
    *   Check Langevin dynamics parameters (step size, noise).
    *   The stability of your contrastive divergence training.

**[[Question] How can I determine OCR confidence level when using a VLM? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1oaum1r/how_can_i_determine_ocr_confidence_level_when/)**
*  **Summary:** A user asks how to determine OCR confidence level when using a VLM (Visual Language Model).
*  **Emotion:** The emotion is neutral.
*  **Top 3 Points of View:**
    *   Run a spellchecker over the result, pop a warning if it raises eyebrow too much.
    *   One approach that works surprisingly well is running the same extraction twice with slightly different prompts and checking for consistency.

**[[Question] Got new 5070ti gpu, have access to 16gb vram. What things can I do with it for AI? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1oayrh9/got_new_5070ti_gpu_have_access_to_16gb_vram_what/)**
*  **Summary:**  A user asks for suggestions on what AI-related tasks they can perform with their new 5070ti GPU and 16GB of VRAM.
*  **Emotion:** The emotion is neutral.
*  **Top 3 Points of View:**
    *   You should be able to get higher context (128k or the limit) and greater t/s for gpt-oss 20b.
    *   You can run all the Impish models.
    *   I have 4070 and I'm curious what people run with 12gb and at what speed?

**[[Question] N00b looking to get initial hardware to play with (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1oatzjf/n00b_looking_to_get_initial_hardware_to_play_with/)**
*  **Summary:**  A newbie asks for advice on initial hardware to play with local LLMs.
*  **Emotion:** The emotion is positive and neutral.
*  **Top 3 Points of View:**
    *   ChatGPT can answer these questions quite well.
    *   I still think the best value is a 12Gb 3060 if you can find one being sold locally.
    *   Yes, your GPU's VRAM needs to be at least the size of the model to load it.

**[[Opinion] The next breakthrough is high computer low memory , not MOE (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1oatd53/the_next_breakthrough_is_high_computer_low_memory/)**
*  **Summary:** The post claims that the next breakthrough is high compute low memory, not MOE (Mixture of Experts)
*  **Emotion:** The emotion is neutral.
*  **Top 3 Points of View:**
    *   Compute is actually way more expensive and more difficult to scale than memory ***.
    *   With LLMs the memory bandwidth constraint tends to bind hard because of the nature of the autoregressive generation.
    *   Is there already a coined term for people gaslit by ai thinking to be the next almighty technical genius?

**[[Shitpost] I came from the future and in the future we all laugh at MoEs and "Thinkers" ðŸ¤£ (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1oasmnx/i_came_from_the_future_and_in_the_future_we_all/)**
*  **Summary:** The poster claims that in the future, MoEs and "Thinkers" are laughed at.
*  **Emotion:** The emotion is neutral.
*  **Top 3 Points of View:**
    *   compute is cheaper and faster than memory bandwidth and fast memory.
    *   It probably took you long enough to write all of that with just one hand.
    *   I think both concepts were inspired by an actual brain.

**[[Meme] Its Impossible, Change My Mind (Score: 0)](https://www.reddit.com/gallery/1oau144)**
*  **Summary:** The post claims that you can not determine whether the model is benchmaxed, if they perform very very good in benchmarks but dont use massive amount of reasoning tokens.
*  **Emotion:** The emotion is neutral and negative.
*  **Top 3 Points of View:**
    *   using many tokens doesn't mean that it doesn't already know the answer.
    *   GPT-OSS is proof that any model can spend more time thinking on a topic.
    *   Neither you nor I know enough about the architecture and how the models are trained to meaningfully argue if they would use less tokens if benchmaxed.

**[[Question] Confused about GLM 4.6 running locally. (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1oavg1z/confused_about_glm_46_running_locally/)**
*  **Summary:** The user is confused about why GLM 4.6 is not working as expected when running it locally with ollama.
*  **Emotion:** The emotion is neutral.
*  **Top 3 Points of View:**
    *   Why use Ollama if you have a setup to run GLM 4.6?
    *   Could be a prompt template issue.
    *   Found it, it's right there. Probably nobody with a rig that can actually run GLM-4.6 has ever sanity tested it on ollama with ollama's uploaded quant.

**[[Discussion] Hot take: Recursive reasoning might be the actual path to AGI, not scaling to 1T parameters (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1oavtxb/hot_take_recursive_reasoning_might_be_the_actual/)**
*  **Summary:** The poster suggests that recursive reasoning might be the actual path to AGI instead of scaling to 1T parameters.
*  **Emotion:** The emotion is mostly positive and neutral.
*  **Top 3 Points of View:**
    *   As small models keep improving, they would be even better building blocks.
    *   COT effectively gives you a form of recurence.
    *   "Recursive reasoning" is a fancy way of saying chain-of-thought or iterative reasoning, which LLMs already do.

**[[Question] Best Ollama model for coding? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1oaxowb/best_ollama_model_for_coding/)**
*  **Summary:** A user asks which is the best Ollama model for coding.
*  **Emotion:** The emotion is neutral.
*  **Top 3 Points of View:**
    *   Pretty much nothing is going to get the job done. Might as well signup on [Z.ai](http://Z.ai) and use GLM 4.6 with droid.
    *   Qwen3-coder is a good local model.
    *   What's large coding tasks? Also, I've never used Ollama.
