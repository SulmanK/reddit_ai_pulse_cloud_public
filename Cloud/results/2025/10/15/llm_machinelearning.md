---
title: "Machine Learning Subreddit"
date: "2025-10-15"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "LLM"]
---

# Overall Ranking and Top Discussions
1.  [[P] Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes & More](https://www.reddit.com/r/MachineLearning/comments/1o7160j/p_nanonetsocr2_an_opensource_imagetomarkdown/) (Score: 43)
    * The discussion revolves around an open-source image-to-markdown model.
2.  [[D] ML interviewers, what do you wnat to hear during an interview?](https://www.reddit.com/r/MachineLearning/comments/1o7d963/d_ml_interviewers_what_do_you_wnat_to_hear_during/) (Score: 28)
    *  The discussion is about what ML interviewers want to hear during an interview, specifically for research-level positions.
3.  [[R]: Create a family of pre-trained LLMs of intermediate sizes from a single student-teacher pair](https://www.reddit.com/r/MachineLearning/comments/1o7hywy/r_create_a_family_of_pretrained_llms_of/) (Score: 17)
    * The post is a discussion on creating a family of pre-trained LLMs of intermediate sizes from a single student-teacher pair.
4.  [[D] ICCV 2025 Hawaii](https://www.reddit.com/r/MachineLearning/comments/1o7ar8w/d_iccv_2025_hawaii/) (Score: 7)
    * This thread is about the upcoming ICCV 2025 conference in Hawaii.
5.  [[R] Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity](https://www.reddit.com/r/MachineLearning/comments/1o7ifvy/r_verbalized_sampling_how_to_mitigate_mode/) (Score: 4)
    * This thread discusses a technique called Verbalized Sampling to mitigate mode collapse and unlock diversity in LLMs.
6.  [[D] Curious asymmetry when swapping step order in data processing pipelines](https://www.reddit.com/r/MachineLearning/comments/1o70jyv/d_curious_asymmetry_when_swapping_step_order_in/) (Score: 3)
    * This post discusses the asymmetry observed when swapping the order of steps in data processing pipelines.

# Detailed Analysis by Thread
**[[P] Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes & More (Score: 43)](https://www.reddit.com/r/MachineLearning/comments/1o7160j/p_nanonetsocr2_an_opensource_imagetomarkdown/)**
*  **Summary:** This thread discusses the release of Nanonets-OCR2, an open-source image-to-markdown model capable of handling LaTeX, tables, flowcharts, handwritten documents, and checkboxes.
*  **Emotion:** The overall emotional tone is positive and neutral, with users expressing interest and asking questions.
*  **Top 3 Points of View:**
    *   Users are interested in using it for note-taking in ObsidianMD.
    *   The table extraction feature is of particular interest.
    *   Users are inquiring about the licensing terms for commercial use.

**[[D] ML interviewers, what do you wnat to hear during an interview? (Score: 28)](https://www.reddit.com/r/MachineLearning/comments/1o7d963/d_ml_interviewers_what_do_you_wnat_to_hear_during/)**
*  **Summary:** This thread is a discussion about what machine learning interviewers look for in candidates, particularly for research-oriented roles.
*  **Emotion:** The overall emotional tone is neutral to positive with some negative sentiment, with interviewers sharing their expectations and advice.
*  **Top 3 Points of View:**
    *   Interviewers value fluidity of mind, the ability to pivot, and a linear Google Scholar curve (for research positions).
    *   The ability to discuss and tackle research problems is crucial, beyond just the current tasks.
    *   Research roles typically go to PhDs or those with significant experience and publications. A fresh master's degree might face challenges due to high competition.

**[[R]: Create a family of pre-trained LLMs of intermediate sizes from a single student-teacher pair (Score: 17)](https://www.reddit.com/r/MachineLearning/comments/1o7hywy/r_create_a_family_of_pretrained_llms_of/)**
*  **Summary:** This thread discusses a research paper about creating a family of pre-trained LLMs of varying sizes from a single student-teacher pair using a technique called "boomerang distillation."
*  **Emotion:** The overall emotional tone is neutral, showing intrigue and in-depth analysis of the research.
*  **Top 3 Points of View:**
    *   The "boomerang distillation" method, where knowledge is transferred from a teacher model to a student model and then selectively reintegrated, is considered technically impressive and elegant.
    *   The concept of creating a "family" of models through architectural interpolation is contrasted with the idea of an "emergent family" within a single architecture, raising philosophical questions about model identity.
    *   The technique has potential for democratizing AI by enabling flexible deployment of LLMs on various devices and platforms.

**[[D] ICCV 2025 Hawaii (Score: 7)](https://www.reddit.com/r/MachineLearning/comments/1o7ar8w/d_iccv_2025_hawaii/)**
*  **Summary:** This thread appears to be for people who are attending ICCV 2025 in Hawaii.
*  **Emotion:** The overall emotional tone is positive and neutral, as people are generally expressing interest in the conference.
*  **Top 3 Points of View:**
    *   Users express excitement about attending the conference for the first time.
    *   Attendees are open to connecting with others at the conference.
    *   The thread serves as a means to network and coordinate meetups at the conference.

**[[R] Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity (Score: 4)](https://www.reddit.com/r/MachineLearning/comments/1o7ifvy/r_verbalized_sampling_how_to_mitigate_mode/)**
*  **Summary:** The thread discusses a research paper on Verbalized Sampling, a method to reduce mode collapse and increase the diversity of LLM outputs.
*  **Emotion:** The overall emotional tone is positive and neutral, reflecting a mixture of interest, questioning, and author engagement.
*  **Top 3 Points of View:**
    *   The author is available to answer questions and assist with troubleshooting.
    *   Concerns are raised about whether using probabilities leads to a representative sample, or if it merely causes statistical variation in the training data.
    *   There's a question about the method's effectiveness and the interpretation of how an LLM "accesses" its training corpus.

**[[D] Curious asymmetry when swapping step order in data processing pipelines (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1o70jyv/d_curious_asymmetry_when_swapping_step_order_in/)**
*  **Summary:** This thread discusses the asymmetry observed when the order of steps in data processing pipelines is swapped.
*  **Emotion:** The overall emotional tone is neutral, expressing agreement and providing examples.
*  **Top 3 Points of View:**
    *  The asymmetry is not surprising, as many transformations can negatively impact normalization.
    *  Many image transformations are order-dependent.
    *  Deterministic processes are not necessarily transitive, so order often matters.
