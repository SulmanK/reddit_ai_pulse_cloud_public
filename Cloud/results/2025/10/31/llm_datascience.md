---
title: "Data Science Subreddit"
date: "2025-10-31"
description: "Analysis of top discussions and trends in the datascience subreddit"
tags: ["datascience", "reddit", "analysis"]
---

# Overall Ranking and Top Discussions
1.  [[D] My notebook workflow](https://www.reddit.com/r/datascience/comments/1oku793/my_notebook_workflow/) (Score: 9)
    *   Discusses different approaches to managing and organizing data science projects using notebooks, including using VSCode with Jupyter extensions and focusing on writing good functions.
2.  [Home Insurance Claims Recovery modelling experience (subrogation)](https://www.reddit.com/r/datascience/comments/1okvmiz/home_insurance_claims_recovery_modelling/) (Score: 3)
    *   Asks for advice on modeling home insurance claims recovery, with suggestions to consult an actuary or use ChatGPT.
3.  [/r/askdatascience/comments/1okvrvh/what_are_some_key_issues_with_data_science/](https://www.reddit.com/r/askdatascience/comments/1okvrvh/what_are_some_key_issues_with_data_science/) (Score: 2)
    *   Discusses key issues with data science undergrad degrees, including weak math/stats education and disconnects between academia and real-world business applications.
4.  [From Data to Value: The Architecture of AI Impact](https://i0.wp.com/devnavigator.com/wp-content/uploads/2025/10/image-39.png?resize=1200%2C665&ssl=1) (Score: 0)
    *   A comment expressing skepticism about the simplicity of AI impact architecture in real-world scenarios.
5.  [How to train a LLM as a poor guy?](https://www.reddit.com/r/datascience/comments/1okoiyw/how_to_train_a_llm_as_a_poor_guy/) (Score: 0)
    *   Asks for advice on training a Large Language Model (LLM) with limited resources, with suggestions including using RAG, Google Colab, Kaggle, or smaller models.

# Detailed Analysis by Thread
**[[D] My notebook workflow (Score: 9)](https://www.reddit.com/r/datascience/comments/1oku793/my_notebook_workflow/)**
*  **Summary:** This thread discusses different approaches to managing data science projects using notebooks. Users share their workflows, including using VSCode with Jupyter extensions, keeping notebooks within a repository, and focusing on writing good functions that are documented, typed, and tested.
*  **Emotion:** The overall emotional tone is Positive. The comments express interest and provide constructive suggestions, indicating a helpful and encouraging atmosphere.
*  **Top 3 Points of View:**
    *   Some users find it helpful to keep a folder for notebooks within their repository and use VSCode with the Jupyter extension for a clean setup.
    *   Others emphasize the importance of writing well-documented, typed, and tested functions, even during the exploration phase, for long-term efficiency and reusability.
    *   Connecting VSCode to remote compute resources like Databricks is a challenge that requires attention.

**[Home Insurance Claims Recovery modelling experience (subrogation) (Score: 3)](https://www.reddit.com/r/datascience/comments/1okvmiz/home_insurance_claims_recovery_modelling/)**
*  **Summary:**  This thread is seeking advice on modeling home insurance claims recovery (subrogation). The suggestions are brief.
*  **Emotion:** The emotional tone is Neutral. The comments are short and direct.
*  **Top 3 Points of View:**
    *   Consult an actuary for expertise.
    *   Use ChatGPT for assistance.
    *   Someone expresses general interest in the problem being solved.

**[/r/askdatascience/comments/1okvrvh/what_are_some_key_issues_with_data_science/ (Score: 2)](https://www.reddit.com/r/askdatascience/comments/1okvrvh/what_are_some_key_issues_with_data_science/)**
*  **Summary:** This thread discusses the key issues with data science undergrad degrees. Recurring themes include weak math/stats education, disconnects between academia and real-world business applications, and the overemphasis on highly technical but often impractical skills.
*  **Emotion:** The emotional tone is Neutral. The comments offer critical perspectives on data science education and its relevance to the business world.
*  **Top 3 Points of View:**
    *   Data science programs often lack sufficient rigor in math and statistics, leading to graduates who lack fundamental understanding.
    *   There's a significant disconnect between the skills taught in academia and the actual work performed in business settings, where data cleaning and reliable software development are more important.
    *   Data scientists in business may feel underutilized or relegated to routine tasks, highlighting a lack of understanding of analytics at various levels within companies.

**[From Data to Value: The Architecture of AI Impact (Score: 0)](https://i0.wp.com/devnavigator.com/wp-content/uploads/2025/10/image-39.png?resize=1200%2C665&ssl=1)**
*  **Summary:** The thread involves a single comment expressing skepticism about how easily data turns into value when working with AI in a real-world application.
*  **Emotion:** The emotional tone is Neutral, marked by a hint of skepticism.
*  **Top 3 Points of View:**
    *   The user questions the simplistic and streamlined representations of AI's impact in real-world applications.

**[How to train a LLM as a poor guy? (Score: 0)](https://www.reddit.com/r/datascience/comments/1okoiyw/how_to_train_a_llm_as_a_poor_guy/)**
*  **Summary:** This thread asks for advice on training a Large Language Model (LLM) with limited resources. Suggestions range from using Retrieval-Augmented Generation (RAG) to using cloud services with free or low-cost GPU options and smaller models.
*  **Emotion:** The overall emotional tone is Neutral. The comments are practical and offer various strategies to overcome resource limitations.
*  **Top 3 Points of View:**
    *   Consider using RAG (Retrieval-Augmented Generation) to reduce training time and improve accuracy, especially when dealing with specialized data like medical data.
    *   Explore free or low-cost GPU options on platforms like Google Colab, Kaggle, or spot instances on cloud providers for fine-tuning.
    *   If resources are severely limited, consider using smaller LLMs or alternative approaches like QLoRA with CPU offloading.
