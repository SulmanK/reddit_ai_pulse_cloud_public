---
title: "Machine Learning Subreddit"
date: "2025-10-31"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "LLM"]
---

# Overall Ranking and Top Discussions
1.  [[R] We found LRMs look great…until the problems get harder (AACL 2025)](https://www.reddit.com/r/MachineLearning/comments/1okdq0s/r_we_found_lrms_look_greatuntil_the_problems_get/) (Score: 23)
    *   Discussion about the limitations of Large Reasoning Models (LRMs) when faced with increasingly complex problems, as highlighted in a paper presented at AACL 2025.
2.  [[D] How to benchmark open-ended, real-world goal achievement by computer-using LLMs?](https://www.reddit.com/r/MachineLearning/comments/1okwuyx/d_how_to_benchmark_openended_realworld_goal/) (Score: 2)
    *   A discussion on how to effectively benchmark Large Language Models (LLMs) based on their ability to achieve goals in real-world scenarios, noting the current benchmarks mainly focus on text-based tasks.
3.  [[R] A New Species of Artificial Intelligence: KMS-Stabilized Reasoning with Harmonic Algebra](https://www.reddit.com/r/MachineLearning/comments/1ol14tv/r_a_new_species_of_artificial_intelligence/) (Score: 0)
    *   Discussion around "KMS-Stabilized Reasoning with Harmonic Algebra," a novel approach to artificial intelligence.

# Detailed Analysis by Thread
**[[R] We found LRMs look great…until the problems get harder (AACL 2025) (Score: 23)](https://www.reddit.com/r/MachineLearning/comments/1okdq0s/r_we_found_lrms_look_greatuntil_the_problems_get/)**
*   **Summary:** Discussion about the limitations of Large Reasoning Models (LRMs) when faced with increasingly complex problems, as highlighted in a paper presented at AACL 2025. Some suggest LRMs are like function approximators and rely on shortcuts rather than reflecting actual processes.
*   **Emotion:** The overall emotional tone is neutral, with some positivity.
*   **Top 3 Points of View:**
    *   LRMs perform well until problem complexity increases.
    *   This limitation is expected since LRMs are function approximators.
    *   LRMs rely on shortcuts instead of reflecting actual processes.

**[[D] How to benchmark open-ended, real-world goal achievement by computer-using LLMs? (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1okwuyx/d_how_to_benchmark_openended_realworld_goal/)**
*   **Summary:** A discussion on how to effectively benchmark Large Language Models (LLMs) based on their ability to achieve goals in real-world scenarios, noting the current benchmarks mainly focus on text-based tasks.
*   **Emotion:** Positive.
*   **Top 3 Points of View:**
    *   Benchmarking real-world goal achievement is a key element missing in current LLM evaluations.
    *   Current benchmarks focus too much on text tasks.

**[[R] A New Species of Artificial Intelligence: KMS-Stabilized Reasoning with Harmonic Algebra (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ol14tv/r_a_new_species_of_artificial_intelligence/)**
*   **Summary:** Discussion around "KMS-Stabilized Reasoning with Harmonic Algebra," a novel approach to artificial intelligence.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   The comment "slop" expresses a negative view of the research.
