---
title: "LocalLLaMA Subreddit"
date: "2025-10-23"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "AI", "Benchmarking"]
---

# Overall Ranking and Top Discussions
1.  [Can Qwen3-VL count my push-ups? (Ronnie Coleman voice)](https://v.redd.it/pfn5nm7ypwwf1) (Score: 9)
    * This thread is about using Qwen3-VL to count push-ups, with a link to a GitHub repository.
2.  [What’s the best and most reliable LLM benchmarking site or arena right now?](https://www.reddit.com/r/LocalLLaMA/comments/1oeaucp/whats_the_best_and_most_reliable_llm_benchmarking/) (Score: 6)
    * This thread discusses the best and most reliable LLM benchmarking sites, with users sharing their opinions and experiences.
3.  [What’s the smartest NON thinking model under 40B or so?](https://www.reddit.com/r/LocalLLaMA/comments/1oec0xm/whats_the_smartest_non_thinking_model_under_40b/) (Score: 4)
    *  This thread is asking which is the smartest "NON thinking" model under 40B.
4.  [llama2 may not be as smart as newer LLMs, but it does have personality LOL](https://i.redd.it/1uk9ze6f9wwf1.png) (Score: 4)
    * The thread discusses the personality of Llama2 and the potential for distilling and blending it into newer models.
5.  [LightOn Launches LightOnOCR An OCR Model From 1b Up To 0.9](https://www.reddit.com/gallery/1oe98c8) (Score: 3)
    *  The thread is about LightOn launching LightOnOCR.
6.  [GPT-OSS 20B reasoning low vs medium vs high](https://www.reddit.com/r/LocalLLaMA/comments/1oebgam/gptoss_20b_reasoning_low_vs_medium_vs_high/) (Score: 2)
    *  The thread is discussing the performance of GPT-OSS 20B with different reasoning settings.
7.  [R9700 + 7900XTX If you have these cards, let's share our observations](https://www.reddit.com/r/LocalLLaMA/comments/1oe9itw/r9700_7900xtx_if_you_have_these_cards_lets_share/) (Score: 2)
    * The thread encourages users with R9700 and 7900XTX cards to share their experiences and observations.
8.  [AMD Benchmarks (no, there is none) for Ryzen 395 Hybrid (NPU+GPU) mode](https://www.reddit.com/r/LocalLLaMA/comments/1oe9kzj/amd_benchmarks_no_there_is_none_for_ryzen_395/) (Score: 2)
    * The thread discusses the lack of available benchmarks for AMD Ryzen 395 in Hybrid mode.
9.  [Might the DeepSeek-OCR paper be a key innovation for smarter models?](https://www.reddit.com/r/LocalLLaMA/comments/1oea48t/might_the_deepseekocr_paper_be_a_key_innovation/) (Score: 2)
    * This thread asks if the DeepSeek-OCR paper might be a key innovation for smarter models.
10. [LLM File Organization](https://www.reddit.com/r/LocalLLaMA/comments/1oecdlr/llm_file_organization/) (Score: 1)
    * The thread is about LLM file organization.
11. [Nearly all software for AI is ***! Worse than all other open source software](https://www.reddit.com/r/LocalLLaMA/comments/1oeb6co/nearly_all_software_for_ai_is_ass_worse_than_all/) (Score: 0)
    * The thread is a complaint about the quality of AI software.
12. [Shifting from web development to AI Agent/Workflow Engineering , viable career?](https://www.reddit.com/r/LocalLLaMA/comments/1oebkd6/shifting_from_web_development_to_ai_agentworkflow/) (Score: 0)
    * This thread discusses the viability of shifting from web development to AI Agent/Workflow Engineering.
13. [How much would a GPU boost gpt-oss-120b on a server CPU with 128 GB of RAM at 3-5 tps?](https://www.reddit.com/r/LocalLLaMA/comments/1oebtkr/how_much_would_a_gpu_boost_gptoss120b_on_a_server/) (Score: 0)
    * This thread asks how much a GPU would boost gpt-oss-120b on a server CPU with 128 GB of RAM at 3-5 tps.
14. [Looking to get a Strix Halo for local AI? 100% avoid random no-name brands like Bee-link!](https://i.imgur.com/wAyFNQZ.png) (Score: 0)
    * This thread advises against buying from random, no-name brands like Bee-link when looking for a Strix Halo for local AI.

# Detailed Analysis by Thread
**[Can Qwen3-VL count my push-ups? (Ronnie Coleman voice) (Score: 9)](https://v.redd.it/pfn5nm7ypwwf1)**
*  **Summary:** This thread is a question asking if Qwen3-VL can count push-ups, accompanied by a link to the GitHub repository.
*  **Emotion:** The emotional tone of the thread is neutral.
*  **Top 3 Points of View:**
    *  The thread links to a github repository: [https://github.com/gabber-dev/gabber](https://github.com/gabber-dev/gabber)

**[What’s the best and most reliable LLM benchmarking site or arena right now? (Score: 6)](https://www.reddit.com/r/LocalLLaMA/comments/1oeaucp/whats_the_best_and_most_reliable_llm_benchmarking/)**
*   **Summary:** The thread is a discussion about the best and most reliable LLM benchmarking sites or arenas.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 3 Points of View:**
    *   There is no good and fair benchmarking website.
    *   The current state of LLM benchmarking is a cluster.
    *   The most reliable approach is to create your own eval suite based on real examples from your domain.

**[What’s the smartest NON thinking model under 40B or so? (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1oec0xm/whats_the_smartest_non_thinking_model_under_40b/)**
*   **Summary:** The thread seeks recommendations for the smartest "NON thinking" LLM under 40B.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   If you like Seed 36B, you can set its thinking budget to 0 to disable thinking.
    *   The author is curious if explicit non-thinking models may do better than a model that simply has it's thinking budget turned off.
    *   User has no complaints about the Gemm3 series.

**[llama2 may not be as smart as newer LLMs, but it does have personality LOL (Score: 4)](https://i.redd.it/1uk9ze6f9wwf1.png)**
*   **Summary:** The thread discusses the personality of Llama2 and the potential for distilling and blending it into newer models.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   An earlier model's flavor or personality could be distilled and blended into training data for a more recent model.
    *   I wouldn't be surprised if llama 2 and similarly aged LLMs have better knowledge due to it being pre-trained with less synthetic data
    *   The personality is honestly just AI slop that can still be spotted on the newest models. They're just much better at hiding it.

**[LightOn Launches LightOnOCR An OCR Model From 1b Up To 0.9 (Score: 3)](https://www.reddit.com/gallery/1oe98c8)**
*   **Summary:** The thread is about LightOn launching LightOnOCR, an OCR model.
*   **Emotion:** The emotional tone is positive.
*   **Top 3 Points of View:**
    *   Gemini is useful for text and tables, but not for plots or graphics embedded in PDFs.

**[GPT-OSS 20B reasoning low vs medium vs high (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1oebgam/gptoss_20b_reasoning_low_vs_medium_vs_high/)**
*   **Summary:** The thread is discussing the performance of GPT-OSS 20B with different reasoning settings.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   20B is only useful when it's set to high reasoning.
    *   20B high is better than almost any 70B q4ish model.
    *   Run the AIME 2025 against it and you'll see low fail a significantly larger amount of them than high.

**[R9700 + 7900XTX If you have these cards, let's share our observations (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1oe9itw/r9700_7900xtx_if_you_have_these_cards_lets_share/)**
*   **Summary:** The thread encourages users with R9700 and 7900XTX cards to share their experiences and observations.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   A user is considering getting some AMD cards.

**[AMD Benchmarks (no, there is none) for Ryzen 395 Hybrid (NPU+GPU) mode (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1oe9kzj/amd_benchmarks_no_there_is_none_for_ryzen_395/)**
*   **Summary:** The thread discusses the lack of available benchmarks for AMD Ryzen 395 in Hybrid mode.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   AMD GAIA has benchmarks about it.
    *   There is a graph with token per sec.
    *   A user is asking what is the backend on this.

**[Might the DeepSeek-OCR paper be a key innovation for smarter models? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1oea48t/might_the_deepseekocr_paper_be_a_key_innovation/)**
*   **Summary:** This thread asks if the DeepSeek-OCR paper might be a key innovation for smarter models.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   Image-only input should benefit text models as they could easily learn with them how to predict entire blocks of text instead of just the next token. A full conversation or document can be a sequence of images (which incidentally is also what a video is, at a basic level).

**[LLM File Organization (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1oecdlr/llm_file_organization/)**
*   **Summary:** The thread is about LLM file organization.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   Maybe have an LLM create a short description of each file, make a spreadsheet of them all, then have it organize the items in the sheet into folders and subfolders?

**[Nearly all software for AI is ***! Worse than all other open source software (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1oeb6co/nearly_all_software_for_ai_is_ass_worse_than_all/)**
*   **Summary:** The thread is a complaint about the quality of AI software.
*   **Emotion:** The emotional tone is negative.
*   **Top 3 Points of View:**
    *   Most users are frustrated by the never ending errors, incompatibilities and Python dependency.
    *   Local AI is still in its infancy.
    *   Install LM Studio - it was made for nontechnical users.

**[Shifting from web development to AI Agent/Workflow Engineering , viable career? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1oebkd6/shifting_from_web_development_to_ai_agentworkflow/)**
*   **Summary:** This thread discusses the viability of shifting from web development to AI Agent/Workflow Engineering.
*   **Emotion:** The emotional tone is negative.
*   **Top 3 Points of View:**
    *   AI agent/workflow engineer is not a viable career option.
    *   There's a huge market for automated solutions using agents.
    *   As a junior employee, you need at least two years to find your feet in the working world. After the third year, you will gain confidence.

**[How much would a GPU boost gpt-oss-120b on a server CPU with 128 GB of RAM at 3-5 tps? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1oebtkr/how_much_would_a_gpu_boost_gptoss120b_on_a_server/)**
*   **Summary:** This thread asks how much a GPU would boost gpt-oss-120b on a server CPU with 128 GB of RAM at 3-5 tps.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   You need more VRAM. Just the 120b model needs 60GB. Only the 4060 ti has 16GB and you need more.
    *   An RTX 6000 Pro 96GB will load the entire model and context with VRAM to spare and give you ~ 170 tokens/sec!
    *   A user with a similar setup gets up to 16 tps with this setup, it drops to about 10-11 tps when the context fills up.

**[Looking to get a Strix Halo for local AI? 100% avoid random no-name brands like Bee-link! (Score: 0)](https://i.imgur.com/wAyFNQZ.png)**
*   **Summary:** This thread advises against buying from random, no-name brands like Bee-link when looking for a Strix Halo for local AI.
*   **Emotion:** The emotional tone is positive.
*   **Top 3 Points of View:**
    *   Demand a refund and go buy something in stock.
    *   Beelink is a good brand, but you should have bought it from Micro Center or something.
    *   OEM's that are sub licensing a design from an ODM need to get enough orders to bulk buy the parts and then they have internal build time to make them a sellable product.
