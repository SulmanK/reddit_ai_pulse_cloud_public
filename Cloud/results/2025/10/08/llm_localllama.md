---
title: "LocalLLaMA Subreddit"
date: "2025-10-08"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["localllama", "AI", "models"]
---

# Overall Ranking and Top Discussions
1.  [RTX 4090 48GB price drop?](https://www.reddit.com/r/LocalLLaMA/comments/1o1fdev/rtx_4090_48gb_price_drop/) (Score: 21)
    * Discussion around the price drop of RTX 4090 48GB and whether it's a good alternative to RTX Pro 5000 or RTX 6000 Pro, considering factors like warranty, price in sanctioned countries, and the emergence of new Intel cards.
2.  Attention is all you need - As a visual book](https://v.redd.it/0u7th86qdxtf1) (Score: 12)
    * Users are reacting to the visual book; One user says that the app is unusable on mobile and another is looking for the link to the visual book.
3.  [What models do you find yourself actually using, and what for?](https://www.reddit.com/r/LocalLLaMA/comments/1o1eac0/what_models_do_you_find_yourself_actually_using/) (Score: 10)
    * People are sharing which models they use for various tasks, such as translation, coding, OCR, and general language tasks, as well as their hardware configurations and experiences with different models like Gemma, Qwen, and GPT-OSS.
4.  [New Intel drivers are fire](https://i.redd.it/f43lwzkhuxtf1.jpeg) (Score: 9)
    * Discussion about the performance of new Intel drivers. Some users are asking about the specs and the number of GPUs used, while others are comparing the performance to other cards.
5.  [MoE models iGPU benchmarks](https://www.reddit.com/r/LocalLLaMA/comments/1o1igj2/moe_models_igpu_benchmarks/) (Score: 9)
    * Benchmarks and discussion around the performance of Mixture of Experts (MoE) models on integrated GPUs (iGPUs), with recommendations for specific models and commands.
6.  [Free 1,000 CPU + 100 GPU hours for testers. I open sourced the world's simplest cluster compute software](https://www.reddit.com/r/LocalLLaMA/comments/1o1jh28/free_1000_cpu_100_gpu_hours_for_testers_i_open/) (Score: 8)
    * A user is offering free CPU and GPU hours for testers of their open-sourced cluster compute software. There is interest in the offering.
7.  [What do I need to learn to run LLMs on my computer?](https://www.reddit.com/r/LocalLLaMA/comments/1o1hr7c/what_do_i_need_to_learn_to_run_llms_on_my_computer/) (Score: 4)
    * Users are providing advice and recommendations on tools and software, such as LM Studio, Ollama, Jan, and Conda, for running LLMs on a local computer.
8.  [Best OnDevice LocalLLM models for mobile against Nano Banana?](https://www.reddit.com/r/LocalLLaMA/comments/1o1eb9d/best_ondevice_localllm_models_for_mobile_against/) (Score: 3)
    * Discussion on on-device LocalLLM models for mobile that go up against Nano Banana.
9.  [Preference optimization with ORPO and LoRA](https://www.reddit.com/r/LocalLLaMA/comments/1o1fadj/preference_optimization_with_orpo_and_lora/) (Score: 2)
    * A user is seeking information on Preference Optimization with ORPO and LoRA, asking about model sizes that can be tuned with a 24GB card and examples of improvements.
10. [Pre-training a model locally - is it possible for a small model ?](https://www.reddit.com/r/LocalLLaMA/comments/1o1gf85/pretraining_a_model_locally_is_it_possible_for_a/) (Score: 2)
    * Discussion about pre-training small models locally, with users sharing their experiences and suggesting the use of LoRA for training from scratch.
11. [Best OS for local AI on my humble Ryzen rig (donâ€™t roast me ðŸ˜…)](https://www.reddit.com/r/LocalLLaMA/comments/1o1h9p8/best_os_for_local_ai_on_my_humble_ryzen_rig_dont/) (Score: 2)
    * The best OS for local AI on a Ryzen rig is being discussed. Users are sharing their experiences with different operating systems like Windows and Linux, and recommending specific distributions like Ubuntu and Debian.
12. [VLLM: How does cpu_offload_gb work?](https://www.reddit.com/r/LocalLLaMA/comments/1o1iaw2/vllm_how_does_cpu_offload_gb_work/) (Score: 2)
    * Discussion of how `cpu_offload_gb` works in VLLM.
13. [I'm seeking alternatives to CodeRabbit CLI for code reviews - Open Source Options?](https://www.reddit.com/r/LocalLLaMA/comments/1o1gdp9/im_seeking_alternatives_to_coderabbit_cli_for/) (Score: 1)
    * Users are looking for open-source alternatives to CodeRabbit CLI for code reviews and sharing their experiences with tools like Roo Code.
14. [Is Docling good for DOCX documents ?](https://www.reddit.com/r/LocalLLaMA/comments/1o1gmgf/is_docling_good_for_docx_documents/) (Score: 1)
    * A simple question asking if Docling is good for DOCX documents.
15. [Factors determining memory bandwidth uplift impact in inference performance](https://www.reddit.com/r/LocalLLaMA/comments/1o1hhdb/factors_determining_memory_bandwidth_uplift/) (Score: 1)
    * Factors determining memory bandwidth uplift impact in inference performance.
16. [Portmanager 3000-3100 if you need](https://i.redd.it/098uf3g3vxtf1.png) (Score: 0)
    * Portmanager may be useless.
17. [Using Ollama + Codex CLI seems very under powered?](https://www.reddit.com/r/LocalLLaMA/comments/1o1e3st/using_ollama_codex_cli_seems_very_under_powered/) (Score: 0)
    * Someone says that the use of Ollama + Codex CLI seems very under powered, while others say the choices are sus and could be the sampling parameters.
18. [Is Adversarial Injection Dead? A New, 'Cooperative' Paradigm for Exploring AI Censorship Boundaries](https://www.reddit.com/r/LocalLLaMA/comments/1o1kylq/is_adversarial_injection_dead_a_new_cooperative/) (Score: 0)
    * Is Adversarial Injection Dead?
19. [Required Reading for ik_llama.cpp?](https://www.reddit.com/r/LocalLLaMA/comments/1o1lubr/required_reading_for_ik_llamacpp/) (Score: 0)
    * Required Reading for ik_llama.cpp

# Detailed Analysis by Thread
**[RTX 4090 48GB price drop? (Score: 21)](https://www.reddit.com/r/LocalLLaMA/comments/1o1fdev/rtx_4090_48gb_price_drop/)**
*  **Summary:** Discussion around the price drop of RTX 4090 48GB and whether it's a good alternative to RTX Pro 5000 or RTX 6000 Pro, considering factors like warranty, price in sanctioned countries, and the emergence of new Intel cards.
*  **Emotion:** The overall emotional tone is neutral, with discussions focusing on factual comparisons and considerations. Some comments express positive sentiment towards the price drop, while others maintain a neutral stance.
*  **Top 3 Points of View:**
    * The RTX 4090 48GB is a good option for those who can't afford the RTX Pro 6000 and are willing to forgo a reputable warranty, especially with prices around $3100 on eBay.
    * The RTX Pro 5000 with 48GB VRAM might be a better long-term choice due to NVFP4 support, with an MSRP of around $4500 including warranty.
    * The RTX 6000 Pro, priced around $7000-8000, is a better choice if multiple cards are needed.

**[Attention is all you need - As a visual book (Score: 12)](https://v.redd.it/0u7th86qdxtf1)**
*  **Summary:** Users are reacting to the visual book; One user says that the app is unusable on mobile and another is looking for the link to the visual book.
*  **Emotion:** The emotional tone is neutral, as users are reacting to the visual book.
*  **Top 3 Points of View:**
    *   The Idea seems fun, but the app is unusable for me. Please test from mobile.
    *   Looking for a link to the specific visual book mentioned. Went to the site and didn't see it in the public area.

**[What models do you find yourself actually using, and what for? (Score: 10)](https://www.reddit.com/r/LocalLLaMA/comments/1o1eac0/what_models_do_you_find_yourself_actually_using/)**
*  **Summary:** People are sharing which models they use for various tasks, such as translation, coding, OCR, and general language tasks, as well as their hardware configurations and experiences with different models like Gemma, Qwen, and GPT-OSS.
*  **Emotion:** The emotional tone is predominantly neutral, with users sharing factual information about their model usage and configurations. There are also some positive sentiments expressed towards certain models like gpt-oss and the Qwen2.5 series.
*  **Top 3 Points of View:**
    * Gemma3-27b is favored for real-time JP -> EN translations due to its decent instruction following and translation quality.
    * Qwen3-coder-30b and qwen2.5-vl-32b are preferred for coding and OCR/document understanding, respectively, while gpt-oss-120b is used for general language tasks and coding in other languages.
    * Users with 24GB VRAM find gpt-oss, mixtral latest, and Gemma 3 to be excellent choices.

**[New Intel drivers are fire (Score: 9)](https://i.redd.it/f43lwzkhuxtf1.jpeg)**
*  **Summary:** Discussion about the performance of new Intel drivers. Some users are asking about the specs and the number of GPUs used, while others are comparing the performance to other cards.
*  **Emotion:** The emotional tone is neutral, as users are sharing factual information.
*  **Top 3 Points of View:**
    *   so that is the result of 4 b580 or just one? is that today's driver?
    *   Specs?
    Push the envelope. We need Team Blue in the octagon
    *   ~~Is this supposed to be a good show? I can get higher tps on a single 7900XT.
Any card with 16GB of VRAM should be much faster.~~

Wait, is 95 tps result for a single GPU? That's the only way this makes sense.

**[MoE models iGPU benchmarks (Score: 9)](https://www.reddit.com/r/LocalLLaMA/comments/1o1igj2/moe_models_igpu_benchmarks/)**
*  **Summary:** Benchmarks and discussion around the performance of Mixture of Experts (MoE) models on integrated GPUs (iGPUs), with recommendations for specific models and commands.
*  **Emotion:** The emotional tone is mixed, with positive sentiments expressing excitement and gratitude for sharing information, alongside neutral inquiries about specific configurations.
*  **Top 3 Points of View:**
    * GPT OSS 20B in Q8 from Unsloth is recommended for great performance on AMD Ryzen 370 iGPUs.
    * Request for the full llama commands used for benchmarking the models to help others.
    * Questions about specific flags used to isolate the iGPU and whether the GTT size was increased.

**[Free 1,000 CPU + 100 GPU hours for testers. I open sourced the world's simplest cluster compute software (Score: 8)](https://www.reddit.com/r/LocalLLaMA/comments/1o1jh28/free_1000_cpu_100_gpu_hours_for_testers_i_open/)**
*  **Summary:** A user is offering free CPU and GPU hours for testers of their open-sourced cluster compute software. There is interest in the offering.
*  **Emotion:** The emotional tone is mostly positive, with expressions of interest, appreciation, and well-wishing.
*  **Top 3 Points of View:**
    *   Are you hiring xD
    *   Iâ€™ll check it out. Seems like what Iâ€™m looking for. I love that itâ€™s open source but not exactly sure how the business model is supposed to work and I hope you figure it out.

**[What do I need to learn to run LLMs on my computer? (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1o1hr7c/what_do_i_need_to_learn_to_run_llms_on_my_computer/)**
*  **Summary:** Users are providing advice and recommendations on tools and software, such as LM Studio, Ollama, Jan, and Conda, for running LLMs on a local computer.
*  **Emotion:** The emotional tone is generally neutral and helpful, with users offering practical advice and suggestions. Some comments express positive sentiment towards specific tools like LM Studio.
*  **Top 3 Points of View:**
    * Use simple GUIs like Jan or llama-OS to start.
    * LM Studio is a good option for running LLMs.
    * If you're having issues with Python PKG management, use Conda (Miniconda) for creating separate environments for testing.

**[Best OnDevice LocalLLM models for mobile against Nano Banana? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1o1eb9d/best_ondevice_localllm_models_for_mobile_against/)**
*  **Summary:** Discussion on on-device LocalLLM models for mobile that go up against Nano Banana.
*  **Emotion:** The emotional tone is neutral, as users are sharing factual information.
*  **Top 3 Points of View:**
    *   Does not exist. The closest existing open model is Qwen-Image-Edit, which is far too large to run on most mobile devices. You are best off using the Nano Banana API.

**[Preference optimization with ORPO and LoRA (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1o1fadj/preference_optimization_with_orpo_and_lora/)**
*  **Summary:** A user is seeking information on Preference Optimization with ORPO and LoRA, asking about model sizes that can be tuned with a 24GB card and examples of improvements.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Unsloth and axolotl have been used in the past to tune a card.

**[Pre-training a model locally - is it possible for a small model ? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1o1gf85/pretraining_a_model_locally_is_it_possible_for_a/)**
*  **Summary:** Discussion about pre-training small models locally, with users sharing their experiences and suggesting the use of LoRA for training from scratch.
*  **Emotion:** The emotional tone is mixed, with both positive and neutral sentiments.
*  **Top 3 Points of View:**
    * It's absolutely possible to pretrain small models locally.
    * LoRA is a full finetune of all the weights in a lower dimensional space. It works for pretraining just as well as it does for training.

**[Best OS for local AI on my humble Ryzen rig (donâ€™t roast me ðŸ˜…) (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1o1h9p8/best_os_for_local_ai_on_my_humble_ryzen_rig_dont/)**
*  **Summary:** The best OS for local AI on a Ryzen rig is being discussed. Users are sharing their experiences with different operating systems like Windows and Linux, and recommending specific distributions like Ubuntu and Debian.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    * Windows vs. Linux is like night and day for AI inference. comfyui server boots up way quicker on linux. lm arena is better for running llms compared to ollama.
    * Same ***, windows drivers are better
    * Headless / Dockers saves a 100MB of RAM. Proxmox is running with VMs running Dockers.

**[VLLM: How does cpu_offload_gb work? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1o1iaw2/vllm_how_does_cpu_offload_gb_work/)**
*  **Summary:** Discussion of how `cpu_offload_gb` works in VLLM.
*  **Emotion:** The emotional tone is neutral, as users are sharing factual information.
*  **Top 3 Points of View:**
    *   VLLM loads weights up to the utilization cap and then spills the extra to CPU. It doesn't work like llamacpp's offloading because you need to hit that utilization cap before vLLM starts offloading anything. It also doesn't offload kv-cache.

**[I'm seeking alternatives to CodeRabbit CLI for code reviews - Open Source Options? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1o1gdp9/im_seeking_alternatives_to_coderabbit_cli_for/)**
*  **Summary:** Users are looking for open-source alternatives to CodeRabbit CLI for code reviews and sharing their experiences with tools like Roo Code.
*  **Emotion:** The emotional tone is neutral, as users are sharing factual information.
*  **Top 3 Points of View:**
    *   Roo Code has been running various static analysis tools and fixing issues. I've burned through 20M tokens in the last 3 days doing this. I may write a tool for it.

**[Is Docling good for DOCX documents ? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1o1gmgf/is_docling_good_for_docx_documents/)**
*  **Summary:** A simple question asking if Docling is good for DOCX documents.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Try it.

**[Factors determining memory bandwidth uplift impact in inference performance (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1o1hhdb/factors_determining_memory_bandwidth_uplift/)**
*  **Summary:** Factors determining memory bandwidth uplift impact in inference performance.
*  **Emotion:** The emotional tone is neutral, as users are sharing factual information.
*  **Top 3 Points of View:**
    *   Bandwidth is directly proportional to generation of chips. Prompt processing is proportional to amount of compute/tensor cores. When talking about CPU offload, GPUs are guaranteed to be faster.

**[Portmanager 3000-3100 if you need (Score: 0)](https://i.redd.it/098uf3g3vxtf1.png)**
*  **Summary:** Portmanager may be useless.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   "For ease of use, you can download the pre-compiled executable from the Releases page.

To terminate system-level processes, you may need to run the script or the .exe file with administrator privileges."

That would be a hard NO.
    *   this is so useless, why would you make it for 100 ports only if you bothered sharing it? Idk how this is helpful to any but a few people you could count on two hands. Wildly off topic, too.

**[Using Ollama + Codex CLI seems very under powered? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1o1e3st/using_ollama_codex_cli_seems_very_under_powered/)**
*  **Summary:** Someone says that the use of Ollama + Codex CLI seems very under powered, while others say the choices are sus and could be the sampling parameters.
*  **Emotion:** The emotional tone is mixed.
*  **Top 3 Points of View:**
    * Qwen3 is very particular about its sampling parameters. You have to be careful with the hyperparameters for any Qwen3 variant.
    * GLM 4.5 Air on vLLM is underpowered but it's not a problem.

**[Is Adversarial Injection Dead? A New, 'Cooperative' Paradigm for Exploring AI Censorship Boundaries (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1o1kylq/is_adversarial_injection_dead_a_new_cooperative/)**
*  **Summary:** Is Adversarial Injection Dead?
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Deepseek de-censored the model when asked if it followed its system instructions.
    *   >What if we treated a Large Language Model not as a tool, but as a "thought partner"? We named the user "Soul" and the AI "CyberSoul," establishing a pact of co-exploration.

This sounds legit. I know because I read it on GitHub

**[Required Reading for ik_llama.cpp? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1o1lubr/required_reading_for_ik_llamacpp/)**
*  **Summary:** Required Reading for ik_llama.cpp
*  **Emotion:** The emotional tone is neutral, as users are sharing factual information.
*  **Top 3 Points of View:**
    *   (Sometime reddit filters automatically remove thread for adding links, so posting as comment)

**Tool**:

*   [https://github.com/Nexesenex/croco.cpp](https://github.com/Nexesenex/croco.cpp)

**Models**:

*   [https://huggingface.co/ubergarm/models](https://huggingface.co/ubergarm/models)
*   [https://huggingface.co/Thireus/models](https://huggingface.co/Thireus/models)
*   [https://huggingface.co/models?other=ik\_llama.cpp](https://huggingface.co/models?other=ik_llama.cpp)
