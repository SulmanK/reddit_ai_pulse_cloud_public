---
title: "Machine Learning Subreddit"
date: "2025-10-02"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "LLM"]
---

# Overall Ranking and Top Discussions
1.  [[D] Open source projects to contribute to as an ML research scientist](https://www.reddit.com/r/MachineLearning/comments/1nvvdvl/d_open_source_projects_to_contribute_to_as_an_ml/) (Score: 81)
    *   The discussion revolves around open-source machine-learning projects where people can contribute. Suggestions include projects related to video generation, SQL integration with AI, and Julia programming language.

2.  [[D] I’m looking for papers, preprints, datasets, or reports where an LLM is trained to only know what humans knew before a major scientific breakthrough, and is then asked to propose a new theoretical frameworkwithout using post-breakthrough knowledge and without requiring experimental validation.](https://www.reddit.com/r/MachineLearning/comments/1nvxswc/d_im_looking_for_papers_preprints_datasets_or/) (Score: 36)
    *   This thread discusses the idea of training LLMs on pre-breakthrough knowledge and asking them to propose new frameworks. People are sharing relevant papers and discussing the challenges and motivations behind such an experiment.

3.  [[D] The job market is weird](https://www.reddit.com/r/MachineLearning/comments/1nvwkdt/d_the_job_market_is_weird/) (Score: 32)
    *   The conversation touches on the current state of the ML job market, especially for junior and mid-level positions. People discuss the increasing expectations for developers and the impact of LLMs on workload distribution.

4.  [[D] How much should researchers (especially in ML domain) rely on LLMs for their work?](https://www.reddit.com/r/MachineLearning/comments/1nwaunk/d_how_much_should_researchers_especially_in_ml/) (Score: 18)
    *   This discussion centers on the ethical and practical considerations of researchers using LLMs in their work. It covers various use cases like summarization, code generation, and brainstorming, and highlights the importance of maintaining oversight and critical thinking.

5.  [[D] AAAI 26 Social Impact Track](https://www.reddit.com/r/MachineLearning/comments/1nvoh20/d_aaai_26_social_impact_track/) (Score: 10)
    *   This thread involves people sharing and discussing the reviews they received for their submissions to the AAAI 26 Social Impact Track. They are also trying to determine the scoring range used in the review process.

6.  [[R] Maths PhD student - Had an idea on diffusion](https://www.reddit.com/r/MachineLearning/comments/1nw6jqf/r_maths_phd_student_had_an_idea_on_diffusion/) (Score: 9)
    *   A math PhD student shares an idea about using diffusion models. The discussion revolves around whether diffusion models are suitable for the use case, alternative approaches, and relevant research papers.

7.  [[D] Self-Promotion Thread](https://www.reddit.com/r/MachineLearning/comments/1nvrmw5/d_selfpromotion_thread/) (Score: 4)
    *   This is a thread for self-promotion. Users share their projects related to PyTorch, Hugging Face, and Jax.

8.  [[D] Will fine-tuning LLaMA 3.2 11B Instruct on text-only data degrade its vision capabilities?](https://www.reddit.com/r/MachineLearning/comments/1nw8ql3/d_will_finetuning_llama_32_11b_instruct_on/) (Score: 2)
    *   The discussion revolves around the impact of fine-tuning LLaMA 3.2 11B on text-only data on its vision capabilities. Users discuss the potential for the model to "forget" pretraining data and ways to mitigate this.

# Detailed Analysis by Thread
**[[D] Open source projects to contribute to as an ML research scientist (Score: 81)](https://www.reddit.com/r/MachineLearning/comments/1nvvdvl/d_open_source_projects_to_contribute_to_as_an_ml/)**
*   **Summary:** The discussion is about suggesting open source projects for ML research scientists to contribute to.
*   **Emotion:** The overall emotional tone is neutral, with some positive sentiments expressing excitement about specific projects.
*   **Top 3 Points of View:**
    *   Contributing to existing projects like Matplotlib can be beneficial.
    *   SQLv2, which merges SQL with AI, is a new standard where contribution is encouraged.
    *   Checking issue trackers and Discord communities of projects you already use is a good way to find opportunities.

**[[D] I’m looking for papers, preprints, datasets, or reports where an LLM is trained to only know what humans knew before a major scientific breakthrough, and is then asked to propose a new theoretical frameworkwithout using post-breakthrough knowledge and without requiring experimental validation. (Score: 36)](https://www.reddit.com/r/MachineLearning/comments/1nvxswc/d_im_looking_for_papers_preprints_datasets_or/)**
*   **Summary:** The thread is a request for papers, datasets, and reports about training LLMs with pre-breakthrough knowledge. It got pretty impressive results, but it used graph-native ML models.
*   **Emotion:** The emotional tone is generally positive, reflecting interest and enthusiasm in the research area.
*   **Top 3 Points of View:**
    *   Using LLMs trained on pre-2020 datasets might be a good starting point.
    *   It is unlikely anyone has trained LLMs based on specifical chronological periods.
    *   There are adjacent research direction of benchmarking the forecasting abilities of LLMs.

**[[D] The job market is weird (Score: 32)](https://www.reddit.com/r/MachineLearning/comments/1nvwkdt/d_the_job_market_is_weird/)**
*   **Summary:** The conversation centers around the current state of the ML job market, with an emphasis on the challenges faced by junior and mid-level applicants.
*   **Emotion:** The overall tone is negative, reflecting frustration and concern about the job market.
*   **Top 3 Points of View:**
    *   The job market is tough, with few junior/new grad positions available.
    *   Companies are increasingly expecting developers to "own" projects and handle deployment and scaling, even at mid-level positions.
    *   Big tech is reducing/freezing hiring and Finance seems to be doing well.

**[[D] How much should researchers (especially in ML domain) rely on LLMs for their work? (Score: 18)](https://www.reddit.com/r/MachineLearning/comments/1nwaunk/d_how_much_should_researchers_especially_in_ml/)**
*   **Summary:** The discussion is about how much ML researchers should rely on LLMs for their work, and the ethical considerations involved.
*   **Emotion:** The emotional tone is mostly neutral, with a mix of positive and negative sentiments depending on the specific application of LLMs.
*   **Top 3 Points of View:**
    *   LLMs are useful for summarizing papers, searching for ideas, and generating boilerplate code.
    *   LLMs shouldn't be used for planning or generating novel research ideas, as it might lead to unoriginal work.
    *   Using LLMs for writing assistance is acceptable if the researcher is open about it and the writing is secondary to the core research.

**[[D] AAAI 26 Social Impact Track (Score: 10)](https://www.reddit.com/r/MachineLearning/comments/1nvoh20/d_aaai_26_social_impact_track/)**
*   **Summary:**  People sharing their experiences and review scores from the AAAI 26 Social Impact Track submissions.
*   **Emotion:**  The overall tone is neutral, with a hint of positive sentiment from those who feel they can address the reviewers' concerns.
*   **Top 3 Points of View:**
    *   Reviewers made fair points.
    *   Need more experiments.
    *   Uncertain about the score range.

**[[R] Maths PhD student - Had an idea on diffusion (Score: 9)](https://www.reddit.com/r/MachineLearning/comments/1nw6jqf/r_maths_phd_student_had_an_idea_on_diffusion/)**
*   **Summary:** A math PhD student shared an idea about using diffusion models, and others responded with feedback, alternative approaches, and relevant research papers.
*   **Emotion:** The thread maintains a neutral tone, with a slight positive leaning due to helpful suggestions and shared resources.
*   **Top 3 Points of View:**
    *   Diffusion models may not be necessary for the specific use case, and simpler approaches might suffice.
    *   More context is needed to determine the best architecture.
    *   Training a diffusion model could be done as constructing a dataset of pairs of vectors and then training a model on it.

**[[D] Self-Promotion Thread (Score: 4)](https://www.reddit.com/r/MachineLearning/comments/1nvrmw5/d_selfpromotion_thread/)**
*   **Summary:** Users share their projects, tools, and resources related to machine learning.
*   **Emotion:** Neutral
*   **Top 3 Points of View:**
    *   Tensorlink is a peer-to-peer platform for running PyTorch models.
    *   Jax can be learned in an interactive way using the self-paced tool.

**[[D] Will fine-tuning LLaMA 3.2 11B Instruct on text-only data degrade its vision capabilities? (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1nw8ql3/d_will_finetuning_llama_32_11b_instruct_on/)**
*   **Summary:** The discussion centers on the potential impact of fine-tuning LLaMA 3.2 11B on text-only data on its vision capabilities.
*   **Emotion:** The tone is neutral.
*   **Top 3 Points of View:**
    *   Fine-tuning will cause the model to start to forget its pretraining data.
    *   PEFT will reduce the amount of forgetting.
