---
title: "Stable Diffusion Subreddit"
date: "2025-10-18"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [[D] Wan 2.2 i2V Quality Tip (For Noobs)](https://www.reddit.com/r/StableDiffusion/comments/1o9zcyj/wan_22_i2v_quality_tip_for_noobs/) (Score: 18)
    * Users discuss tips for improving image-to-video quality using Wan 2.2.
2.  [[D] Brie's Qwen Edit Lazy Repose workflow](https://www.reddit.com/r/StableDiffusion/comments/1o9zqer/bries_qwen_edit_lazy_repose_workflow/) (Score: 11)
    *  A user shares a workflow and model for Qwen Edit and receives positive feedback, with another user suggesting increasing the steps for better results.
3.  [[D] Training a Qwen Image LORA on a 3080ti in 2 and a half hours on Onetrainer.](https://www.reddit.com/r/StableDiffusion/comments/1oa1wp3/training_a_qwen_image_lora_on_a_3080ti_in_2_and_a/) (Score: 10)
    *  Users discuss the possibility of training Qwen Image LORA on a 3080ti using Onetrainer, sharing experiences with different batch sizes and VRAM usage.
4.  [Best way to iterate through many prompts in comfyui?](https://i.redd.it/beetiwyc7wvf1.png) (Score: 9)
    *  Users discuss various methods for iterating through multiple prompts in ComfyUI, including using the Impact-Pack's String Selector, ComfyRoll's multi-prompting node, and automating via the API.
5.  [Open-source release! Face-to-Photo Transform ordinary face photos into stunning portraits.](https://www.reddit.com/r/StableDiffusion/comments/1o9zxe2/opensource_release_facetophoto_transform_ordinary/) (Score: 8)
    *  Users discuss a new open-source tool for transforming face photos into portraits.
6.  [About that WAN T2V 2.2 and "speed up" LORAs.](https://www.reddit.com/r/StableDiffusion/comments/1o9wyqj/about_that_wan_t2v_22_and_speed_up_loras/) (Score: 6)
    *  Users discuss configurations and performance of WAN T2V 2.2 with "speed up" LORAs.
7.  [[D] Has anyone managed to fully animate a still image (not just use it as reference) with ControlNet in an image-to-video workflow?](https://www.reddit.com/r/StableDiffusion/comments/1oa0y8x/has_anyone_managed_to_fully_animate_a_still_image/) (Score: 2)
    *  Users discuss methods and tools for animating still images using ControlNet in image-to-video workflows.
8.  [[D] GGUF vs fp8](https://www.reddit.com/r/StableDiffusion/comments/1oa1cwk/gguf_vs_fp8/) (Score: 2)
    *  Users compare GGUF and FP8, discussing speed, quality, VRAM usage, and the ability to handle multiple LoRAs.
9.  [What are the telltale signs of the different models?](https://www.reddit.com/r/StableDiffusion/comments/1o9yuv7/what_are_the_telltale_signs_of_the_different/) (Score: 2)
    *  Users discuss the characteristic visual artifacts and features of different Stable Diffusion models.
10. [Does eye direction matter when training LORA?](https://www.reddit.com/r/StableDiffusion/comments/1o9zq29/does_eye_direction_matter_when_training_lora/) (Score: 2)
    *  Users discuss whether eye direction in training images affects the resulting LORA and suggest ways to influence eye direction in generated images.
11. [[D] Mixing Epochs HIGH/LOW?](https://www.reddit.com/r/StableDiffusion/comments/1oa5n6v/mixing_epochs_highlow/) (Score: 1)
    *  A user asks about the possibility of mixing different epochs when training a model.
12. [Anyone knows what app is that?](https://i.redd.it/g8o7nb20ovvf1.jpeg) (Score: 1)
    *  Users identify an app shown in an image as Airtable and discuss its potential use cases.
13. [Wan video always having artifacts/weird lines?](https://www.reddit.com/r/StableDiffusion/comments/1o9ye3a/wan_video_always_having_artifactsweird_lines/) (Score: 1)
    *  Users report and share examples of artifacts and weird lines in videos generated with Wan.
14. [Why is my inpaint not working no matter what I do?](https://www.reddit.com/r/StableDiffusion/comments/1oa0b9m/why_is_my_inpaint_not_working_no_matter_what_i_do/) (Score: 0)
    *  A user asks for help with inpainting issues and receives suggestions regarding models, LoRAs, and denoise strength.
15. [[D] I want to keep up to date](https://www.reddit.com/r/StableDiffusion/comments/1o9ysdf/i_want_to_keep_up_to_date/) (Score: 0)
    * A user asks for advice on how to keep up-to-date on AI.
16. [Is it possible to match the prompt adherence level of chatgpt/gemini/grok with a locally running model?](https://www.reddit.com/r/StableDiffusion/comments/1o9wc2o/is_it_possible_to_match_the_prompt_adherence/) (Score: 0)
    *  Users discuss whether local models can achieve the same level of prompt adherence as large language models like ChatGPT, Gemini, and Grok, and suggest using models like Qwen Image Edit.
17. [[D] what is wrong with this?](https://www.reddit.com/r/StableDiffusion/comments/1oa1jkv/what_is_wrong_with_this/) (Score: 0)
    *  A user asks for help identifying the cause of image generation problems and receives suggestions regarding resolution, finetuning, and IP-Adapter.
18. [Can OpenSource video have a character jump down from higher up, pull out a laser gun from behind her back, fire laser gun on the way down & land on a red motorcycle parked by the curb, then back up realistically and with weighted physics with the motorcycle? Grok  background music and sound effects.](https://v.redd.it/vpsxea9kzwvf1) (Score: 0)
    *  A user asks if open-source video generation can achieve a complex action sequence and receives mixed responses.
19. [[D] Recommended hardware (sorry)](https://www.reddit.com/r/StableDiffusion/comments/1oa3lba/recommended_hardware_sorry/) (Score: 0)
    *  Users discuss recommended hardware configurations for AI video workflows, focusing on GPUs like the RTX 4090 and the upcoming 5090.
20. [Generating 2D pixel art 16x16 spritesheets](https://www.reddit.com/r/StableDiffusion/comments/1oa4zcf/generating_2d_pixel_art_16x16_spritesheets/) (Score: 0)
    * A user asks about generating 2D pixel art 16x16 spritesheets.

# Detailed Analysis by Thread
**[[D] Wan 2.2 i2V Quality Tip (For Noobs) (Score: 18)](https://www.reddit.com/r/StableDiffusion/comments/1o9zcyj/wan_22_i2v_quality_tip_for_noobs/)**
*  **Summary:** Users discuss tips for improving image-to-video quality using Wan 2.2, including adjusting steps in KSampler nodes and using different resizing techniques.
*  **Emotion:** Overall, the emotional tone is neutral, with some positive sentiment expressed for helpful tips and neutral sentiment regarding testing results and observations.
*  **Top 3 Points of View:**
    *   Increasing steps in KSampler nodes can improve quality, but may double generation time.
    *   Using lower resolution sources can sometimes yield better quality results than high-resolution sources.
    *   Destructive image resizing with cubic interpolation or resize nodes with supersampling can help preserve details.

**[[D] Brie's Qwen Edit Lazy Repose workflow (Score: 11)](https://www.reddit.com/r/StableDiffusion/comments/1o9zqer/bries_qwen_edit_lazy_repose_workflow/)**
*  **Summary:** A user shares a workflow and model for Qwen Edit and receives positive feedback, with another user suggesting increasing the steps for better results.
*  **Emotion:** The overall tone is positive, expressing gratitude and satisfaction with the shared workflow.
*  **Top 3 Points of View:**
    *   The shared model and workflow work great.
    *   Increasing the steps can improve results.
    *   Gratitude is expressed for the resource.

**[[D] Training a Qwen Image LORA on a 3080ti in 2 and a half hours on Onetrainer. (Score: 10)](https://www.reddit.com/r/StableDiffusion/comments/1oa1wp3/training_a_qwen_image_lora_on_a_3080ti_in_2_and_a/)**
*  **Summary:** Users discuss the possibility of training Qwen Image LORA on a 3080ti using Onetrainer, sharing experiences with different batch sizes and VRAM usage.
*  **Emotion:** The overall sentiment is neutral to positive, with curiosity, excitement, and inquiry regarding training parameters and possibilities.
*  **Top 3 Points of View:**
    *   It is possible to train Qwen Image LORA on a 3080ti using Onetrainer.
    *   Experimenting with batch sizes can optimize VRAM usage.
    *   There is interest in tutorials and more information on getting started with Onetrainer.

**[Best way to iterate through many prompts in comfyui? (Score: 9)](https://i.redd.it/beetiwyc7wvf1.png)**
*  **Summary:** Users discuss various methods for iterating through multiple prompts in ComfyUI, including using the Impact-Pack's String Selector, ComfyRoll's multi-prompting node, and automating via the API.
*  **Emotion:** The emotional tone is mostly neutral, with a slight positive leaning due to the helpful and informative nature of the responses.
*  **Top 3 Points of View:**
    *   The Impact-Pack's String Selector is a useful tool for selecting prompts from a text block.
    *   Automating via the API allows for more control and integration with other AI models.
    *   ComfyRoll has an excellent multi-prompting node.

**[Open-source release! Face-to-Photo Transform ordinary face photos into stunning portraits. (Score: 8)](https://www.reddit.com/r/StableDiffusion/comments/1o9zxe2/opensource_release_facetophoto_transform_ordinary/)**
*  **Summary:** Users discuss a new open-source tool for transforming face photos into portraits.
*  **Emotion:** The overall sentiment is mixed, with curiosity and some skepticism.
*  **Top 3 Points of View:**
    *   Question of wheather it is outpainting
    *   One user considers the tool's claim to be better than InfiniteYou "pretty bold."
    *   Further exploration and comparison with existing tools are needed.

**[About that WAN T2V 2.2 and "speed up" LORAs. (Score: 6)](https://www.reddit.com/r/StableDiffusion/comments/1o9wyqj/about_that_wan_t2v_22_and_speed_up_loras/)**
*  **Summary:** Users discuss configurations and performance of WAN T2V 2.2 with "speed up" LORAs.
*  **Emotion:** The emotional tone is neutral, with users sharing technical information and settings.
*  **Top 3 Points of View:**
    *   Specific configurations for Wan2.2 T2V A14B - Q4\_K\_S with "speed up" LORAs are shared.
    *   Details on steps, samplers, and VAE settings are provided.
    *   Performance metrics, such as generation time, are mentioned.

**[[D] Has anyone managed to fully animate a still image (not just use it as reference) with ControlNet in an image-to-video workflow? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1oa0y8x/has_anyone_managed_to_fully_animate_a_still_image/)**
*  **Summary:** Users discuss methods and tools for animating still images using ControlNet in image-to-video workflows.
*  **Emotion:** The overall tone is neutral and inquisitive.
*  **Top 3 Points of View:**
    *   Wan Animate may be useful for this purpose.
    *   Anisora3.2 is suggested as a possible solution.

**[[D] GGUF vs fp8 (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1oa1cwk/gguf_vs_fp8/)**
*  **Summary:** Users compare GGUF and FP8, discussing speed, quality, VRAM usage, and the ability to handle multiple LoRAs.
*  **Emotion:** The emotional tone is neutral, focusing on technical comparisons and performance.
*  **Top 3 Points of View:**
    *   FP8 is faster but potentially lower quality than Q8 GGUF.
    *   GGUF is slower but may offer better quality with Q8.
    *   FP8 can handle more LoRAs without losing quality compared to GGUF.

**[What are the telltale signs of the different models? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1o9yuv7/what_are_the_telltale_signs_of_the_different/)**
*  **Summary:** Users discuss the characteristic visual artifacts and features of different Stable Diffusion models.
*   **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   SD 1.5 / SDXL models often have messed-up hands and limbs.
    *   Flux models are known for the "Flux chin" and characters wearing earrings.
    *   Qwen models tend to produce images with plastic-looking skin (without LoRAs).

**[Does eye direction matter when training LORA? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1o9zq29/does_eye_direction_matter_when_training_lora/)**
*  **Summary:** Users discuss whether eye direction in training images affects the resulting LORA and suggest ways to influence eye direction in generated images.
*   **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   Eye direction can be influenced by adding terms like "candid" or "looking away" to the prompt.
    *   A LoRA trained on images with consistent eye direction will tend to reproduce that direction.
    *   Creating a more varied dataset can help to produce a LoRA that generates images with different eye directions.

**[[D] Mixing Epochs HIGH/LOW? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1oa5n6v/mixing_epochs_highlow/)**
*   **Summary:** A user asks about the possibility of mixing different epochs when training a model.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   User asks if different epochs can be mixed

**[Anyone knows what app is that? (Score: 1)](https://i.redd.it/g8o7nb20ovvf1.jpeg)**
*   **Summary:** Users identify an app shown in an image as Airtable and discuss its potential use cases.
*   **Emotion:** Mixed, with neutral and positive sentiments.
*   **Top 3 Points of View:**
    *   The app is identified as Airtable.
    *   Airtable is used in companies as a database.
    *   The use case of Airtable shown in the image is questioned.

**[Wan video always having artifacts/weird lines? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1o9ye3a/wan_video_always_having_artifactsweird_lines/)**
*   **Summary:** Users report and share examples of artifacts and weird lines in videos generated with Wan.
*   **Emotion:** Mixed, with neutral and negative sentiments.
*   **Top 3 Points of View:**
    *   User points to a gif showing the artifacts/weird lines
    *   One user reports seeing butterflies and fireflies

**[Why is my inpaint not working no matter what I do? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1oa0b9m/why_is_my_inpaint_not_working_no_matter_what_i_do/)**
*   **Summary:** A user asks for help with inpainting issues and receives suggestions regarding models, LoRAs, and denoise strength.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    * Suggestion to test denoise strenghts
    * Suggestion to not use Turbo or Lightning model or LoRA

**[[D] I want to keep up to date (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1o9ysdf/i_want_to_keep_up_to_date/)**
*   **Summary:** A user asks for advice on how to keep up-to-date on AI.
*   **Emotion:** Neutral
*   **Top 3 Points of View:**
    * Use the app Noesion to keep up-to-date

**[Is it possible to match the prompt adherence level of chatgpt/gemini/grok with a locally running model? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1o9wc2o/is_it_possible_to_match_the_prompt_adherence/)**
*   **Summary:** Users discuss whether local models can achieve the same level of prompt adherence as large language models like ChatGPT, Gemini, and Grok, and suggest using models like Qwen Image Edit.
*   **Emotion:** Mixed, with neutral sentiments
*   **Top 3 Points of View:**
    *   Use a locally running model, such as  qwen image edit, via Wan2gp.
    *   Strong prompt adherence is what the new models offer.

**[[D] what is wrong with this? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1oa1jkv/what_is_wrong_with_this/)**
*   **Summary:** A user asks for help identifying the cause of image generation problems and receives suggestions regarding resolution, finetuning, and IP-Adapter.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   The resolution is too low.
    *   A proper finetune is preferred over the base SDXL model.
    *   Newer models like Flux Kontext/Qwen Image Edit are better at character consistency than IP-Adapter.

**[Can OpenSource video have a character jump down from higher up, pull out a laser gun from behind her back, fire laser gun on the way down & land on a red motorcycle parked by the curb, then back up realistically and with weighted physics with the motorcycle? Grok  background music and sound effects. (Score: 0)](https://v.redd.it/vpsxea9kzwvf1)**
*   **Summary:** A user asks if open-source video generation can achieve a complex action sequence and receives mixed responses.
*   **Emotion:** Mixed, with positive and neutral sentiments
*   **Top 3 Points of View:**
    *   It can, but we don't need more slop.
    *   I mean... yes... but also... why?

**[[D] Recommended hardware (sorry) (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1oa3lba/recommended_hardware_sorry/)**
*   **Summary:** Users discuss recommended hardware configurations for AI video workflows, focusing on GPUs like the RTX 4090 and the upcoming 5090.
*   **Emotion:** Mixed, with positive and neutral sentiments.
*   **Top 3 Points of View:**
    *   The Nvidia RTX 4090 is still the second best GPU for non-professional users.
    *   Prioritize getting a 5090, max out the system memory, and cut costs on the other components. This setup will handle AI video workflows.
    *   The main question will be if you can even get hold of a 4090 now.

**[Generating 2D pixel art 16x16 spritesheets (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1oa4zcf/generating_2d_pixel_art_16x16_spritesheets/)**
*   **Summary:** A user asks about generating 2D pixel art 16x16 spritesheets.
*   **Emotion:** Neutral
*   **Top 3 Points of View:**
    *   There are a few pixel art specific models on civit and hf
