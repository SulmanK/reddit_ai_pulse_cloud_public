---
title: "LocalLLaMA Subreddit"
date: "2025-10-13"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "LocalAI", "Open Source"]
---

# Overall Ranking and Top Discussions
1.  [[D] Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes & More](https://www.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/) (Score: 177)
    *   The post discusses a new open-source image-to-markdown model, Nanonets-OCR2, and its features.

2.  [Ring-1T, the open-source trillion-parameter thinking model built on the Ling 2.0 architecture.](https://huggingface.co/inclusionAI/Ring-1T) (Score: 106)
    *   This thread is about the release of Ring-1T, a trillion-parameter open-source model, and users are discussing its performance and availability.

3.  [Guys here is my theory.,,](https://www.reddit.com/r/LocalLLaMA/comments/1o5smq2/guys_here_is_my_theory/) (Score: 82)
    *   The post presents a theory about a network of connected devices to access expert knowledge, and users are discussing its feasibility and applications.

4.  [Drummer's Cydonia Redux 22B v1.1 and Behemoth ReduX 123B v1.1 - Feel the nostalgia without all the stupidity!](https://huggingface.co/TheDrummer/Cydonia-Redux-22B-v1.1) (Score: 38)
    *   This post announces the release of Cydonia Redux 22B v1.1 and Behemoth ReduX 123B v1.1 models and invites users to try them.

5.  [4x4090 build running gpt-oss:20b locally - full specs](https://www.reddit.com/r/LocalLLaMA/comments/1o5qx6p/4x4090_build_running_gptoss20b_locally_full_specs/) (Score: 23)
    *   The thread showcases a 4x4090 build running gpt-oss:20b locally, and users are discussing its performance and suggesting optimizations.

6.  [Geoffrey Hinton explains Neural Nets/LLMs to Jon Stewart](https://www.youtube.com/watch?v=jrK3PsD3APk) (Score: 9)
    *   This thread shares a video of Geoffrey Hinton explaining Neural Nets/LLMs to Jon Stewart, with users praising the discussion.

7.  [It has been 4 hrs since the release of nanochat from Karpathy and no sign of it here! A new full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase](https://github.com/karpathy/nanochat) (Score: 8)
    *   This post highlights the release of nanochat by Karpathy, a minimal implementation of an LLM, and prompts discussion about its use.

8.  [I wrote a 2025 deep dive on why long system prompts quietly hurt context windows, speed, and cost](https://www.reddit.com/r/LocalLLaMA/comments/1o5p4ed/i_wrote_a_2025_deep_dive_on_why_long_system/) (Score: 6)
    *   The post shares a deep dive analysis on the negative impacts of long system prompts, and users are discussing its insights and implications.

9.  [Comparing Popular AI Evaluation Platforms for 2025](https://www.reddit.com/r/LocalLLaMA/comments/1o5t7dr/comparing_popular_ai_evaluation_platforms_for_2025/) (Score: 4)
    *   This thread compares different AI evaluation platforms available in 2025 and recommends Maxim AI as a tool for combining evaluation and observability.

10. [What is the best non Instruct-tuned model?](https://www.reddit.com/r/LocalLLaMA/comments/1o5tokd/what_is_the_best_non_instructtuned_model/) (Score: 4)
    *   Users are discussing what the best non-instruction tuned model is.

11. [Local Build Recommendation 10k USD Budget](https://www.reddit.com/r/LocalLLaMA/comments/1o5q9ob/local_build_recommendation_10k_usd_budget/) (Score: 2)
    *   Users are providing advice on building a local machine learning setup with a budget of $10,000.

12. [GLM-4.6-UD-IQ2_M b0rked?](https://www.reddit.com/r/LocalLLaMA/comments/1o5pu88/glm46udiq2_m_b0rked/) (Score: 1)
    *   The thread discusses an issue with the GLM-4.6-UD-IQ2_M model and its compatibility, later resolved by rebuilding.

13. [how do i make ollama3 uncensored locally?](https://www.reddit.com/r/LocalLLaMA/comments/1o5n4u6/how_do_i_make_ollama3_uncensored_locally/) (Score: 0)
    *   The post asks about how to create an uncensored version of ollama3 locally, but it is pointed out that "ollama3" does not exist.

14. [how to know if X LLM could run reasonably on my hardware ?](https://www.reddit.com/r/LocalLLaMA/comments/1o5nlrc/how_to_know_if_x_llm_could_run_reasonably_on_my/) (Score: 0)
    *   The thread is a question about how to determine if a specific LLM can run adequately on a user's hardware.

15. [Ollama vs Llama CPP + Vulkan on IrisXE IGPU](https://www.reddit.com/r/LocalLLaMA/comments/1o5pvb7/ollama_vs_llama_cpp_vulkan_on_irisxe_igpu/) (Score: 0)
    *   This post compares the performance of Ollama versus Llama CPP with Vulkan on an IrisXE IGPU.

# Detailed Analysis by Thread
**[[D] Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes & More (Score: 177)](https://www.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/)**
*   **Summary:** This thread announces the release of Nanonets-OCR2, an open-source image-to-markdown model capable of handling LaTeX, tables, flowcharts, handwritten documents, and checkboxes. Users are expressing interest, asking about comparisons to other models like Mistral OCR and docling, inquiring about the license, and reporting issues with the demo code.
*   **Emotion:** The overall emotional tone is positive, with users expressing excitement and appreciation for the new model. There is also a sense of curiosity and a desire to understand how it compares to existing solutions.
*   **Top 3 Points of View:**
    *   Enthusiasm for the new open-source OCR model and its capabilities.
    *   Inquiries and requests for comparisons against existing OCR solutions like Mistral OCR and docling.
    *   Questions regarding the licensing and practical issues, such as demo code functionality.

**[Ring-1T, the open-source trillion-parameter thinking model built on the Ling 2.0 architecture. (Score: 106)](https://huggingface.co/inclusionAI/Ring-1T)**
*   **Summary:** The thread discusses Ring-1T, a newly released open-source trillion-parameter model. Users are commenting on its long thinking traces, API speed, and comparing it to other models like Gemini, DeepSeek, and Claude 4.5 Sonnet. Some users also express a desire to run it locally despite its size.
*   **Emotion:** The emotional tone is mixed, with initial excitement and surprise at the release of such a large open-source model. However, some express skepticism about its performance compared to other models.
*   **Top 3 Points of View:**
    *   Excitement and surprise regarding the release of a trillion-parameter open-source model.
    *   Comparison of Ring-1T's performance against other models, including concerns about whether it truly surpasses existing SOTA models.
    *   Desire to run the model locally and discussion about its large context window.

**[Guys here is my theory.,, (Score: 82)](https://www.reddit.com/r/LocalLLaMA/comments/1o5smq2/guys_here_is_my_theory/)**
*   **Summary:** This thread revolves around a user's theory about creating a global network of connected devices for accessing expert knowledge. Discussions include its ecological and economic benefits, potential pricing, API uptime, and comparison to existing solutions like Mechanical Turk.
*   **Emotion:** The overall emotional tone is mixed, with some users expressing excitement and support for the theory, while others raise concerns about practicality, pricing, and uptime. There's also a hint of sarcasm and skepticism.
*   **Top 3 Points of View:**
    *   Support for the idea, highlighting its ecological and economic advantages.
    *   Concerns about pricing, API uptime, and suitability for B2B SAAS applications.
    *   Comparison to existing solutions like Mechanical Turk, and sarcastic comments about the theory's novelty.

**[Drummer's Cydonia Redux 22B v1.1 and Behemoth ReduX 123B v1.1 - Feel the nostalgia without all the stupidity! (Score: 38)](https://huggingface.co/TheDrummer/Cydonia-Redux-22B-v1.1)**
*   **Summary:** The thread announces the release of Cydonia Redux 22B v1.1 and Behemoth ReduX 123B v1.1 models. Users are discussing the models, with some expressing their intention to try them out. The model creator also uses the opportunity to ask for funding support and ideas.
*   **Emotion:** The emotional tone is generally positive, with users expressing excitement and willingness to try out the new models. The model creator also conveys a sense of hope and need for support.
*   **Top 3 Points of View:**
    *   Enthusiasm for trying out the newly released models, particularly from users familiar with previous versions.
    *   Inquiries about specific aspects of the models, such as whether the 123B model is a MoE model.
    *   The model creator's request for funding and suggestions for monetization, highlighting the challenges of maintaining free open-source projects.

**[4x4090 build running gpt-oss:20b locally - full specs (Score: 23)](https://www.reddit.com/r/LocalLLaMA/comments/1o5qx6p/4x4090_build_running_gptoss20b_locally_full_specs/)**
*   **Summary:** The thread is about a user showcasing their 4x4090 build running gpt-oss:20b locally. Discussions revolve around the build's performance, with many users recommending the use of VLLM for optimization. Some users also inquire about the build's cost and specific goals.
*   **Emotion:** The emotional tone is mixed, with initial admiration for the impressive build, but also skepticism about the choice of model and its optimization. There's a strong focus on improving performance.
*   **Top 3 Points of View:**
    *   Admiration for the impressive 4x4090 build.
    *   Strong recommendation to use VLLM for significant performance improvements.
    *   Skepticism regarding the choice of gpt-oss-20b model given the powerful hardware and questions about the specific use case.

**[Geoffrey Hinton explains Neural Nets/LLMs to Jon Stewart (Score: 9)](https://www.youtube.com/watch?v=jrK3PsD3APk)**
*   **Summary:** The thread shares a video of Geoffrey Hinton explaining Neural Nets/LLMs to Jon Stewart. Users praise the discussion, highlighting Stewart's ability to engage with a complex topic and Hinton's insightful explanations.
*   **Emotion:** The emotional tone is overwhelmingly positive, with users appreciating the informative and engaging discussion between Hinton and Stewart.
*   **Top 3 Points of View:**
    *   Appreciation for Geoffrey Hinton's ability to explain complex AI concepts in an accessible manner.
    *   Praise for Jon Stewart's engagement and facilitation of the discussion.
    *   General enjoyment and recommendation of the video for those interested in AI.

**[It has been 4 hrs since the release of nanochat from Karpathy and no sign of it here! A new full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase (Score: 8)](https://github.com/karpathy/nanochat)**
*   **Summary:** The thread highlights the release of nanochat by Karpathy, an implementation of an LLM, with one user questioning the value of training a kindergartener model.
*   **Emotion:** The emotional tone is somewhat neutral, with one user curious about why they would train a model that is not very smart.
*   **Top 3 Points of View:**
    *   The initial poster is excited about the Karpathy release.
    *   Another user is curious about the point of training and running a weak model.

**[I wrote a 2025 deep dive on why long system prompts quietly hurt context windows, speed, and cost (Score: 6)](https://www.reddit.com/r/LocalLLaMA/comments/1o5p4ed/i_wrote_a_2025_deep_dive_on_why_long_system/)**
*   **Summary:** This post discusses a deep dive analysis on the negative impacts of long system prompts, discussing the impact on context windows, speed, and cost. One user asked about personalization, and another user is appreciative of the insights.
*   **Emotion:** The emotional tone is very positive, with users appreciating the new perspectives.
*   **Top 3 Points of View:**
    *   The author of the deep dive presents the idea that long prompts can hurt system performance.
    *   Another user wants to know how to address personalization.
    *   Another user thanks the author for the insight.

**[Comparing Popular AI Evaluation Platforms for 2025 (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1o5t7dr/comparing_popular_ai_evaluation_platforms_for_2025/)**
*   **Summary:** The thread is about comparing AI Evaluation Platforms.
*   **Emotion:** The emotional tone is positive, with the sentiment that Maxim AI is a good tool for evaluation and observabilty.
*   **Top 3 Points of View:**
    *   The author of the post compares the evaluation platforms.
    *   Another user mentions that Maxim AI is a good evaluation tool.

**[What is the best non Instruct-tuned model? (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1o5tokd/what_is_the_best_non_instructtuned_model/)**
*   **Summary:** This thread asks about which models are the best non-instruct tuned models.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   One user claims any model is a base model.

**[Local Build Recommendation 10k USD Budget (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1o5q9ob/local_build_recommendation_10k_usd_budget/)**
*   **Summary:** The thread asks about advice on building a local machine learning setup.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   The author of the post wants help building a local machine learning setup.
    *   Another user recommends waiting for the m5 studio.
    *   Another user recommends an RTX Pro 6000 96GB.

**[GLM-4.6-UD-IQ2_M b0rked? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1o5pu88/glm46udiq2_m_b0rked/)**
*   **Summary:** This thread discusses an issue with the GLM-4.6-UD-IQ2_M model and its compatibility.
*   **Emotion:** The emotional tone is negative, the poster stating that it is b0rked, then changes to negative when they say they are an idiot.
*   **Top 3 Points of View:**
    *   There is a possible incompatibility between the model and Llama.cpp.
    *   The model works after a rebuild.

**[how do i make ollama3 uncensored locally? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1o5n4u6/how_do_i_make_ollama3_uncensored_locally/)**
*   **Summary:** This thread asks about how to create an uncensored version of ollama3 locally.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   The author wants an uncensored model.
    *   There is no ollama3.
    *   The author should find uncensored models on huggingface.

**[how to know if X LLM could run reasonably on my hardware ? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1o5nlrc/how_to_know_if_x_llm_could_run_reasonably_on_my/)**
*   **Summary:** This thread asks about how to determine if a specific LLM can run adequately on a user's hardware.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   The author is asking for advice.
    *   A user provided a link to a calculator.
    *   A user provided a formula.

**[Ollama vs Llama CPP + Vulkan on IrisXE IGPU (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1o5pvb7/ollama_vs_llama_cpp_vulkan_on_irisxe_igpu/)**
*   **Summary:** This post compares the performance of Ollama versus Llama CPP with Vulkan on an IrisXE IGPU.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   The author is asking for information.
    *   A user claims that ollama is a bad llama.cpp wrapper.
