---
title: "LocalLLaMA Subreddit"
date: "2025-10-29"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "local models", "AI"]
---

# Overall Ranking and Top Discussions
1. [[D] DeepSeek may have found a new way to improve AI’s ability to remember](https://www.technologyreview.com/2025/10/29/1126932/deepseek-ocr-visual-compression) (Score: 18)
    *   The discussion centers on DeepSeek's potential advancements in AI memory and the desire for llama.cpp to support all released models. There's also anticipation for the release of v4 and hopes for training on Huawei GPUs to reduce Nvidia GPU costs.
2.  [Here's the best prompt you will ever need to test the new LLMs](https://i.redd.it/n2yilqu2k3yf1.png) (Score: 17)
    *   Users are sharing and testing prompts, comparing the performance of different LLMs, and discussing the potential for training data contamination.
3.  [I miss hybrid/toggleable thinking for Qwen3](https://www.reddit.com/r/LocalLLaMA/comments/1oja7na/i_miss_hybridtoggleable_thinking_for_qwen3/) (Score: 5)
    *   Users debate the merits of hybrid vs. split models for Qwen3, with some missing the hybrid approach and others preferring the separation. They discuss the challenges of using reasoning models with limited resources.
4.  [Where my fine tuners at?](https://www.reddit.com/r/LocalLLaMA/comments/1ojdapo/where_my_fine_tuners_at/) (Score: 4)
    *   Users express interest in learning how to fine-tune models, with one user seeking guidance on using their RTX Pro 6000 GPUs.
5.  [4x RTX 3090 Setup for Wan2.2-TI2V-5B (FP16)](https://www.reddit.com/r/LocalLLaMA/comments/1ojdx3y/4x_rtx_3090_setup_for_wan22ti2v5b_fp16/) (Score: 3)
    *   A user questions the need for a 4x RTX 3090 setup, suggesting that Wan2.2 can run with less VRAM and that ComfyUI can effectively swap model parts to RAM.
6.  [Issue with GLM 4.6 and OpenRouter?](https://www.reddit.com/r/LocalLLaMA/comments/1ojd3wu/issue_with_glm_46_and_openrouter/) (Score: 2)
    *   The discussion centers around issues encountered with GLM 4.6 on OpenRouter, specifically problems with providers automatically closing assistant responses, which breaks prefill functionality.
7.  [TTS - Open Source Chatterbox vs the New Cartesia Sonic 3](https://v.redd.it/oslfb5idf3yf1) (Score: 1)
    *   The conversation is about TTS, or text-to-speech, and how voice cloning may need some improvements.
8.  [Rig Help Wanted](https://www.reddit.com/r/LocalLLaMA/comments/1oj9pzi/rig_help_wanted/) (Score: 1)
    *   A user seeks advice on building a rig for running large MoE models, considering Macs and alternatives like Threadripper or Epyc-based workstations, weighing the pros and cons of each option.
9.  [Does Apple have their own language model?](https://www.reddit.com/r/LocalLLaMA/comments/1oj8z4q/does_apple_have_their_own_language_model/) (Score: 0)
    *   The discussion revolves around whether Apple has its own language model, with participants mentioning the multimodal LLM used in Apple Intelligence and the MLX framework for Apple silicon.
10. [Why aren't more people using local models?](https://www.reddit.com/r/LocalLLaMA/comments/1oj7ts7/why_arent_more_people_using_local_models/) (Score: 0)
    *   The discussion explores why local models aren't more widely adopted, citing factors like the cost of hardware, the complexity of setup, the convenience of cloud-based services, and the limitations of smaller models.
11. [2 x DGX Spark! Give me your non-inference workloads](https://i.redd.it/vjd12ghi03yf1.jpeg) (Score: 0)
    *   The post is about requesting non-inference workloads.
12. [StenoAI: Open Source LocalLLM AI Meeting Notes Taker with Whisper Transcription & LLama 3.2 Summaries](https://www.reddit.com/r/LocalLLaMA/comments/1ojba4q/stenoai_open_source_localllm_ai_meeting_notes/) (Score: 0)
    *   The discussion is about StenoAI, an open source tool that takes meeting notes with Whisper Transcription & Llama 3.2 Summaries
13. [Since Character.ai is ruined by age verification, what's the best local models you know of for 16GB Vram? (Quantized is fine)](https://www.reddit.com/r/LocalLLaMA/comments/1ojdh0d/since_characterai_is_ruined_by_age_verification/) (Score: 0)
    *   The discussion focuses on finding the best local models for roleplaying with 16GB VRAM, as well as the issues with age verification.
14. [Enough math and reasoning and benchmarks; what local models are the most fun to talk to? What are the all time greats you keep coming back to?](https://www.reddit.com/r/LocalLLaMA/comments/1oje5uy/enough_math_and_reasoning_and_benchmarks_what/) (Score: 0)
    *   Users are sharing their favorite local models for engaging conversation, with Qwen3 80B A3B being recommended for those with sufficient VRAM.

# Detailed Analysis by Thread
**[DeepSeek may have found a new way to improve AI’s ability to remember (Score: 18)](https://www.technologyreview.com/2025/10/29/1126932/deepseek-ocr-visual-compression)**
*  **Summary:** The discussion centers on DeepSeek's potential advancements in AI memory and the desire for llama.cpp to support all released models. There's also anticipation for the release of v4 and hopes for training on Huawei GPUs to reduce Nvidia GPU costs.
*  **Emotion:** The emotional tone is generally neutral, with a hint of positive sentiment due to anticipation and hope.
*  **Top 3 Points of View:**
    *   Need to ban new research on LLMs until llama.cpp adds support for all models already released.
    *   Eagerness for the release of v4.
    *   Desire for training on Huawei GPUs to lower Nvidia GPU prices.

**[Here's the best prompt you will ever need to test the new LLMs (Score: 17)](https://i.redd.it/n2yilqu2k3yf1.png)**
*  **Summary:** Users are sharing and testing prompts, comparing the performance of different LLMs, and discussing the potential for training data contamination.
*  **Emotion:** The emotional tone is mostly neutral with a hint of positive sentiment as users share hilarious content.
*  **Top 3 Points of View:**
    *   Some models fail at this prompt, while others succeed.
    *   Concern that training data may be contaminated.
    *   The prompt is considered hilarious.

**[I miss hybrid/toggleable thinking for Qwen3 (Score: 5)](https://www.reddit.com/r/LocalLLaMA/comments/1oja7na/i_miss_hybridtoggleable_thinking_for_qwen3/)**
*  **Summary:** Users debate the merits of hybrid vs. split models for Qwen3, with some missing the hybrid approach and others preferring the separation. They discuss the challenges of using reasoning models with limited resources.
*  **Emotion:** The overall emotional tone is negative.
*  **Top 3 Points of View:**
    *   Some users miss the hybrid/toggleable thinking feature.
    *   Others prefer the split models.
    *   Some users are stuck using reasoning models due to resource limitations.

**[Where my fine tuners at? (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1ojdapo/where_my_fine_tuners_at/)**
*  **Summary:** Users express interest in learning how to fine-tune models, with one user seeking guidance on using their RTX Pro 6000 GPUs.
*  **Emotion:** Positive sentiment.
*  **Top 3 Points of View:**
    *   Interest in learning how to fine-tune models.
    *   Desire for guidance on using specific hardware for fine-tuning.

**[4x RTX 3090 Setup for Wan2.2-TI2V-5B (FP16) (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1ojdx3y/4x_rtx_3090_setup_for_wan22ti2v5b_fp16/)**
*  **Summary:** A user questions the need for a 4x RTX 3090 setup, suggesting that Wan2.2 can run with less VRAM and that ComfyUI can effectively swap model parts to RAM.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    * Questioning of hardware requirements
    * Suggestion of alternative configurations.

**[Issue with GLM 4.6 and OpenRouter? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1ojd3wu/issue_with_glm_46_and_openrouter/)**
*  **Summary:** The discussion centers around issues encountered with GLM 4.6 on OpenRouter, specifically problems with providers automatically closing assistant responses, which breaks prefill functionality.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   There is an issue with GLM-4.6 providers on OpenRouter
    *   Certain providers automatically close the assistant response, which breaks prefill.

**[TTS - Open Source Chatterbox vs the New Cartesia Sonic 3 (Score: 1)](https://v.redd.it/oslfb5idf3yf1)**
*  **Summary:** The conversation is about TTS, or text-to-speech, and how voice cloning may need some improvements.
*  **Emotion:** Negative Sentiment.
*  **Top 3 Points of View:**
    *   voice cloning isnt quite there.
**[Rig Help Wanted (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1oj9pzi/rig_help_wanted/)**
*  **Summary:** A user seeks advice on building a rig for running large MoE models, considering Macs and alternatives like Threadripper or Epyc-based workstations, weighing the pros and cons of each option.
*  **Emotion:** Positive sentiment.
*  **Top 3 Points of View:**
    *   Consideration of Macs as an option.
    *   Threadripper workstations as an alternative.
    *   Epyc servers/workstations with GPU.

**[Does Apple have their own language model? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1oj8z4q/does_apple_have_their_own_language_model/)**
*  **Summary:** The discussion revolves around whether Apple has its own language model, with participants mentioning the multimodal LLM used in Apple Intelligence and the MLX framework for Apple silicon.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   Apple has foundation models.
    *   The on-device model is small and heavily quantised.

**[Why aren't more people using local models? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1oj7ts7/why_arent_more_people_using_local_models/)**
*  **Summary:** The discussion explores why local models aren't more widely adopted, citing factors like the cost of hardware, the complexity of setup, the convenience of cloud-based services, and the limitations of smaller models.
*  **Emotion:** Negative.
*  **Top 3 Points of View:**
    *   Graphics cards cost a lot.
    *   The early days of AI are over.
    *   The problem with edge devices is simple: battery life.

**[2 x DGX Spark! Give me your non-inference workloads (Score: 0)](https://i.redd.it/vjd12ghi03yf1.jpeg)**
*  **Summary:** The post is about requesting non-inference workloads.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   Inference benchmarks would be very interesting
    *   Train nanochat on this boxes.
    *   Run some benchmarks on a MoE model

**[StenoAI: Open Source LocalLLM AI Meeting Notes Taker with Whisper Transcription & LLama 3.2 Summaries (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1ojba4q/stenoai_open_source_localllm_ai_meeting_notes/)**
*  **Summary:** The discussion is about StenoAI, an open source tool that takes meeting notes with Whisper Transcription & Llama 3.2 Summaries
*  **Emotion:** Positive.
*  **Top 3 Points of View:**
    *   Whisper is good.

**[Since Character.ai is ruined by age verification, what's the best local models you know of for 16GB Vram? (Quantized is fine) (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1ojdh0d/since_characterai_is_ruined_by_age_verification/)**
*  **Summary:** The discussion focuses on finding the best local models for roleplaying with 16GB VRAM, as well as the issues with age verification.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   Seek the best local models you know of for 16GB Vram.
    *   Some are looking for roleplaying.
    *   Some are experiencing an issue with age verification.

**[Enough math and reasoning and benchmarks; what local models are the most fun to talk to? What are the all time greats you keep coming back to? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1oje5uy/enough_math_and_reasoning_and_benchmarks_what/)**
*  **Summary:** Users are sharing their favorite local models for engaging conversation, with Qwen3 80B A3B being recommended for those with sufficient VRAM.
*  **Emotion:** Positive.
*  **Top 3 Points of View:**
    *   Suggesting users Qwen3 80B A3B is the best overall for engaging conversation if you got ~64GB of VRAM to work with.
    *   Today I stumbled randomly on a old small 7B model from the olden times, that is very philosophical
