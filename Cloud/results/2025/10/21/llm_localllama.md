---
title: "LocalLLaMA Subreddit"
date: "2025-10-21"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "Local", "AI"]
---

# Overall Ranking and Top Discussions
1.  [[D] Qwen3-VL-2B and Qwen3-VL-32B Released](https://i.redd.it/n4rx9o72phwf1.jpeg) (Score: 270)
    * Discussing the release of Qwen3-VL-2B and Qwen3-VL-32B models, including benchmarks and comparisons to other models.
2.  [DeepSeek-OCR AI can scan an entire microfiche sheet and not just cells and retain 100% of the data in seconds...](https://www.reddit.com/gallery/1ocgun0) (Score: 74)
    * A discussion about DeepSeek-OCR AI's capabilities in scanning microfiche sheets, with skepticism about the claims.
3.  [Nvidia quietly released RTX Pro 5000 Blackwell 72Gb](https://www.reddit.com/r/LocalLLaMA/comments/1och0jn/nvidia_quietly_released_rtx_pro_5000_blackwell/) (Score: 66)
    *  Discussing the release of the Nvidia RTX Pro 5000 Blackwell GPU, its specs, and its potential use for local LLM development.
4.  [Comparison new qwen 32b-vl vs qwen 30a3-vl](https://www.reddit.com/gallery/1ocko1m) (Score: 29)
    *  Comparing the new Qwen 32b-vl model against the Qwen 30a3-vl model.
5.  [Getting most out of your local LLM setup](https://www.reddit.com/r/LocalLLaMA/comments/1oclug7/getting_most_out_of_your_local_llm_setup/) (Score: 21)
    * A discussion about how to optimize local LLM setups, with a focus on tools and backends.
6.  [I built an offline-first voice AI with <1 s latency on my Mac M3](https://www.reddit.com/r/LocalLLaMA/comments/1ocjhug/i_built_an_offlinefirst_voice_ai_with_1_s_latency/) (Score: 9)
    *  A user showcasing an offline-first voice AI built on a Mac M3, and discussing its features and performance.
7.  [OpenCode Chat - a slimmer version of OC. From 20k tokens init to 5k.](https://github.com/IgorWarzocha/opencode-chat/releases/tag/opencode-chat-v0.1.0) (Score: 8)
    * Announcing the release of OpenCode Chat, a streamlined version of OpenCode.
8.  [Qwen3-VL kinda sucks in LM Studio](https://www.reddit.com/gallery/1ock0lc) (Score: 7)
    * Discussing issues with Qwen3-VL performance in LM Studio, including image downscaling problems.
9.  [Llama-Embed-Nemotron-8B Takes the Top Spot on MMTEB Multilingual Retrieval Leaderboard](https://www.reddit.com/r/LocalLLaMA/comments/1ocj4w8/llamaembednemotron8b_takes_the_top_spot_on_mmteb/) (Score: 5)
    * Discussing the performance of Llama-Embed-Nemotron-8B on the MMTEB multilingual retrieval leaderboard and comparing it with other models like Qwen.
10. [Are there LLMs I can run via LM Studio that have voice input and output?](https://www.reddit.com/r/LocalLLaMA/comments/1och72d/are_there_llms_i_can_run_via_lm_studio_that_have/) (Score: 3)
    * Asking about LLMs that can be run with voice input/output through LM Studio.
11. [Tool calling frustrations with Qwen3-30B-A3B-Instruct-GGUF](https://www.reddit.com/r/LocalLLaMA/comments/1oclysw/tool_calling_frustrations_with/) (Score: 3)
    * A discussion about tool calling issues experienced with Qwen3-30B-A3B-Instruct-GGUF.
12. [Embeddings](https://www.reddit.com/r/LocalLLaMA/comments/1ocjgbf/embeddings/) (Score: 2)
    * A question about which embeddings to use, with a recommendation for Qwen 8B.
13. [What has been your experience building with a diffusion LLM?](https://www.reddit.com/r/LocalLLaMA/comments/1ocj7uf/what_has_been_your_experience_building_with_a/) (Score: 2)
    * Discussion about building with Diffusion Language Models.
14. [ByteDance model efficiency](https://www.reddit.com/r/LocalLLaMA/comments/1ockanx/bytedance_model_efficiency/) (Score: 2)
    * Discussing the efficiency of ByteDance models.
15. [Build an Excel Agent](https://www.reddit.com/r/LocalLLaMA/comments/1ocmv3j/build_an_excel_agent/) (Score: 0)
    *  A question about building an Excel agent.

# Detailed Analysis by Thread
**[ [D] Qwen3-VL-2B and Qwen3-VL-32B Released (Score: 270)](https://i.redd.it/n4rx9o72phwf1.jpeg)**
*  **Summary:**  The thread discusses the release of the Qwen3-VL-2B and Qwen3-VL-32B models, including comparisons to previous versions and other models like Qwen 30B A3B. Users share benchmark results, discuss model performance, and express excitement about the advancements. Some are looking for a way to compare different models to decide which to use.
*  **Emotion:** The overall emotional tone is positive and neutral, with excitement about the new model releases.
*  **Top 3 Points of View:**
    *   The new Qwen3-VL models are a significant improvement over previous versions.
    *   The comparison to the Qwen 30B A3B model is important for users deciding which model to use.
    *   A simple chart to compare models at a glance would be helpful.

**[ DeepSeek-OCR AI can scan an entire microfiche sheet and not just cells and retain 100% of the data in seconds... (Score: 74)](https://www.reddit.com/gallery/1ocgun0)**
*  **Summary:**  The thread revolves around the DeepSeek-OCR AI's alleged ability to scan microfiche sheets quickly and accurately. Many users are skeptical of the claims, especially the "100% data retention" claim without verification. Some also question whether the technology would be released by US companies as openly as it is by Chinese entities.
*  **Emotion:** The emotional tone is mixed, with skepticism and positivity.
*  **Top 3 Points of View:**
    *   The claims about DeepSeek-OCR's performance need validation.
    *   It's interesting how openly China is releasing OCR work.
    *   The resolution and processing methods of microfiche scanning need more explanation.

**[ Nvidia quietly released RTX Pro 5000 Blackwell 72Gb (Score: 66)](https://www.reddit.com/r/LocalLLaMA/comments/1och0jn/nvidia_quietly_released_rtx_pro_5000_blackwell/)**
*  **Summary:**  This thread is about the quiet release of the Nvidia RTX Pro 5000 Blackwell GPU. Users are discussing its specifications, particularly the memory (initially reported as 72GB but later questioned), and its potential use for running large language models locally. They also compare it to other GPUs and discuss the pricing.
*  **Emotion:**  The emotional tone is mostly positive, with excitement about a new GPU that could be ideal for local LLM development, mixed with some negativity.
*  **Top 3 Points of View:**
    *   The RTX Pro 5000 Blackwell is a good balance between high performance and cost.
    *   The 72GB (or 48GB) memory is a significant advantage for running large models.
    *   It's unfortunate that Intel and AMD don't compete in this market segment.

**[ Comparison new qwen 32b-vl vs qwen 30a3-vl (Score: 29)](https://www.reddit.com/gallery/1ocko1m)**
*  **Summary:**  The thread discusses the comparison between the new Qwen 32B-VL model and the Qwen 30A3-VL model. Users share their experiences and observations, noting that the 32B model is generally better in benchmarks but slower than the 30A3-VL. They also consider its applicability to tasks like document parsing.
*  **Emotion:** The overall emotional tone is neutral, with users sharing practical experiences and comparing models.
*  **Top 3 Points of View:**
    *   The 32B model has a slight quality increase but sacrifices speed.
    *   The applicability of these models for document parsing tasks compared to OCR-based models is a key question.
    *   Some users have experienced issues with the 32B model outputting "think" tags, leading them to revert to the 30B model.

**[ Getting most out of your local LLM setup (Score: 21)](https://www.reddit.com/r/LocalLLaMA/comments/1oclug7/getting_most_out_of_your_local_llm_setup/)**
*  **Summary:**  The thread is about optimizing local LLM setups. Users are sharing tips, tools, and backends to improve the performance and functionality of their local LLM environments.
*  **Emotion:** The overall emotional tone is positive, with gratitude for shared information and enthusiasm for improving local LLM setups.
*  **Top 3 Points of View:**
    *   OpenCode is a great fit for local LLM and agentic coding.
    *   Tailscale can solve many networking issues.
    *   "Transformers serve" is an interesting new backend option for language models without vLLM or llama.cpp support.

**[ I built an offline-first voice AI with <1 s latency on my Mac M3 (Score: 9)](https://www.reddit.com/r/LocalLLaMA/comments/1ocjhug/i_built_an_offlinefirst_voice_ai_with_1_s_latency/)**
*  **Summary:**  The thread showcases a user's project: an offline-first voice AI with sub-second latency on a Mac M3. Users commend the work and ask questions about the models used and the RAM requirements. There's also a suggestion for improving the latency further using speculative transcription.
*  **Emotion:** The overall emotional tone is positive, with users expressing admiration and curiosity about the project.
*  **Top 3 Points of View:**
    *   The demo is impressive and the latency is notable.
    *   The implementation of voice activity detection (VAD) is a key feature.
    *   The choice of LLM and its RAM requirements are important considerations.

**[ OpenCode Chat - a slimmer version of OC. From 20k tokens init to 5k. (Score: 8)](https://github.com/IgorWarzocha/opencode-chat/releases/tag/opencode-chat-v0.1.0)**
*  **Summary:**  The thread announces the release of OpenCode Chat, a more streamlined version of the OpenCode project with a reduced initial token count.
*  **Emotion:** The emotional tone is positive.
*  **Top 3 Points of View:**
    * The idea is appreciated.

**[ Qwen3-VL kinda sucks in LM Studio (Score: 7)](https://www.reddit.com/gallery/1ock0lc)**
*  **Summary:**  The thread discusses issues with the performance of Qwen3-VL in LM Studio. Users are reporting that the model underperforms, and the discussion points to LM Studio's image downscaling as a possible cause. Alternative solutions like using Hyperlink app are suggested.
*  **Emotion:** The emotional tone is negative.
*  **Top 3 Points of View:**
    *   LM Studio's image downscaling negatively affects Qwen3-VL's performance.
    *   The 30b model works better, but the 4B model works well on iPhones
    *   Hyperlink app by Nexa is a better option.

**[ Llama-Embed-Nemotron-8B Takes the Top Spot on MMTEB Multilingual Retrieval Leaderboard (Score: 5)](https://www.reddit.com/r/LocalLLaMA/comments/1ocj4w8/llamaembednemotron8b_takes_the_top_spot_on_mmteb/)**
*  **Summary:**  This thread discusses the performance of Llama-Embed-Nemotron-8B on the MMTEB multilingual retrieval leaderboard. Users are comparing it to Qwen models and questioning whether it supports matryoshka embedding.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Llama-Embed-Nemotron-8B wins over Qwen 8B by votes, but its mean task score is between Qwen 8B and 4B.
    *   It's unclear whether the Nemotron model supports matryoshka embedding like Qwen.

**[ Are there LLMs I can run via LM Studio that have voice input and output? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1och72d/are_there_llms_i_can_run_via_lm_studio_that_have/)**
*  **Summary:**  The thread is a question asking for LLMs that can be run via LM Studio with voice input and output. Users provide suggestions, including using AnythingLLM, Handy and specific models like Gemma 3 or Qwen 3.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   AnythingLLM offers out-of-the-box integration for TTS and STT.
    *   Handy is a parallel application that works well with LM Studio for voice input and output.
    *   Using whisper and coqui can yield great results for simple translation apps.

**[ Tool calling frustrations with Qwen3-30B-A3B-Instruct-GGUF (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1oclysw/tool_calling_frustrations_with/)**
*  **Summary:**  Users are discussing frustrations experienced with tool calling when using the Qwen3-30B-A3B-Instruct-GGUF model. They propose alternative models and configurations.
*  **Emotion:** The emotional tone is mixed, with frustration and positivity.
*  **Top 3 Points of View:**
    *  Qwen coder sucks. Glm 4.5 air is good.
    *  Context size and VRAM can affect tool calling performance.
    *  The thinking version with Roo code works better for tool calling.

**[ Embeddings (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1ocjgbf/embeddings/)**
*  **Summary:**  The thread asks about which embeddings to use.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Qwen 8B is recommended.

**[ What has been your experience building with a diffusion LLM? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1ocj7uf/what_has_been_your_experience_building_with_a/)**
*  **Summary:**  This thread discusses the experiences of users building with diffusion LLMs. The main points include the trade-offs between diffusion and autoregressive models, diffusion models' unsuitability for cloud deployment, and the potential for small on-device reasoning LLMs.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Diffusion models are faster but can struggle with context and coherence, while autoregressive models are slower but generate higher quality text.
    *   Diffusion LLMs don't make sense in the cloud due to high compute density and lack of benefits from batching.
    *   Diffusion LLMs' potential lies in small on-device reasoning LLMs that cooperate with a cloud-based LLM.

**[ ByteDance model efficiency (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1ockanx/bytedance_model_efficiency/)**
*  **Summary:**  The thread discusses the efficiency of ByteDance models.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   The model is running on a CPU with 13 tps.
    *   Confusion exists regarding which ByteDance model is being discussed.
    *   Qwen coder, GLM 4.5 Air, or GPT OSS 120B are the models that need to be compared.

**[ Build an Excel Agent (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1ocmv3j/build_an_excel_agent/)**
*  **Summary:**  The thread discusses the need for an AI agent to build an Excel Agent
*  **Emotion:** The emotional tone is negative.
*  **Top 3 Points of View:**
    *   Overthinking and overcomplicating something extremely simple, need for AI Agent or use OCR etc.
