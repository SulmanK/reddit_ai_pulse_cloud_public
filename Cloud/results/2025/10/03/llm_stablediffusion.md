---
title: "Stable Diffusion Subreddit"
date: "2025-10-03"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [On-AI-R #1: Camille - Complex AI-Driven Musical Performance](https://v.redd.it/xi9ttqrquwsf1) (Score: 42)
    *   Discussion about a complex AI-driven musical performance and the hardware requirements to render it in 4K.
2.  [Ming-UniVision: The First Unified Autoregressive MLLM with Continuous Vision Tokens.](https://i.redd.it/xw6e3wic6xsf1.png) (Score: 36)
    *   A post introducing Ming-UniVision, the first unified autoregressive MLLM with continuous vision tokens, along with questions about its implementation and the meaning of MLLM.
3.  [Wan 2.2 i2v with Dyno lora and Qwen based images (both workflows included)](https://v.redd.it/zw9a9ewj4xsf1) (Score: 36)
    *   A showcase of image-to-video generation using Wan 2.2 with Dyno LoRA and Qwen, including workflow details and some feedback on the results.
4.  [Ovi is pretty good! 2 mins on an RTX Pro 6000](https://v.redd.it/uc5i2giliwsf1) (Score: 33)
    *   A demonstration of Ovi, running on an RTX Pro 6000, with users discussing its capabilities and comparing it to other tools.
5.  [Night Drive Cat](https://v.redd.it/l2zc4crhkxsf1) (Score: 11)
    *   A user shares a post and another user says that the post genuinely made them laugh.
6.  [Animal Winter Olympics üêíüêß‚õ∑Ô∏è | Satirical News Montage | APE NEWS 6min. Is that more than slog?](https://youtu.be/2E4ENshewOk?si=d8gEaIVvJN0KDLjR) (Score: 5)
    *   A user shares a satirical news montage of an Animal Winter Olympics and how it took a week to create the news segment and around 500 videos were made with 100 used.
7.  [VHS filters work great with AI footage (WAN 2.2 + NTSC-RS)](https://v.redd.it/wxwqhth99ysf1) (Score: 4)
    *   A user shares WAN 2.2 + NTSC-RS and how VHS filters work great with AI footage.
8.  [Anyone using eGPU for image generation ?](https://www.reddit.com/r/StableDiffusion/comments/1nx5lel/anyone_using_egpu_for_image_generation/) (Score: 3)
    *   A discussion about using eGPUs for image generation, including their limitations and comparisons to renting GPUs.
9.  [Some samples with Qwen 2509](https://www.reddit.com/r/StableDiffusion/comments/1nx090c/some_samples_with_qwen_2509/) (Score: 2)
    *   A user shares some samples with Qwen 2509.
10. [How Do I Become "Literate" In Local AI Tools/Techniques? (I Don't Want To Rely On Tutorials Forever)](https://www.reddit.com/r/StableDiffusion/comments/1nx0rx7/how_do_i_become_literate_in_local_ai/) (Score: 1)
    *   A discussion on how to become proficient in local AI tools and techniques beyond relying on tutorials.
11. [Which online providers offer Wan and SeaDream with the most creative freedom?](https://www.reddit.com/r/StableDiffusion/comments/1nx43fp/which_online_providers_offer_wan_and_seadream/) (Score: 1)
    *   A question about which online providers offer Wan and SeaDream with the most creative freedom.
12. [I want to train a Lora for WAN 2.2 on high and low noise. Do I need to change any of the data for the low and high noise models, or can I leave the same settings, or the same for high and low noise?](https://www.reddit.com/r/StableDiffusion/comments/1nx6mt9/i_want_to_train_a_lora_for_wan_22_on_high_and_low/) (Score: 1)
    *   A question about training a LoRA for WAN 2.2 on high and low noise and asking if any of the data needs to be changed for the low and high noise models, or can the user leave the same settings, or the same for high and low noise?
13. [For some reason, the Ideogram V3 model and Google's Nano Banana are very similar...](https://i.redd.it/1c4278wc4ysf1.png) (Score: 0)
    *   A user thinks that the Ideogram V3 model and Google's Nano Banana are very similar.
14. [Looking for recommendations for generating product images with ai.](https://www.reddit.com/r/StableDiffusion/comments/1nx21t2/looking_for_recommendations_for_generating/) (Score: 0)
    *   A user is looking for recommendations for generating product images with ai.
15. [How to create story telling videos for YouTube with ai?](https://www.reddit.com/r/StableDiffusion/comments/1nx298i/how_to_create_story_telling_videos_for_youtube/) (Score: 0)
    *   A user is asking how to create story telling videos for YouTube with ai.
16. [QWEN  IMAGEN Y LORAS](https://www.reddit.com/r/StableDiffusion/comments/1nx53wg/qwen_imagen_y_loras/) (Score: 0)
    *   A user asks about Qwen Imagen y Loras.

# Detailed Analysis by Thread
**[On-AI-R #1: Camille - Complex AI-Driven Musical Performance (Score: 42)](https://v.redd.it/xi9ttqrquwsf1)**
*   **Summary:**  A showcase of a complex, AI-driven musical performance named Camille. The discussion focuses on the hardware required to render it in 4K resolution.
*   **Emotion:** Neutral. The discussion is primarily technical and informational.
*   **Top 3 Points of View:**
    *   Inquiry about the hardware requirements.
    *   (Only one viewpoint present)

**[Ming-UniVision: The First Unified Autoregressive MLLM with Continuous Vision Tokens. (Score: 36)](https://i.redd.it/xw6e3wic6xsf1.png)**
*   **Summary:**  Introduction of Ming-UniVision, a new MLLM. Discussions include the possibility of GGUF implementations and the meaning of the acronym "MLLM."
*   **Emotion:** Neutral. Primarily informational with requests for clarification.
*   **Top 3 Points of View:**
    *   Speculation about GGUF availability.
    *   Request for explanation of "MLLM."
    *   (Only two viewpoints present)

**[Wan 2.2 i2v with Dyno lora and Qwen based images (both workflows included) (Score: 36)](https://v.redd.it/zw9a9ewj4xsf1)**
*   **Summary:**  Showcase of Wan 2.2 image-to-video generation using Dyno LoRA and Qwen-based images. Feedback includes comparisons to other versions and aesthetic preferences.
*   **Emotion:** Mixed, but primarily Neutral. Positive comments about the work, but also some critical feedback.
*   **Top 3 Points of View:**
    *   Questioning the effectiveness of Dyno LoRA without comparison.
    *   Criticism of the results compared to older versions (Wan 2.1).
    *   Positive feedback, expressing appreciation and interest in further improvements.

**[Ovi is pretty good! 2 mins on an RTX Pro 6000 (Score: 33)](https://v.redd.it/uc5i2giliwsf1)**
*   **Summary:**  A demonstration of the AI tool "Ovi" running on an RTX Pro 6000. Discussions involve its capabilities, comparisons to other tools, and user experiences.
*   **Emotion:** Mixed. Neutral to Positive. Contains positive reactions and technical questions. A negative comment is present, but not dominant.
*   **Top 3 Points of View:**
    *   Enthusiasm and desire for continuation/more content.
    *   Questions about T2V vs. I2V capabilities.
    *   Inquiries about running Ovi on specific platforms (Runpod) and troubleshooting issues.

**[Night Drive Cat (Score: 11)](https://v.redd.it/l2zc4crhkxsf1)**
*   **Summary:**  A user shares a post and another user says that the post genuinely made them laugh.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   One user found the post humorous.
    *   (Only one viewpoint present)

**[Animal Winter Olympics üêíüêß‚õ∑Ô∏è | Satirical News Montage | APE NEWS 6min. Is that more than slog? (Score: 5)](https://youtu.be/2E4ENshewOk?si=d8gEaIVvJN0KDLjR)**
*   **Summary:**  A user shares a satirical news montage of an Animal Winter Olympics and how it took a week to create the news segment and around 500 videos were made with 100 used.
*   **Emotion:** Positive.
*   **Top 3 Points of View:**
    *   It took almost a week to create the news segment. About 500 videos were made and only 100 were used.
    *   (Only one viewpoint present)

**[VHS filters work great with AI footage (WAN 2.2 + NTSC-RS) (Score: 4)](https://v.redd.it/wxwqhth99ysf1)**
*   **Summary:**  A user shares WAN 2.2 + NTSC-RS and how VHS filters work great with AI footage.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   VHS filters work great with AI footage.
    *   (Only one viewpoint present)

**[Anyone using eGPU for image generation ? (Score: 3)](https://www.reddit.com/r/StableDiffusion/comments/1nx5lel/anyone_using_egpu_for_image_generation/)**
*   **Summary:**  A discussion about using eGPUs for image generation, including their limitations and comparisons to renting GPUs.
*   **Emotion:** Mixed. Contains both positive and negative comments.
*   **Top 3 Points of View:**
    *   Experience using an eGPU setup with a 5080 and 5090.
    *   Recommendation against using eGPUs due to bandwidth limitations and cost-effectiveness of renting.
    *   Sharing a link to a previous discussion about eGPU performance.

**[Some samples with Qwen 2509 (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1nx090c/some_samples_with_qwen_2509/)**
*   **Summary:**  A user shares some samples with Qwen 2509.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   A user shared a link.
    *   (Only one viewpoint present)

**[How Do I Become "Literate" In Local AI Tools/Techniques? (I Don't Want To Rely On Tutorials Forever) (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1nx0rx7/how_do_i_become_literate_in_local_ai/)**
*   **Summary:**  A discussion on how to become proficient in local AI tools and techniques beyond relying on tutorials.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   To learn by doing; making up projects.
    *   Read the documentation for the tools that you are using.
    *   Start with a simple UI like Invoke or Forge and make the jump to Comfy later.

**[Which online providers offer Wan and SeaDream with the most creative freedom? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1nx43fp/which_online_providers_offer_wan_and_seadream/)**
*   **Summary:**  A question about which online providers offer Wan and SeaDream with the most creative freedom.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   If you pay for Fal.ai you can disable their filter when using the API.
    *   Freepik seems to leave the censorship up to the end model.
    *   (Only two viewpoints present)

**[I want to train a Lora for WAN 2.2 on high and low noise. Do I need to change any of the data for the low and high noise models, or can I leave the same settings, or the same for high and low noise? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1nx6mt9/i_want_to_train_a_lora_for_wan_22_on_high_and_low/)**
*   **Summary:**  A question about training a LoRA for WAN 2.2 on high and low noise and asking if any of the data needs to be changed for the low and high noise models, or can the user leave the same settings, or the same for high and low noise?
*   **Emotion:** Positive.
*   **Top 3 Points of View:**
    *   Use Diffusion Pipe to train models and don't change the settings for high or low noise models training.
    *   The resolution for wan is 720p. You shouldn't use more pixels than 720x720 square or cropping to 16:9.
    *   You can use the T2v lora with i2v wan.

**[For some reason, the Ideogram V3 model and Google's Nano Banana are very similar... (Score: 0)](https://i.redd.it/1c4278wc4ysf1.png)**
*   **Summary:**  A user thinks that the Ideogram V3 model and Google's Nano Banana are very similar.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   The models look different.
    *   Why are the models suspiciously similar?
    *   (Only two viewpoints present)

**[Looking for recommendations for generating product images with ai. (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1nx21t2/looking_for_recommendations_for_generating/)**
*   **Summary:**  A user is looking for recommendations for generating product images with ai.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   Download ComfyUI and generate images with a recent image diffusion model such as Flux Krea, Wan 2.2, Qwen, ChromaHD, etc.
    *   (Only one viewpoint present)

**[How to create story telling videos for YouTube with ai? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1nx298i/how_to_create_story_telling_videos_for_youtube/)**
*   **Summary:**  A user is asking how to create story telling videos for YouTube with ai.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   Download ComfyUI and generate images with a recent image diffusion model such as Flux Krea, Wan 2.2, Qwen, ChromaHD, etc.
    *   (Only one viewpoint present)

**[QWEN  IMAGEN Y LORAS (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1nx53wg/qwen_imagen_y_loras/)**
*   **Summary:**  A user asks about Qwen Imagen y Loras.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   The lora trained on qwen, on civitai look for a lora.
    *   [https://civitai.com/search/models?baseModel=Qwen&modelType=LORA&modelType=LoCon&sortBy=models\_v9](https://civitai.com/search/models?baseModel=Qwen&modelType=LORA&modelType=LoCon&sortBy=models_v9)
    *   (Only two viewpoints present)
