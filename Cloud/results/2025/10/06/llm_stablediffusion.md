---
title: "Stable Diffusion Subreddit"
date: "2025-10-06"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [[D] Qwen Image Edit 2509 lightx2v LoRA's just released - 4 or 8 step](https://www.reddit.com/r/StableDiffusion/comments/1nzl1c2/qwen_image_edit_2509_lightx2v_loras_just_released/) (Score: 128)
    *   Users are discussing the newly released Qwen Image Edit 2509 lightx2v LoRA models for Stable Diffusion, focusing on improvements in prompt adherence and general usage.
2.  ["Neural Growth" WAN2.2 FLF2V first/last frames animation](https://youtu.be/brARorFpwkA?si=a9XdHJwOHjZ0kN2B) (Score: 14)
    *   The post showcases a "Neural Growth" animation created using WAN2.2 FLF2V, detailing the workflow and tools used.
3.  [Parallel universes](https://v.redd.it/jada64ah6itf1) (Score: 9)
    *   The post features a collection of images depicting parallel universes, generated using Stable Diffusion with different checkpoints and negative prompts.
4.  [Tinkering on a sandbox for real-time interactive generation starting with LongLive-1.3B](https://v.redd.it/zcjhbpk7xitf1) (Score: 5)
    *   The poster is working on a real-time interactive generation sandbox, and other users are suggesting how best to bind user inputs to prompt adjustments.
5.  [Qwen doesn't do it. Kontext doesn't do it. What do we have that takes "person A" and puts them in "scene B"?](https://www.reddit.com/r/StableDiffusion/comments/1nzs18o/qwen_doesnt_do_it_kontext_doesnt_do_it_what_do_we/) (Score: 3)
    *   Users are seeking solutions for combining a person from one image and placing them into a different scene using Stable Diffusion, discussing different approaches using Qwen image edit and other tools.
6.  [Wan Animate only supports one person](https://www.reddit.com/r/StableDiffusion/comments/1nzs1to/wan_animate_only_supports_one_person/) (Score: 3)
    *   A user is inquiring about the limitations of Wan Animate in handling multiple subjects, with another user suggesting workarounds.
7.  [Fairy Tail - Fan animation - Wan and Chatterbox/Xtts-v2](https://v.redd.it/ixxc603q0itf1) (Score: 2)
    *   A user shares a "Fairy Tail" fan animation created using Wan and Chatterbox/Xtts-v2, described as a "dangerous watch of the week."
8.  [Wan 2.2 T2V problem - various blemishes and marks on the video](https://www.reddit.com/r/StableDiffusion/comments/1nznwlh/wan_22_t2v_problem_various_blemishes_and_marks_on/) (Score: 2)
    *   A user is experiencing issues with blemishes and marks when using Wan 2.2 T2V, and other users suggest using the right model or setting.
9.  [How do you create a consistent character to use across videos?](https://www.reddit.com/r/StableDiffusion/comments/1nzpr3e/how_do_you_create_a_consistent_character_to_use/) (Score: 2)
    *   Users are discussing methods for creating consistent characters across videos using Stable Diffusion, including using LoRAs, Flux, Qwen image edit, and Nana Banana.
10. [Looking for help with QWEN Image Edit 2509](https://i.redd.it/1ib4evaeajtf1.png) (Score: 1)
    *   A user is requesting help with QWEN Image Edit 2509. Others suggest using "ImageScaleToTotalPixel" to resize the image or check if the image size is the correct resolution.
11. [Qwen Edit 2509 unconsistent outputs (HEEEELP)](https://www.reddit.com/gallery/1nzrdvy) (Score: 1)
    *   A user is experiencing inconsistent outputs with Qwen Edit 2509 and seeking help. Suggested solutions include using a "canny" controlnet, using the first Qwen edit, or using a Qwen 2509 LoRA to transform artsy images into realistic ones.
12. [Help we moving from A1111-forge to ComfyUI](https://www.reddit.com/r/StableDiffusion/comments/1nzo5u5/help_we_moving_from_a1111forge_to_comfyui/) (Score: 1)
    *   A user is requesting assistance with transitioning from A1111-forge to ComfyUI, particularly regarding upscaling, FluxGuidance nodes, and LoRA implementation.
13. [16GB VRAM and qwen_image_edit_2509?](https://www.reddit.com/r/StableDiffusion/comments/1nzs7um/16gb_vram_and_qwen_image_edit_2509/) (Score: 1)
    *   Users are sharing their experiences using qwen_image_edit_2509 with 16GB VRAM cards, discussing potential issues and alternative versions like the Nunchaku version.
14. [Choose 1, 2 or 3? and can you tell me why you don't like the other 2?](https://v.redd.it/xl9uc0x3nitf1) (Score: 0)
    *   Users are providing feedback on different video options, focusing on voice quality, lip sync accuracy, and the presence of generative artifacts.
15. [Would it be a good idea creating a Stable Diffusion Challenge Subreddit?](https://www.reddit.com/r/StableDiffusion/comments/1nzm1vy/would_it_be_a_good_idea_creating_a_stable/) (Score: 0)
    *   A user is suggesting creating a Stable Diffusion challenge subreddit, and other users are discussing existing alternatives like Civitai bounties, Discord servers, and the challenges of maintaining engagement on Reddit.
16. [help](https://www.reddit.com/r/StableDiffusion/comments/1nzm5o0/help/) (Score: 0)
    *   A user is requesting help. The other users suggests using ComfyUI with Flux dev/krea or Qwen.
17. [[Paid job] Looking for a ForgeUI expert to help with game asset creation](https://www.reddit.com/r/StableDiffusion/comments/1nzmxxo/paid_job_looking_for_a_forgeui_expert_to_help/) (Score: 0)
    *   A user is looking for a ForgeUI expert to help with game asset creation, and other users are recommending ComfyUI and linking to relevant resources.
18. [Best AI coding Agent Opensource/Free for coding?](https://www.reddit.com/r/StableDiffusion/comments/1nzpvxu/best_ai_coding_agent_opensourcefree_for_coding/) (Score: 0)
    *   Users are discussing various open-source and free AI coding agents, comparing their capabilities, resource requirements, and integration with IDEs.

# Detailed Analysis by Thread
**[Qwen Image Edit 2509 lightx2v LoRA's just released - 4 or 8 step (Score: 128)](https://www.reddit.com/r/StableDiffusion/comments/1nzl1c2/qwen_image_edit_2509_lightx2v_loras_just_released/)**
*  **Summary:** Users are discussing the new Qwen Image Edit 2509 lightx2v LoRA models, focusing on improvements in prompt adherence and comparing it to previous versions. They are also discussing potential uses and limitations of the new models.
*  **Emotion:** The overall emotional tone is neutral, with some positive sentiment expressed regarding the perceived improvements in prompt adherence.
*  **Top 3 Points of View:**
    *   The new LoRA improves prompt adherence.
    *   Users are wondering if the Edit version can be used for regular image generation
    *   Users are waiting for the release of NSFW loras.

**["Neural Growth" WAN2.2 FLF2V first/last frames animation (Score: 14)](https://youtu.be/brARorFpwkA?si=a9XdHJwOHjZ0kN2B)**
*  **Summary:** A user shares an animation created using the WAN2.2 FLF2V workflow in ComfyUI and describes the tools and workflow used.
*  **Emotion:** The emotional tone is primarily neutral.
*  **Top 3 Points of View:**
    *   The animation was created using ComfyUI and Resolve.
    *   Images were created in FLUX.
    *   The user provided a link to the official ComfyUI documentation.

**[Parallel universes (Score: 9)](https://v.redd.it/jada64ah6itf1)**
*  **Summary:** The post showcases AI-generated images of parallel universes, created using different checkpoints and negative prompts.
*  **Emotion:** The overall emotional tone is positive.
*  **Top 2 Points of View:**
    *   Some images were generated using different checkpoints and negative prompts.
    *   The middle image is considered "really cool".

**[Tinkering on a sandbox for real-time interactive generation starting with LongLive-1.3B (Score: 5)](https://v.redd.it/zcjhbpk7xitf1)**
*  **Summary:** A user is working on a sandbox for real-time interactive image generation. A commenter suggests binding keys to prompt adjustments for a more interactive experience.
*  **Emotion:** The overall emotional tone is positive.
*  **Top 1 Points of View:**
    *   The suggestion is to have the user bind keys to specific prompts for real-time control.

**[Qwen doesn't do it. Kontext doesn't do it. What do we have that takes "person A" and puts them in "scene B"? (Score: 3)](https://www.reddit.com/r/StableDiffusion/comments/1nzs18o/qwen_doesnt_do_it_kontext_doesnt_do_it_what_do_we/)**
*  **Summary:** Users are seeking methods for combining a person from one image and placing them into another scene, discussing the limitations of Qwen and Kontext, and suggesting alternative workflows.
*  **Emotion:** The emotional tone is mostly neutral, with some negative sentiment from users who have had unsuccessful attempts.
*  **Top 3 Points of View:**
    *   Qwen image edit 2509 can take 3 images and integrate them, and there is a workflow template in ComfyUI.
    *   The default Qwen Image Edit 2509 workflow can do this. Templates->Image->Qwen Image Edit 2509.
    *   Some users have had unsuccessful attempts.

**[Wan Animate only supports one person (Score: 3)](https://www.reddit.com/r/StableDiffusion/comments/1nzs1to/wan_animate_only_supports_one_person/)**
*  **Summary:** A user is inquiring about the limitations of Wan Animate in handling multiple subjects, with another user suggesting workarounds.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 1 Points of View:**
    *   If they are interacting, do them separately.

**[Fairy Tail - Fan animation - Wan and Chatterbox/Xtts-v2 (Score: 2)](https://v.redd.it/ixxc603q0itf1)**
*  **Summary:** A user shares a "Fairy Tail" fan animation created using Wan and Chatterbox/Xtts-v2, described as a "dangerous watch of the week."
*  **Emotion:** The emotional tone is negative.
*  **Top 1 Points of View:**
    *   It is a "dangerous watch of the week".

**[Wan 2.2 T2V problem - various blemishes and marks on the video (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1nznwlh/wan_22_t2v_problem_various_blemishes_and_marks_on/)**
*  **Summary:** A user is experiencing issues with blemishes and marks when using Wan 2.2 T2V, and other users suggest using the right model or setting.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 1 Points of View:**
    *   You are using the wrong models or settings.

**[How do you create a consistent character to use across videos? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1nzpr3e/how_do_you_create_a_consistent_character_to_use/)**
*  **Summary:** Users are discussing methods for creating consistent characters across videos using Stable Diffusion, including using LoRAs, Flux, Qwen image edit, and Nana Banana.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 2 Points of View:**
    *   Create a Flux or Qwen LoRA. Then use the LORA to generate the image, and use image with WAN2.2 image2vid.
    *   Create a super detailed description prompt of your character or if you want 100% likeness, create a lora and use a i2v model

**[Looking for help with QWEN Image Edit 2509 (Score: 1)](https://i.redd.it/1ib4evaeajtf1.png)**
*  **Summary:** A user is requesting help with QWEN Image Edit 2509. Others suggest using "ImageScaleToTotalPixel" to resize the image or check if the image size is the correct resolution.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 2 Points of View:**
    *   Use "ImageScaleToTotalPixel" to resize your image.
    *   Have you checked the image size?

**[Qwen Edit 2509 unconsistent outputs (HEEEELP) (Score: 1)](https://www.reddit.com/gallery/1nzrdvy)**
*  **Summary:** A user is experiencing inconsistent outputs with Qwen Edit 2509 and seeking help. Suggested solutions include using a "canny" controlnet, using the first Qwen edit, or using a Qwen 2509 LoRA to transform artsy images into realistic ones.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Happens when you use the image on the "TextEncodeQwenEditPlus" but you dont enter the latent into the Reference Latent.
    *   If you want a realistic version of the image, you can use 'canny' controlnet between the image and the text encode.
    *   The first Qwen edit is better for these kinds of tasks (transforming colored drawings into other styles).

**[Help we moving from A1111-forge to ComfyUI (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1nzo5u5/help_we_moving_from_a1111forge_to_comfyui/)**
*  **Summary:** A user is requesting assistance with transitioning from A1111-forge to ComfyUI, particularly regarding upscaling, FluxGuidance nodes, and LoRA implementation.
*  **Emotion:** The emotional tone is mixed, with some negative sentiment due to the challenges of upscaling with basic nodes.
*  **Top 3 Points of View:**
    *   In the second pass, you upscale either pixels with ESRGAN/similar models or latents and do img2img.
    *   Upscaling with the basic nodes is annoying, so you need to add a resize afterwards.
    *   To adjust the weight, you just change the "Strength model".

**[16GB VRAM and qwen_image_edit_2509? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1nzs7um/16gb_vram_and_qwen_image_edit_2509/)**
*  **Summary:** Users are sharing their experiences using qwen_image_edit_2509 with 16GB VRAM cards, discussing potential issues and alternative versions like the Nunchaku version.
*  **Emotion:** The emotional tone is neutral, with some positive sentiment from users who have had success.
*  **Top 3 Points of View:**
    *   I use the f8 version with my 5060ti 16gb VRAM and 48 gb RAM with no issues.
    *   RTX 4080 Super, 16GB, No problem with qwen\_image\_edit\_2509\_fp8\_e4m3fn.safetensors.
    *   Nunchaku version of Nunchaku Qwen Edit 2509 doesn't have LORA support yet.

**[Choose 1, 2 or 3? and can you tell me why you don't like the other 2? (Score: 0)](https://v.redd.it/xl9uc0x3nitf1)**
*  **Summary:** Users are providing feedback on different video options, focusing on voice quality, lip sync accuracy, and the presence of generative artifacts.
*  **Emotion:** The emotional tone is negative, with users expressing dissatisfaction with the voice quality and lip sync.
*  **Top 3 Points of View:**
    *   The voice over is so bad.
    *   All three *** equally because the voice in the audio is a nasal, ageless, neutral, corporate sounding voice that doesn't fit any human.
    *   Lips don't match the speech.

**[Would it be a good idea creating a Stable Diffusion Challenge Subreddit? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1nzm1vy/would_it_be_a_good_idea_creating_a_stable/)**
*  **Summary:** A user is suggesting creating a Stable Diffusion challenge subreddit, and other users are discussing existing alternatives like Civitai bounties, Discord servers, and the challenges of maintaining engagement on Reddit.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Photoshop battles but Ai?
    *   Civitai has a bounties system and regular site-wide challenges.
    *   Reddit format isn't conducive to making people engage in that way.

**[help (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1nzm5o0/help/)**
*  **Summary:** A user is requesting help. The other users suggests using ComfyUI with Flux dev/krea or Qwen.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 2 Points of View:**
    *   Yes, Flux can make *** in cars.
    *   Yes comfyui with a model like Flux dev/krea, or Qwen, combined with a character or style Lora that emulates the amateur look.

**[[Paid job] Looking for a ForgeUI expert to help with game asset creation (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1nzmxxo/paid_job_looking_for_a_forgeui_expert_to_help/)**
*  **Summary:** A user is looking for a ForgeUI expert to help with game asset creation, and other users are recommending ComfyUI and linking to relevant resources.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   I send you a DM.
    *   A professional would be much more likey to use ComfyUI for generation as it is the industry standard.
    *   I recommend you check out [this post](https://old.reddit.com/r/StableDiffusion/comments/1nsrx4l/vnccs_visual_novel_character_creation_suite/).

**[Best AI coding Agent Opensource/Free for coding? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1nzpvxu/best_ai_coding_agent_opensourcefree_for_coding/)**
*  **Summary:** Users are discussing various open-source and free AI coding agents, comparing their capabilities, resource requirements, and integration with IDEs.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Cline VS Code extension and gpt oss 120b.
    *   r/LocalLlama
    *   CursorAI as the IDE, and chatGPT 5 as the agent.
