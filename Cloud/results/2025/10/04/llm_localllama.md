---
title: "LocalLLaMA Subreddit"
date: "2025-10-04"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "AI", "Local Models"]
---

# Overall Ranking and Top Discussions
1.  [GPT-1 Thinking 2.6m coming soon](https://i.redd.it/2ln0mw87m4tf1.png) (Score: 245)
    *   A new GPT-1 Thinking model with 2.6 million parameters is announced.
2.  Why are AI labs in China not focused on creating new search engines? (Score: 143)
    *   Discussion on why Chinese AI labs aren't developing search engines, focusing on the prevalence of super apps and different internet ecosystems in China.
3.  [gpt-oss 120B is running at 20t/s with $500 AMD M780 iGPU mini PC and 96GB DDR5 RAM](https://www.reddit.com/r/LocalLLaMA/comments/1nxztlx/gptoss_120b_is_running_at_20ts_with_500_amd_m780/) (Score: 76)
    *   Users discuss running gpt-oss 120B at 20 tokens/second using a mini PC with an AMD iGPU, exploring hardware configurations and performance.
4.  [My mildly janky setup](https://www.reddit.com/gallery/1nxx9r9) (Score: 40)
    *   A user showcases a "janky" hardware setup involving an Acer laptop, external GPU, and PSU, leading to discussion and appreciation of unconventional builds.
5.  Open source text-to-image Hunyuan 3.0 by Tencent is now #1 in LMArena, Beating proprietary models like Nano Banana and SeeDream 4 for the first time (Score: 24)
    *   Tencent's open-source text-to-image model, Hunyuan 3.0, tops the LMArena rankings, sparking discussion and skepticism about the ranking system.
6.  New Build for local LLM (Score: 16)
    *   A user shares a new local LLM build, prompting questions about performance, cost, and potential issues like airflow.
7.  Qwen3-VL-30B-A3B-Instruct & Thinking are here! (Score: 12)
    *   Announcement and discussion about the availability of the Qwen3-VL-30B-A3B-Instruct & Thinking model, with users anticipating GGUF versions.
8.  [My janky way of getting 2 GPUs into my rig](https://www.reddit.com/gallery/1nxyw0a) (Score: 8)
    *   A user showcases a setup for running two GPUs, prompting positive reactions regarding its ingenuity.
9.  HP Launches ZGX Nano G1n AI Workstation, Powered By NVIDIA's GB10 Superchip (Score: 4)
    *   Discussion of the HP ZGX Nano G1n AI workstation, including its specifications, potential performance, and comparisons to alternatives like Macbooks.
10. [What are the best models for legal work in Oct 2025?](https://www.reddit.com/r/LocalLLaMA/comments/1nxyfwr/what_are_the_best_models_for_legal_work_in_oct/) (Score: 3)
    *   A user asks about the best LLMs for legal work, leading to recommendations, insights, and discussions about the accuracy of LLMs in the legal field.
11. [Any resources on how to prepare data for fine tuning?](https://www.reddit.com/r/LocalLLaMA/comments/1ny1z91/any_resources_on_how_to_prepare_data_for_fine/) (Score: 3)
    *   A user seeks resources on preparing data for fine-tuning LLMs, and another user recommends Unsloth's documentation.
12. Which Open-Source / Local LLMs work best for Offensive Security? + What Hardware Setup Is Realistic? (Score: 2)
    *   A user asks about LLMs best suited for offensive security, but some are questioning its intentions and encourages the user to test on their own.
13. [What model do you think this website uses?](https://www.reddit.com/r/LocalLLaMA/comments/1nxzaw4/what_model_do_you_think_this_website_uses/) (Score: 2)
    *   Users speculate about the model used by a website, with one commenter suggesting the possibility of the website training its own models given its funding.
14. [Any good local alternatives to Claude?](https://www.reddit.com/r/LocalLLaMA/comments/1ny2ot3/any_good_local_alternatives_to_claude/) (Score: 2)
    *   A user seeks local alternatives to Claude, with recommendations including GLM4.6, Qwen3-Coder-30B-A3B-Instruct, and gpt-oss.
15. [Any quality ios chat with custom models?](https://www.reddit.com/r/LocalLLaMA/comments/1ny1ess/any_quality_ios_chat_with_custom_models/) (Score: 1)
    *   A user asks about iOS chat apps that support custom models, with the response noting limitations in the iOS ecosystem and suggesting web-based interfaces or paid options.
16. Gemini 3.0 & Deepseek R2 (Score: 1)
    *   Discussion surrounding the upcoming Gemini 3.0 and Deepseek R2 models, with some reservations towards the previous Deepseek models.
17. [What are some repetitive text patterns you see a lot from your AI?](https://www.reddit.com/r/LocalLLaMA/comments/1ny1wzr/what_are_some_repetitive_text_patterns_you_see_a/) (Score: 0)
    *   Users share repetitive text patterns they've observed in AI-generated text, including lists, repeated names, m-dashes, emojis, and specific phrases.

# Detailed Analysis by Thread
**[GPT-1 Thinking 2.6m coming soon (Score: 245)](https://i.redd.it/2ln0mw87m4tf1.png)**
*  **Summary:**  The post announces the upcoming release of GPT-1 Thinking 2.6m.
*  **Emotion:** The overall emotional tone is neutral, with elements of positivity.
*  **Top 3 Points of View:**
    *   Excitement about the new model and preparing GPUs to run it.
    *   The model looks promising.
    *   Question about when GGUF version will be released.

**[Why are AI labs in China not focused on creating new search engines? (Score: 143)](https://i.redd.it/4glawt4k84tf1.jpeg)**
*  **Summary:**  This thread discusses the reasons why AI labs in China might not be focused on creating new search engines, considering the existing dominance of super apps and the fragmented nature of the Chinese internet ecosystem.
*  **Emotion:** The emotional tone is predominantly neutral, with some tinges of negativity.
*  **Top 3 Points of View:**
    *   The Chinese web ecosystem is made up of silos within Chinese Big Tech, rendering traditional search engines less useful.
    *   People in China prefer searching within super apps like WeChat, Alipay, and Douyin, making web search less relevant.
    *   Tech funding operates on hype, and currently, AI is prioritized over search engines.

**[gpt-oss 120B is running at 20t/s with $500 AMD M780 iGPU mini PC and 96GB DDR5 RAM (Score: 76)](https://www.reddit.com/r/LocalLLaMA/comments/1nxztlx/gptoss_120b_is_running_at_20ts_with_500_amd_m780/)**
*  **Summary:**  The thread discusses the performance of running the gpt-oss 120B model on a mini PC with an AMD iGPU, focusing on token generation speed and hardware configurations.
*  **Emotion:** The overall emotion is positive and neutral, with enthusiastic engagement from users.
*  **Top 3 Points of View:**
    *   The iGPU is mostly improving pp performance, but tg is still limited by the memory bandwidth speed.
    *   The performance of running the GPT-OSS-120b and Qwen 235b model on the same platform.
    *   The speed is 11 tokens/ s with mini pc ryzen 7940hs, 780M and 64 GB 5600 mhz ddr5.

**[My mildly janky setup (Score: 40)](https://www.reddit.com/gallery/1nxx9r9)**
*  **Summary:**  A user shares their "janky" setup involving a laptop with an external GPU and power supply, highlighting unconventional hardware configurations.
*  **Emotion:** The overall tone is positive and neutral, appreciating the creativity.
*  **Top 3 Points of View:**
    *   Appreciation for the janky, unconventional hardware setup.
    *   Comparison on how cheap is the build.
    *   Describing the components such as 6-year old Acer with Athlon 300U, 32GB DDR4.

**[Open source text-to-image Hunyuan 3.0 by Tencent is now #1 in LMArena, Beating proprietary models like Nano Banana and SeeDream 4 for the first time (Score: 24)](https://i.redd.it/whxcmf68r4tf1.jpeg)**
*  **Summary:**  Tencent's Hunyuan 3.0 tops the LMArena rankings for text-to-image models, generating discussion and skepticism.
*  **Emotion:** The tone is predominantly neutral, with a few negative comments questioning the LMArena ranking system.
*  **Top 3 Points of View:**
    *   Questioning LMArena ranking system.
    *   The image quality is not the best.
    *   Acknowledge that it is a Huge W.

**[New Build for local LLM (Score: 16)](https://i.redd.it/3xz2zcko95tf1.png)**
*  **Summary:**  A user showcases their new local LLM build, sparking discussion about performance, cost, and potential cooling issues.
*  **Emotion:** The overall emotion is neutral with the focus on specs and potential problems.
*  **Top 3 Points of View:**
    *   Questions on how much did the build cost and how is the heat management.
    *   Suggesting a surge protector to protect the $60K investment.
    *   Questioning why it is in the office because it can be too loud and hot to place near the body.

**[Qwen3-VL-30B-A3B-Instruct & Thinking are here! (Score: 12)](https://i.redd.it/bx7mh9pr35tf1.png)**
*  **Summary:**  The thread announces the release of Qwen3-VL-30B-A3B-Instruct & Thinking and users are waiting for the GGUF version.
*  **Emotion:** The overall tone is anticipation and positive.
*  **Top 3 Points of View:**
    *   Waiting for GGUF version.
    *   Open llms are the best soft power strategy china has implemented so far.
    *    The model Qwen3-VL-30B is being updated. Mistral 3.2 24B is the local model to beat IMO for VL.

**[My janky way of getting 2 GPUs into my rig (Score: 8)](https://www.reddit.com/gallery/1nxyw0a)**
*  **Summary:**  A user displays a creative setup for running two GPUs.
*  **Emotion:** The overall tone is positive, appreciating the user's setup.
*  **Top 3 Points of View:**
    *   Positive reaction to the janky but functional setup.
    *   Someone says they took the coward way and bought a 1200w PSU.
    *   Comment on Cheetos on the floor.

**[HP Launches ZGX Nano G1n AI Workstation, Powered By NVIDIA's GB10 Superchip (Score: 4)](https://wccftech.com/hp-launches-zgx-nano-g1n-ai-workstation-powered-by-nvidias-gb10-superchip/)**
*  **Summary:**  This thread discusses the HP ZGX Nano G1n AI workstation, its specifications, potential performance, and comparisons to alternatives.
*  **Emotion:** The overall tone is neutral with some negative sentiments regarding the workstation's performance.
*  **Top 3 Points of View:**
    *   The workstation is practically dead on arrival with that 273gbps memory bandwidth and a macbook with an M3 Max could outperform this for cheaper.
    *   Nvidia's marketing is better than their products these days.
    *   The workstation is just a cut down blackwell in an soc package with some lpddr5 memory and a few arm cores.

**[What are the best models for legal work in Oct 2025? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1nxyfwr/what_are_the_best_models_for_legal_work_in_oct/)**
*  **Summary:**  A user asks for the best models for legal work.
*  **Emotion:** The tone is neutral, with some leaning towards positive.
*  **Top 3 Points of View:**
    *   It has been suggested that IBM claims the data is curated and copyright-free so whatever law firm you're working for won't be in trouble for copyright infringement with Granite-4.
    *   Llm is used for legal/tax analysis, analysis of law and research in documents. I get best results with Gemma 27b, Qwq 3 32b, glm air 4.5, qwen 3 235b and bytedance Seed 36b
    *   It has been suggested that Llama3.3 70B, Llama3.1 405B, and even a sparse MoE like gpt-oss-120B do better here.

**[Any resources on how to prepare data for fine tuning? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1ny1z91/any_resources_on_how_to_prepare_data_for_fine/)**
*  **Summary:**  User requests resources for data preparation in fine-tuning.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Unsloth documentation is a good source to reference.

**[Which Open-Source / Local LLMs work best for Offensive Security? + What Hardware Setup Is Realistic? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1nxwykv/which_opensource_local_llms_work_best_for/)**
*  **Summary:** The thread asks about LLMs for offensive security.
*  **Emotion:** Positive, but some skepticism on the intent
*  **Top 3 Points of View:**
    *   The original poster's plan sounds great.
    *   If you work in this field, you should experiment and find out yourself.

**[What model do you think this website uses? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1nxzaw4/what_model_do_you_think_this_website_uses/)**
*  **Summary:** The thread discusses what model a website uses.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   They may have trained their own model, as they raised $125 million in 2024.

**[Any good local alternatives to Claude? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1ny2ot3/any_good_local_alternatives_to_claude/)**
*  **Summary:** A user seeks local alternatives to Claude.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   If you get a 128gb kit you can do glm 4.6.
    *   For non-complex, gpt-oss is great.
    *   Qwen3-Coder-30B-A3B-Instruct can be relatively fast and effective.

**[Any quality ios chat with custom models? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1ny1ess/any_quality_ios_chat_with_custom_models/)**
*  **Summary:** User asks about iOS chat apps that support custom models.
*  **Emotion:** Negative
*  **Top 3 Points of View:**
    *   The ios app ecosystem for local LLMs is limited.

**[Gemini 3.0 & Deepseek R2 (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1ny33x8/gemini_30_deepseek_r2/)**
*  **Summary:** Discussion surrounding the upcoming Gemini 3.0 and Deepseek R2 models.
*  **Emotion:** Negative
*  **Top 3 Points of View:**
    *   Looking forward to Gemini 3.0 and Deepseek R2.
    *   gemini 3 will be good because of its tps and price
    *   With how dissapointing the last deepseek models were, I don't think deepseek is the king of open source models. GLM is now the king imo.

**[What are some repetitive text patterns you see a lot from your AI? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1ny1wzr/what_are_some_repetitive_text_patterns_you_see_a/)**
*  **Summary:** This thread is about repetitive patterns found in AI text generation.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Lists.
    *   Repeated names.
    *   M-dashes.
