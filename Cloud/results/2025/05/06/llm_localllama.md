---
title: "LocalLLaMA Subreddit"
date: "2025-05-06"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "Local AI", "AI"]
---

# Overall Ranking and Top Discussions
1.  [New SOTA music generation model](https://v.redd.it/gf0uynfhz6ze1) (Score: 334)
    * Discusses the release of a new, potentially groundbreaking, open-source music generation model and its capabilities.
2.  [Nvidia to drop CUDA support for Maxwell, Pascal, and Volta GPUs with the next major Toolkit release](https://www.reddit.com/r/LocalLLaMA/comments/1kg7768/nvidia_to_drop_cuda_support_for_maxwell_pascal/) (Score: 73)
    *  Concerns Nvidia's decision to end CUDA support for older GPU architectures, prompting discussions about hardware upgrades and alternative solutions.
3.  [How long before we start seeing ads intentionally shoved into LLM training data?](https://www.reddit.com/r/LocalLLaMA/comments/1kg9mjs/how_long_before_we_start_seeing_ads_intentionally/) (Score: 50)
    * Raises concerns about the potential for injecting advertisements and biases into LLM training data and the implications for the integrity of AI models.
4.  [Running Qwen3-235B-A22B, and LLama 4 Maverick locally at the same time on a 6x RTX 3090 Epyc system. Qwen runs at 25 tokens/second on 5x GPU. Maverick runs at 20 tokens/second on one GPU, and CPU.](https://youtu.be/36pDNgBSktY) (Score: 28)
    *  Shows off and discusses the performance of running large language models (LLMs) locally on a high-end, multi-GPU system.
5.  [The real reason OpenAI bought WindSurf](https://i.redd.it/knqgtodvs7ze1.jpeg) (Score: 6)
    *  Speculates about the motivations behind OpenAI's acquisition of WindSurf, focusing on data collection and vertical integration.
6.  [Audio transcribe options?](https://www.reddit.com/r/LocalLLaMA/comments/1kga4m9/audio_transcribe_options/) (Score: 3)
    *  Asks for recommendations on audio transcription tools and shares experiences with Whisper and faster-whisper.
7.  [Base vs Instruct for embedding models. What's the difference?](https://www.reddit.com/r/LocalLLaMA/comments/1kg7zsb/base_vs_instruct_for_embedding_models_whats_the/) (Score: 2)
    *  Discusses the differences between base and instruct embedding models, particularly in the context of custom embedding workflows.
8.  [How to share compute accross different machines?](https://www.reddit.com/r/LocalLLaMA/comments/1kg7rkp/how_to_share_compute_accross_different_machines/) (Score: 1)
    * Seeks advice on how to share computing resources across different machines with varying architectures for LLM tasks.
9.  [I have 4x3090, what is the cheapest options to create a local LLM?](https://www.reddit.com/r/LocalLLaMA/comments/1kg94o2/i_have_4x3090_what_is_the_cheapest_options_to/) (Score: 1)
    *  Asks for recommendations on the most cost-effective hardware setup for creating a local LLM using a 4x3090 GPU configuration.
10. [Best model to run on a homelab machine on ollama](https://www.reddit.com/r/LocalLLaMA/comments/1kga99k/best_model_to_run_on_a_homelab_machine_on_ollama/) (Score: 1)
    * Inquires about the best LLM model to run on a home lab machine using Ollama, considering specific tasks like development assistance or summarization.
11. [i dont think from now we should considered the claude in the ai race . there valuation is going to be down no doubt . there will be no legacy bcz its never started . they just relevant in the last year this year they will be vanished in the year nobody will ever know there name](https://i.redd.it/6ls7tz2ql6ze1.png) (Score: 0)
    * Shares an opinion that Claude will soon become irrelevant in the AI landscape.
12. [AGI current progress and when it will be achieved 100%](https://www.reddit.com/gallery/1kgbpes) (Score: 0)
    *  Presents an optimistic view on the progress toward achieving AGI, based on ChatGPT's estimates.
13. [Local VLM for Chart/Image Analysis and understanding on base M3 Ultra? Qwen 2.5 & Gemma 27B Not Cutting It.](https://www.reddit.com/r/LocalLLaMA/comments/1kg80ps/local_vlm_for_chartimage_analysis_and/) (Score: 0)
    * Seeks recommendations for a local VLM (Vision Language Model) capable of chart and image analysis, particularly on an M3 Ultra Mac.
14. [What are the main use cases for smaller models?](https://www.reddit.com/r/LocalLLaMA/comments/1kg8u6m/what_are_the_main_use_cases_for_smaller_models/) (Score: 0)
    * Explores potential use cases for smaller LLMs, including offline work, home/business applications, and accessibility for users with limited hardware.
15. [Not happy with ~32B models. What's the minimum size of an LLM to be truly useful for engineering tasks?](https://www.reddit.com/r/LocalLLaMA/comments/1kgbolo/not_happy_with_32b_models_whats_the_minimum_size/) (Score: 0)
    * Asks about the minimum LLM size to be truly useful for engineering tasks, particularly complex, multi-faceted problems.
16. [Still build your own RAG eval system in 2025?](https://www.reddit.com/r/LocalLLaMA/comments/1kgbqbo/still_build_your_own_rag_eval_system_in_2025/) (Score: 0)
    *  Asks if it's still necessary to build your own Retrieval-Augmented Generation (RAG) evaluation system.
17. [Recently saved an MSI Trident 3 from the local eWaste facility. Looking for ideas?](https://www.reddit.com/r/LocalLLaMA/comments/1kgce5w/recently_saved_an_msi_trident_3_from_the_local/) (Score: 0)
    * Solicits suggestions for uses for a recovered MSI Trident 3 system, focusing on local LLM applications.
18. [From my local FB Marketplace...](https://www.reddit.com/r/LocalLLaMA/comments/1kgcigi/from_my_local_fb_marketplace/) (Score: 0)
    *  Shares a listing from Facebook Marketplace and speculates on the hardware specifications.

# Detailed Analysis by Thread
**[[D] New SOTA music generation model (Score: 334)](https://v.redd.it/gf0uynfhz6ze1)**
*   **Summary:** The discussion revolves around a new open-source music generation model. Users are sharing their experiences with the model, discussing its strengths and weaknesses, and comparing it to existing solutions like Suno and Udio. Key features like Apache licensing, LORA support, and hardware requirements are also being discussed.
*   **Emotion:** The overall emotional tone is primarily **Positive**. There's excitement about the potential of the new model, its open-source nature, and its flexibility. While some users point out areas for improvement (e.g., vocal quality), the general sentiment is optimistic.
*   **Top 3 Points of View:**
    *   The model represents a significant advancement for open-source music generation.
    *   The instrumentals are impressive, but the vocal generation needs improvement.
    *   LORA support and the Apache license are key advantages for the community.

**[Nvidia to drop CUDA support for Maxwell, Pascal, and Volta GPUs with the next major Toolkit release (Score: 73)](https://www.reddit.com/r/LocalLLaMA/comments/1kg7768/nvidia_to_drop_cuda_support_for_maxwell_pascal/)**
*   **Summary:** This thread discusses Nvidia's decision to discontinue CUDA support for Maxwell, Pascal, and Volta GPUs in the upcoming Toolkit release. Users are expressing concerns about the impact on older hardware, discussing upgrade strategies, and exploring alternative solutions like Vulkan and ROCm.
*   **Emotion:** The overall emotional tone is mixed, but leaning towards **Neutral**. There's some disappointment and frustration with Nvidia's decision, balanced by pragmatic discussions about adapting to the change. Some users are hopeful for discounted hardware as a result.
*   **Top 3 Points of View:**
    *   Nvidia's decision will force upgrades for users relying on older GPUs.
    *   The end of CUDA support will create challenges for maintaining older systems.
    *   Some users are looking forward to potentially cheaper Volta cards.

**[How long before we start seeing ads intentionally shoved into LLM training data? (Score: 50)](https://www.reddit.com/r/LocalLLaMA/comments/1kg9mjs/how_long_before_we_start_seeing_ads_intentionally/)**
*   **Summary:** The discussion centers on the increasing likelihood of advertisements being intentionally injected into LLM training data. Users are discussing potential methods of ad injection, the implications for model integrity, and the possibility of biased or manipulative outputs. The conversation also touches on the ethical considerations of using LLMs for advertising and persuasion.
*   **Emotion:** The overall emotional tone is a mix of **Neutral** and **Negative**. There's a sense of unease and concern about the potential for LLMs to be manipulated for advertising purposes. Users are also wary of more insidious forms of bias being introduced into the models.
*   **Top 3 Points of View:**
    *   Ad injection into LLM training data is becoming increasingly likely.
    *   This practice raises ethical concerns about bias and manipulation.
    *   There is a need for safeguards to prevent the insertion of ads and biases into LLMs.

**[Running Qwen3-235B-A22B, and LLama 4 Maverick locally at the same time on a 6x RTX 3090 Epyc system. Qwen runs at 25 tokens/second on 5x GPU. Maverick runs at 20 tokens/second on one GPU, and CPU. (Score: 28)](https://youtu.be/36pDNgBSktY)**
*   **Summary:** This thread documents the performance of running Qwen3-235B-A22B and LLama 4 Maverick models locally on a high-end system equipped with multiple RTX 3090 GPUs. The discussion focuses on the achieved tokens-per-second rates and the hardware configurations used. Users also discuss quantization methods and their impact on performance.
*   **Emotion:** The emotional tone is largely **Neutral** and informative. The discussion revolves around technical details and performance metrics, with users sharing their own experiences and hardware setups.
*   **Top 3 Points of View:**
    *   It is feasible to run very large language models locally with sufficient hardware resources.
    *   Different quantization methods can significantly impact performance.
    *   Shared expert configurations are a desirable feature.

**[The real reason OpenAI bought WindSurf (Score: 6)](https://i.redd.it/knqgtodvs7ze1.jpeg)**
*   **Summary:** The thread discusses the possible reasons behind OpenAI's acquisition of WindSurf. The main themes revolve around data acquisition, vertical integration, and OpenAI's overall strategy in the AI market. Some users suggest that the acquisition indicates a focus on near-term revenue generation rather than a pure pursuit of AGI.
*   **Emotion:** The emotional tone is primarily **Neutral**. Users are engaging in speculative discussions about OpenAI's motivations, with a focus on strategic analysis rather than strong emotional reactions.
*   **Top 3 Points of View:**
    *   OpenAI acquired WindSurf primarily for its data assets and code data.
    *   The acquisition suggests a need for OpenAI to generate revenue in the short term.
    *   Zapier would have been a more strategic acquisition for OpenAI.

**[Audio transcribe options? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1kga4m9/audio_transcribe_options/)**
*   **Summary:** This thread is a straightforward request for recommendations on audio transcription tools. Users are sharing their experiences with different options, primarily focusing on Whisper and faster-whisper.
*   **Emotion:** The emotional tone is largely **Neutral** and helpful. Users are simply providing suggestions and sharing their experiences with different tools.
*   **Top 3 Points of View:**
    *   Whisper is a viable option for audio transcription.
    *   faster-whisper is worth exploring for potentially improved performance.
    *   Success has been seen with Whisper on larger audio files.

**[Base vs Instruct for embedding models. What's the difference? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1kg7zsb/base_vs_instruct_for_embedding_models_whats_the/)**
*   **Summary:** The thread explores the distinctions between base and instruct embedding models. The discussion emphasizes that "instruct" models are designed to generate text embeddings tailored to specific tasks without requiring additional training.
*   **Emotion:** The emotional tone is predominantly **Neutral**, characterized by an informational exchange.
*   **Top 3 Points of View:**
    *   "Instruct" models are designed to generate embeddings without further training.
    *   "Instruct" models are only useful if already using custom embedding workflows.
    *   "Instruct" models may simplify specialized embedding models into one.

**[How to share compute accross different machines? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1kg7rkp/how_to_share_compute_accross_different_machines/)**
*   **Summary:** The thread seeks advice on sharing compute resources across machines with different architectures, specifically a Mac, Nvidia GPU, and Intel Arc GPU.
*   **Emotion:** The emotional tone is primarily **Neutral**, characterized by an informational request and potential solutions.
*   **Top 3 Points of View:**
    *   The best option is to sell some equipment and rebuild for uniform architecture.
    *   vLLM supports distributed inference, but the execution environment must be the same.
    *   Llama rpc server can be used, but the slower GPUs may not contribute much.

**[I have 4x3090, what is the cheapest options to create a local LLM? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1kg94o2/i_have_4x3090_what_is_the_cheapest_options_to/)**
*   **Summary:** This thread is asking about the cheapest options to create a local LLM, given the user already has 4x3090 GPUs.  Options discussed include motherboards and CPUs.
*   **Emotion:** The overall emotion is **Positive** as users are helping the original poster find a solution and are giving recommendations.
*   **Top 3 Points of View:**
    *   An X399 motherboard and CPU are cheaper, but DDR4 memory is expensive.
    *   An Epyc motherboard is more expensive, but the CPU and ECC RDIMM memory are much cheaper.
    *   For inferencing, PCIe lanes don't matter that much, so an old mining rig could work.

**[Best model to run on a homelab machine on ollama (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1kga99k/best_model_to_run_on_a_homelab_machine_on_ollama/)**
*   **Summary:** The thread asks what the best model is to run on a homelab machine on Ollama.  There is no discussion about the intended use case of the model.
*   **Emotion:** The overall emotion is **Neutral** as posters are mainly asking clarifying questions to better understand how to help.
*   **Top 3 Points of View:**
    *   A homelab could try Qwen3-30b-a3b-q8 as it recently had an update.
    *   The response depends on the need (Development assistant? Summarization? Baby monitor?).

**[i dont think from now we should considered the claude in the ai race . there valuation is going to be down no doubt . there will be no legacy bcz its never started . they just relevant in the last year this year they will be vanished in the year nobody will ever know there name (Score: 0)](https://i.redd.it/6ls7tz2ql6ze1.png)**
*   **Summary:** The original poster believes Claude's valuation is going down and that they will be irrelevant.  Other posters disagree and/or criticize the post itself.
*   **Emotion:** The overall emotion is **Neutral** to **Negative** as posters mainly disagree with the original statement or critique its formatting and content.
*   **Top 3 Points of View:**
    *   The poster believes Claude's AI will be forgotten soon.
    *   Others believe benchmarks can't be trusted.
    *   Others are still crying about limited usage with Claude.

**[AGI current progress and when it will be achieved 100% (Score: 0)](https://www.reddit.com/gallery/1kgbpes)**
*   **Summary:** The original poster shares a ChatGPT conversation in which the estimated date for AGI is between 2100-2300.
*   **Emotion:** The overall emotion is **Neutral** as other posters disagree with the estimates and/or critique the methodology to obtain the estimates.
*   **Top 3 Points of View:**
    *   AGI will be achieved between 2100-2300.
    *   "Extrapolating by 2-3 points into 100 years" is not proper data analysis.
    *   There is no global consensus on what is meant by AGI.

**[Local VLM for Chart/Image Analysis and understanding on base M3 Ultra? Qwen 2.5 & Gemma 27B Not Cutting It. (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1kg80ps/local_vlm_for_chartimage_analysis_and/)**
*   **Summary:** The original poster is requesting recommendations for a Local VLM to perform chart/image analysis.
*   **Emotion:** The overall emotion is **Neutral** as other posters are offering suggestions.
*   **Top 3 Points of View:**
    *   Try "Pixtral-Large-Instruct-2411-exl2-5.0bpw."

**[What are the main use cases for smaller models? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1kg8u6m/what_are_the_main_use_cases_for_smaller_models/)**
*   **Summary:** The thread is asking what the main use cases are for smaller models.
*   **Emotion:** The overall emotion is **Positive** and **Neutral** as other posters are offering many recommendations.
*   **Top 3 Points of View:**
    *   Get work done on a plane without Wi-Fi.
    *   Feasibility for any home or business project.
    *   Summarization tasks.

**[Not happy with ~32B models. What's the minimum size of an LLM to be truly useful for engineering tasks? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1kgbolo/not_happy_with_32b_models_whats_the_minimum_size/)**
*   **Summary:** The original poster is not happy with 32B models, and wants to know what the minimum size LLM is to be truly useful for engineering tasks such as designing a solar system.
*   **Emotion:** The overall emotion is **Neutral** as other posters are asking clarifying questions and/or offering recommendations.
*   **Top 3 Points of View:**
    *   Split the objective into smaller tasks handled by multiple agents.
    *   It's not as much about the size of the model as about its internal knowledge and core capabilities.
    *   There is a skill and knowledge gap issue on how to use these LLMs.

**[Still build your own RAG eval system in 2025? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1kgbqbo/still_build_your_own_rag_eval_system_in_2025/)**
*   **Summary:** The original poster asks about building your own RAG eval system in 2025.
*   **Emotion:** The overall emotion is **Positive** as other posters believe RAG is still evolving quickly.
*   **Top 3 Points of View:**
    *   RAGs are still evolving quickly.

**[Recently saved an MSI Trident 3 from the local eWaste facility. Looking for ideas? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1kgce5w/recently_saved_an_msi_trident_3_from_the_local/)**
*   **Summary:** The original poster saved an MSI Trident 3 and is looking for ideas.
*   **Emotion:** The overall emotion is **Neutral** as other posters offer suggestions and opinions.
*   **Top 3 Points of View:**
    *   Qwen 3 30B-A3B is the right model.
    *   A dynamic quant of Qwen3 4B sounds like a good match.
    *   Any 8B model at Q4 will fit.

**[From my local FB Marketplace... (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1kgcigi/from_my_local_fb_marketplace/)**
*   **Summary:** The original poster shares a listing from Facebook Marketplace and speculates on the hardware specifications.
*   **Emotion:** The overall emotion is **Neutral** as other posters are just speculating on the hardware specs.
*   **Top 3 Points of View:**
    *   The specs are going to be a ryzen 7 1st gen with a 1030 with a 250w psu with molex connector.
