---
title: "LocalLLaMA Subreddit"
date: "2025-05-21"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "LocalAI", "OpenSource"]
---

# Overall Ranking and Top Discussions
1.  [Anyone else feel like LLMs aren't actually getting that much better?](https://www.reddit.com/r/LocalLLaMA/comments/1ks1ncf/anyone_else_feel_like_llms_arent_actually_getting/) (Score: 95)
    *  The post discusses the perceived improvements, or lack thereof, in large language models (LLMs) over time.
2.  [Mistral's new Devstral coding model running on a single RTX 4090 with 54k context using Q4KM quantization with vLLM](https://i.redd.it/ddhhql5ap52f1.png) (Score: 75)
    *  The thread focuses on the performance and capabilities of Mistral's new Devstral coding model, especially when running on specific hardware configurations.
3.  [Broke down and bought a Mac Mini - my processes run 5x faster](https://www.reddit.com/r/LocalLLaMA/comments/1ks5sh4/broke_down_and_bought_a_mac_mini_my_processes_run/) (Score: 17)
    *  The post highlights the performance benefits of using a Mac Mini for certain processes.
4.  [Devstral with vision support (from ngxson)](https://www.reddit.com/r/LocalLLaMA/comments/1ks5nul/devstral_with_vision_support_from_ngxson/) (Score: 7)
    *   The post discusses Devstral with vision support.
5.  [Arc pro b60 48gb vram](https://www.reddit.com/r/LocalLLaMA/comments/1ks47wv/arc_pro_b60_48gb_vram/) (Score: 6)
    *   The post discusses the Arc pro b60 48gb vram.
6.  [New to the PC world and want to run a llm locally and need input](https://www.reddit.com/r/LocalLLaMA/comments/1ks0zee/new_to_the_pc_world_and_want_to_run_a_llm_locally/) (Score: 5)
    *   The post requests input for running a LLM locally on a new PC.
7.  [Public ranking for open source models?](https://www.reddit.com/r/LocalLLaMA/comments/1ks2j74/public_ranking_for_open_source_models/) (Score: 4)
    *   The post discusses public rankings for open source models.
8.  [Falcon-H1 by tiiuae.](https://i.redd.it/pqhnjil1x62f1.jpeg) (Score: 3)
    *   The post introduces Falcon-H1 by tiiuae.
9.  [Startups: Collaborative Coding with Windsurf/Cursor](https://www.reddit.com/r/LocalLLaMA/comments/1ks2u4z/startups_collaborative_coding_with_windsurfcursor/) (Score: 2)
    *   The post discusses startups and collaborative coding with Windsurf/Cursor.
10. [Reliable function calling with vLLM](https://www.reddit.com/r/LocalLLaMA/comments/1ks5oxb/reliable_function_calling_with_vllm/) (Score: 2)
    *  The post investigates methods for reliable function calling using vLLM, a library for serving language models.
11. [NVLink On 2x 3090 Question](https://www.reddit.com/r/LocalLLaMA/comments/1ks6236/nvlink_on_2x_3090_question/) (Score: 2)
    *  The post asks about NVLink on 2x 3090.
12. [Bosgame M5 AI Mini PC - $1699 | AMD Ryzen AI Max+ 395, 128gb LPDDR5, and 2TB SSD](https://www.bosgamepc.com/products/bosgame-m5-ai-mini-desktop-ryzen-ai-max-395) (Score: 1)
    *   The post introduces Bosgame M5 AI Mini PC - $1699.
13. [What is tps of qwen3 30ba3b on igpu 780m?](https://www.reddit.com/r/LocalLLaMA/comments/1ks6mlc/what_is_tps_of_qwen3_30ba3b_on_igpu_780m/) (Score: 1)
    *   The post asks what is tps of qwen3 30ba3b on igpu 780m.
14. [Models become so good, companies are selling illusion of a working brain](https://i.redd.it/2uo5fi9e362f1.png) (Score: 0)
    *   The post discusses how models are becoming so good, companies are selling illusion of a working brain.
15. [Perchance RP/RPG story interface for local model?](https://i.redd.it/h8nikytye62f1.jpeg) (Score: 0)
    *   The post discusses Perchance RP/RPG story interface for local model.
16. [Should I add 64gb RAM to my current PC ?](https://www.reddit.com/r/LocalLLaMA/comments/1ks2101/should_i_add_64gb_ram_to_my_current_pc/) (Score: 0)
    *   The post asks if should I add 64gb RAM to my current PC.
17. [ChatGPT’s Impromptu Web Lookups... Can Open Source Compete?](https://www.reddit.com/r/LocalLLaMA/comments/1ks3oi1/chatgpts_impromptu_web_lookups_can_open_source/) (Score: 0)
    *   The post asks if ChatGPT’s Impromptu Web Lookups... Can Open Source Compete?

# Detailed Analysis by Thread
**[Anyone else feel like LLMs aren't actually getting that much better? (Score: 95)](https://www.reddit.com/r/LocalLLaMA/comments/1ks1ncf/anyone_else_feel_like_llms_arent_actually_getting/)**
*  **Summary:** The main question posed in the post is whether large language models are showing significant improvements over time. Some users feel improvements are massive, while others believe progress has plateaued, especially compared to the advancements seen between GPT-3.5 and more recent models. The discussion also includes the impact of quantization and edge devices on LLM performance.
*  **Emotion:** The overall emotional tone is Neutral. There are mixed sentiments.
*  **Top 3 Points of View:**
    *   LLMs have improved significantly, especially when comparing GPT-3.5 to newer models like o3 or Gemini Pro, making daily tasks easier.
    *   LLMs haven't shown much improvement in the past 8 months, but year-on-year improvements are still massive.
    *   Improvements are mainly in quantization and edge devices, allowing GPT4o-level LLMs to run on high-end consumer GPUs.

**[Mistral's new Devstral coding model running on a single RTX 4090 with 54k context using Q4KM quantization with vLLM (Score: 75)](https://i.redd.it/ddhhql5ap52f1.png)**
*  **Summary:** This thread discusses the performance of Mistral's new Devstral coding model when running on an RTX 4090 with specific configurations like Q4KM quantization and vLLM. Users share their experiences and excitement about the model's capabilities in coding tasks, including variable hunting and regex searches.
*  **Emotion:** The overall emotional tone is Positive, with users expressing excitement and happiness about the model's performance.
*  **Top 3 Points of View:**
    *   The Devstral model performs well when quantized, though fine-tuning may be needed to realign tokenization.
    *   Users are excited about the performance of Mistral models, which often exceed benchmarks.
    *   The model is being used for tasks such as hunting misnamed variables and performing REGEX searches on codebases.

**[Broke down and bought a Mac Mini - my processes run 5x faster (Score: 17)](https://www.reddit.com/r/LocalLLaMA/comments/1ks5sh4/broke_down_and_bought_a_mac_mini_my_processes_run/)**
*  **Summary:** The post discusses the user's experience of significantly faster processes after switching to a Mac Mini. Other users inquired about the specific processes and hardware configurations, while some suggested alternative solutions like using a Thunderbolt eGPU or mobile workstation laptops.
*  **Emotion:** The overall emotional tone is Positive, reflecting satisfaction with the Mac Mini's performance.
*  **Top 3 Points of View:**
    *   Mac Mini with 512 GB of unified RAM significantly improves programming and processing speeds.
    *   A Thunderbolt eGPU paired with a Beelink could offer faster performance than a Mac Mini.
    *   The performance difference between Mac Mini and Ryzen 9 AI CPUs is an area of interest, and the OS (macOS vs Windows/WSL) might be a factor.

**[Devstral with vision support (from ngxson) (Score: 7)](https://www.reddit.com/r/LocalLLaMA/comments/1ks5nul/devstral_with_vision_support_from_ngxson/)**
*  **Summary:** This thread briefly discusses the integration of vision support into the Devstral model, possibly using components from Mistral Small.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 2 Points of View:**
    *   The integration of vision support "works".
    *   It may be redundant to duplicate files instead of using the vision encoder from Mistral Small directly.

**[Arc pro b60 48gb vram (Score: 6)](https://www.reddit.com/r/LocalLLaMA/comments/1ks47wv/arc_pro_b60_48gb_vram/)**
*  **Summary:** This thread discusses the specifications and limitations of the Arc pro b60, particularly its 48GB VRAM configuration and memory bandwidth.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   The 48GB VRAM is effectively 2x24GB and not functionally combined, limiting its use for tasks requiring a single large memory space.
    *   Memory bandwidth is considered "meh" (unsatisfactory).
    *   The power draw of the card (400W per dual GPU) could limit the number of cards in a server setup.

**[New to the PC world and want to run a llm locally and need input (Score: 5)](https://www.reddit.com/r/LocalLLaMA/comments/1ks0zee/new_to_the_pc_world_and_want_to_run_a_llm_locally/)**
*  **Summary:** The post is a request for advice from someone new to PC and wanting to run LLMs locally. Users suggest different software (Ollama, LMStudio, OpenWebUI) and models (Mistral Small 3.1 24B, Qwen 30B MoE, Gemma 3 27B) suitable for the user's hardware.
*  **Emotion:** The overall emotional tone is Positive, as users are offering helpful suggestions and recommendations.
*  **Top 3 Points of View:**
    *   Ollama is a great way to get started, especially when used with Docker.
    *   OpenWebUI + Ollama is the easiest way to have web search and file uploads.
    *   LMStudio is highly recommended.

**[Public ranking for open source models? (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1ks2j74/public_ranking_for_open_source_models/)**
*  **Summary:** This thread is a discussion on the availability and potential need for public rankings of open-source LLMs.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 1 Points of View:**
    *   Public rankings already exist on platforms like livebench.ai and lmarena.ai.

**[Falcon-H1 by tiiuae. (Score: 3)](https://i.redd.it/pqhnjil1x62f1.jpeg)**
*  **Summary:** This thread is about a new LLM model, Falcon-H1.
*  **Emotion:** The overall emotional tone is Mixed.
*  **Top 2 Points of View:**
    *   Creating an arbitrary benchmark to show a small increase is seen as a negative sign.
    *   The user is going to test the model to see if it's comparable to Qwen in terms of speed and performance.

**[Startups: Collaborative Coding with Windsurf/Cursor (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1ks2u4z/startups_collaborative_coding_with_windsurfcursor/)**
*  **Summary:** This thread discusses collaborative coding with Windsurf/Cursor in the context of startups, questioning how AI affects the SDLC.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 1 Points of View:**
    *   The impact of AI on how code is built, merged, and shipped shouldn't be significant, with traditional SDLC and version control remaining important.

**[Reliable function calling with vLLM (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1ks5oxb/reliable_function_calling_with_vllm/)**
*  **Summary:** The post discusses reliable function calling with vLLM.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   Using `Do not use in-line tool_call syntax; use only the tool_call array.` in the Qwen system prompt can ensure correct formatting.
    *   Qwen2.5 7B works well for function calling.
    *   Qwen3 with custom template and default parser are also reliable.

**[NVLink On 2x 3090 Question (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1ks6236/nvlink_on_2x_3090_question/)**
*  **Summary:** This thread discusses the requirements and benefits of using NVLink with two 3090 GPUs.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   NVLink's primary benefit is for training and fine-tuning, not inference.
    *   Motherboard requirements are primarily related to slot spacing.
    *   Linux handles NVLink better, whereas Windows may require explicit SLI compatibility on the motherboard.

**[Bosgame M5 AI Mini PC - $1699 | AMD Ryzen AI Max+ 395, 128gb LPDDR5, and 2TB SSD (Score: 1)](https://www.bosgamepc.com/products/bosgame-m5-ai-mini-desktop-ryzen-ai-max-395)**
*  **Summary:** This thread discusses the Bosgame M5 AI Mini PC.
*  **Emotion:** The overall emotional tone is Mixed.
*  **Top 2 Points of View:**
    *   It is interesting when cheaper, entry-level models will appear with a small number of cores but with full-fledged memory.
    *   This says it has 8533mhz ram, which is interesting.

**[What is tps of qwen3 30ba3b on igpu 780m? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1ks6mlc/what_is_tps_of_qwen3_30ba3b_on_igpu_780m/)**
*  **Summary:** This thread discusses the tokens per second (TPS) of qwen3 30ba3b on igpu 780m.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 1 Points of View:**
    *   The poster is getting 19.5 tokens/s of generation speed with a 780m, and llama.cpp. (Qwen3-30B-A3B-Q6_K.gguf)

**[Models become so good, companies are selling illusion of a working brain (Score: 0)](https://i.redd.it/2uo5fi9e362f1.png)**
*  **Summary:** This thread discusses the perception of AI models and the marketing around them, with some questioning the actual intelligence involved.
*  **Emotion:** The overall emotional tone is Mixed.
*  **Top 3 Points of View:**
    *   It would be interesting to see how AI agents would handle clients who have no idea what they want.
    *   Some believe that the LLMs are writing like wow emoji.
    *   Some people are selling smoke and mirrors, and some are selling better smoke and mirrors but aren't advertising it as something that isn't smoke and mirrors.

**[Perchance RP/RPG story interface for local model? (Score: 0)](https://i.redd.it/h8nikytye62f1.jpeg)**
*  **Summary:** This thread is about a user seeking advice on creating a local model interface inspired by Perchance for role-playing and RPG stories.
*  **Emotion:** The overall emotional tone is Positive.
*  **Top 1 Points of View:**
    *   The poster is studying HTML for the first time in my life. I want to clone a portable version that uses local models with the LM Studio server, but I'm new to coding.

**[Should I add 64gb RAM to my current PC ? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1ks2101/should_i_add_64gb_ram_to_my_current_pc/)**
*  **Summary:** This thread discusses whether to add 64GB of RAM to a PC for running LLMs.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   AMD doesn't like 4x RAM, and you can't mix match RAM when putting in 4x.
    *   With 64GB RAM, it'd be easier to load larger models, albeit with slower speeds.
    *   Adding RAM would be better than getting a second GPU.

**[ChatGPT’s Impromptu Web Lookups... Can Open Source Compete? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1ks3oi1/chatgpts_impromptu_web_lookups_can_open_source/)**
*  **Summary:** This thread discusses whether open source can compete with ChatGPT's impromptu web lookups.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   AnythingLLM already has an agent to do this.
    *   Goose + Qwen2.5 7B (or higher) + A search MCP is the best combination for actual tool calling that I've seen.
    *   The poster asks about tool calling.
