---
title: "Machine Learning Subreddit"
date: "2025-05-23"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "research"]
---

# Overall Ranking and Top Discussions
1.  [[R] Tsinghua University, Stanford University, CMU, and Tencent jointly released a benchmark, named RBench-V, for visual reasoning.](https://www.reddit.com/r/MachineLearning/comments/1kte2nu/r_tsinghua_university_stanford_university_cmu_and/) (Score: 78)
    *   The thread discusses a new benchmark for visual reasoning, RBench-V, released by Tsinghua University, Stanford University, CMU, and Tencent.
2.  [What to prepare before starting a ML PhD - 3 months! [D]](https://www.reddit.com/r/MachineLearning/comments/1ktm35z/what_to_prepare_before_starting_a_ml_phd_3_months/) (Score: 27)
    *   The thread discusses what someone should do to prepare for a Machine Learning PhD program in the 3 months before starting.
3.  [[D] Researcher communities like this one?](https://www.reddit.com/r/MachineLearning/comments/1ktg0ey/d_researcher_communities_like_this_one/) (Score: 17)
    *   This thread is a question about research communities for machine learning.
4.  [[N] [D] kumo.ai releases a "Relational Foundation Model", KumoRFM](https://www.reddit.com/r/MachineLearning/comments/1ktmvkl/n_d_kumoai_releases_a_relational_foundation_model/) (Score: 6)
    *   This thread discusses the release of a "Relational Foundation Model", KumoRFM, by kumo.ai.
5.  [Replace Attention mechanism with FAVOR +](https://arxiv.org/pdf/2009.14794) (Score: 5)
    *   The thread discusses replacing the attention mechanism with FAVOR+ in transformers.
6.  [[R] Best Practices for Image Classification Consensus with Large Annotator Teams](https://www.reddit.com/r/MachineLearning/comments/1ktcodg/r_best_practices_for_image_classification/) (Score: 4)
    *   The thread is about best practices for achieving consensus in image classification with large annotator teams.
7.  [[D] Publication advice](https://www.reddit.com/r/MachineLearning/comments/1ktgdf5/d_publication_advice/) (Score: 4)
    *   The thread is seeking publication advice for a potentially non-novel contribution in a low-resource language.
8.  [[D] Challenges in ML for Rare Time Series Events – Looking for insights from others in this space](https://www.reddit.com/r/MachineLearning/comments/1ktahso/d_challenges_in_ml_for_rare_time_series_events/) (Score: 2)
    *   The thread discusses challenges in using ML for rare time series events and seeks insights from others in the field.
9.  [[D] What are the research papers and methods that led to Deepmind’s Veo 3?](https://www.reddit.com/r/MachineLearning/comments/1ktt2ze/d_what_are_the_research_papers_and_methods_that/) (Score: 2)
    *   The thread is asking about the research papers and methods that led to Deepmind's Veo 3.
10. [[P] Football & AI Project](https://www.reddit.com/r/MachineLearning/comments/1kt5bea/p_football_ai_project/) (Score: 1)
    *   This thread is about a Football & AI Project.
11. [[P] Running LLMs on 8× H100s… but sometimes you have to let AI be the artist too](https://www.reddit.com/gallery/1ktdryq) (Score: 0)
    *   The thread is about running LLMs on 8x H100s.

# Detailed Analysis by Thread
**[[R] Tsinghua University, Stanford University, CMU, and Tencent jointly released a benchmark, named RBench-V, for visual reasoning. (Score: 78)](https://www.reddit.com/r/MachineLearning/comments/1kte2nu/r_tsinghua_university_stanford_university_cmu_and/)**
*   **Summary:** This thread discusses a new visual reasoning benchmark, RBench-V, released by Tsinghua University, Stanford University, CMU, and Tencent. Users discuss the difficulty of the benchmark, compare it to GPT-4o's abilities, and question the performance of human experts on the task.
*   **Emotion:** The overall emotional tone is neutral, with some positivity related to the release of the benchmark and some skepticism regarding its difficulty and the reported performance of models and humans.
*   **Top 3 Points of View:**
    *   The benchmark is a valuable contribution to the field.
    *   The benchmark is surprisingly difficult, even for current state-of-the-art models.
    *   The claimed performance of "human experts" seems questionable given the difficulty of the benchmark.

**[What to prepare before starting a ML PhD - 3 months! [D] (Score: 27)](https://www.reddit.com/r/MachineLearning/comments/1ktm35z/what_to_prepare_before_starting_a_ml_phd_3_months/)**
*   **Summary:**  This thread focuses on advice for someone about to start a Machine Learning PhD, specifically what to do in the 3 months beforehand. The general consensus is to relax and enjoy the time off, rather than stressing about technical preparation. Some suggest light technical review, exploring research interests, or focusing on personal well-being.
*   **Emotion:** The dominant emotion is positive, with an encouraging and supportive tone. Commenters emphasize relaxation and enjoyment before the demanding PhD program.
*   **Top 3 Points of View:**
    *   Take time off and relax before the PhD program begins.
    *   Explore research interests in a low-pressure way.
    *   Focus on personal well-being and health.

**[[D] Researcher communities like this one? (Score: 17)](https://www.reddit.com/r/MachineLearning/comments/1ktg0ey/d_researcher_communities_like_this_one/)**
*   **Summary:** This thread asks about other research communities similar to the subreddit. One user recommends the AdversarialML subreddit, while another suggests the Cohere Labs Discord.
*   **Emotion:** The overall tone is neutral and informative.
*   **Top 3 Points of View:**
    *   Looking for recommendations for researcher communities.
    *   Advertising a new adversarial ML/AI research subreddit.
    *   Suggesting Cohere Labs Discord.

**[[N] [D] kumo.ai releases a "Relational Foundation Model", KumoRFM (Score: 6)](https://www.reddit.com/r/MachineLearning/comments/1ktmvkl/n_d_kumoai_releases_a_relational_foundation_model/)**
*   **Summary:** This thread announces the release of KumoRFM, a "Relational Foundation Model" by kumo.ai. One commenter mentions that the idea behind it has been around for some time, and provides a link to a write-up by one of the founders of scikit-learn.
*   **Emotion:** The overall tone is neutral and informative.
*   **Top 3 Points of View:**
    *   Announcing the release of KumoRFM.
    *   Pointing out the prior art and related work.
    *   Mentioning Jure is working on it.

**[Replace Attention mechanism with FAVOR + (Score: 5)](https://arxiv.org/pdf/2009.14794)**
*   **Summary:** This thread discusses replacing the attention mechanism in transformers with FAVOR+.  The commenter questions if it's worth switching given the existing optimizations for original transformers.
*   **Emotion:** The emotional tone is neutral and somewhat skeptical.
*   **Top 3 Points of View:**
    *   Questioning the practical benefit of replacing attention mechanisms given existing transformer optimizations.
    *   No other points of view were explicitly given

**[[R] Best Practices for Image Classification Consensus with Large Annotator Teams (Score: 4)](https://www.reddit.com/r/MachineLearning/comments/1ktcodg/r_best_practices_for_image_classification/)**
*   **Summary:** This thread addresses the challenge of achieving consensus in image classification when working with large annotator teams.  Commenters discuss using reference documents with examples, periodic check-in meetings, and the potential reasons for disagreements among annotators.
*   **Emotion:** The overall tone is neutral and seeks practical advice.
*   **Top 3 Points of View:**
    *   Sharing experience with smaller annotation teams.
    *   Inquiring about narrow margins and reasons for disagreement.

**[[D] Publication advice (Score: 4)](https://www.reddit.com/r/MachineLearning/comments/1ktgdf5/d_publication_advice/)**
*   **Summary:** This thread is about getting advice on publishing a potentially non-novel work. The commenters recommend publishing on arXiv or other conferences or workshops that focus on specific languages, like LREC or VarDial.
*   **Emotion:** The emotional tone is mixed, with some being supportive and others being negative about the prospects for a top-tier publication.
*   **Top 3 Points of View:**
    *   Even non-novel work is worth publishing.
    *   Consider publishing on arXiv or workshops focusing on specific languages.
    *   The work may not be novel enough for a machine learning journal.

**[[D] Challenges in ML for Rare Time Series Events – Looking for insights from others in this space (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1ktahso/d_challenges_in_ml_for_rare_time_series_events/)**
*   **Summary:** The thread discusses the challenges in applying machine learning to rare time series events, seeking advice from others in the field. One comment suggests using autoencoders and weighing samples.
*   **Emotion:** The emotional tone is neutral, with a focus on problem-solving.
*   **Top 3 Points of View:**
    *   Consider using autoencoders with ConvNeXt for feature extraction.
    *   Weigh samples by an adversity metric.

**[[D] What are the research papers and methods that led to Deepmind’s Veo 3? (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1ktt2ze/d_what_are_the_research_papers_and_methods_that/)**
*   **Summary:** The thread seeks information on the research papers and methods behind Deepmind's Veo 3.  One commenter suggests that Deepmind's access to YouTube data is a significant advantage.
*   **Emotion:** The tone is inquisitive and neutral.
*   **Top 3 Points of View:**
    *   Suggesting that Deepmind's access to YouTube data is the key.

**[[P] Football & AI Project (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1kt5bea/p_football_ai_project/)**
*   **Summary:** The thread provides links to a GitHub repository and website for a Football & AI project.
*   **Emotion:** The overall tone is neutral.
*   **Top 3 Points of View:**
    *   Presenting a football & AI project.

**[[P] Running LLMs on 8× H100s… but sometimes you have to let AI be the artist too (Score: 0)](https://www.reddit.com/gallery/1ktdryq)**
*   **Summary:** This thread is about running LLMs on 8x H100s.
*   **Emotion:** The overall tone is neutral.
*   **Top 3 Points of View:**
    *   Asking about image generation speed
