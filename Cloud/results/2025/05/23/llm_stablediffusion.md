---
title: "Stable Diffusion Subreddit"
date: "2025-05-23"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stable diffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [Loop Anything with Wan2.1 VACE](https://v.redd.it/h46ktsyhoj2f1) (Score: 145)
    * This thread discusses a looping workflow using Wan2.1 VACE for stable diffusion, with users sharing their experiences and asking questions about specific nodes.
2.  [LayerDiffuse: generating transparent images from prompts (complete guide)](https://i.redd.it/2y4svebp6k2f1.png) (Score: 41)
    *  This thread is about generating transparent images from prompts using LayerDiffuse. Users are discussing the availability of the tool and its performance compared to previous versions.
3.  [Vace 14B multi-image conditioning test (aka "Try and top that, Veo you corpo b...ch!")](https://v.redd.it/6dh0ymc5pk2f1) (Score: 7)
    *  This thread showcases an experiment using Kijai's WanVideoVaceEncode node for multi-image conditioning, achieving impressive results with strategically placed input frames.
4.  [Training LORA for body part shape](https://www.reddit.com/r/StableDiffusion/comments/1ktka9e/training_lora_for_body_part_shape/) (Score: 7)
    *  Users are discussing issues related to training a LoRA for body part shape, including compatibility problems between SDXL and SD1.5 models.
5.  [Does regularization images matter in LoRA trainings?](https://www.reddit.com/r/StableDiffusion/comments/1ktlj3p/does_regularization_images_matter_in_lora/) (Score: 4)
    *  This thread discusses the impact of regularization images on LoRA training, with users warning against using generated images and highlighting the importance of the ratio of training to regularization images.
6.  [How much does performance differ when using an eGPU compared to it's desktop equivalent?](https://www.reddit.com/r/StableDiffusion/comments/1ktoxp6/how_much_does_performance_differ_when_using_an/) (Score: 2)
    *  Users discuss performance differences between eGPUs and desktop GPUs for stable diffusion, focusing on VRAM usage and PCIe lane configurations.
7.  [Real slow generations using Wan2.1 I2V (720 or 480, GGUF or safetensors)](https://www.reddit.com/r/StableDiffusion/comments/1ktqnsr/real_slow_generations_using_wan21_i2v_720_or_480/) (Score: 2)
    *  Users are discussing the slow generation speeds of Wan2.1 I2V and suggesting solutions like CausVid, torch compile, and teacache.
8.  [Is Skip Layer Guidance a thing in SwarmUi for WAN?](https://www.reddit.com/r/StableDiffusion/comments/1ktmoy2/is_skip_layer_guidance_a_thing_in_swarmui_for_wan/) (Score: 1)
    *  This thread is about finding skip layer guidance in SwarmUI for WAN, with users pointing to KJNodes and the Extensions menu.
9.  [Can you spot any inconsistencies in this output anything that would scream Ai ?](https://i.redd.it/9is517vrtj2f1.jpeg) (Score: 0)
    *  Users are trying to identify inconsistencies in an AI-generated image, pointing out issues with perspective, details in the Eiffel Tower, and anatomical inaccuracies.
10. [look too ai when I faceswap with reactor](https://i.redd.it/mhwqogehvj2f1.png) (Score: 0)
    *  Users are discussing how to improve face-swapped images to make them look less AI-generated, suggesting the use of ControlNet and InsightFace.
11. [Is there any way to avoid these soft/gradient/swirly effects? (I'm using "gpt-image-1" model API from OpenAI)](https://i.redd.it/s1bug8jejk2f1.png) (Score: 0)
    *  Users are giving advice on how to avoid soft/gradient/swirly effects when using the "gpt-image-1" model API from OpenAI, suggesting post-processing or using different models.
12. [What's the best local 3D model AI generator that can run on a 3060 with 12gb of vram?](https://www.reddit.com/r/StableDiffusion/comments/1ktlp9x/whats_the_best_local_3d_model_ai_generator_that/) (Score: 0)
    *  This thread discusses the best local 3D model AI generators for a 3060 with 12GB VRAM, recommending ComfyUI, Forge, InvokeAI, Fooocus, and Stability Matrix.
13. [how do i prune a Flux Lora](https://www.reddit.com/r/StableDiffusion/comments/1ktn1u2/how_do_i_prune_a_flux_lora/) (Score: 0)
    *  Users are discussing how to prune a Flux LoRA, with suggestions to use Lora Block Weight node for comfyui.
14. [Model](https://www.reddit.com/r/StableDiffusion/comments/1ktqnc5/model/) (Score: 0)
    *  A user asks, "What is an art object?"
15. [How the *** do you guys use this thing?](https://www.reddit.com/r/StableDiffusion/comments/1ktsko6/how_the_hell_do_you_guys_use_this_thing/) (Score: 0)
    *  Users discuss their workflows for using Stable Diffusion, including using ComfyUI, training LoRAs, and refining images in Photoshop.
16. [I have created AI Minecraft Short Music Video - What Do You Think - is it worth making full clip?](https://www.youtube.com/shorts/WuIhYsxXgso) (Score: 0)
    *  A user asks if their AI Minecraft short music video is worth making into a full clip and receives a negative response.

# Detailed Analysis by Thread
**[Loop Anything with Wan2.1 VACE (Score: 145)](https://v.redd.it/h46ktsyhoj2f1)**
*  **Summary:** This thread discusses a looping workflow using Wan2.1 VACE for stable diffusion. Users share their experiences, ask questions about specific nodes like UNetTemporalAttentionMultiply, and express gratitude for the shared workflow.
*  **Emotion:** The overall emotional tone is positive, with users expressing excitement and appreciation for the shared workflow.
*  **Top 3 Points of View:**
    * The looping workflow is very interesting and helpful.
    * The UNetTemporalAttentionMultiply node is unfamiliar and its function is unclear.
    * Users are grateful for the shared workflow.

**[LayerDiffuse: generating transparent images from prompts (complete guide) (Score: 41)](https://i.redd.it/2y4svebp6k2f1.png)**
*  **Summary:**  This thread is about generating transparent images from prompts using LayerDiffuse.  Users discuss the availability of the tool, expressing frustration that it's advertised as a guide but uses a closed API and hasn't been released for ComfyUI yet. Others inquire about running it locally and compare its performance to previous versions.
*  **Emotion:** The emotional tone is mixed, with some negativity due to the closed API and lack of ComfyUI support, but also interest in the potential of the tool.
*  **Top 3 Points of View:**
    * The "guide" is misleading because it advertises a closed API.
    * It's a shame that LayerDiffusion for Flux is not released on ComfyUI yet.
    * Users want to know if they can run this locally and how it compares to older versions.

**[Vace 14B multi-image conditioning test (aka "Try and top that, Veo you corpo b...ch!") (Score: 7)](https://v.redd.it/6dh0ymc5pk2f1)**
*  **Summary:**  This thread showcases an experiment using Kijai's WanVideoVaceEncode node for multi-image conditioning, achieving impressive results with strategically placed input frames. The user expresses satisfaction with the outcome, particularly with character consistency.
*  **Emotion:** The overall tone is positive, showcasing excitement and satisfaction with the results.
*  **Top 3 Points of View:**
    * Kijai's WanVideoVaceEncode node allows flexible configuration of conditioning images and masks.
    * The user is happy with the results of his experiment.
    * Users appreciate the motion and character consistency, though note some AI-generated artifacts in the shadows.

**[Training LORA for body part shape (Score: 7)](https://www.reddit.com/r/StableDiffusion/comments/1ktka9e/training_lora_for_body_part_shape/)**
*  **Summary:** Users are discussing issues related to training a LoRA for body part shape. The primary issue is compatibility problems between SDXL and SD1.5 models, leading to the LoRA having no effect. Additionally, they discuss the number of training steps.
*  **Emotion:** The overall emotional tone is neutral, offering technical advice and troubleshooting.
*  **Top 3 Points of View:**
    * Using a Lora trained on SD1.5 with SDXL is incompatible.
    * Train the LoRA against pony directly or use a SD1.5 model.
    * 6500 steps is too much for training, recommending 2k-2.5k steps.

**[Does regularization images matter in LoRA trainings? (Score: 4)](https://www.reddit.com/r/StableDiffusion/comments/1ktlj3p/does_regularization_images_matter_in_lora/)**
*  **Summary:** This thread discusses the impact of regularization images on LoRA training. Users caution against using generated images for training and emphasize the importance of the ratio of training images to regularization images.
*  **Emotion:** The emotional tone is neutral, providing cautionary advice and highlighting potential pitfalls in LoRA training.
*  **Top 3 Points of View:**
    * Training on generated images can self-pollute the dataset.
    * A large number of regularization images compared to training images can render the LoRA ineffective.
    * Questioning the user for training on AI images.

**[How much does performance differ when using an eGPU compared to it's desktop equivalent? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1ktoxp6/how_much_does_performance_differ_when_using_an/)**
*  **Summary:** Users discuss the performance differences between using an eGPU compared to a desktop GPU for stable diffusion. The main focus is on VRAM usage and PCIe lane configurations and whether the model is offloading to the ram.
*  **Emotion:** The emotional tone is neutral, offering technical explanations and advice on hardware configurations.
*  **Top 3 Points of View:**
    * Performance is the same if the AI model is completely in VRAM.
    * Performance decreases when offloading to RAM.
    * It is more cost effective to build a desktop with a PCIe 4 slot instead of using eGPU.

**[Real slow generations using Wan2.1 I2V (720 or 480, GGUF or safetensors) (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1ktqnsr/real_slow_generations_using_wan21_i2v_720_or_480/)**
*  **Summary:** Users are discussing the slow generation speeds experienced with Wan2.1 I2V, and suggesting solutions to improve performance. These include using CausVid, torch compile, and teacache. Frame count, resolution, and layers offloaded were also mentioned as contributing factors.
*  **Emotion:** The emotional tone is neutral with a hint of frustration, as users seek solutions to improve slow generation times.
*  **Top 3 Points of View:**
    * GUFF only speeds up things when it allow its small size to fit in VRAM; CausVid is needed.
    * Solutions like triton, torch compile, and either teacache or causvid can significantly reduce generation time.
    * Generation speed depends on framecount, stepcount, resolution, and the amount of layers offloaded.

**[Is Skip Layer Guidance a thing in SwarmUi for WAN? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1ktmoy2/is_skip_layer_guidance_a_thing_in_swarmui_for_wan/)**
*  **Summary:** This thread addresses the question of finding skip layer guidance in SwarmUI for WAN. Users suggest that it can be found via KJNodes or through Server > Extensions in Swarm.
*  **Emotion:** The emotional tone is neutral and helpful.
*  **Top 3 Points of View:**
    * Skip Layer Guidance is available under KJNodes.
    * Users can install KJNodes.
    * It is under Server > Extensions in Swarm.

**[Can you spot any inconsistencies in this output anything that would scream Ai ? (Score: 0)](https://i.redd.it/9is517vrtj2f1.jpeg)**
*  **Summary:** Users are scrutinizing an AI-generated image to identify inconsistencies that reveal its artificial origin. They point out flaws in perspective, inaccuracies in the depiction of the Eiffel Tower, and anatomical anomalies in the figures.
*  **Emotion:** The emotional tone is analytical and critical, as users meticulously dissect the image for imperfections.
*  **Top 3 Points of View:**
    * Perspective on the right side buildings is off, the depth of field is skewed and the top parts of the buildings are inaccurate.
    * The mid section of the Eiffel Tower especially looks more like its Tokyo counterpart.
    * The right hand index finger of the women is unrealstic.

**[look too ai when I faceswap with reactor (Score: 0)](https://i.redd.it/mhwqogehvj2f1.png)**
*  **Summary:** This thread focuses on improving face-swapped images created with Reactor to appear less AI-generated. Suggestions include the use of ControlNet and InsightFace.
*  **Emotion:** The emotional tone is neutral and solution-oriented, with users providing specific advice.
*  **Top 3 Points of View:**
    * The leg in the image has problems.
    * Try using ControlNet and then Insight Face for improvement.

**[Is there any way to avoid these soft/gradient/swirly effects? (I'm using "gpt-image-1" model API from OpenAI) (Score: 0)](https://i.redd.it/s1bug8jejk2f1.png)**
*  **Summary:** This thread addresses the issue of soft, gradient, and swirly effects in images generated using OpenAI's "gpt-image-1" model API. Users recommend post-processing in tools like Photoshop or Paint, or experimenting with different models.
*  **Emotion:** The emotional tone is neutral and practical, offering solutions for mitigating unwanted effects.
*  **Top 3 Points of View:**
    * Post-processing in Photoshop can help fix the artifacts.
    * Some models will produce less of these artifacts.

**[What's the best local 3D model AI generator that can run on a 3060 with 12gb of vram? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1ktlp9x/whats_the_best_local_3d_model_ai_generator_that/)**
*  **Summary:** This thread discusses suitable local 3D model AI generators for a system with a 3060 GPU with 12GB VRAM. Users recommend various front ends like ComfyUI, Forge, InvokeAI, Fooocus, and Stability Matrix, noting that they essentially run the same models, with the user interface and features being the main differentiators.
*  **Emotion:** The emotional tone is informative and helpful, providing a range of options and considerations.
*  **Top 3 Points of View:**
    * ComfyUI, Forge, InvokeAI, Fooocus and Stability Matrix are all running the same models.
    * ComfyUI is the best maintained, most powerful and flexible, but has a steeper learning curve.
    * Stability Matrix has a built-in basic inference engine.

**[how do i prune a Flux Lora (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1ktn1u2/how_do_i_prune_a_flux_lora/)**
*  **Summary:** This thread explores the process of pruning a Flux LoRA. Suggestions include using the Lora Block Weight node for ComfyUI to selectively apply LoRA blocks during inference.
*  **Emotion:** The emotional tone is inquisitive and exploratory.
*  **Top 3 Points of View:**
    * Not sure about pruning the Lora itself, but there is a Lora Block Weight node for comfyui that allows you to chose which lora blocks to apply during inference.
    * What would pruning the blocks do?
    * I'm interested in that as well.

**[Model (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1ktqnc5/model/)**
*  **Summary:**  The thread consists of a single question: "What is an art object?"
*  **Emotion:** The emotional tone is neutral, as it is a simple question.
*  **Top 3 Points of View:**
    * What is an art object?

**[How the *** do you guys use this thing? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1ktsko6/how_the_hell_do_you_guys_use_this_thing/)**
*  **Summary:** This thread details various workflows for using Stable Diffusion. Users share their processes, including using ComfyUI, training LoRAs on subjects and art styles, and refining images in Photoshop.
*  **Emotion:** The emotional tone is informative and somewhat technical, as users describe their complex workflows.
*  **Top 3 Points of View:**
    * Users use ComfyUI to craft workflows using either SD15, SDXL, or Flux.
    * They train LoRAs of people that they use as subjects and train them on art styles.
    * Users mix and match components to compose images that they refine in Photoshop.

**[I have created AI Minecraft Short Music Video - What Do You Think - is it worth making full clip? (Score: 0)](https://www.youtube.com/shorts/WuIhYsxXgso)**
*  **Summary:** A user asks for feedback on their AI-generated Minecraft short music video, inquiring if it's worth making a full clip. The response is simply, "No."
*  **Emotion:** The emotional tone is blunt and negative.
*  **Top 3 Points of View:**
    * The video is not worth making a full clip.
