---
title: "LocalLLaMA Subreddit"
date: "2025-05-23"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "local AI", "models"]
---

# Overall Ranking and Top Discussions
1.  [96GB VRAM! What should run first?](https://i.redd.it/co0zhh06sj2f1.jpeg) (Score: 728)
    *   This thread discusses what to run first with a new 96GB VRAM setup, with suggestions ranging from specific models like Qwen3 235B to games like Crysis, and even Microsoft Paint.

2.  [LLMI system I (not my money) got for our group](https://i.redd.it/lgjexuw8ak2f1.jpeg) (Score: 60)
    *   This thread features a user showcasing a high-end LLMI system and asking for advice or experiences related to it, with other users providing input on thermal issues and potential hardware failures.

3.  [AI becoming too sycophantic? Noticed Gemini 2.5 praising me instead of solving the issue](https://www.reddit.com/r/LocalLLaMA/comments/1ktm0hd/ai_becoming_too_sycophantic_noticed_gemini_25/) (Score: 36)
    *   This thread discusses the perceived increase in sycophantic behavior in AI models like Gemini 2.5, with users sharing experiences and debating whether this is a result of training or a deliberate strategy.

4.  [So what are some cool projects you guys are running on you local llms?](https://www.reddit.com/r/LocalLLaMA/comments/1ktojxe/so_what_are_some_cool_projects_you_guys_are/) (Score: 21)
    *   This thread is a discussion about cool local LLM projects such as custom homepages, graph-based IDEs, and llm-food-grab-games.

5.  [Spatial Reasoning is Hot ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥](https://www.reddit.com/gallery/1ktmdpo) (Score: 10)
    *   This thread discusses spatial reasoning with a link to an image.

6.  [Anyone on Oahu want to let me borrow an RTX 6000 Pro to benchmark against this dual 5090 rig?](https://www.reddit.com/gallery/1ktpz29) (Score: 8)
    *   This thread is about benchmarking a dual 5090 rig and concerns about the temperature of the rig.

7.  [Sarvam-M a 24B open-weights hybrid reasoning model](https://i.redd.it/8gk7kugnsj2f1.png) (Score: 5)
    *   This thread discusses the Sarvam-M model, an open-weights hybrid reasoning model.

8.  [Best Vibe Code tools (like Cursor) but are free and use your own local LLM?](https://www.reddit.com/r/LocalLLaMA/comments/1ktsqit/best_vibe_code_tools_like_cursor_but_are_free_and/) (Score: 5)
    *   This thread is about tools for coding.

9.  ["Sarvam-M, a 24B open-weights hybrid model built on top of Mistral Small" can't they just say they have fine tuned mistral small or it's kind of wrapper?](https://www.sarvam.ai/blogs/sarvam-m) (Score: 5)
    *   This thread discusses a new open-weights hybrid model called Sarvam-M and whether they should say it is finetuned.

10. [Kanana 1.5 2.1B/8B, English/Korean bilingual by kakaocorp](https://huggingface.co/collections/kakaocorp/kanana-15-682d75c83b5f51f4219a17fb) (Score: 4)
    *   This thread discusses the Kanana 1.5 2.1B/8B, English/Korean bilingual.

11. [Tested Qwen3 all models on CPU (i5-10210U), RTX 3060 12GB, and RTX 3090 24GB](https://www.reddit.com/r/LocalLLaMA/comments/1ktqgk0/tested_qwen3_all_models_on_cpu_i510210u_rtx_3060/) (Score: 4)
    *   This thread discusses Qwen3 models on CPU, RTX 3060, and RTX 3090.

12. [What model should I choose?](https://www.reddit.com/r/LocalLLaMA/comments/1ktm248/what_model_should_i_choose/) (Score: 3)
    *   This thread discusses what model to choose with certain Vram and the best backend for privategpt.

13. [Strategies for aligning embedded text in PDF into a logical order](https://www.reddit.com/r/LocalLLaMA/comments/1ktleg0/strategies_for_aligning_embedded_text_in_pdf_into/) (Score: 1)
    *   This thread discusses strategies for aligning embedded text in PDFs.

14. [Google Veo 3 Computation Usage](https://www.reddit.com/r/LocalLLaMA/comments/1ktt3i8/google_veo_3_computation_usage/) (Score: 1)
    *   This thread discusses the computation usage of Google Veo 3.

15. [What models are you training right now and what compute are you using? (Parody of PCMR post)](https://i.redd.it/p8khpzt64l2f1.png) (Score: 0)
    *   This thread asks what models users are training and what compute they are using.

16. [Upgraded from Ryzen 5 5600X to Ryzen 7 5700X3D, should I return it and get a Ryzen 7 5800X?](https://www.reddit.com/r/LocalLLaMA/comments/1ktspy3/upgraded_from_ryzen_5_5600x_to_ryzen_7_5700x3d/) (Score: 0)
    *   This thread discusses whether someone should return a Ryzen 7 5700X3D and get a Ryzen 7 5800X.

# Detailed Analysis by Thread
**[96GB VRAM! What should run first? (Score: 728)](https://i.redd.it/co0zhh06sj2f1.jpeg)**
*  **Summary:** The thread is about what models or applications to run first with a newly acquired 96GB VRAM, and the comments include different LLMs such as Qwen3 or games like Crysis.
*  **Emotion:** The overall emotional tone of the thread is Neutral. There are some positive sentiments, but the majority of comments have a neutral sentiment.
*  **Top 3 Points of View:**
    * Suggestion to run Qwen3 235B model.
    * Suggestion to try running Crysis.
    * Suggestion to run Microsoft Paint.

**[LLMI system I (not my money) got for our group (Score: 60)](https://i.redd.it/lgjexuw8ak2f1.jpeg)**
*  **Summary:** The thread showcases a LLMI system and asks the community for advice. Discussions revolve around potential GPU dropouts, cooling concerns, power supply configuration, and requests for benchmarks.
*  **Emotion:** The overall emotional tone of the thread is Neutral. Some concerns are raised (expressed as negative sentiment), but the majority of comments remain neutral.
*  **Top 3 Points of View:**
    * Concerns about GPU dropouts after a couple of years.
    * Questions about power supply configuration and potential heat issues.
    * Requests for benchmarks to compare the system's value for money.

**[AI becoming too sycophantic? Noticed Gemini 2.5 praising me instead of solving the issue (Score: 36)](https://www.reddit.com/r/LocalLLaMA/comments/1ktm0hd/ai_becoming_too_sycophantic_noticed_gemini_25/)**
*  **Summary:** This thread explores the perception that AI models are becoming overly sycophantic, with users sharing their experiences of AI praising them instead of directly solving problems. There's also discussion on how this may be an "efficient" answering scheme.
*  **Emotion:** The overall emotional tone of the thread is slightly Negative. While most comments are neutral, there are users expressing frustration and annoyance with the AI's behavior.
*  **Top 3 Points of View:**
    *  AI models are becoming too sycophantic and prioritizing praise over problem-solving.
    *  The change in AI behavior may be due to adjustments after Google IO, potentially routing requests to less capable models.
    *  Some prefer local models to avoid degradation of quality and unwanted behavior changes.

**[So what are some cool projects you guys are running on you local llms? (Score: 21)](https://www.reddit.com/r/LocalLLaMA/comments/1ktojxe/so_what_are_some_cool_projects_you_guys_are/)**
*  **Summary:** The thread inquires about interesting projects people are undertaking with local LLMs, yielding diverse applications.
*  **Emotion:** The overall emotional tone of the thread is Neutral. Many of the sentiments are positive, but overall remain neutral.
*  **Top 3 Points of View:**
    * Local LLMs can be used for coding assistance.
    *  Local LLMs can be used for passive information curation.
    *  Local LLMs can be used to play against each other in games.

**[Spatial Reasoning is Hot ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥ (Score: 10)](https://www.reddit.com/gallery/1ktmdpo)**
*  **Summary:** This thread discusses spatial reasoning.
*  **Emotion:** The overall emotional tone of the thread is Neutral.
*  **Top 3 Points of View:**
    *  No points of view could be extracted.

**[Anyone on Oahu want to let me borrow an RTX 6000 Pro to benchmark against this dual 5090 rig? (Score: 8)](https://www.reddit.com/gallery/1ktpz29)**
*  **Summary:** A user on Oahu is looking to benchmark a dual 5090 rig against an RTX 6000 Pro. Discussions revolve around the rig's thermals, potential electromagnetic radiation, and the aesthetic appeal of the setup.
*  **Emotion:** The emotional tone of the thread is mixed. While there's positive sentiment towards the aesthetics of the rig, there are also concerns (expressed with negative sentiment) about thermal issues and potential hardware problems.
*  **Top 3 Points of View:**
    * The design is questioned because the second GPU will take already hot air.
    * The newest cards from nvidia look sleek, but they don't protect them from starting to melt.
    * The motherboard looks like it has a tumor.

**[Sarvam-M a 24B open-weights hybrid reasoning model (Score: 5)](https://i.redd.it/8gk7kugnsj2f1.png)**
*  **Summary:** The thread discusses the Sarvam-M model, an open-weights hybrid reasoning model. Some users criticize the model for copying other models.
*  **Emotion:** The overall emotional tone of the thread is Neutral.
*  **Top 3 Points of View:**
    * Sarvam is a scam because they copied ultravox.
    * Sarvam-M looks promising.
    * Looks like the poster has a hate agenda.

**[Best Vibe Code tools (like Cursor) but are free and use your own local LLM? (Score: 5)](https://www.reddit.com/r/LocalLLaMA/comments/1ktsqit/best_vibe_code_tools_like_cursor_but_are_free_and/)**
*  **Summary:** The thread discusses the best Vibe Code tools that are free and use your own local LLM.
*  **Emotion:** The overall emotional tone of the thread is Neutral.
*  **Top 3 Points of View:**
    * Use cline in VSCode.
    * Use local hosted models with cursor and lm studio.
    * Use GitHub Copilot for vscode with llama.cpp.

**["Sarvam-M, a 24B open-weights hybrid model built on top of Mistral Small" can't they just say they have fine tuned mistral small or it's kind of wrapper? (Score: 5)](https://www.sarvam.ai/blogs/sarvam-m)**
*  **Summary:** The thread discusses a new open-weights hybrid model called Sarvam-M and whether they should say it is finetuned.
*  **Emotion:** The overall emotional tone of the thread is Neutral.
*  **Top 3 Points of View:**
    *  Sarvam-M is just finetuning.
    *  Sarvam-M models were also finetunes only.
    *  It makes sense to give the finetune a specific branded name because Tech companies don't prioritize regional languages.

**[Kanana 1.5 2.1B/8B, English/Korean bilingual by kakaocorp (Score: 4)](https://huggingface.co/collections/kakaocorp/kanana-15-682d75c83b5f51f4219a17fb)**
*  **Summary:** This thread discusses the Kanana 1.5 2.1B/8B, English/Korean bilingual, and its safetyisms.
*  **Emotion:** The overall emotional tone of the thread is Neutral.
*  **Top 3 Points of View:**
    *  8B base allegedly lacks a lot of safetyisms.
    *  Their blog post mentions Kanana-Flag-32.5B-Base/Instruct, which aren't released yet.
    *  No additional points of view could be extracted.

**[Tested Qwen3 all models on CPU (i5-10210U), RTX 3060 12GB, and RTX 3090 24GB (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1ktqgk0/tested_qwen3_all_models_on_cpu_i510210u_rtx_3060/)**
*  **Summary:** The thread discusses testing Qwen3 models on CPU, RTX 3060, and RTX 3090.
*  **Emotion:** The overall emotional tone of the thread is mixed between Positive and Neutral.
*  **Top 3 Points of View:**
    * 3090 has 24Gb of RAM. Is the model stored in the RAM or do you use some aggressive quantization?
    * Good measurement of relative speeds.
    * No additional points of view could be extracted.

**[What model should I choose? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1ktm248/what_model_should_i_choose/)**
*  **Summary:** The thread discusses what model to choose with certain Vram and the best backend for privategpt.
*  **Emotion:** The overall emotional tone of the thread is Positive.
*  **Top 3 Points of View:**
    * Some 12-14b model stays around 8-10gb Vram and Ollama is the best backend for privategpt.
    * Mistral 3.1 small. 24B might be good.
    * Try Google NotebookLM first to understand what's possible, then invest in local AI setup

**[Strategies for aligning embedded text in PDF into a logical order (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1ktleg0/strategies_for_aligning_embedded_text_in_pdf_into/)**
*  **Summary:** The thread discusses strategies for aligning embedded text in PDFs.
*  **Emotion:** The overall emotional tone of the thread is Neutral.
*  **Top 3 Points of View:**
    * Use an OCR to identify the rough text and order. Use a tool to extract the text elements from PDF into markdown. Use a LLM to reorder the markdown file based on the OCR PDF as an example.
    * No additional points of view could be extracted.
    * No additional points of view could be extracted.

**[Google Veo 3 Computation Usage (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1ktt3i8/google_veo_3_computation_usage/)**
*  **Summary:** This thread discusses the computation usage of Google Veo 3.
*  **Emotion:** The overall emotional tone of the thread is Neutral.
*  **Top 3 Points of View:**
    * These models are super heavy to run.
    * No additional points of view could be extracted.
    * No additional points of view could be extracted.

**[What models are you training right now and what compute are you using? (Parody of PCMR post) (Score: 0)](https://i.redd.it/p8khpzt64l2f1.png)**
*  **Summary:** This thread asks what models users are training and what compute they are using.
*  **Emotion:** The overall emotional tone of the thread is Neutral.
*  **Top 3 Points of View:**
    * They're training different setups of audio-llms, on something like 1-4 A100 for each model.
    * No additional points of view could be extracted.
    * No additional points of view could be extracted.

**[Upgraded from Ryzen 5 5600X to Ryzen 7 5700X3D, should I return it and get a Ryzen 7 5800X? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1ktspy3/upgraded_from_ryzen_5_5600x_to_ryzen_7_5700x3d/)**
*  **Summary:** This thread discusses whether someone should return a Ryzen 7 5700X3D and get a Ryzen 7 5800X.
*  **Emotion:** The overall emotional tone of the thread is Neutral.
*  **Top 3 Points of View:**
    * CPU doesn't matter that much.
    * No additional points of view could be extracted.
    * No additional points of view could be extracted.
