---
title: "LocalLLaMA Subreddit"
date: "2025-05-12"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LocalLLM", "AI", "Models"]
---

# Overall Ranking and Top Discussions
1.  [[D] Meta has released an 8B BLT model](https://ai.meta.com/blog/meta-fair-updates-perception-localization-reasoning/?utm_source=twitter&utm_medium=organic%20social&utm_content=video&utm_campaign=fair) (Score: 63)
    *   Discussion about Meta's release of an 8B BLT model, comparing it to other models like Eva;yte, and questioning its novelty.
2.  [Qwen suggests adding presence penalty when using Quants](https://www.reddit.com/gallery/1kkuq7m) (Score: 46)
    *   Users discussing the impact of presence penalty when using Qwen models with quantization, particularly for coding tasks and extended context lengths.
3.  [Qwen3 throughput benchmarks on 2x 3090, almost 1000 tok/s using 4B model and vLLM as the inference engine](https://www.reddit.com/r/LocalLLaMA/comments/1kkvqti/qwen3_throughput_benchmarks_on_2x_3090_almost/) (Score: 29)
    *   Sharing Qwen3 throughput benchmarks using 2x 3090 GPUs, discussing various configurations, and comparing performance with other setups.
4.  [Latest Open/Local Vision Language Model 2025 Update: Agentic models, video LMs, multimodal RAG and more!](https://www.reddit.com/r/LocalLLaMA/comments/1kkxguj/latest_openlocal_vision_language_model_2025/) (Score: 21)
    *   Discussion of the latest updates in open/local vision language models for 2025, including agentic models, video LMs, and multimodal RAG.
5.  [Building local Manus alternative AI agent app using Qwen3, MCP, Ollama - what did I learn](https://www.reddit.com/r/LocalLLaMA/comments/1kkyzaz/building_local_manus_alternative_ai_agent_app/) (Score: 8)
    *   A user shares their experience building a local Manus alternative AI agent app using Qwen3, MCP, and Ollama, noting that Qwen 2.5 is better than Qwen 3 32Bs in tool calling.
6.  [Best local inference provider?](https://www.reddit.com/r/LocalLLaMA/comments/1kkx4ev/best_local_inference_provider/) (Score: 7)
    *   A discussion about the best local inference providers, with recommendations for VLLM, ik_llama_cpp, Llama CPP, and Ollama based on different needs like performance, memory split, features, and ease of use.
7.  [Predicting sales conversion probability from conversations using pure Reinforcement Learning](https://www.reddit.com/r/LocalLLaMA/comments/1kl0uvv/predicting_sales_conversion_probability_from/) (Score: 3)
    *   Discussion about a Reinforcement Learning model that predicts sales conversion probability from conversations, outperforming LLM-only approaches.
8.  [Chatbots, Music and Solar Systems galore! More fun and quirkiness with Qwen 3 8b!](https://www.youtube.com/watch?v=gepZOxpSyFQ) (Score: 3)
    *   A user showcases the capabilities of the Qwen 3 8b model, including chatbot creation, music generation, and a realistic solar system model.
9.  [What is the best way to return code snippets in a structured output?](https://www.reddit.com/r/LocalLLaMA/comments/1kkvrjh/what_is_the_best_way_to_return_code_snippets_in_a/) (Score: 2)
    *   Discussion on the best way to return code snippets in a structured output, with suggestions for structured extraction and using context-free grammar.
10. [what's the best way to choose and fine-tune llms on hugging face?](https://www.reddit.com/r/LocalLLaMA/comments/1kkxiio/whats_the_best_way_to_choose_and_finetune_llms_on/) (Score: 2)
    *   A question about the best way to choose and fine-tune LLMs on Hugging Face.
11. [Which hardware to buy for RAG?](https://www.reddit.com/r/LocalLLaMA/comments/1kky7y2/which_hardware_to_buy_for_rag/) (Score: 1)
    *   Advice needed on hardware for RAG implementation.
12. [Local fine tuning - CPU for 5090](https://www.reddit.com/r/LocalLLaMA/comments/1kkz1z7/local_fine_tuning_cpu_for_5090/) (Score: 1)
    *   Discussion about the optimal CPU for local fine-tuning with a 5090 GPU.
13. [Kokoro-JS with long text support](https://test-kokoro.glitch.me/) (Score: 0)
    *   A project using semantic text splitting to get sensible text chunks.
14. [Project Arbius](https://www.reddit.com/gallery/1kkt0ya) (Score: 0)
    *   Discussion about project Arbius and potential use cases.
15. [A forum that makes its data available to all via a torrent?](https://www.reddit.com/r/LocalLLaMA/comments/1kkt0rc/a_forum_that_makes_its_data_available_to_all_via/) (Score: 0)
    *   A question about a forum that makes its data available via torrent.
16. [Searching local model to comment C code in doxygen style](https://www.reddit.com/r/LocalLLaMA/comments/1kktxaz/searching_local_model_to_comment_c_code_in/) (Score: 0)
    *   Discussion about finding a local model to comment C code in Doxygen style.
17. [Need Local Llama](https://www.reddit.com/r/LocalLLaMA/comments/1kkuuxz/need_local_llama/) (Score: 0)
    *   A question about the context window of local LLMs and solutions.
18. [What is stopping a LLM from using a fixed function in a workflow to basic tasks like calculate numbers, time, etc.?](https://www.reddit.com/r/LocalLLaMA/comments/1kkvptm/what_is_stopping_a_llm_from_using_a_fixed/) (Score: 0)
    *   Discussion on why LLMs don't use fixed functions for basic tasks like calculations.

# Detailed Analysis by Thread
**[[D] Meta has released an 8B BLT model (Score: 63)](https://ai.meta.com/blog/meta-fair-updates-perception-localization-reasoning/?utm_source=twitter&utm_medium=organic%20social&utm_content=video&utm_campaign=fair)**
*  **Summary:** The discussion revolves around Meta's release of an 8B BLT model. Users are comparing it to other recent models, questioning if it is truly novel, and discussing its licensing and potential applications in robotics. Some users also feel a need for Llama 4 or 4.1.
*  **Emotion:** The overall emotional tone is neutral, with users primarily focused on technical aspects and comparisons. There are some hints of positive sentiment regarding the potential of the model for robotics.
*  **Top 3 Points of View:**
    *   The model isn't really new, being a rehash of models released and discussed a month/year ago.
    *   The 8B BLT model is not a single 8B model, but rather two models (7B and 1B).
    *   The community is waiting for Meta to release Llama 4.

**[Qwen suggests adding presence penalty when using Quants (Score: 46)](https://www.reddit.com/gallery/1kkuq7m)**
*  **Summary:** The discussion centers around Qwen's suggestion to add a presence penalty when using quantized models. Users are sharing their experiences, particularly in coding tasks, and noting that it seems to help with reducing repetitions, especially with extended context lengths.
*  **Emotion:** The emotional tone is mostly neutral, with some positive sentiment expressed regarding the improved performance in coding tasks.
*  **Top 3 Points of View:**
    *   Presence penalty improves performance on coding tasks with quantized models.
    *   Presence penalty is most effective when dealing with extended context lengths to avoid repetitions.
    *   If you don't have the problem, don't fix the car; only adjust the presence penalty if repetitions are an issue.

**[Qwen3 throughput benchmarks on 2x 3090, almost 1000 tok/s using 4B model and vLLM as the inference engine (Score: 29)](https://www.reddit.com/r/LocalLLaMA/comments/1kkvqti/qwen3_throughput_benchmarks_on_2x_3090_almost/)**
*  **Summary:**  This thread discusses Qwen3 throughput benchmarks using 2x 3090 GPUs with vLLM. Users are sharing their results with different configurations (e.g., TP, DP, FP8, FP16), OS (WSL, Ubuntu), and hardware setups, as well as discussing the impact of PCIe bandwidth.
*  **Emotion:** The overall emotional tone is neutral, with users sharing data and insights in an informative manner. Positive sentiment is present when users express thanks for the benchmarks and share their intention to replicate them.
*  **Top 3 Points of View:**
    *   dp is slower than tp
    *   CUDA graphs take up VRAM.
    *   PCIe bandwidth affects throughput.

**[Latest Open/Local Vision Language Model 2025 Update: Agentic models, video LMs, multimodal RAG and more! (Score: 21)](https://www.reddit.com/r/LocalLLaMA/comments/1kkxguj/latest_openlocal_vision_language_model_2025/)**
*  **Summary:** The thread focuses on the latest advancements in Open/Local Vision Language Models for 2025. The main topics include Agentic models, video LMs and multimodal RAG. Users show their appreciation and ask for model recommendations for object detection.
*  **Emotion:** The emotional tone is positive, reflecting appreciation for the contributions made by Hugging Face in the open source AI space.
*  **Top 3 Points of View:**
    *   Hugging Face's contributions to open source AI are greatly appreciated.
    *   Seeking recommendations for object detection models.
    *   There is general enthusiasm for the advancements in Vision Language Models.

**[Building local Manus alternative AI agent app using Qwen3, MCP, Ollama - what did I learn (Score: 8)](https://www.reddit.com/r/LocalLLaMA/comments/1kkyzaz/building_local_manus_alternative_ai_agent_app/)**
*  **Summary:** A user shares their experiences building a local AI agent app using Qwen3, MCP, and Ollama, concluding that Qwen 2.5 is better for tool calling than Qwen 3 32Bs.
*  **Emotion:** The overall emotional tone is positive due to the user's experience and sharing the learnings.
*  **Top 3 Points of View:**
    *   Qwen 2.5 is superior to Qwen 3 32Bs in tool calling based on personal experience.

**[Best local inference provider? (Score: 7)](https://www.reddit.com/r/LocalLLaMA/comments/1kkx4ev/best_local_inference_provider/)**
*  **Summary:** Users are discussing and recommending different local inference providers based on varying needs, such as performance, CPU+GPU split, features, and ease of use.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   VLLM for performance
    *   ik_llama_cpp for CPU+GPU split
    *   Llama CPP for features + control

**[Predicting sales conversion probability from conversations using pure Reinforcement Learning (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1kl0uvv/predicting_sales_conversion_probability_from/)**
*  **Summary:** A user highlights the performance and capabilities of the SalesRLAgent model, which uses reinforcement learning to predict sales conversion probability from conversations, noting its high accuracy and open-source nature.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   SalesRLAgent achieves high accuracy in conversion prediction.

**[Chatbots, Music and Solar Systems galore! More fun and quirkiness with Qwen 3 8b! (Score: 3)](https://www.youtube.com/watch?v=gepZOxpSyFQ)**
*  **Summary:** A user showcases the capabilities of the Qwen 3 8b model, including chatbot creation, music generation, and a realistic solar system model.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   Qwen 3 8b is capable of creating chatbots, music, and realistic solar system models.

**[What is the best way to return code snippets in a structured output? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1kkvrjh/what_is_the_best_way_to_return_code_snippets_in_a/)**
*  **Summary:** Discussion about structured extraction and using context-free grammar for returning code snippets in a structured output.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   Using structured extraction is a viable approach.
    *   Context-free grammar can structure the output using tools like Guidance AI.

**[what's the best way to choose and fine-tune llms on hugging face? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1kkxiio/whats_the_best_way_to_choose_and_fine_tune_llms_on/)**
*  **Summary:** A question about the best way to choose and fine-tune LLMs on Hugging Face.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   The user is looking for resources on LLMs on Hugging Face.

**[Which hardware to buy for RAG? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1kky7y2/which_hardware_to_buy_for_rag/)**
*  **Summary:** Advice needed on hardware for RAG implementation.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   The size of the model affects which hardware to buy for RAG

**[Local fine tuning - CPU for 5090 (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1kkz1z7/local_fine_tuning_cpu_for_5090/)**
*  **Summary:** Discussion about the optimal CPU for local fine-tuning with a 5090 GPU.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   AMD is a good value.
    *   Intel AMX is the way to go for bigger models.

**[Kokoro-JS with long text support (Score: 0)](https://test-kokoro.glitch.me/)**
*  **Summary:** A project using semantic text splitting to get sensible text chunks.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   Project Kokoro-JS uses semantic text splitting to get sensible text chunks.

**[Project Arbius (Score: 0)](https://www.reddit.com/gallery/1kkt0ya)**
*  **Summary:** Discussion about project Arbius and potential use cases.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   Suspicions about crypto + Amica.

**[A forum that makes its data available to all via a torrent? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1kkt0rc/a_forum_that_makes_its_data_available_to_all_via/)**
*  **Summary:** A question about a forum that makes its data available via torrent.
*  **Emotion:** Positive.
*  **Top 3 Points of View:**
    *   The user is asking the question and someone responded with "nice try glowie".

**[Searching local model to comment C code in doxygen style (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1kktxaz/searching_local_model_to_comment_c_code_in/)**
*  **Summary:** Discussion about finding a local model to comment C code in Doxygen style.
*  **Emotion:** Positive.
*  **Top 3 Points of View:**
    *   Llama.cpp backend running Qwen3 models works ok.

**[Need Local Llama (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1kkuuxz/need_local_llama/)**
*  **Summary:** A question about the context window of local LLMs and solutions.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   LLM performance falls drastically after about 33k tokens.

**[What is stopping a LLM from using a fixed function in a workflow to basic tasks like calculate numbers, time, etc.? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1kkvptm/what_is_stopping_a_llm_from_using_a_fixed/)**
*  **Summary:** Discussion on why LLMs don't use fixed functions for basic tasks like calculations.
*  **Emotion:** Negative.
*  **Top 3 Points of View:**
    *   It is annoying to produce training data that effectively teaches the model to use it.
