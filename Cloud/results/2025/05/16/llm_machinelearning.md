---
title: "Machine Learning Subreddit"
date: "2025-05-16"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "NLP"]
---

# Overall Ranking and Top Discussions
1.  [[D] Who do you all follow for genuinely substantial ML/AI content?](https://www.reddit.com/r/MachineLearning/comments/1ko64s6/d_who_do_you_all_follow_for_genuinely_substantial/) (Score: 48)
    *   The thread is about who people follow for substantial Machine Learning and AI content.
2.  [[D] presenting a paper virtually in ACL findings - should we?](https://www.reddit.com/r/MachineLearning/comments/1knvsib/d_presenting_a_paper_virtually_in_acl_findings/) (Score: 16)
    *   The thread discusses the value of presenting a paper virtually at ACL findings, with some suggesting it's not worth the money or time.
3.  [[P] I trained an AI to beat the first level of Doom!](https://www.reddit.com/r/MachineLearning/comments/1ko83yq/p_i_trained_an_ai_to_beat_the_first_level_of_doom/) (Score: 12)
    *   The thread showcases an AI trained to beat the first level of Doom, with users asking about training time.
4.  [[D] At what cost are we training chatbots?](https://www.reddit.com/r/MachineLearning/comments/1knj3bj/d_at_what_cost_are_we_training_chatbots/) (Score: 6)
    *   The discussion revolves around the environmental and ethical costs associated with training chatbots, particularly in comparison to other technologies and industries.
5.  [[R] NeurIPS Dataset Anonymization on HuggingFace](https://www.reddit.com/r/MachineLearning/comments/1knkl5g/r_neurips_dataset_anonymization_on_huggingface/) (Score: 6)
    *   The thread discusses methods for anonymizing datasets on HuggingFace for NeurIPS submissions.
6.  [[P] TTSDS2 - Multlingual TTS leaderboard](https://www.reddit.com/r/MachineLearning/comments/1knwaf7/p_ttsds2_multlingual_tts_leaderboard/) (Score: 6)
    *   This thread is about the TTSDS2 multilingual text-to-speech leaderboard, and users appreciate the benchmarks and data.
7.  [[P] Why I Used CNN+LSTM Over CNN for CCTV Anomaly Detection (>99% Validation Accuracy)](https://www.reddit.com/gallery/1ko5voc) (Score: 4)
    *   The thread is about using CNN+LSTM for CCTV anomaly detection, and users discuss alternative approaches like 3D convolutions.
8.  [[R] NeurIPS 2025: Changing Title](https://www.reddit.com/r/MachineLearning/comments/1knjidk/r_neurips_2025_changing_title/) (Score: 3)
    *   The thread is about whether it is permissable to change the title of a paper submitted to NeurIPS 2025.
9.  [[D] What is an acceptable Gini impurity threshold for decision tree splits in practice?](https://www.reddit.com/r/MachineLearning/comments/1knutpa/d_what_is_an_acceptable_gini_impurity_threshold/) (Score: 3)
    *   The thread discusses what an acceptable Gini impurity threshold is for decision tree splits.
10. [[D] Looking for PhD topic/general future research directions in NLP/ML](https://www.reddit.com/r/MachineLearning/comments/1knv7vy/d_looking_for_phd_topicgeneral_future_research/) (Score: 0)
    *   The thread revolves around advice for choosing a PhD topic in NLP/ML and identifying promising research directions.
11. [[R] could anyone help tell me what is this onnx file and how to remake it? ive have been trying to figure out for hours with little to nothing to show for it](https://www.reddit.com/r/MachineLearning/comments/1ko9fsq/r_could_anyone_help_tell_me_what_is_this_onnx/) (Score: 0)
    *   The thread is a request for assistance in understanding and recreating an ONNX file.

# Detailed Analysis by Thread
**[[D] Who do you all follow for genuinely substantial ML/AI content? (Score: 48)](https://www.reddit.com/r/MachineLearning/comments/1ko64s6/d_who_do_you_all_follow_for_genuinely_substantial/)**
*   **Summary:**  The thread asks for recommendations on who to follow for high-quality Machine Learning and AI content.  Suggestions range from specific individuals like Sebastian Raschka and Yannic Kilcher to resources like journals, arXiv papers, newsletters, and podcasts.
*   **Emotion:** The overall emotional tone is Neutral, with the majority of comments expressing neutral sentiment while offering resources and suggestions. There is a single instance of "Positive" sentiment, when recommending a podcast.
*   **Top 3 Points of View:**
    *   Follow researchers on Google Scholar and read their papers.
    *   Journals and arXiv papers are better sources than content creators for in-depth understanding.
    *   Specific individuals like Sebastian Raschka, Yannic Kilcher, Maxime Labonne, and Chip Huyen are recommended.

**[[D] presenting a paper virtually in ACL findings - should we? (Score: 16)](https://www.reddit.com/r/MachineLearning/comments/1knvsib/d_presenting_a_paper_virtually_in_acl_findings/)**
*   **Summary:** The thread discusses the value of virtually presenting a paper at ACL findings. The general consensus is that it's not worth the money or time, as attendance is likely to be low.
*   **Emotion:** The emotional tone is mixed, with both Positive and Negative sentiments expressed. The positive sentiment congratulates the user on their paper, whereas the negative sentiment expresses doubt about the virtual presentation's worth.
*   **Top 3 Points of View:**
    *   Presenting virtually is not worth the money.
    *   Attendance at virtual sessions will be low.
    *   Focus on in-person events instead.

**[[P] I trained an AI to beat the first level of Doom! (Score: 12)](https://www.reddit.com/r/MachineLearning/comments/1ko83yq/p_i_trained_an_ai_to_beat_the_first_level_of_doom/)**
*   **Summary:** The thread is a showcase of an AI trained to beat the first level of Doom. Users express interest and inquire about the training duration.
*   **Emotion:** The emotional tone is positive and neutral. Users express genuine interest and ask questions.
*   **Top 3 Points of View:**
    *   The project is well-received.
    *   Users are curious about the technical details (training time).

**[[D] At what cost are we training chatbots? (Score: 6)](https://www.reddit.com/r/MachineLearning/comments/1knj3bj/d_at_what_cost_are_we_training_chatbots/)**
*   **Summary:** The thread delves into the costs associated with training chatbots, touching on environmental impact (energy consumption, waste heat) and ethical concerns.  Users compare the impact of AI to other technologies and industries.
*   **Emotion:** The emotional tone is predominantly Neutral, with users presenting arguments and posing questions.
*   **Top 3 Points of View:**
    *   The environmental cost of AI training should be considered.
    *   The focus on LLMs might be disproportionate compared to other industries with high environmental impact.
    *   Issues with methane-fuelled gas turbines relate more to kleptocracy and deregulation than AI itself.

**[[R] NeurIPS Dataset Anonymization on HuggingFace (Score: 6)](https://www.reddit.com/r/MachineLearning/comments/1knkl5g/r_neurips_dataset_anonymization_on_huggingface/)**
*   **Summary:** The discussion centers around how to anonymize datasets on HuggingFace for submission to NeurIPS.  Alternatives like using an anonymous Google user or zipping the data are suggested. The option of single-blind submission, which may negate the need for anonymization, is also mentioned.
*   **Emotion:** The emotional tone is mostly Neutral, focused on providing helpful solutions and information.
*   **Top 3 Points of View:**
    *   Creating an anonymous Google user for Github and Huggingface is a viable anonymization strategy.
    *   Zipping the data can be another way to anonymize it.
    *   Single-blind submission doesn't necessarily require anonymization.

**[[P] TTSDS2 - Multlingual TTS leaderboard (Score: 6)](https://www.reddit.com/r/MachineLearning/comments/1knwaf7/p_ttsds2_multlingual_tts_leaderboard/)**
*   **Summary:** This thread features the presentation of a multilingual TTS leaderboard (TTSDS2). Users express positive feedback, appreciating the benchmarks and data provided.
*   **Emotion:** The emotional tone is highly Positive, with users expressing appreciation for the resource.
*   **Top 3 Points of View:**
    *   The leaderboard is considered a valuable resource.
    *   Benchmarks and data are appreciated.

**[[P] Why I Used CNN+LSTM Over CNN for CCTV Anomaly Detection (>99% Validation Accuracy) (Score: 4)](https://www.reddit.com/gallery/1ko5voc)**
*   **Summary:** The thread presents a project using CNN+LSTM for CCTV anomaly detection.  Discussion includes questions about alternative approaches like 3D convolutions and minor criticisms about the presentation style.
*   **Emotion:** The emotional tone is mixed, with Positive feedback on the results, Neutral inquiries, and a slight negativity towards the presentation.
*   **Top 3 Points of View:**
    *   The results are acknowledged as positive.
    *   3D convolutions are suggested as an alternative to LSTM.
    *   Emojis in blog-style posts are not appreciated by everyone.

**[[R] NeurIPS 2025: Changing Title (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1knjidk/r_neurips_2025_changing_title/)**
*   **Summary:** This is a question asking if the author can change the paper title for NeurIPS 2025. The consensus is that it's fine as long as it doesn't represent a completely new idea, to avoid placeholder submissions.
*   **Emotion:** Neutral - this is just an informational thread.
*   **Top 3 Points of View:**
    *   Changing the title is allowed as long as the core idea stays the same.
    *   The main concern is preventing placeholder submissions.

**[[D] What is an acceptable Gini impurity threshold for decision tree splits in practice? (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1knutpa/d_what_is_an_acceptable_gini_impurity_threshold/)**
*   **Summary:** The thread is about finding the optimal Gini impurity threshold for decision trees. The main point is that this depends on the application and dataset.
*   **Emotion:** The emotional tone is neutral, offering advice and caveats.
*   **Top 3 Points of View:**
    *   The ideal threshold is data-dependent.
    *   Hyperparameter optimization is beneficial.
    *   Feature pruning can be performed by comparison with a random feature.

**[[D] Looking for PhD topic/general future research directions in NLP/ML (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1knv7vy/d_looking_for_phd_topicgeneral_future_research/)**
*   **Summary:** The thread requests advice on choosing a PhD topic in NLP/ML. Suggestions include focusing on compressing symbolic reasoning into reusable prompts and using conclusion sections of recent papers to identify possible research directions.
*   **Emotion:** The emotion is neutral, providing advice.
*   **Top 3 Points of View:**
    *   Compressing symbolic reasoning or task structure into prompts could be a promising research area, especially under low-resource constraints.
    *   Reading the conclusion sections of recent papers can reveal possible directions for future work.
    *   Finding a topic that one finds interesting is important.

**[[R] could anyone help tell me what is this onnx file and how to remake it? ive have been trying to figure out for hours with little to nothing to show for it (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ko9fsq/r_could_anyone_help_tell_me_what_is_this_onnx/)**
*   **Summary:** The user asks for help in understanding and remaking an ONNX file. The main suggestion is to use Netron to visualize the file's structure.
*   **Emotion:** The emotional tone is positive, offering helpful advice.
*   **Top 3 Points of View:**
    *   Netron can be used to visualize the structure of the ONNX file.
