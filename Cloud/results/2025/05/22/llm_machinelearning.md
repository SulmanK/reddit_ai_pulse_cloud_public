---
title: "Machine Learning Subreddit"
date: "2025-05-22"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "NLP"]
---

# Overall Ranking and Top Discussions
1.  [[D] Google already out with a Text- Diffusion Model](https://www.reddit.com/r/MachineLearning/comments/1ksdn9b/d_google_already_out_with_a_text_diffusion_model/) (Score: 196)
    * Discusses Google's new text diffusion model and its potential compared to traditional LLMs.
2.  [[D] ICLR submissions should not be public on Openreview](https://www.reddit.com/r/MachineLearning/comments/1ksjgei/d_iclr_submissions_should_not_be_public_on/) (Score: 60)
    *  Addresses the transparency of ICLR submissions on Openreview and the potential for plagiarism or being "maliciously followed up."
3.  [[D] For ML academics, how many times do you resubmit a rejected paper to the big three conferences before seeking alternatives?](https://www.reddit.com/r/MachineLearning/comments/1ksr9uo/d_for_ml_academics_how_many_times_do_you_resubmit/) (Score: 35)
    *  Explores the common practice of resubmitting rejected papers to major conferences in the machine learning field and how many attempts are usually made before considering other venues.
4.  [[N] Datadog releases SOTA time series foundation model and an observability benchmark](https://www.reddit.com/r/MachineLearning/comments/1ksszls/n_datadog_releases_sota_time_series_foundation/) (Score: 19)
    *  Highlights the release of Datadog's new time series foundation model and its related observability benchmark.
5.  [[Q] [D] What are the state-of-the-art techniques for large context sizes?](https://www.reddit.com/r/MachineLearning/comments/1kslzht/q_d_what_are_the_stateoftheart_techniques_for/) (Score: 7)
    *  Queries and discusses the most advanced techniques for handling large context sizes in machine learning models.
6.  [[D] state space estimation vs ML](https://www.reddit.com/r/MachineLearning/comments/1ksqkrz/d_state_space_estimation_vs_ml/) (Score: 2)
    *  Asks for clarification and discusses the relationship between state space estimation and machine learning models.
7.  [[D] How to keep improving in Machine Learning](https://www.reddit.com/r/MachineLearning/comments/1ksw92x/d_how_to_keep_improving_in_machine_learning/) (Score: 2)
    *  Presents different opinions of how to improve in Machine Learning, and whether or not current methods are effective.
8.  [[R] Group-based recommendation](https://www.reddit.com/r/MachineLearning/comments/1ksb1vc/r_groupbased_recommendation/) (Score: 1)
    *  Seeks recommendations and advice on group-based recommendation systems.
9.  [[D] Suggestions for Poster making.](https://www.reddit.com/r/MachineLearning/comments/1ksk7j8/d_suggestions_for_poster_making/) (Score: 0)
    *   Asks for suggestions on poster creation, with Latex and Canva suggested.
10. [[D] How to use tensor flow on Linux??](https://www.reddit.com/r/MachineLearning/comments/1kswrbn/d_how_to_use_tensor_flow_on_linux/) (Score: 0)
    *   Asks how to use Tensor Flow on Linux. Uv and Conda suggested as the solutions.

# Detailed Analysis by Thread

**[ [D] Google already out with a Text- Diffusion Model (Score: 196)](https://www.reddit.com/r/MachineLearning/comments/1ksdn9b/d_google_already_out_with_a_text_diffusion_model/)**
*  **Summary:**  This thread discusses Google's new text diffusion model, comparing it to traditional LLMs and exploring its potential advantages. Users share links to related projects, express excitement about the new approach, and discuss potential applications and challenges.
*  **Emotion:** The overall emotional tone of the thread is Neutral, with some instances of positive sentiment expressing excitement about the new model.
*  **Top 3 Points of View:**
    *   Diffusion models may be more effective than autoregressive models due to their ability to refine tokens at the start of a sequence.
    *   Tools used for image diffusion models can be adapted for text diffusion, such as inpainting and controlnets.
    *   There's excitement about Google thinking outside the transformer autoregressive box.

**[ [D] ICLR submissions should not be public on Openreview (Score: 60)](https://www.reddit.com/r/MachineLearning/comments/1ksjgei/d_iclr_submissions_should_not_be_public_on/)**
*  **Summary:** This thread revolves around the debate of whether ICLR submissions should be public on Openreview. Some argue that the transparency helps establish timestamps and prevents scooping, while others express concerns about plagiarism and the potential for reviewers or their groups to exploit the knowledge.
*  **Emotion:** The overall emotional tone is Neutral, with some negative sentiment due to concerns about plagiarism. However, some comments also show positive sentiment, highlighting the benefits of transparency.
*  **Top 3 Points of View:**
    *   ICLR's transparency is beneficial because it provides proof of authorship and helps prevent scooping.
    *   The public nature of ICLR submissions increases the risk of plagiarism and exploitation by reviewers.
    *   Publishing submissions on ArXiv can provide similar protection against plagiarism.

**[ [D] For ML academics, how many times do you resubmit a rejected paper to the big three conferences before seeking alternatives? (Score: 35)](https://www.reddit.com/r/MachineLearning/comments/1ksr9uo/d_for_ml_academics_how_many_times_do_you_resubmit/)**
*  **Summary:** This thread discusses the practice of resubmitting rejected papers to major machine learning conferences. Academics share their personal experiences, strategies, and opinions on how many times to resubmit before seeking alternative venues. The consensus seems to be that it depends on the quality of the reviews and the amount of improvement made between submissions.
*  **Emotion:** The overall emotional tone is Neutral, with some positive sentiment expressing the idea that resubmissions can lead to stronger papers.
*  **Top 3 Points of View:**
    *   The number of resubmissions depends on the amount of effort required to improve the paper and how close it is to acceptance.
    *   It's important to address legitimate criticism from reviewers and improve the paper accordingly.
    *   If the reason for rejection is a lack of novelty, resubmitting is unlikely to be successful.

**[ [N] Datadog releases SOTA time series foundation model and an observability benchmark (Score: 19)](https://www.reddit.com/r/MachineLearning/comments/1ksszls/n_datadog_releases_sota_time_series_foundation/)**
*  **Summary:** This thread discusses Datadog's release of a new time series foundation model and a related observability benchmark. Some users express skepticism about the effectiveness of general-purpose time series models, while others share their own benchmark results and discuss the potential benefits of using a global model instead of local ones.
*  **Emotion:** The overall emotional tone is Neutral, with some negative sentiment from those skeptical of the approach.
*  **Top 3 Points of View:**
    *   Large, general-purpose time series models may not be effective for all tasks because each time series is governed by its own underlying stochastic process.
    *   Time-series foundation models sometimes outperform local baseline models.
    *   It's unclear what signals and patterns a foundational time series model is supposed to learn.

**[ [Q] [D] What are the state-of-the-art techniques for large context sizes? (Score: 7)](https://www.reddit.com/r/MachineLearning/comments/1kslzht/q_d_what_are_the_stateoftheart_techniques_for/)**
*  **Summary:** This thread discusses techniques for handling large context sizes in machine learning models. It includes mentions of specific architectures like Google's Titans and the use of TPUs. Some users also raise concerns about potential security risks related to finetuning in long context regions.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   Google's Titans architecture is a potential solution for long context.
    *   Attention Mechanisms are just ways of enabling either dynamic or heuristic means of sampling ‘differently’ within a sequence
    *   There are potential risks associated with finetuning in long context regions.

**[ [D] state space estimation vs ML (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1ksqkrz/d_state_space_estimation_vs_ml/)**
*  **Summary:** This thread explores the relationship between state space estimation and machine learning. Users ask for clarification about the meaning of "state space estimation" and suggest resources such as papers and books.
*  **Emotion:** The overall emotional tone is Neutral, with some positive sentiment from those sharing helpful resources.
*  **Top 3 Points of View:**
    *   The definition of "state space estimation" needs clarification.
    *   An old paper with clear discussion and examples of the relationships of ML models is recommended.
    *   Kalman filters can be examined from the perspective of PGMs.

**[ [D] How to keep improving in Machine Learning (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1ksw92x/d_how_to_keep_improving_in_machine_learning/)**
*  **Summary:** This thread discusses various approaches to improving in machine learning. Some users advise reading research papers and playing with math, while others suggest focusing on meaningful projects and abandoning "fake competitions". There's also advice for high school students to continue participating in valuable programs.
*  **Emotion:** The overall emotional tone is Negative, with people suggesting to abandon competitions and focus on meaningful projects.
*  **Top 3 Points of View:**
    *   Reading research papers and practicing math are essential for improvement.
    *   Forget about fake competitions. Focus on meaningful work.
    *   You should go to ArXiv and search for topics that are interesting to you.

**[ [R] Group-based recommendation (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1ksb1vc/r_groupbased_recommendation/)**
*  **Summary:** Asks for recommendations on group-based recommendation.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   User-based collaborative filtering.
    *   What specific scenario?

**[ [D] Suggestions for Poster making. (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ksk7j8/d_suggestions_for_poster_making/)**
*  **Summary:** A simple request for poster-making suggestions.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   Use latex and canva.

**[ [D] How to use tensor flow on Linux?? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1kswrbn/d_how_to_use_tensor_flow_on_linux/)**
*  **Summary:** Asking how to use Tensor Flow on Linux.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   Use uv... much better than other options.
    *   Use conda
    *   Use Google or YouTube  "mambaforge" or "uv pip"
