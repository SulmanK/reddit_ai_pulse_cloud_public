~~~
post_id: 1kui17w
subreddit: localllama
post_score: 81
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kui17w/openhands_devstral_is_utter_crap_as_of_may_2025/
comment_id: mu1qn91
summary_date: 2025-05-24
post_content: OpenHands + Devstral is utter *** as of May 2025 (24G VRAM)
comment_body: I appreciate you testing this, we need more tests.
I was also wondering about roo code and qwq/qwen3, but my pc is currently having issues (idk if qwen3 is better at function calling or not but qwq is supposed to be decent)
comment_summary: I appreciate you testing this, we need more tests.
I was also wondering about roo code and qwq/qwen3, but my pc is currently having issues (idk if qwen3 is better at function calling or not but qwq is supposed to be decent)
sentiment_score: 0.639236569404602
sentiment_label: Positive
~~~
post_id: 1kui17w
subreddit: localllama
post_score: 81
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kui17w/openhands_devstral_is_utter_crap_as_of_may_2025/
comment_id: mu1rjkr
summary_date: 2025-05-24
post_content: OpenHands + Devstral is utter *** as of May 2025 (24G VRAM)
comment_body: How much was the context size?
comment_summary: How much was the context size?
sentiment_score: 0.6613736748695374
sentiment_label: Neutral
~~~
post_id: 1kui17w
subreddit: localllama
post_score: 81
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kui17w/openhands_devstral_is_utter_crap_as_of_may_2025/
comment_id: mu206as
summary_date: 2025-05-24
post_content: OpenHands + Devstral is utter *** as of May 2025 (24G VRAM)
comment_body: OpenHands is OpenDevin.

OpenDevin was always ***.

Changing the name of a project won't make it good.

The smaller the model, the more the quantisation will affect it, if you have to run a 24B model at Q4_K_M, maybe you don't have the harware to pass judgement on said model.
comment_summary: OpenHands is OpenDevin.

OpenDevin was always ***.

Changing the name of a project won't make it good.

The smaller the model, the more the quantisation will affect it, if you have to run a 24B model at Q4_K_M, maybe you don't have the harware to pass judgement on said model.
sentiment_score: 0.808410108089447
sentiment_label: Neutral
~~~
post_id: 1kui17w
subreddit: localllama
post_score: 81
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kui17w/openhands_devstral_is_utter_crap_as_of_may_2025/
comment_id: mu22xvy
summary_date: 2025-05-24
post_content: OpenHands + Devstral is utter *** as of May 2025 (24G VRAM)
comment_body: > The press release boasts:

>    Devstral is light enough to run on a single RTX 4090

\* with lossy compression that loses up to 75% of the information encoded in the weights.

I don't know if it performs any better with fp16 weights but I will say that I am slowly getting tired of people *only* commenting on performance of q4 or even lower quantized LLMs. Before complaining that a model is bad, they should really try a version that has not been lobotomized. *Then* the complaint is valid.
comment_summary: Devstral is light enough to run on a single RTX. It has lossy compression that loses up to 75% of the information encoded in the weights.
sentiment_score: 0.4533820152282715
sentiment_label: Neutral
~~~
post_id: 1kui17w
subreddit: localllama
post_score: 81
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kui17w/openhands_devstral_is_utter_crap_as_of_may_2025/
comment_id: mu1tlgz
summary_date: 2025-05-24
post_content: OpenHands + Devstral is utter *** as of May 2025 (24G VRAM)
comment_body: It's been pretty decent with cline. Still not as good as commercial models like Claude, but noticeably better than Qwen30A3 IME and still reasonably fast.
comment_summary: It's been pretty decent with cline. Still not as good as commercial models like Claude, but noticeably better than Qwen30A3 IME and still reasonably fast.
sentiment_score: 0.576450765132904
sentiment_label: Positive
~~~
post_id: 1kui17w
subreddit: localllama
post_score: 81
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kui17w/openhands_devstral_is_utter_crap_as_of_may_2025/
comment_id: mu1qpjr
summary_date: 2025-05-24
post_content: OpenHands + Devstral is utter *** as of May 2025 (24G VRAM)
comment_body: Did you run devstral with default parameters in ollama? By default, it will be initialized to have context length of a mere 2048 tokens; so if you didn't change it manually, then you booted up a model that has attention span less than gpt 3.5. Could very well explain your results.
comment_summary: Did you run devstral with default parameters in ollama? By default, it will be initialized to have context length of a mere 2048 tokens; so if you didn't change it manually, then you booted up a model that has attention span less than gpt 3.5. Could very well explain your results.
sentiment_score: 0.6053609848022461
sentiment_label: Neutral
~~~
post_id: 1kui17w
subreddit: localllama
post_score: 81
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kui17w/openhands_devstral_is_utter_crap_as_of_may_2025/
comment_id: mu1pbyd
summary_date: 2025-05-24
post_content: OpenHands + Devstral is utter *** as of May 2025 (24G VRAM)
comment_body: Mistral models have been disappointing for a while now. Nemo was the last good one
comment_summary: Mistral models have been disappointing for a while now. Nemo was the last good one
sentiment_score: 0.851542055606842
sentiment_label: Negative
~~~
post_id: 1kui17w
subreddit: localllama
post_score: 81
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kui17w/openhands_devstral_is_utter_crap_as_of_may_2025/
comment_id: mu1qq58
summary_date: 2025-05-24
post_content: OpenHands + Devstral is utter *** as of May 2025 (24G VRAM)
comment_body: hey this is a real sincere failure narrative. we should post more stuff like this. that being said, I think I got a little bit further with Roo Code (suggested here on this reddit) and Open Router because it was not using quants. At the time all of the providers were bf16. I was able to get probably up to 16 steps and 40-50k context before it would trip on itself. It wasn't perfect, but got further then qwen3 models I was testingg locally. I decided not to test the local of devstral on my 3090, and use the full bf16 on providers, perhaps that is the major difference
comment_summary: With Roo Code and Open Router I got further than with qwen3. I decided not to test the local of devstral on my 3090 and use the full bf16 on providers.
sentiment_score: 0.759925127029419
sentiment_label: Neutral
~~~
post_id: 1kui17w
subreddit: localllama
post_score: 81
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kui17w/openhands_devstral_is_utter_crap_as_of_may_2025/
comment_id: mu1stqf
summary_date: 2025-05-24
post_content: OpenHands + Devstral is utter *** as of May 2025 (24G VRAM)
comment_body: I had decent results with openhands and qwen2.5-coder-32b. I’ve tried devstral for several agentic use cases (cline, continue.dev, some custom smolagents) and it’s been horrible at all of them. Phi-4 even beats devstral in my tests. Qwen3 models are the best, but the reasoning tags trip up some agent frameworks.
comment_summary: I had decent results with openhands and qwen2.5-coder-32b. I’ve tried devstral for several agentic use cases (cline, continue.dev, some custom smolagents) and it’s been horrible at all of them. Phi-4 even beats devstral in my tests. Qwen3 models are the best, but the reasoning tags trip up some agent frameworks.
sentiment_score: 0.20247595012187958
sentiment_label: Neutral
~~~
post_id: 1kui17w
subreddit: localllama
post_score: 81
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kui17w/openhands_devstral_is_utter_crap_as_of_may_2025/
comment_id: mu1tisr
summary_date: 2025-05-24
post_content: OpenHands + Devstral is utter *** as of May 2025 (24G VRAM)
comment_body: I tried and was very impressed. I asked for a model view controller object oriented snake game with documentation and for it to cycle the tasks by itself on cline and the result was flawless, I just needed to change the in game clock to 20 from 60 for it to be playable. I tried on q8 on a MacBook.
comment_summary: I asked for a model view controller object oriented snake game with documentation and for it to cycle the tasks by itself on cline and the result was flawless. I just needed to change the in game clock to 20 from 60 to play it.
sentiment_score: 0.6324505805969238
sentiment_label: Positive
~~~
post_id: 1kug045
subreddit: localllama
post_score: 40
post_url: https://v.redd.it/2ibhpziwdr2f1
comment_id: mu191ip
summary_date: 2025-05-24
post_content: Cua : Docker Container for Computer Use Agents
comment_body: If they can give some tutorials on how to modify the agent that would be great.

Currently seems only can change system prompt, model provider, and linked to a lume VM.

Currently VLM are not good enough, to build a click farm or a downvoting machine from just prompt.
comment_summary: If they can give some tutorials on how to modify the agent that would be great.

Currently seems only can change system prompt, model provider, and linked to a lume VM.

Currently VLM are not good enough, to build a click farm or a downvoting machine from just prompt.
sentiment_score: 0.7869009375572205
sentiment_label: Neutral
~~~
post_id: 1kug045
subreddit: localllama
post_score: 40
post_url: https://v.redd.it/2ibhpziwdr2f1
comment_id: mu1kh32
summary_date: 2025-05-24
post_content: Cua : Docker Container for Computer Use Agents
comment_body: Linux support?
comment_summary: Linux support?
sentiment_score: 0.7578195929527283
sentiment_label: Neutral
~~~
post_id: 1kug045
subreddit: localllama
post_score: 40
post_url: https://v.redd.it/2ibhpziwdr2f1
comment_id: mu1qbd7
summary_date: 2025-05-24
post_content: Cua : Docker Container for Computer Use Agents
comment_body: Y not just docker?
comment_summary: Y not just docker?
sentiment_score: 0.6696398258209229
sentiment_label: Neutral
~~~
post_id: 1kug045
subreddit: localllama
post_score: 40
post_url: https://v.redd.it/2ibhpziwdr2f1
comment_id: mu1p52h
summary_date: 2025-05-24
post_content: Cua : Docker Container for Computer Use Agents
comment_body: Create an agent to lower the *** music. I can't understand anything
comment_summary: Create an agent to lower the *** music. I can't understand anything
sentiment_score: 0.8678309917449951
sentiment_label: Neutral
~~~
post_id: 1kua2u0
subreddit: localllama
post_score: 39
post_url: https://i.redd.it/elvym2oe0q2f1.png
comment_id: mu0hwjc
summary_date: 2025-05-24
post_content: On the go native GPU inference and chatting with Gemma 3n E4B on an old S21 Ultra Snapdragon!
comment_body: Can I import gguf files in it?
comment_summary: Can I import gguf files in it?
sentiment_score: 0.546271562576294
sentiment_label: Neutral
~~~
post_id: 1kua2u0
subreddit: localllama
post_score: 39
post_url: https://i.redd.it/elvym2oe0q2f1.png
comment_id: mu01wxx
summary_date: 2025-05-24
post_content: On the go native GPU inference and chatting with Gemma 3n E4B on an old S21 Ultra Snapdragon!
comment_body: This is nice to see running Gemma 3n E4B on an old S21 Ultra is impressive!
Did you need to quantize the model or tweak anything to make it smooth?

They are capable of multimodal input, handling text, image, video, and audio input, did you try those ?
comment_summary: This is nice to see running Gemma 3n E4B on an old S21 Ultra is impressive!
Did you need to quantize the model or tweak anything to make it smooth?

They are capable of multimodal input, handling text, image, video, and audio input, did you try those ?
sentiment_score: 0.6915597319602966
sentiment_label: Neutral
~~~
post_id: 1kua2u0
subreddit: localllama
post_score: 39
post_url: https://i.redd.it/elvym2oe0q2f1.png
comment_id: mu04ryu
summary_date: 2025-05-24
post_content: On the go native GPU inference and chatting with Gemma 3n E4B on an old S21 Ultra Snapdragon!
comment_body: This looks amazing! What app is this?
comment_summary: This looks amazing! What app is this?
sentiment_score: 0.831569492816925
sentiment_label: Positive
~~~
post_id: 1kua2u0
subreddit: localllama
post_score: 39
post_url: https://i.redd.it/elvym2oe0q2f1.png
comment_id: mtzw4e5
summary_date: 2025-05-24
post_content: On the go native GPU inference and chatting with Gemma 3n E4B on an old S21 Ultra Snapdragon!
comment_body: Google's Edge Gallery app works on Galaxy S20+, too, at ~4 tokens per second...in case anyone needed to know that.

Clarifying: It can run Gemma 3n E4B.
comment_summary: Google's Edge Gallery app works on Galaxy S20+, too, at ~4 tokens per second...in case anyone needed to know that.

Clarifying: It can run Gemma 3n E4B.
sentiment_score: 0.9330529570579529
sentiment_label: Neutral
~~~
post_id: 1kua2u0
subreddit: localllama
post_score: 39
post_url: https://i.redd.it/elvym2oe0q2f1.png
comment_id: mu0cd79
summary_date: 2025-05-24
post_content: On the go native GPU inference and chatting with Gemma 3n E4B on an old S21 Ultra Snapdragon!
comment_body: Somehow it keeps crashing on my galaxy s22+.
comment_summary: Somehow it keeps crashing on my galaxy s22+.
sentiment_score: 0.9080684185028076
sentiment_label: Neutral
~~~
post_id: 1kuejfp
subreddit: localllama
post_score: 33
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kuejfp/new_gemma_3n_is_amazing_wish_they_suported_pc_gpu/
comment_id: mu1xack
summary_date: 2025-05-24
post_content: New gemma 3n is amazing, wish they suported pc gpu inference
comment_body: The model is in .task format, which is basically a zip archive with the model binary and the tokenizer in tflite format. If you can run tflite, you can run the model.

I wanted to convert it to regular safetensors, but it's not that simple. My plan was to use tflite2onnx to convert it to onnx, then convert it to torch and then load it and save to safetensors. The code for inference is not available, but I think I can vibecode it from model graph.

However, converting via tflite2onnx did not work so the plan failed :(
comment_summary: The model is in .task format, which is a zip archive with the model binary and the tokenizer in tflite format. The code for inference is not available, but I think I can vibecode it from model graph.
sentiment_score: 0.7586204409599304
sentiment_label: Neutral
~~~
post_id: 1kuejfp
subreddit: localllama
post_score: 33
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kuejfp/new_gemma_3n_is_amazing_wish_they_suported_pc_gpu/
comment_id: mu10ddf
summary_date: 2025-05-24
post_content: New gemma 3n is amazing, wish they suported pc gpu inference
comment_body: Not exactly answering your question, but I've spent some time tinkering that might help.

I got it running briefly on Mac with web mediapipe. But then it immediately crashed after I updated Chrome and couldn't get it working again.

Seems like there's a bug/ issue in the mediapipe js genai tasks package. Active issue on GitHub that'll be picked up by a Google dev.
comment_summary: Web mediapipe crashed after I updated Chrome and I couldn't get it working again. There's an active issue on GitHub that will be picked up by Google dev.
sentiment_score: 0.6360247731208801
sentiment_label: Neutral
~~~
post_id: 1kuejfp
subreddit: localllama
post_score: 33
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kuejfp/new_gemma_3n_is_amazing_wish_they_suported_pc_gpu/
comment_id: mu1jtgt
summary_date: 2025-05-24
post_content: New gemma 3n is amazing, wish they suported pc gpu inference
comment_body: Wait 3n only works on mobile? How? Is it just because the inferencing is done via ARM?
comment_summary: Wait 3n only works on mobile? How? Is it just because the inferencing is done via ARM?
sentiment_score: 0.62896728515625
sentiment_label: Neutral
~~~
post_id: 1kuejfp
subreddit: localllama
post_score: 33
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kuejfp/new_gemma_3n_is_amazing_wish_they_suported_pc_gpu/
comment_id: mu19q9r
summary_date: 2025-05-24
post_content: New gemma 3n is amazing, wish they suported pc gpu inference
comment_body: idk, I was super unimpressed that it refused to engage in simple nonpurposeful conversation. Nonetheless, I don't see why you couldn't at least run it in an emulator on PC if you wanted.
comment_summary: idk, I was super unimpressed that it refused to engage in simple nonpurposeful conversation. Nonetheless, I don't see why you couldn't at least run it in an emulator on PC if you wanted.
sentiment_score: 0.5634315609931946
sentiment_label: Negative
~~~
post_id: 1kuejfp
subreddit: localllama
post_score: 33
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kuejfp/new_gemma_3n_is_amazing_wish_they_suported_pc_gpu/
comment_id: mu0xq7d
summary_date: 2025-05-24
post_content: New gemma 3n is amazing, wish they suported pc gpu inference
comment_body: WayDroid?
comment_summary: WayDroid?
sentiment_score: 0.8402590751647949
sentiment_label: Neutral
~~~
post_id: 1kuejfp
subreddit: localllama
post_score: 33
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kuejfp/new_gemma_3n_is_amazing_wish_they_suported_pc_gpu/
comment_id: mu177wv
summary_date: 2025-05-24
post_content: New gemma 3n is amazing, wish they suported pc gpu inference
comment_body: Why? Wouldn't gemma3 4b or anything bigger make more sense? Qwen3 4b is even better.
comment_summary: Why? Wouldn't gemma3 4b or anything bigger make more sense? Qwen3 4b is even better.
sentiment_score: 0.43953534960746765
sentiment_label: Neutral
~~~
post_id: 1kuejfp
subreddit: localllama
post_score: 33
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kuejfp/new_gemma_3n_is_amazing_wish_they_suported_pc_gpu/
comment_id: mu1tu3c
summary_date: 2025-05-24
post_content: New gemma 3n is amazing, wish they suported pc gpu inference
comment_body: I thought they announced it'd be coming to desktop inference after they worked with partners.
comment_summary: I thought they announced it'd be coming to desktop inference after they worked with partners.
sentiment_score: 0.8365468382835388
sentiment_label: Neutral
~~~
post_id: 1kuejfp
subreddit: localllama
post_score: 33
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kuejfp/new_gemma_3n_is_amazing_wish_they_suported_pc_gpu/
comment_id: mu150pj
summary_date: 2025-05-24
post_content: New gemma 3n is amazing, wish they suported pc gpu inference
comment_body: There's a ton of cool bleeding edge stuff happening in Gemma 3n. It's basically "pre-quantized" in an extremely sophisticated way, and it uses a bunch of new primitives that I haven't heard of anyone using before.


Google really put a lot of fancy expertise into this model, if I understand correctly.


Usually the fastest way to run really unusual new models is to wait for vLLM to support the new model class. Llama.cpp lags behind, and Ollama behind that.
comment_summary: Gemma 3n is a new pre-quantized model developed by Google. Llama lags behind it.
sentiment_score: 0.7347872257232666
sentiment_label: Neutral
~~~
post_id: 1kudhxg
subreddit: localllama
post_score: 22
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kudhxg/cosmosreason1_physical_ai_common_sense_and/
comment_id: mu0po8s
summary_date: 2025-05-24
post_content: Cosmos-Reason1: Physical AI Common Sense and Embodied Reasoning Models
comment_body: How to use it with the video?
comment_summary: How to use it with the video?
sentiment_score: 0.5884822010993958
sentiment_label: Neutral
~~~
post_id: 1kuimwg
subreddit: localllama
post_score: 22
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kuimwg/nvlink_vs_no_nvlink_devstral_small_2x_rtx_3090/
comment_id: mu23br6
summary_date: 2025-05-24
post_content: NVLink vs No NVLink: Devstral Small 2x RTX 3090 Inference Benchmark with vLLM
comment_body: 6 months ago I was getting some groceries and someone put up a local pickup offer for a 4-slot NVLINK about 3 streets away for about 30 USD. On ebay they were already over 200 at that time, plus shipping and tax. Felt a bit bad about it, but it also felt like the universe really wants to help me.

Similar experience, and I was wondering whether having 16x/16x 4.0 instead of 8x/8x 4.0 bifurcation would have a similar unimpressive impact.

EDIT: I am also happy to try out some benchmarks, if someone sends me a compose yaml or exact script to run it. Ryzen 7600, 2x48GB ddr5, 2x3090, 1200w PSU.
comment_summary: Someone put up a local pickup offer for a 4-slot NVLINK about 3 streets away for about 30 USD. It was already over 200 on ebay at that time, plus shipping and tax.
sentiment_score: 0.5423979163169861
sentiment_label: Neutral
~~~
post_id: 1kuimwg
subreddit: localllama
post_score: 22
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kuimwg/nvlink_vs_no_nvlink_devstral_small_2x_rtx_3090/
comment_id: mu1z450
summary_date: 2025-05-24
post_content: NVLink vs No NVLink: Devstral Small 2x RTX 3090 Inference Benchmark with vLLM
comment_body: I know it may not be cutting edge, but curious if NVLink improves llama.cpp's split-mode row performance given it's generally significantly slower that split-mode layer without NVLink
comment_summary: I know it may not be cutting edge, but curious if NVLink improves llama.cpp's split-mode row performance given it's generally significantly slower that split-mode layer without NVLink
sentiment_score: 0.862796425819397
sentiment_label: Neutral
~~~
post_id: 1kuimwg
subreddit: localllama
post_score: 22
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kuimwg/nvlink_vs_no_nvlink_devstral_small_2x_rtx_3090/
comment_id: mu1v9tf
summary_date: 2025-05-24
post_content: NVLink vs No NVLink: Devstral Small 2x RTX 3090 Inference Benchmark with vLLM
comment_body: I would avoid using ggufs w/ vLLM, the support is not stellar yet. Just for fun, try an fp8 quant, and an awq / int4 one. When fully using both GPUs with tp I think nvlink is 10-20% faster. Also, try to run as many parallel sessions as you can (when starting vLLM it will tell you how many based on available ram and seq ***)
comment_summary: When fully using both GPUs with tp, nvlink is 10-20% faster. Try to run as many parallel sessions as you can.
sentiment_score: 0.3466348648071289
sentiment_label: Neutral
~~~
post_id: 1kuimwg
subreddit: localllama
post_score: 22
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kuimwg/nvlink_vs_no_nvlink_devstral_small_2x_rtx_3090/
comment_id: mu1wvte
summary_date: 2025-05-24
post_content: NVLink vs No NVLink: Devstral Small 2x RTX 3090 Inference Benchmark with vLLM
comment_body: I bet can get more from NVLink if try to use bigger model that fills the VRAM on both cards.
comment_summary: I bet can get more from NVLink if try to use bigger model that fills the VRAM on both cards.
sentiment_score: 0.764309287071228
sentiment_label: Neutral
~~~
post_id: 1kuimwg
subreddit: localllama
post_score: 22
post_url: https://www.reddit.com/r/LocalLLaMA/comments/1kuimwg/nvlink_vs_no_nvlink_devstral_small_2x_rtx_3090/
comment_id: mu25w0f
summary_date: 2025-05-24
post_content: NVLink vs No NVLink: Devstral Small 2x RTX 3090 Inference Benchmark with vLLM
comment_body: I believe from other posts here (search nvlink) that vllm excels in throughput, so 4 concurrent requests is unlikely to benefit from nvlink. Now, if your use case only requires 4 threads, then your assessment is sound. You might as well also just use ollama or llama.cpp.

Also, what is the test script from Claude? Can
