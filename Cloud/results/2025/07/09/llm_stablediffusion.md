---
title: "Stable Diffusion Subreddit"
date: "2025-07-09"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["AI", "image generation", "StableDiffusion"]
---

# Overall Ranking and Top Discussions
1.  [What's everyone using AI image gen for?](https://www.reddit.com/r/StableDiffusion/comments/1lvnt9l/whats_everyone_using_ai_image_gen_for/) (Score: 22)
    *   The discussion revolves around the various uses people have found for AI image generation, from artistic expression to commercial applications and even less serious purposes.
2.  [New LTXV IC-Lora Tutorial – Quick Video Walkthrough](https://v.redd.it/iblcoqlgtvbf1) (Score: 20)
    *   A user seeks advice on LTXV IC-Lora Tutorial and wonders if speed and ease (LTXV) or quality and detail (WAN 2.1) should be prioritized for prototyping.
3.  [Some wan2.1 text2image results.](https://www.reddit.com/r/StableDiffusion/comments/1lvp2rf/some_wan21_text2image_results/) (Score: 8)
    *   Users are discussing the quality and potential of the wan2.1 model for text-to-image generation, with some considering it the next model to watch.
4.  [Flux Kontext - any tricks to change the background without it looking like a photoshop edit ?](https://i.redd.it/lw806wsx7wbf1.png) (Score: 3)
    *   Users are sharing tips and tricks on how to seamlessly change backgrounds in images using Flux Kontext, avoiding a "photoshopped" look.
5.  [Wan 2.1 Question: Switching from Native to Wrapper (Workflow included)](https://i.redd.it/ytnd4gumvvbf1.png) (Score: 3)
    *   A user seeks help with switching from Native to Wrapper using Wan 2.1, and has included a workflow.
6.  [What do people use to caption video clips when training?](https://www.reddit.com/r/StableDiffusion/comments/1lvof9k/what_do_people_use_to_caption_video_clips_when/) (Score: 1)
    *   A user is asking about tools to caption video clips for training purposes.
7.  [Higgsfiled Soul model and a lora trainer](https://www.reddit.com/r/StableDiffusion/comments/1lvo613/higgsfiled_soul_model_and_a_lora_trainer/) (Score: 0)
    *   The post is about a new model and Lora trainer, but some users are skeptical about its release and suspect it's an advertisement.
8.  [Flux Kontext on mac via remote gradio interface or something similar? (not not through ComfyUI or drawthings)](https://www.reddit.com/r/StableDiffusion/comments/1lvobm4/flux_kontext_on_mac_via_remote_gradio_interface/) (Score: 0)
    *   A user inquires about running Flux Kontext on a Mac through a remote Gradio interface, outside of ComfyUI or Drawthings.
9.  [Anyone had success using flux Kontext to remove dark shadows caused by the flash on a photo?](https://www.reddit.com/r/StableDiffusion/comments/1lvocb8/anyone_had_success_using_flux_kontext_to_remove/) (Score: 0)
    *   A user is asking if anyone has had success using Flux Kontext to remove dark shadows caused by flash in photos.
10. [Topaz or no Topaz?](https://www.reddit.com/r/StableDiffusion/comments/1lvqiv2/topaz_or_no_topaz/) (Score: 0)
    *   The discussion revolves around whether to use Topaz for upscaling, with users sharing alternative tools and workflows.
11. [Three month old account is  making $$$ “drawing” pets across all the large subreddits. Theoretically, how easy would their art be to make with AI?](https://www.reddit.com/r/StableDiffusion/comments/1lvr53w/three_month_old_account_is_making_drawing_pets/) (Score: 0)
    *   A user questions whether AI could be used to create pet portraits.
12. [Why does Controlnet works better with illustrious model](https://www.reddit.com/r/StableDiffusion/comments/1lvrs24/why_does_controlnet_works_better_with_illustrious/) (Score: 0)
    *   A user asks why ControlNet works better with the Illustrious model, and gets the reply that different CNs are designed for use with different SDXL models.
13. [Using img2img through Replicate's API and it's just outputting cartoonish, abstract garbage. Need some serious help.](https://www.reddit.com/r/StableDiffusion/comments/1lvsj0m/using_img2img_through_replicates_api_and_its_just/) (Score: 0)
    *   A user seeks help with troubleshooting cartoonish and abstract outputs when using img2img through Replicate's API.

# Detailed Analysis by Thread
**[What's everyone using AI image gen for? (Score: 22)](https://www.reddit.com/r/StableDiffusion/comments/1lvnt9l/whats_everyone_using_ai_image_gen_for/)**
*  **Summary:** The discussion centers on the diverse applications of AI image generation, spanning artistic creation, commercial purposes, and more lighthearted uses.
*  **Emotion:** Predominantly Neutral, with a slight leaning towards Positive sentiment in some comments.
*  **Top 3 Points of View:**
    *   AI image generation allows individuals without drawing skills to express their artistic vision.
    *   AI is used to create on-brand visuals for SMBs.
    *   It's used for less serious purposes (gooning).

**[New LTXV IC-Lora Tutorial – Quick Video Walkthrough (Score: 20)](https://v.redd.it/iblcoqlgtvbf1)**
*  **Summary:** A user seeks advice on whether to prioritize speed and ease (LTXV) or quality and detail (WAN 2.1) for AI image generation prototyping, considering their 16 GB VRAM and a photos-to-video application.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   AI (Wan 2.1) advises using LTXV because of speed and ease.
    *   User needs advice on LTXV IC-Lora Tutorial.
    *   User is looking for upscale method or LORA method to consider.

**[Some wan2.1 text2image results. (Score: 8)](https://www.reddit.com/r/StableDiffusion/comments/1lvp2rf/some_wan21_text2image_results/)**
*  **Summary:** Users are impressed with the wan2.1 model for text-to-image generation and are discussing its potential, minimum specs, and how trainable it is.
*  **Emotion:** Leans towards Positive, with users expressing admiration for the image quality.
*  **Top 3 Points of View:**
    *   WAN 2.1 is excellent at training
    *   WAN should be kept eye on for image generating.
    *   WAN works great with ultimate upscaler.

**[Flux Kontext - any tricks to change the background without it looking like a photoshop edit ? (Score: 3)](https://i.redd.it/lw806wsx7wbf1.png)**
*  **Summary:** Users share various tips and tricks for seamlessly changing backgrounds in images using Flux Kontext, aiming to avoid a noticeable "photoshopped" appearance.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Use img2img with a low denoise after.
    *   Try adding text to the prompt “add bokeh effect to background”.
    *   Prompt it to include the character in the context of your scene.

**[Wan 2.1 Question: Switching from Native to Wrapper (Workflow included) (Score: 3)](https://i.redd.it/ytnd4gumvvbf1.png)**
*  **Summary:** User posted an image and seeks help with switching from Native to Wrapper using Wan 2.1, and has included a workflow.
*  **Emotion:** Negative
*  **Top 3 Points of View:**
    *   User apologized for his image.
    *   User is switching from Native to Wrapper using Wan 2.1.
    *   User included a workflow.

**[What do people use to caption video clips when training? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1lvof9k/what_do_people_use_to_caption_video_clips_when/)**
*  **Summary:** A user is asking about tools to caption video clips for training purposes.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   DeZoomer video caption tool might be helpful.
    *   Unsure of how good it is.

**[Higgsfiled Soul model and a lora trainer (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1lvo613/higgsfiled_soul_model_and_a_lora_trainer/)**
*  **Summary:** The post is about a new model and Lora trainer, but some users are skeptical about its release and suspect it's an advertisement.
*  **Emotion:** Negative
*  **Top 3 Points of View:**
    *   User asks if weights are released.
    *   Suspect the question is an advertisement for the closed source model.

**[Flux Kontext on mac via remote gradio interface or something similar? (not not through ComfyUI or drawthings) (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1lvobm4/flux_kontext_on_mac_via_remote_gradio_interface/)**
*  **Summary:** A user inquires about running Flux Kontext on a Mac through a remote Gradio interface, outside of ComfyUI or Drawthings.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   BFL only provides their paid API, ComfyUI, and the diffusers interfaces.
    *   You could probably rig up your own Gradio/restUI service using a diffuser backend.
    *   You could just as easily setup ComfyUI and open a workflow that does the same.

**[Anyone had success using flux Kontext to remove dark shadows caused by the flash on a photo? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1lvocb8/anyone_had_success_using_flux_kontext_to_remove/)**
*  **Summary:** A user is asking if anyone has had success using Flux Kontext to remove dark shadows caused by flash in photos.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   A user provided a link of a workflow that might be helpful.
    *   Making a mask on the shadow and using a prompt.
    *   Give example of flux Kontext.

**[Topaz or no Topaz? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1lvqiv2/topaz_or_no_topaz/)**
*  **Summary:** The discussion revolves around whether to use Topaz for upscaling, with users sharing alternative tools and workflows.
*  **Emotion:** Mixed, with both Positive and Negative sentiments expressed.
*  **Top 3 Points of View:**
    *   Topaz does a very nice job.
    *   Alternatively, Waifu2x-Extension-GUI has support for REAL-ESRGAN.
    *   It's better to upscale right after wan has generated samples.

**[Three month old account is  making $$$ “drawing” pets across all the large subreddits. Theoretically, how easy would their art be to make with AI? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1lvr53w/three_month_old_account_is_making_drawing_pets/)**
*  **Summary:** A user questions whether AI could be used to create pet portraits.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   It's easy with AI.
    *   If you can draw, it’s literally easier to draw it.
    *   Find the right Lora and image2image tools in ComfyUI with a lot of inpainting (to remove the background).

**[Why does Controlnet works better with illustrious model (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1lvrs24/why_does_controlnet_works_better_with_illustrious/)**
*  **Summary:** A user asks why ControlNet works better with the Illustrious model.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Each model should have its own CN models.
    *   There are too many CNs for SDXL models to just say that.

**[Using img2img through Replicate's API and it's just outputting cartoonish, abstract garbage. Need some serious help. (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1lvsj0m/using_img2img_through_replicates_api_and_its_just/)**
*  **Summary:** A user seeks help with troubleshooting cartoonish and abstract outputs when using img2img through Replicate's API.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Output image is cartoonish and abstract.
