text
---
title: "Stable Diffusion Subreddit"
date: "2025-07-15"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [Wan Vace T2V - Accept time with actions in the prompt! and os really well!](https://v.redd.it/b0q7dlx2f2df1) (Score: 49)
    *   Discussing a new T2V model that accepts time with actions in the prompt.
2.  [Pusa V1.0 Model Open Source Efficient / Better Wan Model... i think?](https://www.reddit.com/r/StableDiffusion/comments/1m0mol1/pusa_v10_model_open_source_efficient_better_wan/) (Score: 38)
    *   Discussing the Pusa V1.0 model as a potential alternative or improvement to the Wan model.
3.  [Classic Painting Flux LoRA](https://www.reddit.com/gallery/1m0odh3) (Score: 31)
    *   Sharing and discussing a LoRA for converting images to classic painting styles within Flux Kontext Dev.
4.  [Flux Kontext - Ultimate Photo Restoration Tool](https://youtu.be/Dj6ISloUvtU) (Score: 10)
    *   Promoting Flux Kontext as a photo restoration tool, with some users noting a "smooth flux skin" effect.
5.  [WAN2.1 MultiTalk](https://v.redd.it/ifxetb3e72df1) (Score: 8)
    *   Showcasing WAN2.1 MultiTalk, with requests for the workflow and model link.
6.  [Soo.. how can I animate any character from just a static image?.. I am completely new at this.. so any tips is greatly appreciated.](https://www.reddit.com/r/StableDiffusion/comments/1m0lbat/soo_how_can_i_animate_any_character_from_just_a/) (Score: 2)
    *   Asking for tips on animating characters from static images, with suggestions for using Wan 2.1 or FramePack.
7.  [Why isn’t VAE kept trainable in diffusion models?](https://www.reddit.com/r/StableDiffusion/comments/1m0lhog/why_isnt_vae_kept_trainable_in_diffusion_models/) (Score: 2)
    *   Discussing the reasons why Variational Autoencoders (VAEs) are not typically kept trainable in diffusion models.
8.  [What would you tell your former self if just starting out?](https://www.reddit.com/r/StableDiffusion/comments/1m0npli/what_would_you_tell_your_former_self_if_just/) (Score: 1)
    *   Users are sharing advice they would give themselves when first starting out with Stable Diffusion.
9.  [Any tips for writing detailed image gen prompts?](https://www.reddit.com/r/StableDiffusion/comments/1m0otq0/any_tips_for_writing_detailed_image_gen_prompts/) (Score: 1)
    *   Seeking advice on how to write detailed image generation prompts.
10. [Is she AI or a real person?](https://www.reddit.com/gallery/1m0r5xy) (Score: 0)
    *   Discussion about whether an image is of a real person or generated by AI.
11. [Forge UI + Flux Workaround: CUDA error: no kernel image is available for execution on the device](https://www.reddit.com/r/StableDiffusion/comments/1m0jvrf/forge_ui_flux_workaround_cuda_error_no_kernel/) (Score: 0)
    *   Providing a workaround for a CUDA error encountered when using Forge UI with Flux.
12. [ReForge textual inversion/embedding issue?](https://www.reddit.com/r/StableDiffusion/comments/1m0k97e/reforge_textual_inversionembedding_issue/) (Score: 0)
    *   Reporting an issue with textual inversion/embedding in ReForge.
13. [Training Stable Diffusion](https://www.reddit.com/r/StableDiffusion/comments/1m0nkrv/training_stable_diffusion/) (Score: 0)
    *   Asking about the best practices for training Stable Diffusion models.
14. [Help for illustrious model](https://www.reddit.com/r/StableDiffusion/comments/1m0qwc9/help_for_illustrious_model/) (Score: 0)
    *   Requesting assistance with the Illustrious model.

# Detailed Analysis by Thread
**[ Wan Vace T2V - Accept time with actions in the prompt! and os really well! (Score: 49)](https://v.redd.it/b0q7dlx2f2df1)**
*  **Summary:**  Discussion about Wan Vace T2V, a text-to-video model that accepts time with actions in the prompt. Users are sharing their thoughts and observations, asking questions about its functionality and comparing it to other models.
*  **Emotion:** The overall emotional tone is neutral, with users making observations and asking questions. A comment shows a Positive sentiment score.
*  **Top 3 Points of View:**
    *   The model appears to follow the order of the prompt rather than strict timestamps.
    *   Some users believe the T2V claim is misleading because of reference images used.
    *   Some users are interested in the potential for longer videos on consumer hardware.

**[ Pusa V1.0 Model Open Source Efficient / Better Wan Model... i think? (Score: 38)](https://www.reddit.com/r/StableDiffusion/comments/1m0mol1/pusa_v10_model_open_source_efficient_better_wan/)**
*  **Summary:**  Discussion about the Pusa V1.0 model, positioned as a potential open-source and efficient alternative to the Wan model. Users discuss its strengths, weaknesses, and compare it to existing solutions.
*  **Emotion:** The emotional tone is mainly neutral.
*  **Top 3 Points of View:**
    *   Some users believe that the model is not better than the Wan model and produces artifacts.
    *   Others point out that the model uses 720p models, but can generate 8s videos without weird artifacts.
    *   Others note Wan2.1 T2V can handle different timepoints for each frame.

**[ Classic Painting Flux LoRA (Score: 31)](https://www.reddit.com/gallery/1m0odh3)**
*  **Summary:**  A user shares their experience using a "Classic Painting Flux LoRA" in Flux Kontext Dev.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   LoRA slot works in Flux Kontext Dev.
    *   There are noticeable differences with the prompt "Convert this image to a fine oil painting" with and without LoRA.
    *  LoRA better eyes, toenails/fingernails, and slightly finer brush-strokes.

**[ Flux Kontext - Ultimate Photo Restoration Tool (Score: 10)](https://youtu.be/Dj6ISloUvtU)**
*  **Summary:**  Users are commenting on the Flux Kontext photo restoration tool.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 1 Points of View:**
    *   still has a bit of that smooth flux skin based on the thumbnail I'm seeing.

**[ WAN2.1 MultiTalk (Score: 8)](https://v.redd.it/ifxetb3e72df1)**
*  **Summary:**  Users are commenting on WAN2.1 MultiTalk.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 1 Points of View:**
    *   Looks nice, requests for the workflow and model link.

**[ Soo.. how can I animate any character from just a static image?.. I am completely new at this.. so any tips is greatly appreciated. (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1m0lbat/soo_how_can_i_animate_any_character_from_just_a/)**
*  **Summary:**  A user is asking for tips on how to animate a character from a static image.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 2 Points of View:**
    *   Any img2vid model can do that, like Wan 2.1 or FramePack locally.
    *   You want to fly before learning to walk.

**[ Why isn’t VAE kept trainable in diffusion models? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1m0lhog/why_isnt_vae_kept_trainable_in_diffusion_models/)**
*  **Summary:**  Users are discussing the reasons why VAE isn't kept trainable in diffusion models.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   The Variational Auto Encoder doesn't make sense to train with the U-Net of a Diffusion model. In a VAE, you have an encoder, a bottleneck and a decoder.
    *   "AE" part stands for autoencoder, meaning the training is that it takes input images and compress them to a latent space and then decompress them back to pixel space with minimal reconstruction loss.
    *   VAEs operate within the latent space as a distribution of the mean and standard deviation of a given video or image. When trained using a specific VAE, the model learns to denoise latents that fit within this specific latent space.

**[ What would you tell your former self if just starting out? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1m0npli/what_would_you_tell_your_former_self_if_just/)**
*  **Summary:**  Users are sharing advice they would give themselves when first starting out with Stable Diffusion.
*  **Emotion:** The overall emotional tone is neutral, except for one comment that is negative.
*  **Top 3 Points of View:**
    *   As long as you have 4GB Vram you should be able to operate Comfy for image generation.
    *   Organize your loras from the beginning.
    *   learn python, basic venv, filepath POSIX vs Windows, and little bit of torch

**[ Any tips for writing detailed image gen prompts? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1m0otq0/any_tips_for_writing_detailed_image_gen_prompts/)**
*  **Summary:**  Users are looking for tips on writing detailed image generation prompts.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Use 1 of the latest Thinking models. If money is the issue, Gemini 2.5 pro in Google AI Studio is free and has no restrictions.
    *   LLM does prompts from ideas for me. (it does clip\_l and t5 separately).
    *   For current generation image models (Flux, SD3.5, etc) with natural language text encoders, using an LLM to refine your prompt works well.

**[ Is she AI or a real person? (Score: 0)](https://www.reddit.com/gallery/1m0r5xy)**
*  **Summary:**  Discussion about whether an image is of a real person or generated by AI.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   AI, you can create something similar to this using a LoRA model available.
    *   She is real... you even know her name. Just Google it. Her full name is Kristina Alexandrovna Gotfrid... Russian influencer.
    *   Third one definitely makes me sus of AI generated. The classic extended arm cut to presumably her holding the smartphone off-screen to take the photo, but it doesn't look right at all here.

**[ Forge UI + Flux Workaround: CUDA error: no kernel image is available for execution on the device (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1m0jvrf/forge_ui_flux_workaround_cuda_error_no_kernel/)**
*  **Summary:**  Users are providing a workaround for a CUDA error encountered when using Forge UI with Flux.
*  **Emotion:** The overall emotional tone is neutral, expect for one Positive comment.
*  **Top 3 Points of View:**
    *   If the error is related to having a 50xx card, you dont need the nightly, just the latest stable from [pytorch.org](http://pytorch.org) will do.
    *   SEVEN hours today trying to get the same problem fixed and working. Went around in circles with chatgpt, Finally found this video. It worked.  [https://www.youtube.com/watch?v=PtGgjdw5koA&ab\_channel=AIKnowledge2Go](https://www.youtube.com/watch?v=PtGgjdw5koA&ab_channel=AIKnowledge2Go)
    *   which version of forge are you using, because the latest version is not working.

**[ ReForge textual inversion/embedding issue? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1m0k97e/reforge_textual_inversionembedding_issue/)**
*  **Summary:**  Users are reporting an issue with textual inversion/embedding in ReForge.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 1 Points of View:**
    *   Should show the embeddings in question and the models they are for.

**[ Training Stable Diffusion (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1m0nkrv/training_stable_diffusion/)**
*  **Summary:**  Users are asking about the best practices for training Stable Diffusion models.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 2 Points of View:**
    *   For Flux, all you need is 20-50 images if the artist style is consistent. If the style is not consistent, you can train several LoRAs for each style.
    *   You can't re-create the style, but be very close to it. As for amount, there is no hard requirement - more is better, but only if it is actually a good image that adds variety, otherwise it can make it worse.

**[ Help for illustrious model (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1m0qwc9/help_for_illustrious_model/)**
*  **Summary:**  Users are requesting assistance with the Illustrious model.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 2 Points of View:**
    *   What kind of help are you looking for exactly with the model ?
    *   You need to download it to run it, or run it on an online generator like the one on civitai. Dezgo only has really old models.
