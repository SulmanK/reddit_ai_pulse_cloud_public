---
title: "Stable Diffusion Subreddit"
date: "2025-07-30"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [[D] All in one WAN 2.2 model merges: 4-steps, 1 CFG, 1 model speeeeed (both T2V and I2V)](https://huggingface.co/Phr00t/WAN2.2-14B-Rapid-AllInOne) (Score: 69)
    * Users discuss the new WAN 2.2 model merges, focusing on speed and quality, and requesting workflows and comparisons.
2.  [The Improvement from Wan2.2 to Wan2.1 is a bit insane](https://v.redd.it/zx1s724wy1gf1) (Score: 48)
    *  Users discuss the improvements from Wan2.2 model compared to Wan2.1, including speed improvements through latent upscaling.
3.  [WAN 2.2 - I2V 14B - First Person perspective tests](https://v.redd.it/ud90o44w02gf1) (Score: 32)
    *  Users are commenting on the generated first-person perspective videos using Wan 2.2, noting the quality of motions.
4.  [The State of Local Video Generation (Wan 2.2 Update)](https://v.redd.it/fiaaw9g8d2gf1) (Score: 6)
    *  Users discussed the coherent scene and motions with Wan 2.2.
5.  [Wan 2.2 | Good level violence handling](https://v.redd.it/ji96nmo7q1gf1) (Score: 6)
    *  Users discuss the level of violence handling with Wan 2.2, with many making lighthearted comments.
6.  [There are gpl trolls from comfy project that attack [Forge] regularly.](https://www.reddit.com/r/StableDiffusion/comments/1mdf38j/there_are_gpl_trolls_from_comfy_project_that/) (Score: 4)
    *  Users discuss drama and trolling from the Comfy project towards Forge, and the possible motivations behind it.
7.  [Has anyone already trained Lora using Wan 2.2 as base models?](https://www.reddit.com/r/StableDiffusion/comments/1mdfvay/has_anyone_already_trained_lora_using_wan_22_as/) (Score: 4)
    *  Users are discussing the cross-compatibility with LoRAs of Wan2.1_T2V_14B and Wan2.2_T2V_Low_noise checkpoints.
8.  [Where to begin if I want to generate an image with a reference?](https://www.reddit.com/r/StableDiffusion/comments/1mddv0b/where_to_begin_if_i_want_to_generate_an_image/) (Score: 1)
    *  Users provide suggestions on how to generate images with a reference, discussing tools like ComfyUI, Flux Dev, and inpainting techniques.
9.  [Does anyone have the T2V version of Kijai's wan 2.2 workflow already?](https://www.reddit.com/r/StableDiffusion/comments/1mdefav/does_anyone_have_the_t2v_version_of_kijais_wan_22/) (Score: 1)
    *  Users are looking for and discussing the T2V version of Kijai's Wan 2.2 workflow and how to adapt the i2v workflow.
10. [ALL made with Krea.AI - image, video, lip sync, and enhancement](https://v.redd.it/3d7m4oehw1gf1) (Score: 0)
    *  Users are asking which models they use.
11. [I can't take this anymore](https://www.reddit.com/r/StableDiffusion/comments/1mddfwc/i_cant_take_this_anymore/) (Score: 0)
    *  Users are suggesting forge UI or paying someone to do it.
12. [better quality for photorealistic output with wan 2.2.?](https://www.reddit.com/r/StableDiffusion/comments/1mdf0br/better_quality_for_photorealistic_output_with_wan/) (Score: 0)
    *  Users are discussing whether the user is talking about T2V because I2V is excellent.
13. [Current WAN 2.1 I2V takes around 2 hours for a 3 second video. Are there any new workflows or tech that can speed this up? (RTX 3060 12gb)](https://www.reddit.com/r/StableDiffusion/comments/1mdf5np/current_wan_21_i2v_takes_around_2_hours_for_a_3/) (Score: 0)
    *  Users are looking for ways to speed up WAN 2.1 I2V, receiving advice on different models, workflows, and hardware configurations.
14. [How to install Stable Diffusion on Windows?](https://www.reddit.com/r/StableDiffusion/comments/1mdfwva/how_to_install_stable_diffusion_on_windows/) (Score: 0)
    *  Users are discussing the installation of Stable Diffusion on Windows, focusing on the requirement of an Nvidia GPU and potential workarounds for AMD or CPU.
15. [I'm exhausted trying to keep ip](https://www.reddit.com/r/StableDiffusion/comments/1mdgfit/im_exhausted_trying_to_keep_ip/) (Score: 0)
    *  Users are sharing Google Drive Folders.

# Detailed Analysis by Thread
**[[D] All in one WAN 2.2 model merges: 4-steps, 1 CFG, 1 model speeeeed (both T2V and I2V) (Score: 69)](https://huggingface.co/Phr00t/WAN2.2-14B-Rapid-AllInOne)**
*  **Summary:** This thread is about the release of an all-in-one WAN 2.2 model merge, focusing on its speed improvements for both text-to-video (T2V) and image-to-video (I2V) tasks. Users are expressing interest, asking for workflows, hardware specifications, comparisons to previous models, and example outputs.
*  **Emotion:** The emotional tone is generally positive, with expressions of excitement and gratitude towards the model's creator. Some neutral inquiries are present, seeking clarifications and technical details.
*  **Top 3 Points of View:**
    *   Excitement and willingness to test the new model.
    *   Requests for workflow details and hardware recommendations.
    *   Inquiries about quality degradation compared to the original models.

**[The Improvement from Wan2.2 to Wan2.1 is a bit insane (Score: 48)](https://v.redd.it/zx1s724wy1gf1)**
*  **Summary:**  Users are discussing the improvements observed in Wan2.2 compared to Wan2.1. The conversation touches on speed optimizations using latent upscaling techniques, face consistency, prompt understanding and motion. Some users noticed a typo in the title.
*  **Emotion:** The emotional tone is positive, with users expressing excitement about the improvements in the new model. Some users are neutral, pointing out inconsistencies they encountered.
*  **Top 3 Points of View:**
    *   Wan2.2 has improved greatly compared to Wan2.1.
    *   Using latent upscaling at a very low resolution can speed things up a bit.
    *   There are some issues with face consistency.

**[WAN 2.2 - I2V 14B - First Person perspective tests (Score: 32)](https://v.redd.it/ud90o44w02gf1)**
*  **Summary:** The thread discusses first-person perspective video generation using WAN 2.2 - I2V 14B. The user who made the post shares details about their setup, including using Flux for input images and a 3090ti for video generation. The user has found decent results, but also notes that a Lora might help a lot.
*  **Emotion:** The emotional tone is positive, with users noting the nice motions and decent performance.
*  **Top 3 Points of View:**
    *   Motions generated by WAN 2.2 are really nice.
    *   WAN 2.2 - I2V 14B Q5-K-M did some tests to see how it handles first person perspectives.
    *   It did decently well, although a lora for this specifically would probably help a lot.

**[The State of Local Video Generation (Wan 2.2 Update) (Score: 6)](https://v.redd.it/fiaaw9g8d2gf1)**
*  **Summary:**  The thread discusses local video generation using Wan 2.2.
*  **Emotion:** Positive emotion due to coherent scene and motion.
*  **Top 3 Points of View:**
    *   2.2 motions are really really nice
    *   the overall scene is very coherent

**[Wan 2.2 | Good level violence handling (Score: 6)](https://v.redd.it/ji96nmo7q1gf1)**
*  **Summary:**  Users are humorously commenting on a video showcasing Wan 2.2's handling of violence, specifically a scene involving a sword. The general consensus is that the model is following the prompt accurately, although the outcome might not be exactly what was expected.
*  **Emotion:** Neutral with bits of positive due to humorous and joking tone.
*  **Top 3 Points of View:**
    *   The model is following the prompt accurately.
    *   The action resembles a haircut or neck massage.
    *   Users are comparing the scene to other references.

**[There are gpl trolls from comfy project that attack [Forge] regularly. (Score: 4)](https://www.reddit.com/r/StableDiffusion/comments/1mdf38j/there_are_gpl_trolls_from_comfy_project_that/)**
*  **Summary:** The thread discusses alleged attacks by "gpl trolls" from the Comfy project on Forge. Users share anecdotes and opinions about the situation, with some believing the claims and others remaining skeptical. The discussion touches on open-source community values and the potential impact of revenue generation on behavior.
*  **Emotion:** The emotional tone is mixed, with some users expressing concern and others exhibiting a neutral stance. There is a hint of negativity towards the alleged trolls.
*  **Top 3 Points of View:**
    *   The possibility of trolling from the Comfy project is believable.
    *   Trolls are making outlandish statements that are not aligned with open-source community values
    *   Remember that when people say Comfy is the only way to go.

**[Has anyone already trained Lora using Wan 2.2 as base models? (Score: 4)](https://www.reddit.com/r/StableDiffusion/comments/1mdfvay/has_anyone_already_trained_lora_using_wan_22_as/)**
*  **Summary:** A user asks if anyone has trained LoRA using Wan 2.2 as a base model. Another user replied that they know Wan2.1_T2V_14B and Wan2.2_T2V_Low_noise checkpoints are almost completely cross-compatible with LoRAs.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Wan2.1_T2V_14B and Wan2.2_T2V_Low_noise checkpoints are almost completely cross-compatible with LoRAs.

**[Where to begin if I want to generate an image with a reference? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1mddv0b/where_to_begin_if_i_want_to_generate_an_image/)**
*  **Summary:** The thread discusses how to generate images with a reference. The user can run it online on various sites, or you can download it and run it for free. You will need a newer Nvidia GPU to run it well on your own computer. There are also newer models that you can use to automatically edit a picture too like Flux Kontext or Omnigen2.
*  **Emotion:** The emotional tone is neutral, with users offering advice and resources to help the original poster.
*  **Top 3 Points of View:**
    *   ComfyUI is free
    *   You need a newer Nvidia GPU to run it well on your own computer
    *   To fix small details like that you would usually have to use a feature called "inpainting".

**[Does anyone have the T2V version of Kijai's wan 2.2 workflow already? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1mdefav/does_anyone_have_the_t2v_version_of_kijais_wan_22/)**
*  **Summary:**  A user is requesting the T2V version of Kijai's WAN 2.2 workflow. Other users suggest checking the ComfyUI toolbar for templates and offer a workaround involving modifying the existing I2V workflow.
*  **Emotion:** The emotional tone is generally positive, with a helpful and supportive atmosphere.
*  **Top 3 Points of View:**
    *   The T2V workflow might already be available in the ComfyUI templates.
    *   A workaround involves removing image nodes and adding 'WanVideo Empty Embeds'.
    *   Wan is probably working on it.

**[ALL made with Krea.AI - image, video, lip sync, and enhancement (Score: 0)](https://v.redd.it/3d7m4oehw1gf1)**
*  **Summary:** A user posted media created with Krea.AI asking if multitalk was used for lip sync.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Did you use multitalk for the lip sync?

**[I can't take this anymore (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1mddfwc/i_cant_take_this_anymore/)**
*  **Summary:** User is frustrated, with other users suggesting forge UI or paying someone to do it.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   A1111 is old, try Forge UI
    *   You can pay someone to do it for you

**[better quality for photorealistic output with wan 2.2.? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1mdf0br/better_quality_for_photorealistic_output_with_wan/)**
*  **Summary:** A user is asking about image quality with Wan 2.2, and another user asks if the user means T2V.
*  **Emotion:** Positive
*  **Top 3 Points of View:**
    *   Are you talking about T2V because I2V is excellent.

**[Current WAN 2.1 I2V takes around 2 hours for a 3 second video. Are there any new workflows or tech that can speed this up? (RTX 3060 12gb) (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1mdf5np/current_wan_21_i2v_takes_around_2_hours_for_a_3/)**
*  **Summary:** A user is looking for ways to speed up WAN 2.1 I2V generation on their RTX 3060 12gb.  Responses include suggestions for different models, workflows, and the importance of matching resources to hardware capabilities.
*  **Emotion:** The emotional tone is neutral and helpful, with users trying to provide solutions and guidance.
*  **Top 3 Points of View:**
    *   Consider using Hugging Face inference.
    *   Try Wan2.2 1280*704 5B FP16.
    *   You can get faster generation by experimenting with different models and workflows.

**[How to install Stable Diffusion on Windows? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1mdfwva/how_to_install_stable_diffusion_on_windows/)**
*  **Summary:**  A user is asking for help installing Stable Diffusion on Windows, mentioning they don't have an Nvidia GPU. Responses clarify the importance of an Nvidia GPU for optimal performance and suggest alternative solutions for AMD or CPU-based systems.
*  **Emotion:** The emotional tone is generally neutral, with a mix of helpfulness and mild negativity.
*  **Top 3 Points of View:**
    *   Nvidia GPU is required for SD.
    *   Suggests using AMD.
    *   What are the specs?

**[I'm exhausted trying to keep ip (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1mdgfit/im_exhausted_trying_to_keep_ip/)**
*  **Summary:** A user said they are exhausted trying to keep ip, another user links a google drive.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Google Drive Folder
