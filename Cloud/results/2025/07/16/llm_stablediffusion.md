---
title: "Stable Diffusion Subreddit"
date: "2025-07-16"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "video generation"]
---

# Overall Ranking and Top Discussions
1.  [[D] LTXV Just Unlocked Native 60-Second AI Videos](https://v.redd.it/tq7aozwa3adf1) (Score: 93)
    *   Discussion about the new LTXV AI model capable of generating 60-second videos, its performance, and where to find the checkpoint files.
2.  [I found a workflow to insert the 100% me in a scene by using Kontext.](https://www.reddit.com/r/StableDiffusion/comments/1m1hk30/i_found_a_workflow_to_insert_the_100_me_in_a/) (Score: 60)
    *   Discussion about a workflow for inserting oneself into a scene using Kontext, focusing on lighting issues and alternative methods.
3.  [LTXV: 60-Second Long-Form Video Generation: Faster, Cheaper, and More Controllable](https://v.redd.it/erjxfnroz9df1) (Score: 22)
    *   Questions about where to download the LTXV model and how much VRAM is needed to run it.
4.  [Would you try an open source gui-based Diffusion model training and generation platform?](https://i.redd.it/03l37aybaadf1.gif) (Score: 18)
    *   A question asking if users would try an open-source GUI-based diffusion model training and generation platform.
5.  [Live AI image pipeline with SD integration – project from our studio](https://v.redd.it/ie86e3hey9df1) (Score: 6)
    *   A showcase of a live AI image pipeline with Stable Diffusion integration, with users asking if it is open source.
6.  [Transformer Labs (for training) now works with AMD gpus](https://www.reddit.com/r/StableDiffusion/comments/1m1hv9n/transformer_labs_for_training_now_works_with_amd/) (Score: 5)
    *   Confirmation that Transformer Labs now supports AMD GPUs for training.
7.  [How to do this kind of animation?](https://v.redd.it/smskkjj9j9df1) (Score: 3)
    *   Users asking how to create a certain type of animation, with suggestions including Animatediff and Deforum.
8.  [Problem with Lora character after training in Kohya](https://www.reddit.com/gallery/1m1l9wq) (Score: 1)
    *   Help requested for a problem with Lora character after training, and to solve issue about consistency.
9.  [How do you generate a Consistent OC character.. any character male or female? Do I need to use any extensions?](https://www.reddit.com/r/StableDiffusion/comments/1m1hz4y/how_do_you_generate_a_consistent_oc_character_any/) (Score: 1)
    *   A question about generating consistent OC characters in different poses using Stable Diffusion, and if extensions are needed.
10. [WAN V2V Vace Help Needed](https://www.reddit.com/r/StableDiffusion/comments/1m1kbjh/wan_v2v_vace_help_needed/) (Score: 1)
    *   Help request regarding WAN V2V Vace, with suggestions to remove CausVid and upscale the gif.
11. [Turning CAD-like renders into photorealistic images without losing detail — what’s the best workflow?](https://www.reddit.com/r/StableDiffusion/comments/1m1kkyq/turning_cadlike_renders_into_photorealistic/) (Score: 1)
    *   Seeking advice on the best workflow for turning CAD-like renders into photorealistic images while preserving detail.
12. [How to transfer sun highlights from image to image?](https://www.reddit.com/gallery/1m1jfux) (Score: 0)
    *   A question on how to transfer sun highlights from one image to another, with suggestions on using huggingface.co.
13. [Looking for some people to group buy black mixtures AI course with me](https://www.reddit.com/r/StableDiffusion/comments/1m1kjn5/looking_for_some_people_to_group_buy_black/) (Score: 0)
    *   A user seeking others to group buy an AI course, met with skepticism and warnings about the course creator.
14. [What's the Best Way to Generate AI Video Clips Using Stable Diffusion? (Need Help!)](https://www.reddit.com/r/StableDiffusion/comments/1m1kxmj/whats_the_best_way_to_generate_ai_video_clips/) (Score: 0)
    *   Seeking advice on the best way to generate AI video clips using Stable Diffusion.
15. [Video generators](https://www.reddit.com/r/StableDiffusion/comments/1m1mune/video_generators/) (Score: 0)
    *   Listing of video generators and their VRAM requirement.

# Detailed Analysis by Thread
**[[D] LTXV Just Unlocked Native 60-Second AI Videos (Score: 93)](https://v.redd.it/tq7aozwa3adf1)**
*  **Summary:** The thread discusses the release of native 60-second AI video generation by LTXV. Users share their experiences, ask about hardware requirements, and question the quality of example videos. There are also questions regarding where to download the weights.
*  **Emotion:** The overall emotion is neutral, with a mix of positive sentiment ("Cool", "It's the start of a new era") and negative sentiment ("This isn't a very good example video", "Discord invite invalid"), and neutral sentiments regarding the video quality.
*  **Top 3 Points of View:**
    *   Excitement about the potential of native 60-second AI video generation.
    *   Skepticism regarding the quality of current examples and prompt adherence.
    *   Questions about hardware requirements and where to find necessary files/checkpoints.

**[I found a workflow to insert the 100% me in a scene by using Kontext. (Score: 60)](https://www.reddit.com/r/StableDiffusion/comments/1m1hk30/i_found_a_workflow_to_insert_the_100_me_in_a/)**
*  **Summary:** A user shares a workflow for inserting themselves into a scene using Kontext. Other users offer suggestions on improving the results, particularly regarding lighting and consistency, recommending Loras and alternative methods.
*  **Emotion:** The overall emotion is neutral to positive, with helpful suggestions and shared resources.
*  **Top 3 Points of View:**
    *   Kontext is a useful tool, but lighting is a common issue.
    *   LORAs can help fix lighting issues and improve the integration of the subject.
    *   Alternative methods like inpainting and regional prompting exist for more control.

**[LTXV: 60-Second Long-Form Video Generation: Faster, Cheaper, and More Controllable (Score: 22)](https://v.redd.it/erjxfnroz9df1)**
*  **Summary:** The thread revolves around LTXV's 60-second long-form video generation capabilities. Users are primarily asking where to download the required model files and inquiring about the necessary VRAM to run the software, also with negative sentiments on the current version.
*  **Emotion:** Mostly neutral, with hints of negativity due to the mentioned difficulties and cryptic workflow, with users saying "That's actually sad."
*  **Top 3 Points of View:**
    *   Inability to find the model for download.
    *   Concern about the VRAM requirements for running the model.
    *   Frustration with the complexity and lack of documentation for the workflow.

**[Would you try an open source gui-based Diffusion model training and generation platform? (Score: 18)](https://i.redd.it/03l37aybaadf1.gif)**
*  **Summary:** This thread is centered around whether users would be interested in trying an open-source, GUI-based diffusion model training and generation platform.
*  **Emotion:** Positive, indicated by the enthusiastic response "YES, OBVIOUSLY."
*  **Top 3 Points of View:**
    *   Interest in an open-source, GUI-based diffusion model platform.

**[Live AI image pipeline with SD integration – project from our studio (Score: 6)](https://v.redd.it/ie86e3hey9df1)**
*  **Summary:** This thread discusses a live AI image pipeline project with Stable Diffusion integration. The main point of discussion is whether the project is open source.
*  **Emotion:** The overall sentiment is neutral, with users simply inquiring about the project's open-source status.
*  **Top 3 Points of View:**
    *   Inquiry about whether the project is open source.
    *   Speculation that it may not be open source due to lack of information.

**[Transformer Labs (for training) now works with AMD gpus (Score: 5)](https://www.reddit.com/r/StableDiffusion/comments/1m1hv9n/transformer_labs_for_training_now_works_with_amd/)**
*  **Summary:** The thread confirms that Transformer Labs now supports AMD GPUs for training, on both Linux and Windows.
*  **Emotion:** Positive, with confirmation and enthusiasm about the new support.
*  **Top 3 Points of View:**
    *   Confirmation of AMD GPU support for Transformer Labs.

**[How to do this kind of animation? (Score: 3)](https://v.redd.it/smskkjj9j9df1)**
*  **Summary:** A user asks about how to create a specific type of animation. Other users provide suggestions, including using Animatediff, Deforum, and prompt/seed travel techniques.
*  **Emotion:** Neutral overall, with users providing information and suggestions. A bit of negativity can also be seen.
*  **Top 3 Points of View:**
    *   Animatediff is a possible tool.
    *   Deforum is another option.
    *   Prompt/seed travel techniques can also be used.

**[Problem with Lora character after training in Kohya (Score: 1)](https://www.reddit.com/gallery/1m1l9wq)**
*  **Summary:** A user seeks help with a LoRA character training issue in Kohya, where the character's consistency is affected when using multiple LoRAs.
*  **Emotion:** Neutral, with users providing advice and suggestions.
*  **Top 3 Points of View:**
    *   Overfitting may be the cause of the problem.
    *   LoRAs from different models may not stack well.
    *   Strategies like regional conditioning/masking or inpainting can help.

**[How do you generate a Consistent OC character.. any character male or female? Do I need to use any extensions? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1m1hz4y/how_do_you_generate_a_consistent_oc_character_any/)**
*  **Summary:** A user is asking about how to generate a consistent OC character in various poses and whether extensions are needed.
*  **Emotion:** Neutral, with suggestions being offered.
*  **Top 3 Points of View:**
    *   Using Flux Kontext + face swapping + inpainting could help.
    *   Training a LORA would be the best solution.

**[WAN V2V Vace Help Needed (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1m1kbjh/wan_v2v_vace_help_needed/)**
*  **Summary:** A user seeks help with WAN V2V Vace, and other users suggest removing CausVid and upscaling the gif to improve results.
*  **Emotion:** Positive, with users offering assistance.
*  **Top 3 Points of View:**
    *   Removing CausVid may improve results.
    *   Upscaling the gif might help.

**[Turning CAD-like renders into photorealistic images without losing detail — what’s the best workflow? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1m1kkyq/turning_cadlike_renders_into_photorealistic/)**
*  **Summary:** A user is looking for the best workflow to transform CAD-like renders into photorealistic images without losing detail. Suggestions include using control nets, trying txt2img, and using PHRenderFormerWrapper in ComfyUI.
*  **Emotion:** Neutral, with helpful suggestions provided by users.
*  **Top 3 Points of View:**
    *   Using control nets (edge detection, etc.) can help force detail.
    *   Trying txt2img might be better than starting with the source render.
    *   PHRenderFormerWrapper in ComfyUI can help with realistic lighting.

**[How to transfer sun highlights from image to image? (Score: 0)](https://www.reddit.com/gallery/1m1jfux)**
*  **Summary:** A user wants to know how to transfer sun highlights from one image to another. Suggestions include using LBM_relighting, Flux union ControlNet, or Kontext lora.
*  **Emotion:** Neutral, with users offering potential solutions.
*  **Top 3 Points of View:**
    *   LBM_relighting (huggingface space) could help.
    *   Flux union ControlNet (Values mode) can be used.
    *   Kontext lora might be a solution.

**[Looking for some people to group buy black mixtures AI course with me (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1m1kjn5/looking_for_some_people_to_group_buy_black/)**
*  **Summary:** A user is looking for people to group buy an AI course but is met with skepticism and warnings about the course creator allegedly stealing workflows and LoRAs.
*  **Emotion:** Neutral, with a high presence of skepticism and concern.
*  **Top 3 Points of View:**
    *   Skepticism about the listed clients of the course creator.
    *   Warning that the course creator has been flagged for stealing content.
    *   Advice not to spend money on the course and to find free alternatives.

**[What's the Best Way to Generate AI Video Clips Using Stable Diffusion? (Need Help!) (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1m1kxmj/whats_the_best_way_to_generate_ai_video_clips/)**
*  **Summary:** A user asks for advice on the best way to generate AI video clips using Stable Diffusion. The most prominent suggestion is to use Wan 2.1 and ComfyUI, as A1111 and animateddiff are considered outdated.
*  **Emotion:** Neutral, with helpful information and recommendations.
*  **Top 3 Points of View:**
    *   Wan 2.1 is the best tool for video generation.
    *   ComfyUI is worth switching to for motion work.
    *   Wan VACE can recreate real-world poses and actions.

**[Video generators (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1m1mune/video_generators/)**
*  **Summary:** This thread lists video generators and their VRAM requirements.
*  **Emotion:** Neutral.
*  **Top 3 Points of View:**
    *   Wan2.1 is SOTA and needs a lot of VRAM.
    *   Hunyuan is not SOTA but still good and needs a lot of VRAM.
    *   LTX-VIDEO is very fast and doesn't need a lot of VRAM.
