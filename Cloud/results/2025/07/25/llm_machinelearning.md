---
title: "Machine Learning Subreddit"
date: "2025-07-25"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "reddit", "analysis"]
---

# Overall Ranking and Top Discussions
1.  [[D] Tried of the same review pattern](https://www.reddit.com/r/MachineLearning/comments/1m8fynx/d_tried_of_the_same_review_pattern/) (Score: 102)
    *   Discusses the experience of receiving low-effort or seemingly AI-generated reviews for submitted papers, and the frustration with the peer review process.
2.  [[R] NeurIPS 2025 D&B: "The evaluation is limited to 15 open-weights models ... Score: 3"](https://www.reddit.com/r/MachineLearning/comments/1m95ej0/r_neurips_2025_db_the_evaluation_is_limited_to_15/) (Score: 68)
    *   Concerns about the limitations of the NeurIPS 2025 D&B (Datasets and Benchmarks) evaluation, specifically the restriction to 15 open-weight models.
3.  [[D] Review Confidence Guidelines](https://www.reddit.com/r/MachineLearning/comments/1m8rkli/d_review_confidence_guidelines/) (Score: 47)
    *   Discusses the Review Confidence Guidelines
4.  [[N] PapersWithCode sunsets, new HuggingFace Papers UI](https://www.reddit.com/r/MachineLearning/comments/1m8z9zk/n_paperswithcode_sunsets_new_huggingface_papers_ui/) (Score: 30)
    *   Sadness about PapersWithCode closing down and skepticism about Hugging Face becoming a monopoly.
5.  [Help Needed: Accurate Offline Table Extraction from Scanned Forms [P]](https://www.reddit.com/r/MachineLearning/comments/1m8n3yz/help_needed_accurate_offline_table_extraction/) (Score: 3)
    *   Advice on how to perform accurate offline table extraction from scanned forms.
6.  [[D] [MLOps] How to Handle Accuracy Drop in a Few Models During Mass Migration to a New Container?](https://www.reddit.com/r/MachineLearning/comments/1m8tzn2/d_mlops_how_to_handle_accuracy_drop_in_a_few/) (Score: 4)
    *   Deals with how to handle the accuracy drop in a few models during mass migration to a new container.
7.  [[D] BMVC 2025 Results Discussion](https://www.reddit.com/r/MachineLearning/comments/1m95swr/d_bmvc_2025_results_discussion/) (Score: 4)
    *   Discusses the BMVC 2025 Results
8.  [[D] How to calculate the memory needed to train your model on GPU](https://www.reddit.com/r/MachineLearning/comments/1m8ll1d/d_how_to_calculate_the_memory_needed_to_train/) (Score: 4)
    *   The discussion involves calculating the memory needed to train a model on a GPU, including the impact of techniques like LoRA and gradient checkpointing.
9.  [[D]: DDPMs: Training learns to undo entire noise, but at sampling time, noise removed step by step, why?](https://www.reddit.com/r/MachineLearning/comments/1m967t1/d_ddpms_training_learns_to_undo_entire_noise_but/) (Score: 2)
    *   Why Diffusion Models (DDPMs) are trained to undo entire noise but during sampling time, noise is removed step by step.

# Detailed Analysis by Thread
**[[D] Tried of the same review pattern (Score: 102)](https://www.reddit.com/r/MachineLearning/comments/1m8fynx/d_tried_of_the_same_review_pattern/)**
*  **Summary:** The thread discusses the experience of receiving low-effort or seemingly AI-generated reviews for submitted papers, the frustration with the peer review process, and potential solutions such as evaluating reviewers.
*  **Emotion:** The overall emotional tone is negative due to frustration and disillusionment with the review process, however, there is also a neutral tone from people just discussing the matter.
*  **Top 3 Points of View:**
    *   The peer review process is broken, with reviewers giving low-effort or AI-generated feedback.
    *   Authors should be able to evaluate reviewers to improve the quality of reviews.
    *   Submitting to journals is preferable to submitting to ML conferences due to the random nature of conference reviews.

**[[R] NeurIPS 2025 D&B: "The evaluation is limited to 15 open-weights models ... Score: 3" (Score: 68)](https://www.reddit.com/r/MachineLearning/comments/1m95ej0/r_neurips_2025_db_the_evaluation_is_limited_to_15/)**
*  **Summary:** The thread concerns about the limitations of the NeurIPS 2025 D&B (Datasets and Benchmarks) evaluation, specifically the restriction to 15 open-weight models. Some users suggest seeking sponsorship from commercial labs.
*  **Emotion:** The overall emotional tone is slightly positive, with users expressing agreement and offering suggestions, although neutral sentiment is dominant.
*  **Top 3 Points of View:**
    *   The limitation to 15 open-weight models is a significant drawback for evaluating new benchmarks.
    *   Academia should focus on different lines of work instead of competing with industry on PR goose chases.
    *   Including commercial models in evaluations can lead to reproducibility issues.

**[[D] Review Confidence Guidelines (Score: 47)](https://www.reddit.com/r/MachineLearning/comments/1m8rkli/d_review_confidence_guidelines/)**
*  **Summary:** The thread discusses review confidence guidelines for the BMVC conference.
*  **Emotion:** The overall emotional tone is positive.
*  **Top 3 Points of View:**
    *   One person thought the Neurips 1 guide was very adequate
    *   Emergency assigned a paper completely out of my domain this year
    *   5. I'm not like the other students, I know everything
4. I have a paid subscription to chatGPT

**[[N] PapersWithCode sunsets, new HuggingFace Papers UI (Score: 30)](https://www.reddit.com/r/MachineLearning/comments/1m8z9zk/n_paperswithcode_sunsets_new_huggingface_papers_ui/)**
*  **Summary:** The thread discusses the sunsetting of PapersWithCode and the introduction of the new Hugging Face Papers UI. Users express sadness about the loss of PapersWithCode and some are skeptical about Hugging Face consolidating everything, expressing concerns about a potential monopoly.
*  **Emotion:** The overall emotional tone is mixed, with sadness and skepticism being the dominant emotions.
*  **Top 3 Points of View:**
    *   Sadness over PapersWithCode going away, remembering it as a game changer.
    *   Skepticism about Hugging Face consolidating everything, fearing a potential monopoly.
    *   Gratitude towards the creators of PapersWithCode for their efforts.

**[Help Needed: Accurate Offline Table Extraction from Scanned Forms [P] (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1m8n3yz/help_needed_accurate_offline_table_extraction/)**
*  **Summary:** The thread discusses ways on how to perform accurate offline table extraction from scanned forms.
*  **Emotion:** The overall emotional tone is neutral and slightly positive, with people offering helpful suggestions.
*  **Top 3 Points of View:**
    *   Suggests using VLM to convert documents to PDF.
    *   Another suggestion is to use a dinov2 encoder for an RNN or transformer decoder.
    *   To try nano-ocr available on huggingface.

**[[D] [MLOps] How to Handle Accuracy Drop in a Few Models During Mass Migration to a New Container? (Score: 4)](https://www.reddit.com/r/MachineLearning/comments/1m8tzn2/d_mlops_how_to_handle_accuracy_drop_in_a_few/)**
*  **Summary:** The thread discusses how to handle the accuracy drop in a few models during mass migration to a new container.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Suggests to understand where the drop comes from and compare different metrics and specific data points.
    *   Push for the hybrid environment and for the failing models to be fixed with high priority.
    *   Build the container with whatever versions of libraries it was developed on

**[[D] BMVC 2025 Results Discussion (Score: 4)](https://www.reddit.com/r/MachineLearning/comments/1m95swr/d_bmvc_2025_results_discussion/)**
*  **Summary:** This is a thread where people are sharing and discussing the results of their BMVC 2025 submissions.
*  **Emotion:** The emotional tone is primarily negative, with a bit of neutral tone.
*  **Top 3 Points of View:**
    *   Rejected and can't see the reviews
    *   Got rejected and can see meta review. Tbh, very vague review
    *   Got accepted, but can't see the reviews.

**[[D] How to calculate the memory needed to train your model on GPU (Score: 4)](https://www.reddit.com/r/MachineLearning/comments/1m8ll1d/d_how_to_calculate_the_memory_needed_to_train/)**
*  **Summary:** The discussion involves calculating the memory needed to train a model on a GPU, including the impact of techniques like LoRA and gradient checkpointing.
*  **Emotion:** The thread's emotional tone is primarily neutral.
*  **Top 3 Points of View:**
    *   Activation memory is intermediate output of all operations. Rough way to estimate is to sum number of elements in output of all layers.
    *   Not sure it will answer your question about LoRas and so on but I found the calculator at the beginning of this link very useful
    *   Use gradient accumulation. Then double or half BS until it does fit. If a batch size of 1 doesnâ€™t fit, use gradient checkpointing or any of the other million different tricks.

**[[D]: DDPMs: Training learns to undo entire noise, but at sampling time, noise removed step by step, why? (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1m967t1/d_ddpms_training_learns_to_undo_entire_noise_but/)**
*  **Summary:** Why Diffusion Models (DDPMs) are trained to undo entire noise but during sampling time, noise is removed step by step.
*  **Emotion:** The thread's emotional tone is primarily neutral.
*  **Top 3 Points of View:**
    *   The training algorithm is learning to remove noise incrementally. The forward process is defined as markov chain.
    *   If you want to train a model to remove a small step of noise, you can scale the noise to remove it yourself and choose whatever step size you want.
