---
title: "Machine Learning Subreddit"
date: "2025-07-29"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "NLP"]
---

# Overall Ranking and Top Discussions
1.  [[P] BluffMind: Pure LLM powered card game w/ TTS and live dashboard](https://www.reddit.com/gallery/1mbycac) (Score: 23)
    * Discussing a card game powered by LLMs with text-to-speech and a live dashboard.
2.  [[D] First research project – feedback on "Ano", a new optimizer designed for noisy deep RL (also looking for arXiv endorsement)](https://www.reddit.com/r/MachineLearning/comments/1mc8pn4/d_first_research_project_feedback_on_ano_a_new/) (Score: 15)
    * Seeking feedback on a new optimizer designed for noisy deep reinforcement learning, and asking for advice on arXiv endorsement.
3.  [[D] New recent and applied ideas for representation learning? (i.g. Matryoshka, Constrastive learning, etc.)](https://www.reddit.com/r/MachineLearning/comments/1mcf1kl/d_new_recent_and_applied_ideas_for_representation/) (Score: 11)
    *  Exploring new and recent ideas for representation learning, such as Matryoshka and Contrastive learning.
4.  [[D] AAAI-2026 Code Submission](https://www.reddit.com/r/MachineLearning/comments/1mc5jdg/d_aaai2026_code_submission/) (Score: 8)
    *  Discussion about the AAAI-2026 code submission deadline and whether or not to submit code with the paper.
5.  [[R] I turned my NTK notes into an arXiv preprint](https://www.reddit.com/r/MachineLearning/comments/1mbwn1v/r_i_turned_my_ntk_notes_into_an_arxiv_preprint/) (Score: 4)
    *  Discussion about turning NTK notes into an arXiv preprint and the need to cite sources appropriately.
6.  [MSc Statistics Vs MSc Statistics and Data Science at University of bath. [D]](https://www.reddit.com/r/MachineLearning/comments/1mcf1n4/msc_statistics_vs_msc_statistics_and_data_science/) (Score: 3)
    *   Discussing the pros and cons of pursuing an MSc in Statistics versus an MSc in Statistics and Data Science, specifically at the University of Bath.
7.  [[R] Are AUC/ROC curves "black box" metrics?](https://www.reddit.com/r/MachineLearning/comments/1mcff31/r_are_aucroc_curves_black_box_metrics/) (Score: 3)
    * A discussion on the interpretation and utility of AUC/ROC curves, and whether they can be considered "black box" metrics.
8.  [[D] Regression Model for Real Estate](https://www.reddit.com/r/MachineLearning/comments/1mbzx96/d_regression_model_for_real_estate/) (Score: 2)
    *  Discussion on which regression model is best for real estate data.
9.  [[P] Keyword and Phrase Embedding for Query Expansion](https://www.reddit.com/r/MachineLearning/comments/1mbz5fk/p_keyword_and_phrase_embedding_for_query_expansion/) (Score: 1)
    * Discussion about keyword and phrase embedding for query expansion.
10. [[D] Now it's 2025, what's the updated and proper answer to "How to solve the LLM hallucination?"](https://www.reddit.com/r/MachineLearning/comments/1mbre9n/d_now_its_2025_whats_the_updated_and_proper/) (Score: 0)
    * A discussion about the current state of research into Large Language Model hallucination, and potential solutions.
11. [[D] Is there an alternative to the Transformer architecture that can generate good text with less parameters?](https://www.reddit.com/r/MachineLearning/comments/1mcbjgh/d_is_there_an_alternative_to_the_transformer/) (Score: 0)
    * Discussing potential alternatives to the Transformer architecture for generating text with fewer parameters.

# Detailed Analysis by Thread
**[[P] BluffMind: Pure LLM powered card game w/ TTS and live dashboard (Score: 23)](https://www.reddit.com/gallery/1mbycac)**
*  **Summary:** A user presented a card game powered by LLMs with text-to-speech and a live dashboard. Other users showed interest, particularly in the use of LLMs for creative and interactive applications beyond Q&A or coding.
*  **Emotion:** The overall emotional tone is Neutral, with some comments expressing Positive sentiment regarding the project's creativity.
*  **Top 3 Points of View:**
    * The project is cool and demonstrates creative LLM use.
    * It would be interesting to see how the LLM handles bluffing and complex strategies.
    * This could potentially be implemented in online casinos.

**[[D] First research project – feedback on "Ano", a new optimizer designed for noisy deep RL (also looking for arXiv endorsement) (Score: 15)](https://www.reddit.com/r/MachineLearning/comments/1mc8pn4/d_first_research_project_feedback_on_ano_a_new/)**
*  **Summary:** The thread is about a request for feedback on a new optimizer for noisy deep RL, named "Ano". The author is also seeking advice on getting an arXiv endorsement.
*  **Emotion:** The emotional tone is predominantly Neutral, with a touch of admiration for independent researchers' efforts.
*  **Top 3 Points of View:**
    * It's impressive that independent researchers dedicate so much time to research.
    * Submit to a relevant workshop to receive strong feedback.
    * The name of the optimizer might be problematic for Spanish speakers.

**[[D] New recent and applied ideas for representation learning? (i.g. Matryoshka, Constrastive learning, etc.) (Score: 11)](https://www.reddit.com/r/MachineLearning/comments/1mcf1kl/d_new_recent_and_applied_ideas_for_representation/)**
*  **Summary:** The discussion revolves around new approaches to representation learning, including Matryoshka and contrastive learning.
*  **Emotion:** The emotional tone is primarily Neutral, with some Positive sentiment regarding the BYOL approach.
*  **Top 3 Points of View:**
    * Task-specific loss functions lead to superior representations.
    * BYOL is effective for aerial imagery embedding models.
    * JEPA is relevant.

**[[D] AAAI-2026 Code Submission (Score: 8)](https://www.reddit.com/r/MachineLearning/comments/1mc5jdg/d_aaai2026_code_submission/)**
*  **Summary:** The thread discusses the code submission policy for AAAI-2026, debating whether or not to submit code.
*  **Emotion:** The thread's overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    * There's a grace period to submit code after the paper deadline.
    * Whether to submit code depends on the specific paper.
    * Reviewers may not even look at the code.

**[[R] I turned my NTK notes into an arXiv preprint (Score: 4)](https://www.reddit.com/r/MachineLearning/comments/1mbwn1v/r_i_turned_my_ntk_notes_into_an_arxiv_preprint/)**
*  **Summary:** The discussion centers on a user turning their notes on Neural Tangent Kernel (NTK) into an arXiv preprint. The comments focus on proper citation and originality.
*  **Emotion:** The emotional tone is mostly Neutral, with an emphasis on academic integrity.
*  **Top 3 Points of View:**
    * It's important to cite existing NTK work if the results are not novel.
    * Is this user familiar with NTK?
    * Generally positive.

**[MSc Statistics Vs MSc Statistics and Data Science at University of bath. [D] (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1mcf1n4/msc_statistics_vs_msc_statistics_and_data_science/)**
*  **Summary:** A user is seeking advice on whether to pursue an MSc in Statistics or an MSc in Statistics and Data Science at the University of Bath, given their existing experience as a Data Engineer.
*  **Emotion:** The emotional tone is overall Positive, with helpful and encouraging responses.
*  **Top 3 Points of View:**
    * With the user's background, the Stats & DS program is likely a better choice for strengthening statistical foundations.
    * A strong statistics foundation is valuable in industry and will not restrict career options.
    * Either program will work out fine given the user's experience.

**[[R] Are AUC/ROC curves "black box" metrics? (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1mcff31/r_are_aucroc_curves_black_box_metrics/)**
*  **Summary:** The thread explores whether AUC/ROC curves are "black box" metrics, focusing on their interpretation and proper usage.
*  **Emotion:** The emotional tone is Neutral, providing informative and helpful responses.
*  **Top 3 Points of View:**
    * The ROC curve in the post has its x-axis flipped.
    * Probabilities produced by the model should be checked for well-calibration.
    * For imbalanced data, precision recall curves and AUPRC are more suitable.

**[[D] Regression Model for Real Estate (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1mbzx96/d_regression_model_for_real_estate/)**
*  **Summary:** Users discuss suitable regression models for real estate data.
*  **Emotion:** The overall emotional tone is Neutral, with suggestions for appropriate techniques.
*  **Top 3 Points of View:**
    * Apply filters earlier.
    * Use Gradient Boosted Decision Trees, such as LightGBM.
    * Tree-based algorithms work better than simple linear regression with time series data.

**[[P] Keyword and Phrase Embedding for Query Expansion (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1mbz5fk/p_keyword_and_phrase_embedding_for_query_expansion/)**
*  **Summary:** Discussion and suggestions for Keyword and Phrase Embedding for Query Expansion, particularly in Korean text search.
*  **Emotion:** The emotional tone is Positive, offering constructive criticism and helpful recommendations.
*  **Top 3 Points of View:**
    * The approach is on the right track but may be overcomplicated.
    * Proper preprocessing and hybrid approaches yield the biggest wins.
    * Use multilingual models like mBERT or XLM-R, or Korean-specific models like KoBERT.

**[[D] Now it's 2025, what's the updated and proper answer to "How to solve the LLM hallucination?" (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1mbre9n/d_now_its_2025_whats_the_updated_and_proper/)**
*  **Summary:** A discussion about the problem of LLM hallucination and potential solutions in 2025.
*  **Emotion:** The overall emotional tone is Neutral, acknowledging the challenges and offering potential approaches.
*  **Top 3 Points of View:**
    * Hallucinations may be solved by shifting away from stateless AI and using orchestration.
    * Hallucinations are a fundamental consequence of how LLMs work, and haven't been completely solved.
    * Retrieval, tool use, and better prompting make hallucination more manageable.

**[[D] Is there an alternative to the Transformer architecture that can generate good text with less parameters? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1mcbjgh/d_is_there_an_alternative_to_the_transformer/)**
*  **Summary:** This thread discusses alternative architectures to the Transformer that can generate text well with fewer parameters.
*  **Emotion:** The emotional tone is Neutral.
*  **Top 3 Points of View:**
    * Check the Mamba architecture.
    * "Hierarchical Reasoning Models" (HRM) are rumored to require less data to train.
    * Training domain-specific models from scratch is achievable with hundreds of millions of parameters.
