text
---
title: "Stable Diffusion Subreddit"
date: "2025-11-02"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions

1.  [Back to 1.5 and QR Code Monster](https://www.reddit.com/gallery/1ommqxs) (Score: 177)
    *   Discusses the continued relevance and unique capabilities of Stable Diffusion 1.5 combined with QR Code Monster and AnimateDiff, particularly for specific client needs.
2.  [A comparison of 10 different realism LoRa's for Qwen-Image - done by Kimaran on CivitAI](https://imgur.com/a/ME4YKRT) (Score: 34)
    *   Presents a comparison of different realism LoRAs for Qwen-Image, noting subtle differences and potential issues with LoRA's impact on character, pose, and lighting, as well as a typo in the repo for qwen image.
3.  [Is SD 1.5 still relevant? Are there any cool models?](https://www.reddit.com/r/StableDiffusion/comments/1omkh9h/is_sd_15_still_relevant_are_there_any_cool_models/) (Score: 26)
    *   Asks about the current relevance of Stable Diffusion 1.5 and seeks recommendations for interesting models, leading to a discussion about its strengths, weaknesses, and alternatives like SDXL.
4.  [It turns out WDDM driver mode is making our RAM - GPU transfer extremely slower compared to TCC or MCDM mode. Anyone has figured out the bypass NVIDIA software level restrictions?](https://www.reddit.com/r/StableDiffusion/comments/1ommmek/it_turns_out_wddm_driver_mode_is_making_our_ram/) (Score: 22)
    *   Addresses performance issues related to WDDM driver mode affecting RAM-GPU transfer speeds, with some suggesting a shift to Linux for AI development.
5.  [I'm looking to add buildings in this image using InPaint methods but can't manage to have good results, i've tried using the InPaint template from ComfyUI, any help is welcome ( i try to match the style and view of the last image )](https://www.reddit.com/gallery/1omkw1p) (Score: 5)
    *   Seeks advice on improving inpainting results for adding buildings to an image, specifically within ComfyUI, with suggestions involving controlnet techniques and reference images.
6.  [Wan2.1 i2v color matching](https://www.reddit.com/r/StableDiffusion/comments/1omqip3/wan21_i2v_color_matching/) (Score: 2)
    *   Discusses color matching issues with Wan2.1 i2v.
7.  [ComfyUI Wan 2.2 I2V...Is There A Secret Cache Causing Problems?](https://www.reddit.com/r/StableDiffusion/comments/1omplev/comfyui_wan_22_i2vis_there_a_secret_cache_causing/) (Score: 2)
    *   Explores potential caching problems in ComfyUI with Wan 2.2 I2V leading to crashes, and possible solutions involving VRAM management and startup arguments.
8.  [RTX 5060TI or 5070?](https://www.reddit.com/r/StableDiffusion/comments/1omoynb/rtx_5060ti_or_5070/) (Score: 2)
    *   Compares the RTX 5060TI and 5070 for Stable Diffusion tasks, weighing the importance of VRAM versus raw performance, with some advocating for older cards like the 3080ti.
9.  [Need help with Wan 2.2 lora](https://www.reddit.com/r/StableDiffusion/comments/1oml2bi/need_help_with_wan_22_lora/) (Score: 1)
    *   Asks for help training Wan 2.2 Lora.
10. [txt2img Batch Generation?](https://www.reddit.com/r/StableDiffusion/comments/1omml4i/txt2img_batch_generation/) (Score: 1)
    *   Asks about txt2img batch generation.
11. [What's with all the ORANGE in model outputs?](https://www.reddit.com/r/StableDiffusion/comments/1omolin/whats_with_all_the_orange_in_model_outputs/) (Score: 1)
    *   Investigates why model outputs have too much orange.
12. [CAN I?](https://www.reddit.com/r/StableDiffusion/comments/1omn6ha/can_i/) (Score: 0)
    *   Asks about the capability to create content using limited hardware.
13. [Free UGC-style talking videos (ElevenLabs + InfiniteTalk)](https://www.reddit.com/r/StableDiffusion/comments/1omo145/free_ugcstyle_talking_videos_elevenlabs/) (Score: 0)
    *   Discusses the long generation times for free UGC-style talking videos using ElevenLabs + InfiniteTalk.
14. [Created this AI-generated Indian fashion model using Stable Diffusion](https://www.reddit.com/gallery/1omll9k) (Score: 0)
    *   Presents an AI-generated image of an Indian fashion model, prompting feedback on the model's appearance and requests for the workflow.
15. [Current method for local image gen with 9070XT on Windows?](https://www.reddit.com/r/StableDiffusion/comments/1omkm4h/current_method_for_local_image_gen_with_9070xt_on/) (Score: 0)
    *   Seeks the best current method for local image generation on Windows with a 9070XT.
16. [Help/advice to run I2V locally](https://www.reddit.com/r/StableDiffusion/comments/1omm4xg/helpadvice_to_run_i2v_locally/) (Score: 0)
    *   Asks for help running I2V locally.

# Detailed Analysis by Thread

**[[D] Back to 1.5 and QR Code Monster (Score: 177)](https://www.reddit.com/gallery/1ommqxs)**
*   **Summary:** This thread showcases and discusses the use of Stable Diffusion 1.5, QR Code Monster, and AnimateDiff for client projects. It highlights the unique capabilities of this combination and its ongoing relevance, despite newer models being available.
*   **Emotion:** The overall emotional tone is Neutral, with some comments expressing positive sentiments ("this is art", "this is cool") and a recognition of the tool's unique value.
*   **Top 3 Points of View:**
    *   SD1.5 + QR Code Monster + AnimateDiff is the best tool for some client needs, offering unique capabilities.
    *   While old, this toolset is still actively used and being improved upon.
    *   These images are QR codes, and the vintage stuff is cool.

**[A comparison of 10 different realism LoRa's for Qwen-Image - done by Kimaran on CivitAI (Score: 34)](https://imgur.com/a/ME4YKRT)**
*   **Summary:**  The thread discusses a comparison of 10 realism LoRAs for Qwen-Image. Users note the subtle differences between LoRAs and potential issues with them altering key aspects of the generated image. There's also a mention of a typo in the Qwen-Image repository.
*   **Emotion:**  The overall emotional tone is primarily Neutral, with some Positive sentiments ("Looking at it myself I think "analog" is pretty good, I wonder what other people think.", "Thanks.").
*   **Top 3 Points of View:**
    *   LoRA differences are subtle but can impact character, pose, and lighting.
    *   The "analog" LoRA is considered a good option.
    *   There is a typo in the recommended resolution for Qwen-Image.

**[[D] Is SD 1.5 still relevant? Are there any cool models? (Score: 26)](https://www.reddit.com/r/StableDiffusion/comments/1omkh9h/is_sd_15_still_relevant_are_there_any_cool_models/)**
*   **Summary:**  The thread explores the relevance of Stable Diffusion 1.5 in the current landscape of AI image generation. Users discuss its strengths like speed and unique features (e.g., AnimateDiff), as well as alternatives like SDXL and various fine-tuned models.
*   **Emotion:**  The emotional tone is mostly Neutral with a mix of Positive sentiments. Many users share their personal experiences and opinions on different models.
*   **Top 3 Points of View:**
    *   SD 1.5 is still relevant due to its speed and specific features like AnimateDiff.
    *   SDXL is a better alternative for many tasks, offering higher quality and a wide range of fine-tunes.
    *   SD 1.5 can be useful for quickly generating initial images for refinement with other models.

**[It turns out WDDM driver mode is making our RAM - GPU transfer extremely slower compared to TCC or MCDM mode. Anyone has figured out the bypass NVIDIA software level restrictions? (Score: 22)](https://www.reddit.com/r/StableDiffusion/comments/1ommmek/it_turns_out_wddm_driver_mode_is_making_our_ram/)**
*   **Summary:**  This thread centers on the performance impact of WDDM driver mode on RAM-GPU transfer speeds in Stable Diffusion. Users are seeking ways to bypass NVIDIA's restrictions, with some suggesting a switch to Linux.
*   **Emotion:**  The emotional tone is primarily Neutral, with a focus on technical problem-solving.
*   **Top 3 Points of View:**
    *   WDDM driver mode significantly slows down RAM-GPU transfer speeds.
    *   Linux is a preferred platform for AI development and avoids these Windows-specific issues.
    *   Finding workarounds within Windows is challenging and may not be worth the effort.

**[I'm looking to add buildings in this image using InPaint methods but can't manage to have good results, i've tried using the InPaint template from ComfyUI, any help is welcome ( i try to match the style and view of the last image ) (Score: 5)](https://www.reddit.com/gallery/1omkw1p)**
*   **Summary:** The thread is a request for assistance with inpainting buildings into an existing image using ComfyUI. Users provide suggestions involving ControlNet, reference images, and specific workflow recommendations.
*   **Emotion:**  The overall emotional tone is Neutral, focusing on providing technical advice and solutions.
*   **Top 3 Points of View:**
    *   Use ControlNet (e.g., Canny) to guide the inpainting process.
    *   Utilize reference images and solid colored shapes for building outlines in tools like Krita.
    *   Consider using drone footage LoRAs to provide the model with top-view knowledge.

**[Wan2.1 i2v color matching (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1omqip3/wan21_i2v_color_matching/)**
*   **Summary:** The thread focuses on troubleshooting color matching issues specifically within Wan2.1 i2v. The main suggestion involves using wavelet color fix node for the generation, then replace the fried frames with VACE, then color correction in post.
*   **Emotion:** The overall emotional tone is Neutral, with a focus on technical details.
*   **Top 2 Points of View:**
    *   Wavelet color fix node for the generation, then replace the fried frames with VACE, then color correction in post.
    *   2.2 has the exact same problem

**[ComfyUI Wan 2.2 I2V...Is There A Secret Cache Causing Problems? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1omplev/comfyui_wan_22_i2vis_there_a_secret_cache_causing/)**
*   **Summary:** The thread investigates potential VRAM release issues after generations.
*   **Emotion:** The overall emotional tone is Neutral, with a focus on troubleshooting
*   **Top 2 Points of View:**
    *   It's not releasing the vram after generations.
    *   Use --cache-none as additional comfy startup argument and try again. This will load the models one by one and make sure the model is properly flushed out after the first sampler.

**[RTX 5060TI or 5070? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1omoynb/rtx_5060ti_or_5070/)**
*   **Summary:** This thread presents the choice between RTX 5060ti or 5070.
*   **Emotion:** The overall emotional tone is Neutral, with a focus on technical details and recommendations.
*   **Top 3 Points of View:**
    *   16gb for sure.
    *   VRAM is still king. I'm using a 5060TI right now. It's plenty fast and has a lot of VRAM overhead for making larger images.
    *   save up for a 5090 or pick up a 4090. in the meantime a used 3080ti is solid. VRAM is key for loading the model but CPU and ram bottleneck tightly when working on this stuff local. all <5000 series are locked to PCI 4th gen so they're slower. ram + cpu picks up the slack.

**[Need help with Wan 2.2 lora (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1oml2bi/need_help_with_wan_22_lora/)**
*   **Summary:** This thread asks for help with the configuration of the local machine.
*   **Emotion:** The overall emotional tone is Neutral, with a focus on technical details and recommendations.
*   **Top 2 Points of View:**
    *   What was the config of your local machine where you tried training wan 2.2 lora (Trials you did) succesfully? Did it succeed because you trained on a dataset with few pics?
    *   Renting an rtx 6000 pro might seem kinda expensive, but it get the job done really fast. Used datasets of 30 images and it trained it in about 3 hours on runpid I think. (high and low). Good results too.

**[txt2img Batch Generation? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1omml4i/txt2img_batch_generation/)**
*   **Summary:** This thread asks about txt2img batch generation.
*   **Emotion:** The overall emotional tone is Neutral, with a focus on technical details and recommendations.
*   **Top 1 Points of View:**
    *   Maybe this is what you are looking for: [https://github.com/adieyal/comfyui-dynamicprompts](https://github.com/adieyal/comfyui-dynamicprompts)

**[What's with all the ORANGE in model outputs? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1omolin/whats_with_all_the_orange_in_model_outputs/)**
*   **Summary:** The thread explores why model outputs are orange.
*   **Emotion:** The overall emotional tone is Neutral, with a focus on technical details and recommendations.
*   **Top 2 Points of View:**
    *   Your prompt likely includes the word 'Orange' as those birds may be orange-breasted species.
    *   You probably need to specify a color or colors for the flowers on the branches.

**[CAN I? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1omn6ha/can_i/)**
*   **Summary:** This thread explores whether users can create using limited hardware.
*   **Emotion:** The overall emotional tone is Neutral, with a focus on technical details and recommendations.
*   **Top 3 Points of View:**
    *   8GB vram is fine, the biggest technical challenge is your ram and CPU speeds for swapping that data between.
    *    Forge Neo is easy to use, but I'd recommend you to use Comfyui, it is not that difficult nowadays and there are a ton of example workflows you simply drag and drop.
    *   If you want to create fixed characters using Forge, you can use either img2vid as reference or trained character LoRA.

**[Free UGC-style talking videos (ElevenLabs + InfiniteTalk) (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1omo145/free_ugcstyle_talking_videos_elevenlabs/)**
*   **Summary:** This thread explores long generation times using ElevenLabs + InfiniteTalk.
*   **Emotion:** The overall emotional tone is Positive, with focus on the new technologies.
*   **Top 1 Points of View:**
    *   Decent, but these Gen times are still huge :'(

**[Created this AI-generated Indian fashion model using Stable Diffusion (Score: 0)](https://www.reddit.com/gallery/1omll9k)**
*   **Summary:** This thread explores if the model created is face swapping and asks about the workflow.
*   **Emotion:** The overall emotional tone is Positive, with focus on the new model.
*   **Top 3 Points of View:**
    *   Something about her face looks off. Are you face swapping?
    *   workflow?
    *   Indian? She looks Latin American tbh.

**[Current method for local image gen with 9070XT on Windows? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1omkm4h/current_method_for_local_image_gen_with_9070xt_on/)**
*   **Summary:** This thread explores AMD comfyUI installs.
*   **Emotion:** The overall emotional tone is Neutral, with focus on the technical details.
*   **Top 2 Points of View:**
    *   Did you use proper AMD comfyUi install? - [https://github.com/comfyanonymous/ComfyUI/releases/download/v0.3.67/ComfyUI\_windows\_portable\_amd.7z](https://github.com/comfyanonymous/ComfyUI/releases/download/v0.3.67/ComfyUI_windows_portable_amd.7z)
    *   You can run comfyui and SD with ROCm on AMD on Windows 11=.

**[Help/advice to run I2V locally (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1omm4xg/helpadvice_to_run_i2v_locally/)**
*   **Summary:** This thread explores 4Gb Wan2.2 14B.
*   **Emotion:** The overall emotional tone is Neutral, with focus on the technical details.
*   **Top 3 Points of View:**
    *   Try this: [4Gb Wan2.2 14B: r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/1okn7z1/wan22_14b_on_gtx1050_with_4gb_ok/)
    *   Wan I2V? Best thing you could try to use is [ComfyUI-MultiGPU](https://github.com/pollockjj/ComfyUI-MultiGPU) custom node.
    *   Q4 gguf version of Rapid Wan AIO v10 model should work - [https://www.reddit.com/r/comfyui/comments/1mz4fdv/comment/nagn2f2/](https://www.reddit.com/r/comfyui/comments/1mz4fdv/comment/nagn2f2/)
