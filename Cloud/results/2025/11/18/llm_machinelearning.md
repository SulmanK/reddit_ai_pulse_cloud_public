---
title: "Machine Learning Subreddit"
date: "2025-11-18"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machine learning", "AI", "research"]
---

# Overall Ranking and Top Discussions
1.  [[D] Tsinghua ICLR paper withdrawn due to numerous AI generated citations](https://www.reddit.com/r/MachineLearning/comments/1p01c70/d_tsinghua_iclr_paper_withdrawn_due_to_numerous/) (Score: 263)
    *   The discussion centers around a Tsinghua University paper withdrawn from ICLR due to the discovery of numerous AI-generated citations and the broader implications for research integrity.
2.  [[D] Some concerns about the current state of machine learning research](https://www.reddit.com/r/MachineLearning/comments/1ozw2tj/d_some_concerns_about_the_current_state_of/) (Score: 82)
    *   This thread discusses concerns about the current state of machine learning research, including herd behavior and the quality of research.
3.  [[P] PapersWithCode's new open-source alternative: OpenCodePapers](https://www.reddit.com/r/MachineLearning/comments/1p0b96k/p_paperswithcodes_new_opensource_alternative/) (Score: 52)
    *   This thread discusses the launch of OpenCodePapers, a new open-source alternative to PapersWithCode.
4.  [[P] DeepClause - A Neurosymbolic AI System](https://www.reddit.com/r/MachineLearning/comments/1p095xc/p_deepclause_a_neurosymbolic_ai_system/) (Score: 8)
    *   This thread discusses DeepClause - A Neurosymbolic AI System.
5.  [Apple AIML Residency Program 2026 [R]](https://www.reddit.com/r/MachineLearning/comments/1p0lart/apple_aiml_residency_program_2026_r/) (Score: 8)
    *   This thread discusses the Apple AIML Residency Program 2026.
6.  [[D] Is Hot and Cold just embedding similarity?](https://www.reddit.com/r/MachineLearning/comments/1p01ny7/d_is_hot_and_cold_just_embedding_similarity/) (Score: 7)
    *   This thread questions whether the "Hot and Cold" game is simply based on embedding similarity.
7.  [[D] Is it worth the time to publish and prepare for (archival) ACL/EMNLP workshops?](https://www.reddit.com/r/MachineLearning/comments/1p06r3h/d_is_it_worth_the_time_to_publish_and_prepare_for/) (Score: 7)
    *   This thread discusses whether it's worth the time to publish and prepare for archival ACL/EMNLP workshops.
8.  [[D] Has anyone used ONNX Runtime (ORT) + CUDA for multilingual embedding models (e.g., LaBSE) on GPUs?](https://www.reddit.com/r/MachineLearning/comments/1ozzu5z/d_has_anyone_used_onnx_runtime_ort_cuda_for/) (Score: 6)
    *   This thread is for people asking questions or giving tips on using ONNX Runtime (ORT) + CUDA for multilingual embedding models (e.g., LaBSE) on GPUs.
9.  [[D] Upload paper arXiv after acceptance](https://www.reddit.com/r/MachineLearning/comments/1p05h0k/d_upload_paper_arxiv_after_acceptance/) (Score: 4)
    *   This thread discusses uploading a paper to arXiv after acceptance.
10. [[P] A ‚Äúfoveated‚Äù memory layer for LLM agents: +46.7pp accuracy at 256-token context (open-source)](https://www.reddit.com/r/MachineLearning/comments/1ozz6te/p_a_foveated_memory_layer_for_llm_agents_467pp/) (Score: 1)
    *   This thread is about open-sourcing work on a foveated memory layer for LLM agents.
11. [[D] I built a CPU-native memory system that's 527x faster than GPU retrieval. No CUDA. No transformers. 2.27% variance across 150 runs.](https://www.reddit.com/r/MachineLearning/comments/1p01jr1/d_i_built_a_cpunative_memory_system_thats_527x/) (Score: 0)
    *   This thread discusses a CPU-native memory system that claims to be faster than GPU retrieval.
12. [[R], Geometric Sequence - Structured Memory (Yes, this is legit)](https://www.reddit.com/r/MachineLearning/comments/1p04v3q/r_geometric_sequence_structured_memory_yes_this/) (Score: 0)
    *   This thread discusses Geometric Sequence - Structured Memory
13. [[P] How can your AI skills help solve one of the world‚Äôs biggest challenges ‚Äî access to clean water?üíß](https://www.reddit.com/r/MachineLearning/comments/1p07a42/p_how_can_your_ai_skills_help_solve_one_of_the/) (Score: 0)
    *   This thread discusses how AI skills can help solve access to clean water.
14. [[D] I managed to fine-tune Qwen2.5-Omni-3B while keeping multimodal abilities ‚Äî is it actually as hard as it felt?](https://www.reddit.com/r/MachineLearning/comments/1p01hsy/d_i_managed_to_finetune_qwen25omni3b_while/) (Score: 0)
    *   This thread discusses the difficulty of fine-tuning Qwen2.5-Omni-3B while keeping multimodal abilities.

# Detailed Analysis by Thread
**[[D] Tsinghua ICLR paper withdrawn due to numerous AI generated citations (Score: 263)](https://www.reddit.com/r/MachineLearning/comments/1p01c70/d_tsinghua_iclr_paper_withdrawn_due_to_numerous/)**
*   **Summary:** The thread discusses the withdrawal of a Tsinghua University paper from the ICLR conference due to the discovery of numerous AI-generated citations. The discussion revolves around the implications of this incident for research integrity, the pressures faced by researchers in certain institutions, and the overall state of the ML research ecosystem.
*   **Emotion:** The overall emotional tone is Neutral, with some elements of negativity due to concerns about research misconduct.
*   **Top 3 Points of View:**
    *   The withdrawal raises serious concerns about research integrity and the reliability of publications, especially from institutions with high pressure to publish.
    *   The incident highlights systemic issues within the ML research ecosystem, including publication pressure, overburdened advisor supervision, and a collapsing review procedure.
    *   The pressure to publish in high-impact journals in certain regions leads to unethical behavior and the generation of low-quality research.

**[[D] Some concerns about the current state of machine learning research (Score: 82)](https://www.reddit.com/r/MachineLearning/comments/1ozw2tj/d_some_concerns_about_the_current_state_of/)**
*   **Summary:** This thread discusses concerns about the current state of machine learning research, including herd behavior, a focus on trends, and the potential loss of critical thinking. Some commenters argue that these issues are inherent to any research community.
*   **Emotion:** The overall emotional tone is Negative, reflecting concerns about the direction of ML research.
*   **Top 3 Points of View:**
    *   There is a "herd mentality" in ML research, where researchers tend to follow the latest trends and focus on popular topics like LLMs.
    *   Some worry about the loss of critical thinking and the increasing number of low-quality papers in the field due to the rapid growth of the ML community.
    *   Others argue that the observed issues are common in scientific research and that incremental improvements and interdisciplinary work are still valuable.

**[[P] PapersWithCode's new open-source alternative: OpenCodePapers (Score: 52)](https://www.reddit.com/r/MachineLearning/comments/1p0b96k/p_paperswithcodes_new_opensource_alternative/)**
*   **Summary:** This thread announces and discusses OpenCodePapers, a new open-source alternative to PapersWithCode. Users are providing feedback and suggestions for the platform.
*   **Emotion:** The overall emotional tone is Positive, with excitement and approval for the new platform.
*   **Top 3 Points of View:**
    *   The open-source nature of OpenCodePapers is a positive aspect.
    *   The simple, folder-based organization of tasks is preferable to the sorting system in PapersWithCode.
    *   Getting a proper domain is suggested to make the site more professional.

**[[P] DeepClause - A Neurosymbolic AI System (Score: 8)](https://www.reddit.com/r/MachineLearning/comments/1p095xc/p_deepclause_a_neurosymbolic_ai_system/)**
*   **Summary:** This thread discusses DeepClause - A Neurosymbolic AI System.
*   **Emotion:** The overall emotional tone is Positive.
*   **Top 3 Points of View:**
    *   Neurosymbolic AI is a hot research line.

**[Apple AIML Residency Program 2026 [R] (Score: 8)](https://www.reddit.com/r/MachineLearning/comments/1p0lart/apple_aiml_residency_program_2026_r/)**
*   **Summary:** This thread discusses the Apple AIML Residency Program 2026.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   Some users are interested in applying to the program.

**[[D] Is Hot and Cold just embedding similarity? (Score: 7)](https://www.reddit.com/r/MachineLearning/comments/1p01ny7/d_is_hot_and_cold_just_embedding_similarity/)**
*   **Summary:** This thread discusses whether the "Hot and Cold" game is simply based on embedding similarity.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   The "Hot and Cold" game is likely based on embedding similarity.
    *   The stickied comment explains the game.

**[[D] Is it worth the time to publish and prepare for (archival) ACL/EMNLP workshops? (Score: 7)](https://www.reddit.com/r/MachineLearning/comments/1p06r3h/d_is_it_worth_the_time_to_publish_and_prepare_for/)**
*   **Summary:** This thread discusses whether it's worth the time to publish and prepare for archival ACL/EMNLP workshops.
*   **Emotion:** The overall emotional tone is Positive.
*   **Top 3 Points of View:**
    *   Archival workshops are worth the time.
    *   If you are a Master's student applying for PhDs, then it's probably worth it.

**[[D] Has anyone used ONNX Runtime (ORT) + CUDA for multilingual embedding models (e.g., LaBSE) on GPUs? (Score: 6)](https://www.reddit.com/r/MachineLearning/comments/1ozzu5z/d_has_anyone_used_onnx_runtime_ort_cuda_for/)**
*   **Summary:** This thread is for people asking questions or giving tips on using ONNX Runtime (ORT) + CUDA for multilingual embedding models (e.g., LaBSE) on GPUs.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   Change the execution provider if you're using ORT for inference. For Nvidia GPUs the fastest EP is almost always TensorRT.
    *   TensorRT execution provider significantly boosts ONNX Runtime performance for GPU inference.
    *   Using onnx cuda runtimes aren't that fast compared to traditional huggingface these days.

**[[D] Upload paper arXiv after acceptance (Score: 4)](https://www.reddit.com/r/MachineLearning/comments/1p05h0k/d_upload_paper_arxiv_after_acceptance/)**
*   **Summary:** This thread discusses uploading a paper to arXiv after acceptance.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   arXiv does not have any formatting requirements.
    *   Remove the Logo if there is one in your template.

**[[P] A ‚Äúfoveated‚Äù memory layer for LLM agents: +46.7pp accuracy at 256-token context (open-source) (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1ozz6te/p_a_foveated_memory_layer_for_llm_agents_467pp/)**
*   **Summary:** This thread is about open-sourcing work on a foveated memory layer for LLM agents.
*   **Emotion:** The overall emotional tone is Positive.
*   **Top 3 Points of View:**
    *   This user worked on this for 2 months and is okay with the current state.

**[[D] I built a CPU-native memory system that's 527x faster than GPU retrieval. No CUDA. No transformers. 2.27% variance across 150 runs. (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1p01jr1/d_i_built_a_cpunative_memory_system_thats_527x/)**
*   **Summary:** This thread discusses a CPU-native memory system that claims to be faster than GPU retrieval.
*   **Emotion:** The overall emotional tone is Negative.
*   **Top 3 Points of View:**
    *   Some users don't believe the claims.
    *   Some users are skeptical and ask for a GitHub repo.

**[[R], Geometric Sequence - Structured Memory (Yes, this is legit) (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1p04v3q/r_geometric_sequence_structured_memory_yes_this/)**
*   **Summary:** This thread discusses Geometric Sequence - Structured Memory
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   The idea is quite similar to discretized state space models(mamba)
    *   The task-specific transformation doesn't seem to take anything related to the task as input to `memory_to_tokens'.

**[[P] How can your AI skills help solve one of the world‚Äôs biggest challenges ‚Äî access to clean water?üíß (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1p07a42/p_how_can_your_ai_skills_help_solve_one_of_the/)**
*   **Summary:** This thread discusses how AI skills can help solve access to clean water.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   The only comment is "Clanker"

**[[D] I managed to fine-tune Qwen2.5-Omni-3B while keeping multimodal abilities ‚Äî is it actually as hard as it felt? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1p01hsy/d_i_managed_to_finetune_qwen25omni3b_while/)**
*   **Summary:** This thread discusses the difficulty of fine-tuning Qwen2.5-Omni-3B while keeping multimodal abilities.
*   **Emotion:** The overall emotional tone is Negative.
*   **Top 3 Points of View:**
    *   Fine-tuning is normal when working with a new model.
