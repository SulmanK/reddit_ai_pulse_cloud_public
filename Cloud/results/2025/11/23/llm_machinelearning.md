---
title: "Machine Learning Subreddit"
date: "2025-11-23"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "conferences"]
---

# Overall Ranking and Top Discussions
1.  [[D] ML conferences need to learn from AISTATS (Rant/Discussion)](https://www.reddit.com/r/MachineLearning/comments/1p4tme7/d_ml_conferences_need_to_learn_from_aistats/) (Score: 22)
    *   The discussion centers around the quality of reviews in ML conferences, specifically comparing ICLR and AISTATS. Many users believe that ICLR reviewers may be unqualified due to the empirical nature of the papers submitted, which heavily relies on tuning hyperparameters. AISTATS, on the other hand, focuses on rigorous science.
2.  [[D] How do you create clean graphics that you'd find in conference papers, journals and textbooks (like model architecture, flowcharts, plots, tables etc.)?](https://www.reddit.com/r/MachineLearning/comments/1p4t8l8/d_how_do_you_create_clean_graphics_that_youd_find/) (Score: 20)
    *   The main topic of discussion revolves around the tools and techniques people use to create high-quality graphics for research papers.
3.  [[D] What are the best Machine Learning PhD thesis you have read?](https://www.reddit.com/r/MachineLearning/comments/1p4qdf3/d_what_are_the_best_machine_learning_phd_thesis/) (Score: 12)
    *   This thread focuses on recommendations for outstanding Machine Learning PhD theses, with users suggesting specific authors and resources.
4.  [Isn't VICReg essentially gradient-based SFA? [R]](https://www.reddit.com/r/MachineLearning/comments/1p4hx7k/isnt_vicreg_essentially_gradientbased_sfa_r/) (Score: 4)
    *   Users are discussing the relationship between VICReg and Slow Feature Analysis (SFA), specifically whether VICReg can be considered a gradient-based version of SFA.
5.  [[P] Do papers submitted later / with longer titles receive lower review scores?](https://randomfeatures.substack.com/p/do-papers-submitted-later-receive) (Score: 3)
    *   The discussion revolves around whether the timing of submission and the length of titles influence the review scores of research papers.
6.  [[D] Transitioning from physics to an ML PhD](https://www.reddit.com/r/MachineLearning/comments/1p47xtu/d_transitioning_from_physics_to_an_ml_phd/) (Score: 3)
    *   The conversation is about transitioning from a physics background to pursuing a PhD in Machine Learning, with users sharing advice and experiences.
7.  [[D] ARR January 2026 Discussion (ACL 2026)](https://www.reddit.com/r/MachineLearning/comments/1p4svtl/d_arr_january_2026_discussion_acl_2026/) (Score: 0)
    *   This thread is a discussion about the ARR January 2026 cycle for ACL 2026.
8.  [[D] VAST AI GPUs for Development and Deployment](https://www.reddit.com/r/MachineLearning/comments/1p4jzgp/d_vast_ai_gpus_for_development_and_deployment/) (Score: 0)
    *   The discussion centers around the use of VAST AI GPUs for machine learning development and deployment, with users sharing their experiences and alternative suggestions.
9.  [[P] I Built an AI Training Environment That Runs ANY Retro Game](https://youtube.com/watch?v=vp_eePHswm8&si=ompN4Hshhacrzb5J) (Score: 0)
    *   This thread is regarding an AI training environment that runs retro games.

# Detailed Analysis by Thread
**[[D] ML conferences need to learn from AISTATS (Score: 22)](https://www.reddit.com/r/MachineLearning/comments/1p4tme7/d_ml_conferences_need_to_learn_from_aistats/)**
*  **Summary:** The discussion centers around the quality of reviews in ML conferences, specifically comparing ICLR and AISTATS. Many users believe that ICLR reviewers may be unqualified due to the empirical nature of the papers submitted, which heavily relies on tuning hyperparameters. AISTATS, on the other hand, focuses on rigorous science.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   ICLR reviewers are often unqualified due to the empirical focus of the papers.
    *   AISTATS attracts a more passionate and capable reviewer pool because it is less hyped and more focused.
    *   Breaking up large conferences into subsections could improve the quality of reviews.

**[[D] How do you create clean graphics that you'd find in conference papers, journals and textbooks (like model architecture, flowcharts, plots, tables etc.)? (Score: 20)](https://www.reddit.com/r/MachineLearning/comments/1p4t8l8/d_how_do_you_create_clean_graphics_that_youd_find/)**
*  **Summary:** The main topic of discussion revolves around the tools and techniques people use to create high-quality graphics for research papers.
*  **Emotion:** The overall emotional tone is Neutral, with some instances of Positive and Negative sentiments related to personal experiences.
*  **Top 3 Points of View:**
    *   TikZ and Adobe Illustrator are considered the most professional tools for creating graphics.
    *   PowerPoint is a surprisingly effective and accessible option for creating diagrams.
    *   The clarity and informativeness of the graphic are more important than the tool used to create it.

**[[D] What are the best Machine Learning PhD thesis you have read? (Score: 12)](https://www.reddit.com/r/MachineLearning/comments/1p4qdf3/d_what_are_the_best_machine_learning_phd_thesis/)**
*  **Summary:** This thread focuses on recommendations for outstanding Machine Learning PhD theses, with users suggesting specific authors and resources.
*  **Emotion:** The overall emotional tone is Positive and Neutral, with users enthusiastically recommending theses.
*  **Top 3 Points of View:**
    *   David Abel's thesis is highly recommended.
    *   David Duvenaud's exploration of GP kernels is an enjoyable read.
    *   Theses from ETH Zurich and other European schools are often amazing.

**[Isn't VICReg essentially gradient-based SFA? [R] (Score: 4)](https://www.reddit.com/r/MachineLearning/comments/1p4hx7k/isnt_vicreg_essentially_gradientbased_sfa_r/)**
*  **Summary:** Users are discussing the relationship between VICReg and Slow Feature Analysis (SFA), specifically whether VICReg can be considered a gradient-based version of SFA.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   Self-supervised video denoising papers rely heavily on SFA theory.
    *   VICReg is similar to Maximum Autocorrelation Factor decomposition.
    *   Deep learning for signal processing has not done a good job of citing preceding algorithms.

**[[P] Do papers submitted later / with longer titles receive lower review scores? (Score: 3)](https://randomfeatures.substack.com/p/do-papers-submitted-later-receive)**
*  **Summary:** The discussion revolves around whether the timing of submission and the length of titles influence the review scores of research papers.
*  **Emotion:** The overall emotional tone is Neutral, with some Negative sentiment related to the conclusions drawn in the original post.
*  **Top 3 Points of View:**
    *   Higher submission number papers are more likely to be last minute unpolished submissions.
    *   Author count is positively correlated with score at 10+ authors.
    *   The correlation of title lengths with review score is badly concluded.

**[[D] Transitioning from physics to an ML PhD (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1p47xtu/d_transitioning_from_physics_to_an_ml_phd/)**
*  **Summary:** The conversation is about transitioning from a physics background to pursuing a PhD in Machine Learning, with users sharing advice and experiences.
*  **Emotion:** The overall emotional tone is Positive and Neutral, with users offering encouragement and advice.
*  **Top 3 Points of View:**
    *   Many PhDs working on ML currently have a physics background.
    *   It's important to make sure your foundations in math/programming are strong.
    *   Alessandro Barp is a good example of someone with a physics background doing theoretical work in ML.

**[[D] ARR January 2026 Discussion (ACL 2026) (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1p4svtl/d_arr_january_2026_discussion_acl_2026/)**
*   **Summary:** This thread is a discussion about the ARR January 2026 cycle for ACL 2026.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   The users find it too early to think about ARR January 2026 cycle for ACL 2026

**[[D] VAST AI GPUs for Development and Deployment (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1p4jzgp/d_vast_ai_gpus_for_development_and_deployment/)**
*   **Summary:** The discussion centers around the use of VAST AI GPUs for machine learning development and deployment, with users sharing their experiences and alternative suggestions.
*   **Emotion:** The overall emotional tone is Neutral, with some positive experiences reported.
*   **Top 3 Points of View:**
    *   VAST AI GPUs can be used effectively for development and deployment
    *   RunPod is more reliable than vast AI.
    *   VAST is faster than runpod/lambda labs.

**[[P] I Built an AI Training Environment That Runs ANY Retro Game (Score: 0)](https://youtube.com/watch?v=vp_eePHswm8&si=ompN4Hshhacrzb5J)**
*   **Summary:** This thread is regarding an AI training environment that runs retro games.
*   **Emotion:** The overall emotional tone is Positive.
*   **Top 3 Points of View:**
    *   The users find the project to be Cool.
