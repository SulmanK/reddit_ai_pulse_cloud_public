---
title: "Data Science Subreddit"
date: "2025-11-12"
description: "Analysis of top discussions and trends in the datascience subreddit"
tags: ["data science", "AI", "ML"]
---

# Overall Ranking and Top Discussions
1.  [[D] Causal Meta Learners in 2025?](https://www.reddit.com/r/datascience/comments/1oup4qu/causal_meta_learners_in_2025/) (Score: 25)
    *   The discussion revolves around the relevance and usage of Causal Meta Learners in the field of data science, particularly in big tech and areas like healthcare and operations optimization.
2.  [Level of granularity for ATE estimates](https://www.reddit.com/r/datascience/comments/1oupgt5/level_of_granularity_for_ate_estimates/) (Score: 14)
    *   This thread discusses the appropriate level of granularity for Average Treatment Effect (ATE) estimates, exploring the use of hierarchical models and other statistical methods.
3.  [Any idea what EEOC a data scientist should be classified as?](https://www.reddit.com/r/datascience/comments/1ov594o/any_idea_what_eeoc_a_data_scientist_should_be/) (Score: 1)
    *   This post discusses EEOC classification, fair salary, and salary negotiation strategies for data scientists.
4.  [How to prepare for AI Engineering interviews?](https://www.reddit.com/r/datascience/comments/1ovf9k2/how_to_prepare_for_ai_engineering_interviews/) (Score: 1)
    *   This thread provides insights and advice on how to prepare for AI Engineering interviews, with a focus on the variability of interview expectations depending on the company.
5.  [Prediction Pleasure – The Thrill of Being Right](https://www.reddit.com/r/datascience/comments/1ov3nga/prediction_pleasure_the_thrill_of_being_right/) (Score: 0)
    *   This thread discusses the hype surrounding Large Language Models (LLMs) and how they tap into human curiosity and pattern-seeking tendencies.

# Detailed Analysis by Thread
**[[D] Causal Meta Learners in 2025? (Score: 25)](https://www.reddit.com/r/datascience/comments/1oup4qu/causal_meta_learners_in_2025/)**
*   **Summary:** The discussion centers on the current and future applications of causal meta-learners in data science. Some commenters have used them successfully in areas like pricing and account segmentation, while others find they require too much data for practical use. There's also mention of their niche status compared to newer technologies like GenAI and LLMs.
*   **Emotion:** Predominantly positive and neutral, with a general sentiment of curiosity and interest in the topic.
*   **Top 3 Points of View:**
    *   Causal Meta Learners are still relevant, especially in healthcare and operations optimization, and are used for precision-driven projects in big tech.
    *   Some find them impractical for real-world applications due to the large data requirements.
    *   Causal AI is seen as superior to business rules and prediction models for decision-making purposes.

**[Level of granularity for ATE estimates (Score: 14)](https://www.reddit.com/r/datascience/comments/1oupgt5/level_of_granularity_for_ate_estimates/)**
*   **Summary:** The thread discusses the appropriate statistical methods for estimating Average Treatment Effect (ATE) when dealing with hierarchical data (users with multiple sessions of varying lengths). Options include hierarchical mixed models, bootstrapped standard errors, and OLS with clustered standard errors.
*   **Emotion:** Largely neutral, focused on technical statistical concepts and advice.
*   **Top 3 Points of View:**
    *   Hierarchical mixed models can be used but have stringent assumptions.
    *   Bootstrapped standard errors clustered by user are simpler and easier to explain.
    *   Standard industry practice involves the delta method or OLS with clustered standard errors.

**[Any idea what EEOC a data scientist should be classified as? (Score: 1)](https://www.reddit.com/r/datascience/comments/1ov594o/any_idea_what_eeoc_a_data_scientist_should_be/)**
*   **Summary:** The thread addresses a user's question about EEOC classification and potential salary discrepancies. Commenters point out the role of the EEOC in preventing discrimination and suggest focusing on salary negotiation and understanding company pay policies.
*   **Emotion:** Predominantly neutral, with a tone of offering practical advice.
*   **Top 3 Points of View:**
    *   EEOC primarily deals with discrimination based on protected classes, not job classifications.
    *   Salary negotiation should occur before accepting a job offer.
    *   SOC (Standard Occupational Classification) data can be used as a reference point for salary discussions.

**[How to prepare for AI Engineering interviews? (Score: 1)](https://www.reddit.com/r/datascience/comments/1ovf9k2/how_to_prepare_for_ai_engineering_interviews/)**
*   **Summary:** This discussion explores how to prepare for AI engineering interviews. The key takeaway is that the required skills and the interview process can vary significantly from company to company.
*   **Emotion:** Neutral with a hint of uncertainty due to the inconsistent nature of the role.
*   **Top 3 Points of View:**
    *   AI Engineer roles are not standardized - some are SWEs working on AI, others are MLEs.
    *   Consider focusing on MLE interviews instead to avoid the variability of AI Engineer roles.
    *   Certifications can be helpful.

**[Prediction Pleasure – The Thrill of Being Right (Score: 0)](https://www.reddit.com/r/datascience/comments/1ov3nga/prediction_pleasure_the_thrill_of_being_right/)**
*   **Summary:** This thread discusses the hype surrounding Large Language Models (LLMs) and how they tap into human curiosity and pattern-seeking tendencies. Commenters suggest that people often project intelligence onto the output of LLMs, mistaking fluency for understanding.
*   **Emotion:** Neutral, leaning towards critical.
*   **Top 3 Points of View:**
    *   LLMs are often overhyped because people mistake their fluency for understanding.
    *   LLMs tap into our curiosity and pattern-seeking brains.
    *   The hype is a result of a massive hype cycle driven by various factors.
