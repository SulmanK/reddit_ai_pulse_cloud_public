---
title: "Stable Diffusion Subreddit"
date: "2025-11-19"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [Wan-Animate is amazing](https://v.redd.it/7v1i3w8eh82g1) (Score: 266)
    *   Users discuss the Wan-Animate tool, highlighting its capabilities for creating animations and expressing interest in its potential use cases, while also pointing out some limitations related to image quality and workflow complexity.
2.  [Meta Releases Segment Anything Model 3](https://v.redd.it/14edi6ok892g1) (Score: 92)
    *   The discussion revolves around Meta's new Segment Anything Model 3, with users sharing their experiences, noting its improvements over previous versions, and considering its potential applications, such as in manga text/panel segmentation.
3.  [Supertonic - Open-source TTS model running on Raspberry Pi](https://v.redd.it/0hnukx0fr72g1) (Score: 34)
    *   The post is about Supertonic, an open-source TTS model running on Raspberry Pi. Users express interest, but one user questions the open-source classification due to missing training scripts.
4.  [Is it just me who gets this impression ? Is SDXL better than Flux and Qwen for generating art like this ? Is the problem the text encoder ?](https://i.redd.it/0irqnbk9z82g1.jpeg) (Score: 25)
    *   The conversation explores the strengths and weaknesses of various models (SDXL, Flux, Qwen) for generating specific artistic styles. Participants discuss their preferences and methods for achieving desired results, including combining different models.
5.  [【Release】Newbie-Friendly AI Long Video Tutorial: Multi-angle, Character Consistency, Transitions](https://v.redd.it/yi2m9phaw72g1) (Score: 23)
    *   Users thank the poster for sharing the tutorial, and one user humorously compares the video to cultivation novels with Final Fantasy summon cinematics.
6.  [Still doesn't seem to be a robust way of creating extended videos with Wan 2.2](https://www.reddit.com/r/StableDiffusion/comments/1p187pr/still_doesnt_seem_to_be_a_robust_way_of_creating/) (Score: 15)
    *   The thread discusses the challenges of creating long, consistent videos with Wan 2.2. Users share their experiences, frustrations, and alternative methods for achieving longer video generations.
7.  [2 million parameters denoiser model is everything that you need! ( Source code + model + poison detector) Anti Nightshade, Anti Glaze](https://www.reddit.com/r/StableDiffusion/comments/1p1gjb2/2_million_parameters_denoiser_model_is_everything/) (Score: 13)
    *   Users discuss a new denoiser model, showing interest in its potential use in ComfyUI and inquiring about its speed for batch processing.
8. [looking for best Backgrond Remover from videos](https://www.reddit.com/r/StableDiffusion/comments/1p18wdq/looking_for_best_backgrond_remover_from_videos/) (Score: 3)
    * The poster looking for background remover tools from videos.
9.  [I don't want any character to move. How can I do it?](https://v.redd.it/bcwasx1ty82g1) (Score: 2)
    *   Users suggest methods to prevent character movement in generated videos, including using video editing software, adjusting prompts, and utilizing specific camera angle settings in Wan 2.2.
10. [Best websites for dance/movement reference videos for animate workflows](https://www.reddit.com/r/StableDiffusion/comments/1p1icjo/best_websites_for_dancemovement_reference_videos/) (Score: 1)
    * The post is asking about for best websites for dance/movement reference videos for animate workflows, with one user suggested tiktok.
11. [Fastest Wan 2.2 config when VRAM is not a bottleneck?](https://www.reddit.com/r/StableDiffusion/comments/1p1g2sh/fastest_wan_22_config_when_vram_is_not_a/) (Score: 1)
    *   The discussion is about the fastest configuration for Wan 2.2 when VRAM is not a bottleneck, with a user pointing out that CUDA cores are more important than VRAM for speed.
12. [Noticed some AI image tools are way stricter than others… anyone know why?](https://www.reddit.com/r/StableDiffusion/comments/1p18ssa/noticed_some_ai_image_tools_are_way_stricter_than/) (Score: 0)
    *   Users discuss why some AI image tools are stricter than others, attributing it to factors like monetization, payment processor policies, and model knowledge. They suggest using open-source/local AI generators for more freedom.
13. [Best training set up for image generation?](https://www.reddit.com/r/StableDiffusion/comments/1p16o0p/best_training_set_up_for_image_generation/) (Score: 0)
    * The poster is asking about the best training set up for image generation.
14. [Prompt for this? I want my face portrait like this](https://i.redd.it/0uqacxax482g1.jpeg) (Score: 0)
    *   Users offer advice on prompting and tools for generating a face portrait in a specific style, suggesting the use of img2img in Stable Diffusion, commercial AI tools, and tools like Gemini, Kontext, Qwen-edit, and Flux Redux.
15. [How to get continuity between different images (Krita AI)](https://www.reddit.com/r/StableDiffusion/comments/1p1btku/how_to_get_continuity_between_different_images/) (Score: 0)
    * There is only one comment, with one user suggesting the poster draw over it, then refine.
16. [Could somebody help me upscale this video?](https://www.reddit.com/r/StableDiffusion/comments/1p1eoo9/could_somebody_help_me_upscale_this_video/) (Score: 0)
    *   Users discuss the limitations of upscaling video with diffusion models, noting that they add/invent information and may not accurately recreate details.
17. [please help i have been stuck here for the last 2 hours](https://i.redd.it/uv84q5k6u82g1.png) (Score: 0)
    *   Users offer solutions to a ComfyUI error, suggesting the use of ChatGPT, manual cloning of the node repo, and installing missing custom nodes with the ComfyUI manager.

# Detailed Analysis by Thread
**[[Wan-Animate is amazing](https://v.redd.it/7v1i3w8eh82g1) (Score: 266)](https://v.redd.it/7v1i3w8eh82g1)**
*   **Summary:** Users discuss the Wan-Animate tool, highlighting its capabilities for creating animations and expressing interest in its potential use cases, while also pointing out some limitations related to image quality and workflow complexity.
*   **Emotion:** The overall emotional tone is Positive, with a mix of positive and neutral sentiments. Users are generally impressed with the results, but some express concerns about the complexity of the workflow and the quality of the output.
*   **Top 3 Points of View:**
    *   Wan-Animate is an amazing tool for creating animations.
    *   The workflow is too complex and needs to be simplified.
    *   The technology needs to improve in terms of output quality, especially faces and hands.

**[[Meta Releases Segment Anything Model 3](https://v.redd.it/14edi6ok892g1) (Score: 92)](https://v.redd.it/14edi6ok892g1)**
*   **Summary:** The discussion revolves around Meta's new Segment Anything Model 3, with users sharing their experiences, noting its improvements over previous versions, and considering its potential applications, such as in manga text/panel segmentation.
*   **Emotion:** The overall emotional tone is Positive. Users are generally excited about the improvements in the new model and its potential applications.
*   **Top 3 Points of View:**
    *   SAM 3 is a significant improvement over v2.
    *   The 3D spin feature is promising.
    *   It would be nice to have an LLM to find the closest synonym from the list of 214K concepts.

**[[Supertonic - Open-source TTS model running on Raspberry Pi](https://v.redd.it/0hnukx0fr72g1) (Score: 34)](https://v.redd.it/0hnukx0fr72g1)**
*   **Summary:** The post is about Supertonic, an open-source TTS model running on Raspberry Pi. Users express interest, but one user questions the open-source classification due to missing training scripts.
*   **Emotion:** The overall emotional tone is Neutral, with some positive and negative comments. Users are intrigued by the model, but some express reservations about its open-source nature and limitations.
*   **Top 3 Points of View:**
    *   Supertonic is a good use for a Raspberry Pi.
    *   It's questionable whether it is truly open-source without training scripts.
    *   Users are interested in voice cloning and emotion capabilities.

**[[Is it just me who gets this impression ? Is SDXL better than Flux and Qwen for generating art like this ? Is the problem the text encoder ?](https://i.redd.it/0irqnbk9z82g1.jpeg) (Score: 25)](https://i.redd.it/0irqnbk9z82g1.jpeg)**
*   **Summary:** The conversation explores the strengths and weaknesses of various models (SDXL, Flux, Qwen) for generating specific artistic styles. Participants discuss their preferences and methods for achieving desired results, including combining different models.
*   **Emotion:** The overall emotional tone is Neutral, with a slightly positive leaning. Users are engaged in a technical discussion, sharing their experiences and opinions.
*   **Top 3 Points of View:**
    *   SDXL excels in variety of style and "wildness".
    *   Flux and Qwen are better for composition.
    *   SDXL has hundreds of artist styles baked in.

**[[【Release】Newbie-Friendly AI Long Video Tutorial: Multi-angle, Character Consistency, Transitions](https://v.redd.it/yi2m9phaw72g1) (Score: 23)](https://v.redd.it/yi2m9phaw72g1)**
*   **Summary:** Users thank the poster for sharing the tutorial, and one user humorously compares the video to cultivation novels with Final Fantasy summon cinematics.
*   **Emotion:** The overall emotional tone is Positive.
*   **Top 3 Points of View:**
    *   Users are thankful for the tutorial.
    *  The video is seen as promising.
    *   The video's second half is humorously compared to cultivation novels with Final Fantasy summon cinematics.

**[[Still doesn't seem to be a robust way of creating extended videos with Wan 2.2](https://www.reddit.com/r/StableDiffusion/comments/1p187pr/still_doesnt_seem_to_be_a_robust_way_of_creating/) (Score: 15)](https://www.reddit.com/r/StableDiffusion/comments/1p187pr/still_doesnt_seem_to_be_a_robust_way_of_creating/)**
*   **Summary:** The thread discusses the challenges of creating long, consistent videos with Wan 2.2. Users share their experiences, frustrations, and alternative methods for achieving longer video generations.
*   **Emotion:** The overall emotional tone is Mixed, leaning towards negative. Users express frustration with the limitations of Wan 2.2 for creating extended videos.
*   **Top 3 Points of View:**
    *   Creating extended videos with Wan 2.2 is difficult and often results in janky output.
    *   FFLF is a viable method for maintaining consistency in longer videos.
    *   Image editing models are a bottleneck for creating high-quality last frames.

**[[2 million parameters denoiser model is everything that you need! ( Source code + model + poison detector) Anti Nightshade, Anti Glaze](https://www.reddit.com/r/StableDiffusion/comments/1p1gjb2/2_million_parameters_denoiser_model_is_everything/) (Score: 13)](https://www.reddit.com/r/StableDiffusion/comments/1p1gjb2/2_million_parameters_denoiser_model_is_everything/)**
*   **Summary:** Users discuss a new denoiser model, showing interest in its potential use in ComfyUI and inquiring about its speed for batch processing.
*   **Emotion:** The overall emotional tone is Positive.
*   **Top 3 Points of View:**
    *   The model is smart and the user likes the Ghost Loss function.
    *   User is looking for a good denoiser and might fit their need.
    *   Users are wondering how fast is the detector on batches.

**[[looking for best Backgrond Remover from videos](https://www.reddit.com/r/StableDiffusion/comments/1p18wdq/looking_for_best_backgrond_remover_from_videos/) (Score: 3)](https://www.reddit.com/r/StableDiffusion/comments/1p18wdq/looking_for_best_backgrond_remover_from_videos/)**
*   **Summary:** Wan2.2 Animate allows you to create long videos with Character Replacement. Wan Alpha is a tool to generate alpha masks per frame from a video.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   Wan2.2 Animate allows you to create long videos with Character Replacement.
    *   Wan Alpha is a tool to generate alpha masks per frame from a video.
    *   MatAnyOne can be used.

**[[I don't want any character to move. How can I do it?](https://v.redd.it/bcwasx1ty82g1) (Score: 2)](https://v.redd.it/bcwasx1ty82g1)**
*   **Summary:** Users suggest methods to prevent character movement in generated videos, including using video editing software, adjusting prompts, and utilizing specific camera angle settings in Wan 2.2.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   Generate a high-res background and track the footage in Nuke or similar software.
    *   Use a video editor to zoom out.
    *   Describe characters as wax figures or remove "no" from negatives in the prompt.

**[[Best websites for dance/movement reference videos for animate workflows](https://www.reddit.com/r/StableDiffusion/comments/1p1icjo/best_websites_for_dancemovement_reference_videos/) (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1p1icjo/best_websites_for_dancemovement_reference_videos/)**
*   **Summary:** The post is asking about for best websites for dance/movement reference videos for animate workflows, with one user suggested tiktok.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   One user suggested using tiktok.

**[[Fastest Wan 2.2 config when VRAM is not a bottleneck?](https://www.reddit.com/r/StableDiffusion/comments/1p1g2sh/fastest_wan_22_config_when_vram_is_not_a/) (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1p1g2sh/fastest_wan_22_config_when_vram_is_not_a/)**
*   **Summary:** The discussion is about the fastest configuration for Wan 2.2 when VRAM is not a bottleneck, with a user pointing out that CUDA cores are more important than VRAM for speed.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   The bottleneck of Wan is not really in VRAM when it comes to speed but it's a lot more in cuda cores, so you'd have to pick the fastest GPU and in this case that would be the B200 indeed.

**[[Noticed some AI image tools are way stricter than others… anyone know why?](https://www.reddit.com/r/StableDiffusion/comments/1p18ssa/noticed_some_ai_image_tools_are_way_stricter_than/) (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1p18ssa/noticed_some_ai_image_tools_are_way_stricter_than/)**
*   **Summary:** Some image models are not trained on certain things and do not know the terms used. In general the sites implement the filter not the actual image model. If you want maximum freedom and control, you should check if your HW allows for local execution.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *  If you want maximum freedom and control, you should check if your HW allows for local execution.
    *  Visa and Mastercard block game stores from payment processing unless they remove adult content from their products.
    *  Every model you used was trained on a unique dataset. Unless the dataset has examples of what you're trying to generate, it doesn't know how to represent what you are prompting. LoRAs (adapters) for models can introduce new concepts to the model.

**[[Best training set up for image generation?](https://www.reddit.com/r/StableDiffusion/comments/1p16o0p/best_training_set_up_for_image_generation/) (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1p16o0p/best_training_set_up_for_image_generation/)**
*   **Summary:** The easiest is probably to train a Flux LORA on fal.ai or something similar and then to use it in Krea.  If you want to train locally, maybe favor Qwen using OneTrainer (not Comfy).  Note, Qwen-Image... not Qwen-Edit.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   The easiest is probably to train a Flux LORA on fal.ai or something similar and then to use it in Krea.  If you want to train locally, maybe favor Qwen using OneTrainer (not Comfy).  Note, Qwen-Image... not Qwen-Edit.

**[[Prompt for this? I want my face portrait like this](https://i.redd.it/0uqacxax482g1.jpeg) (Score: 0)](https://i.redd.it/0uqacxax482g1.jpeg)**
*   **Summary:** The prompt for the face looks like thick line black ink, low-brow style, with greyscale in the negative.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   The prompt for the face looks like thick line black ink, low-brow style, with greyscale in the negative.
    *   If you are using Stable Diffusion, upload the image and adapt the prompt below.
    *   Probably just paste your portrait and the example into a tool like Gemini and ask for it.  Otherwise, maybe try Kontext or Qwen-edit.  You can also use Flux Redux

**[[How to get continuity between different images (Krita AI)](https://www.reddit.com/r/StableDiffusion/comments/1p1btku/how_to_get_continuity_between_different_images/) (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1p1btku/how_to_get_continuity_between_different_images/)**
*   **Summary:** Draw over it, then refine
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   Draw over it, then refine

**[[Could somebody help me upscale this video?](https://www.reddit.com/r/StableDiffusion/comments/1p1eoo9/could_somebody_help_me_upscale_this_video/) (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1p1eoo9/could_somebody_help_me_upscale_this_video/)**
*   **Summary:** No, there is no information. Hope you find it by finding the same rims. Looks like steel rims ?
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   I'd upscale it for you if I thought it would help, but any diffusion upscaler is going to add/invent information
    *   No, there is no information. Hope you find it by finding the same rims. Looks like steel rims ?

**[[please help i have been stuck here for the last 2 hours](https://i.redd.it/uv84q5k6u82g1.png) (Score: 0)](https://i.redd.it/uv84q5k6u82g1.png)**
*   **Summary:** There is an issue with the C:\Users\Admin\Documents\ComfyUI-VideoHelperSuite module for custom nodes. The issue is quite old and requires installing some of the dependencies.
*   **Emotion:** The overall emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   Send entire errors to ChatGPT. most of the errors for comfyUi would be solved their. also always use Conda enviroment for comfyui.
    *   There is an issue with the C:\Users\Admin\Documents\ComfyUI-VideoHelperSuite module for custom nodes. The issue is quite old and requires installing some of the dependencies.
    *   Sometimes comfyui manager can't find nodes. You can manually clone the node repo in custom nodes folder.
