---
title: "Stable Diffusion Subreddit"
date: "2025-11-14"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [Depth Anything 3: Recovering the Visual Space from Any Views ( Code , Model available). lot of examples on project page.](https://v.redd.it/q4hvxjd7s91g1) (Score: 66)
    *   The discussion revolves around a new model called Depth Anything 3, which recovers visual space from any view, with users showing interest in its applications and technical requirements.
2.  [Please, how do I make these kind of artstyle?](https://www.reddit.com/gallery/1ox2831) (Score: 25)
    *   Users are asking for advice on how to replicate a specific art style using stable diffusion, and other users are offering suggestions on LoRAs, checkpoints, and prompts to achieve the desired aesthetic.
3.  [WIP] Another ComfyUI interface, personal project. I can release to the public if there is enough interest.](https://i.redd.it/mhuuomr03a1g1.png) (Score: 6)
    *   The post presents a work-in-progress ComfyUI interface, and users express interest in its release, with one user requesting mobile-friendliness and another desiring a better UI overall.
4.  [Baseline Qwen Image workflows don't replicate for multiple people. Is there something weird going on?](https://i.redd.it/pronc6smb91g1.png) (Score: 4)
    *   The thread discusses inconsistencies in Qwen image workflows across different users, with potential causes including code changes, seed variability, hardware differences, and the use of specific LoRAs.
5.  [WAN 2.2 just keeps outputting basically the same even with random seeds.](https://www.reddit.com/r/StableDiffusion/comments/1owzkix/wan_22_just_keeps_outputting_basically_the_same/) (Score: 4)
    *   Users are troubleshooting issues with WAN 2.2, where the output remains consistent despite using random seeds. Solutions involve clearing the torch cache and adjusting LoRA settings.
6.  [Prevent your images, workflows and model from being deleted during ConfyUI update](https://www.reddit.com/r/StableDiffusion/comments/1ox2uc3/prevent_your_images_workflows_and_model_from/) (Score: 3)
    *   A user requests a video tutorial on preventing data loss during ComfyUI updates.
7.  [Wan 2.2 Vs Grok img2video quality](https://www.reddit.com/r/StableDiffusion/comments/1ox061e/wan_22_vs_grok_img2video_quality/) (Score: 2)
    *   The discussion compares the image-to-video quality of WAN 2.2 and Grok, with users noting the higher quality of Grok due to potentially more powerful hardware and suggesting rendering at higher resolutions for WAN.
8.  [Looking to hire someone for the animated visuals for my teaching project](https://www.reddit.com/r/StableDiffusion/comments/1ox0lrd/looking_to_hire_someone_for_the_animated_visuals/) (Score: 2)
    *   A user is looking to hire someone for animated visuals and links to their project, while another user suggests providing more details about the project.
9.  [I am getting OOM on 16G Tesla T4 with InfiniteTalk Kijai WF- Using GGUF but still- I think it should work - what is wrong here?](https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_I2V_InfiniteTalk_example_03.json) (Score: 2)
    *   A user is experiencing out-of-memory errors and seeks help, with suggestions to update nodes in ComfyUI.
10. [Testing the new StableGen the new version uses Qwen-edit. Iâ€™m getting better textures with it.](https://v.redd.it/n3bb3fla1a1g1) (Score: 2)
    *   A user reports improved textures using the new StableGen with Qwen-edit, and asks about the differences between StableGen and using a deepmap on a 3D model.
11. [Am i the only one facing difficulties with wan 2.2 character consistency?](https://www.reddit.com/r/StableDiffusion/comments/1ox55xx/am_i_the_only_one_facing_difficulties_with_wan_22/) (Score: 1)
    *   A user asks about character consistency issues with WAN 2.2, and another user asks for training details.
12. [WAN 2.2 generated minor! How to avoid!?](https://www.reddit.com/r/StableDiffusion/comments/1ox817d/wan_22_generated_minor_how_to_avoid/) (Score: 1)
    *   A user is asking how to avoid generating inappropriate content with WAN 2.2, and other users suggest avoiding age-related terms and deleting such content.
13. [Beginner here, I trained a Character Lora with AI toolkit for wan2.2 i2v but my results weren't great. Anyone got any tips?](https://www.reddit.com/r/StableDiffusion/comments/1ox75ea/beginner_here_i_trained_a_character_lora_with_ai/) (Score: 1)
    *   A beginner seeks tips on improving character LoRA training with AI toolkit for WAN 2.2 i2v, and receives advice on verifying LoRA usage and trying text-to-image generation.
14. [Â°â€§ðŸ«§â‹†.à³ƒà¿”*:ï½¥](https://v.redd.it/he1x9un4w81g1) (Score: 0)
    *   A user shares an image, which is described as having SD 1.5 lighting vibes and style.
15. [AliveMoment is a scam!](https://i.redd.it/l26t53e9v81g1.png) (Score: 0)
    *   A user claims AliveMoment is a scam, prompting recommendations for ComfyUI and warnings against Facebook ads.
16. [Stable Diffusion Prompts](https://www.reddit.com/r/StableDiffusion/comments/1ox2qon/stable_diffusion_prompts/) (Score: 0)
    *   The thread discusses the nature of AI and stable diffusion prompts, with a user explaining that AI doesn't "understand" anything and that the emotion is added by the viewer.
17. [So I made this image generator site... (No sign-up, free)](https://i.redd.it/9bmnmlmhq91g1.jpeg) (Score: 0)
    *   A user showcases their image generator site, receiving praise for its ease of use and suggestions for improvements like adding img2img and full HD image support.
18. [Midjourney/Meta AI Model is now FREE and UNLIMITED](https://www.youtube.com/watch?v=Wf0hIDD7NNI) (Score: 0)
    *   A user shares information about a free and unlimited Midjourney/Meta AI model, but it is noted that the model is not local or open source.

# Detailed Analysis by Thread
**[Depth Anything 3: Recovering the Visual Space from Any Views ( Code , Model available). lot of examples on project page. (Score: 66)](https://v.redd.it/q4hvxjd7s91g1)**
*   **Summary:** The discussion revolves around a new model called Depth Anything 3, which recovers visual space from any view, with users showing interest in its applications and technical requirements.
*   **Emotion:** The overall emotional tone is Neutral, with elements of Positive sentiment regarding the model's potential.
*   **Top 3 Points of View:**
    *   The model looks promising for architectural applications, specifically in cloud point modeling.
    *   There's a question about whether the model can process 360 videos directly or if pre-processing is required.
    *   Users are interested in the minimum VRAM size required to run the model.

**[Please, how do I make these kind of artstyle? (Score: 25)](https://www.reddit.com/gallery/1ox2831)**
*   **Summary:** Users are asking for advice on how to replicate a specific art style using stable diffusion, and other users are offering suggestions on LoRAs, checkpoints, and prompts to achieve the desired aesthetic.
*   **Emotion:** The emotional tone is largely Neutral, focused on providing helpful information and resources.
*   **Top 3 Points of View:**
    *   Using specific LoRAs and checkpoints, especially those labeled "retro" or "90s anime," can help achieve the desired art style.
    *   Experimentation and combining multiple LoRAs at different weights is key to achieving the specific look.
    *   CLIP Interrogator 2 can be used to analyze the style of existing images and generate prompts.

**[WIP] Another ComfyUI interface, personal project. I can release to the public if there is enough interest. (Score: 6)](https://i.redd.it/mhuuomr03a1g1.png)**
*   **Summary:** The post presents a work-in-progress ComfyUI interface, and users express interest in its release, with one user requesting mobile-friendliness and another desiring a better UI overall.
*   **Emotion:** Positive, with users expressing excitement and a desire for UI improvements.
*   **Top 3 Points of View:**
    *   There is a strong interest in a better UI for ComfyUI.
    *   Mobile-friendliness is a desirable feature.
    *   The current UI is perceived as a "cobbled together mess," shifting focus away from art and towards wiring.

**[Baseline Qwen Image workflows don't replicate for multiple people. Is there something weird going on? (Score: 4)](https://i.redd.it/pronc6smb91g1.png)**
*   **Summary:** The thread discusses inconsistencies in Qwen image workflows across different users, with potential causes including code changes, seed variability, hardware differences, and the use of specific LoRAs.
*   **Emotion:** Predominantly Neutral, focused on troubleshooting and identifying potential causes of the inconsistencies.
*   **Top 3 Points of View:**
    *   Code changes and updates to ComfyUI can cause variations in output over time.
    *   Seed variability can be influenced by factors like graphics card driver versions and hardware.
    *   The use of specific LoRAs, such as the 8-step lightning LoRA, can significantly alter the output.

**[WAN 2.2 just keeps outputting basically the same even with random seeds. (Score: 4)](https://www.reddit.com/r/StableDiffusion/comments/1owzkix/wan_22_just_keeps_outputting_basically_the_same/)**
*   **Summary:** Users are troubleshooting issues with WAN 2.2, where the output remains consistent despite using random seeds. Solutions involve clearing the torch cache and adjusting LoRA settings.
*   **Emotion:** Neutral and solution-oriented, with users providing technical advice.
*   **Top 3 Points of View:**
    *   The issue may be related to the torch cache, and clearing memory and node cache might resolve it.
    *   Character LoRAs trained only on pictures might be the problem.
    *   Adjusting LoRA strength with scheduling using Kijai's nodes can help.

**[Prevent your images, workflows and model from being deleted during ConfyUI update (Score: 3)](https://www.reddit.com/r/StableDiffusion/comments/1ox2uc3/prevent_your_images_workflows_and_model_from/)**
*   **Summary:** A user requests a video tutorial on preventing data loss during ComfyUI updates.
*   **Emotion:** Neutral, a simple request.
*   **Top 3 Points of View:**
    *   A user wants a video tutorial.

**[Wan 2.2 Vs Grok img2video quality (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1ox061e/wan_22_vs_grok_img2video_quality/)**
*   **Summary:** The discussion compares the image-to-video quality of WAN 2.2 and Grok, with users noting the higher quality of Grok due to potentially more powerful hardware and suggesting rendering at higher resolutions for WAN.
*   **Emotion:** Neutral, with a hint of positivity towards Grok's quality.
*   **Top 3 Points of View:**
    *   Grok likely uses significantly more powerful hardware, which contributes to its higher quality.
    *   To achieve better quality with WAN, higher resolution rendering is necessary.
    *   WAN is improving with new training.

**[Looking to hire someone for the animated visuals for my teaching project (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1ox0lrd/looking_to_hire_someone_for_the_animated_visuals/)**
*   **Summary:** A user is looking to hire someone for animated visuals and links to their project, while another user suggests providing more details about the project.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   Offering services by linking an example video.
    *   Requesting more information to know what is needed.

**[I am getting OOM on 16G Tesla T4 with InfiniteTalk Kijai WF- Using GGUF but still- I think it should work - what is wrong here? (Score: 2)](https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_I2V_InfiniteTalk_example_03.json)**
*   **Summary:** A user is experiencing out-of-memory errors and seeks help, with suggestions to update nodes in ComfyUI.
*   **Emotion:** Neutral, a request for help with a technical issue.
*   **Top 3 Points of View:**
    *   A user is requesting help to resolve an out-of-memory error.
    *   Updating nodes is a potential solution for resolving errors

**[Testing the new StableGen the new version uses Qwen-edit. Iâ€™m getting better textures with it. (Score: 2)](https://v.redd.it/n3bb3fla1a1g1)**
*   **Summary:** A user reports improved textures using the new StableGen with Qwen-edit, and asks about the differences between StableGen and using a deepmap on a 3D model.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   A user reports getting better textures with the new StableGen using Qwen-edit.
    *   Asking what the difference is between using StableGen and using a deepmap on a 3D model view.

**[Am i the only one facing difficulties with wan 2.2 character consistency? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1ox55xx/am_i_the_only_one_facing_difficulties_with_wan_22/)**
*   **Summary:** A user asks about character consistency issues with WAN 2.2, and another user asks for training details.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   A user is having character consistency issues with WAN 2.2.
    *   WAN is really consistent if you do a good training run. Need to know more about the training.

**[WAN 2.2 generated minor! How to avoid!? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1ox817d/wan_22_generated_minor_how_to_avoid/)**
*   **Summary:** A user is asking how to avoid generating inappropriate content with WAN 2.2, and other users suggest avoiding age-related terms and deleting such content.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   A user is trying to avoid generating inappropriate content.
    *   A user suggested not using age/size/height difference stuff, otherwise the weights are going to inevitably give you some wrong images.
    *   A user said the best way to handle it is to delete it and not post it publicly on the internet.

**[Beginner here, I trained a Character Lora with AI toolkit for wan2.2 i2v but my results weren't great. Anyone got any tips? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1ox75ea/beginner_here_i_trained_a_character_lora_with_ai/)**
*   **Summary:** A beginner seeks tips on improving character LoRA training with AI toolkit for WAN 2.2 i2v, and receives advice on verifying LoRA usage and trying text-to-image generation.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   A user is asking for tips about his Character Lora trained with AI toolkit for wan2.2 i2v
    *   Another user asked if the Lora was added to the workflow.
    *   Another user said to test T2I before I2V.
**[Â°â€§ðŸ«§â‹†.à³ƒà¿”*:ï½¥ (Score: 0)](https://v.redd.it/he1x9un4w81g1)**
*   **Summary:** A user shares an image, which is described as having SD 1.5 lighting vibes and style.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   The image is considered to have lighting vibes and style similar to SD 1.5.

**[AliveMoment is a scam! (Score: 0)](https://i.redd.it/l26t53e9v81g1.png)**
*   **Summary:** A user claims AliveMoment is a scam, prompting recommendations for ComfyUI and warnings against Facebook ads.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   A user claims AliveMoment is a scam.
    *   A user recommends ComfyUI as a free, local alternative.
    *   A user warns against trusting Facebook ads.

**[Stable Diffusion Prompts (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1ox2qon/stable_diffusion_prompts/)**
*   **Summary:** The thread discusses the nature of AI and stable diffusion prompts, with a user explaining that AI doesn't "understand" anything and that the emotion is added by the viewer.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   Image generation works by turning noise into an image based on a description.
    *   AI doesn't actually "understand" anything.
    *   Llms are just predictive text engines.

**[So I made this image generator site... (No sign-up, free) (Score: 0)](https://i.redd.it/9bmnmlmhq91g1.jpeg)**
*   **Summary:** A user showcases their image generator site, receiving praise for its ease of use and suggestions for improvements like adding img2img and full HD image support.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   A user shares their image generator site (No sign-up, free)
    *   A user ask how to generate the images so quickly even on a 3090 GPU
    *   A user says they need to plan on how to manage and maintain the service in the long run before adding many new features.

**[Midjourney/Meta AI Model is now FREE and UNLIMITED (Score: 0)](https://www.youtube.com/watch?v=Wf0hIDD7NNI)**
*   **Summary:** A user shares information about a free and unlimited Midjourney/Meta AI model, but it is noted that the model is not local or open source.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   A user shares that Midjourney/Meta AI Model is now FREE and UNLIMITED
    *   The model is not local and not open source.
    *   Is similar to midjourney for free.
