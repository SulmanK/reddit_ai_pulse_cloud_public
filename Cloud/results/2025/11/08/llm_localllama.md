---
title: "LocalLLaMA Subreddit"
date: "2025-11-08"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "AI", "Local Models"]
---

# Overall Ranking and Top Discussions
1.  [I've been trying to make a real production service that uses LLM and it turned into a pure agony. Here are some of my "experiences".](https://www.reddit.com/r/LocalLLaMA/comments/1orw0fz/ive_been_trying_to_make_a_real_production_service/) (Score: 80)
    * This thread discusses the challenges of using LLMs in production services.
2.  [Here comes another bubble (AI edition)](https://v.redd.it/bnjv3qq0g20g1) (Score: 18)
    * This thread seems to be about the possibility of an AI bubble bursting.
3.  [Another day, another model - But does it really matter to everyday users?](https://i.redd.it/8tf3l0pyu20g1.png) (Score: 16)
    *  The discussion revolves around whether new LLMs significantly impact everyday users, questioning the importance of benchmarks and focusing on practical usability.
4.  [AesCoder 4B Debuts as the Top WebDev Model on Design Arena](https://www.reddit.com/r/LocalLLaMA/comments/1orwirm/aescoder_4b_debuts_as_the_top_webdev_model_on/) (Score: 8)
    *  This thread highlights the debut of AesCoder 4B as a top web development model.
5.  [How does ChatGPT know when to use web search? Is it using tool calling underneath?](https://www.reddit.com/r/LocalLLaMA/comments/1orwepr/how_does_chatgpt_know_when_to_use_web_search_is/) (Score: 5)
    *  This thread explores how ChatGPT decides when to utilize web search, wondering if it involves tool calling.
6.  [Free Week of Observer Max as a thank you to r/LocalLLaMA!](https://i.redd.it/bx60nlgw3yzf1.png) (Score: 4)
    *  This thread discusses a free week of Observer Max offered to the subreddit.
7.  [Which are the current best/your favorite LLM quants/models for high-end PCs?](https://www.reddit.com/r/LocalLLaMA/comments/1orv5ab/which_are_the_current_bestyour_favorite_llm/) (Score: 2)
    *  This is a discussion about the best LLM models for high-end PCs.
8.  [Future of LLMs?](https://www.reddit.com/r/LocalLLaMA/comments/1orxjr2/future_of_llms/) (Score: 1)
    * This thread discusses the future prospects and challenges of LLMs.
9.  [Advice on 5070 ti + 5060 ti 16 GB for TensorRT/VLLM](https://www.reddit.com/r/LocalLLaMA/comments/1orw7s1/advice_on_5070_ti_5060_ti_16_gb_for_tensorrtvllm/) (Score: 1)
    *  This post asks for advice on using 5070 ti and 5060 ti GPUs for TensorRT/VLLM, focusing on hardware considerations.
10. [Dual GPU ( 2 x 5070 TI SUPER 24 GB VRAM ) or one RTX 5090 for LLM?.....or mix of them?](https://www.reddit.com/r/LocalLLaMA/comments/1oryygc/dual_gpu_2_x_5070_ti_super_24_gb_vram_or_one_rtx/) (Score: 1)
    *  This thread asks whether to use dual GPUs or a single RTX 5090 for LLM tasks.
11. [Maximising performance in mixed GPU system - llama.cpp/llama-server](https://www.reddit.com/r/LocalLLaMA/comments/1orwq0g/maximising_performance_in_mixed_gpu_system/) (Score: 1)
    *  This post discusses maximizing performance in mixed GPU systems using llama.cpp/llama-server.
12. [how to feed my local AI tech documentation?](https://www.reddit.com/r/LocalLLaMA/comments/1orxfu1/how_to_feed_my_local_ai_tech_documentation/) (Score: 1)
    *  This thread discusses methods for feeding local AI systems with technical documentation.
13. [What if AI didn’t live in the cloud anymore?](https://i.redd.it/2uxr1fmbq20g1.jpeg) (Score: 0)
    * This thread explores the idea of AI existing outside the cloud.
14. [Kimi K2 Thinking benchmark](https://www.reddit.com/r/LocalLLaMA/comments/1orxyln/kimi_k2_thinking_benchmark/) (Score: 0)
    *  This thread discusses the Kimi K2 Thinking benchmark.
15. [Zero-Knowledge AI inference](https://www.reddit.com/r/LocalLLaMA/comments/1orye15/zeroknowledge_ai_inference/) (Score: 0)
    * This thread focuses on the possibility of Zero-Knowledge AI inference.
16. [I want to learn how to build and use AI agents to make money — total beginner here](https://www.reddit.com/r/LocalLLaMA/comments/1orwokq/i_want_to_learn_how_to_build_and_use_ai_agents_to/) (Score: 0)
    * This thread explores building and using AI agents to make money.
17. [hello community please help! seems like our model outperformed Open AI realtime, google live and sesame](https://www.reddit.com/r/LocalLLaMA/comments/1orzdbt/hello_community_please_help_seems_like_our_model/) (Score: 0)
    *  The thread discusses a model that claims to have outperformed Open AI realtime, google live and sesame.

# Detailed Analysis by Thread
**[ I've been trying to make a real production service that uses LLM and it turned into a pure agony. Here are some of my "experiences". (Score: 80)](https://www.reddit.com/r/LocalLLaMA/comments/1orw0fz/ive_been_trying_to_make_a_real_production_service/)**
*   **Summary:** The original poster (OP) describes the difficulties encountered while trying to create a production service using LLMs, which turned out to be more challenging than expected. Commenters provide suggestions and share their own experiences.
*   **Emotion:** Predominantly Neutral, with some Positive sentiments.
*   **Top 3 Points of View:**
    *   LLMs should be grounded with traditional code and structured output using tools like pydantic AI.
    *   LLMs are best used within a scripted decision tree where they interpret and generate responses with preset, structured JSON.
    *   LLMs are currently only suitable for use cases that can tolerate errors and hallucinations.

**[Here comes another bubble (AI edition) (Score: 18)](https://v.redd.it/bnjv3qq0g20g1)**
*   **Summary:** The thread discusses the potential for an AI bubble and its possible burst. Users reflect on lessons learned.
*   **Emotion:** Predominantly Neutral, with some Positive sentiments.
*   **Top 3 Points of View:**
    *   The AI bubble may burst.
    *   The burst of the bubble could lead to cheaper GPUs.
    *   The post is much better than it has any right to be.

**[Another day, another model - But does it really matter to everyday users? (Score: 16)](https://i.redd.it/8tf3l0pyu20g1.png)**
*   **Summary:** The discussion revolves around whether new LLMs significantly impact everyday users, questioning the importance of benchmarks and focusing on practical usability.
*   **Emotion:** A mix of Neutral and Positive sentiments. A single Negative comment regarding the usefulness of benchmarks.
*   **Top 3 Points of View:**
    *   Benchmarks are becoming less relevant for regular users due to overfitting.
    *   Claude and GLM models are preferred for coding and creative tasks because they "just work".
    *   Price and efficiency are important factors as Chinese labs keep pushing the limits.

**[AesCoder 4B Debuts as the Top WebDev Model on Design Arena (Score: 8)](https://www.reddit.com/r/LocalLLaMA/comments/1orwirm/aescoder_4b_debuts_as_the_top_webdev_model_on/)**
*   **Summary:** This thread highlights the debut of AesCoder 4B as a top web development model.
*   **Emotion:** Predominantly Neutral.
*   **Top 3 Points of View:**
    *   The poster wants to know where to download the model.
    *   The model's main focus is web UI development.
    *   Uploaded some unquantized GGUF's.

**[How does ChatGPT know when to use web search? Is it using tool calling underneath? (Score: 5)](https://www.reddit.com/r/LocalLLaMA/comments/1orwepr/how_does_chatgpt_know_when_to_use_web_search_is/)**
*   **Summary:** This thread explores how ChatGPT decides when to utilize web search, wondering if it involves tool calling.
*   **Emotion:** Predominantly Neutral.
*   **Top 3 Points of View:**
    *   Tool instructions are based within the chat template.
    *   Web search calling is instructed for queries requiring up to date information.
    *   It's streaming tool calling.

**[Free Week of Observer Max as a thank you to r/LocalLLaMA! (Score: 4)](https://i.redd.it/bx60nlgw3yzf1.png)**
*   **Summary:** This thread discusses a free week of Observer Max offered to the subreddit. Users suggest monitoring email, and queuing for music festival tickets.
*   **Emotion:** Mix of Positive and Neutral sentiments.
*   **Top 3 Points of View:**
    *   Want to monitor email for certain keywords and notify different contacts based on the content.
    *   Clicking the mouse is a safe and limited computer interaction.
    *   Want to monitor the queue for music festivals and text when their turn is up.

**[Which are the current best/your favorite LLM quants/models for high-end PCs? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1orv5ab/which_are_the_current_bestyour_favorite_llm/)**
*   **Summary:** This is a discussion about the best LLM models for high-end PCs.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   Qwen3 80b q4 k m

**[Future of LLMs? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1orxjr2/future_of_llms/)**
*   **Summary:** This thread discusses the future prospects and challenges of LLMs.
*   **Emotion:** Neutral
*   **Top 3 Points of View:**
    *   The next but illegal step is to plug those "smart" LLMs to a Database of all the book in the world.
    *   AI companies wanting you to use their own "AI Browser" is the new rage.
    *   In long run 10 years from now there is none.

**[Advice on 5070 ti + 5060 ti 16 GB for TensorRT/VLLM (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1orw7s1/advice_on_5070_ti_5060_ti_16_gb_for_tensorrtvllm/)**
*   **Summary:** This post asks for advice on using 5070 ti and 5060 ti GPUs for TensorRT/VLLM, focusing on hardware considerations.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   It will work and it will give you more usable VRAM but not quite 2x.
    *   Don't go under 1000W for PSU.
    *   Pcie lanes from cpu can be important.

**[Dual GPU ( 2 x 5070 TI SUPER 24 GB VRAM ) or one RTX 5090 for LLM?.....or mix of them? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1oryygc/dual_gpu_2_x_5070_ti_super_24_gb_vram_or_one_rtx/)**
*   **Summary:** This thread asks whether to use dual GPUs or a single RTX 5090 for LLM tasks.
*   **Emotion:** Mix of Positive and Neutral.
*   **Top 3 Points of View:**
    *   Whichever gets you the most VRAM with options for future expansion
    *   One 5090.
    *   The new R9700 GPUs are pushing 7900XTX even lower from their sale price.

**[Maximising performance in mixed GPU system - llama.cpp/llama-server (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1orwq0g/maximising_performance_in_mixed_gpu_system/)**
*   **Summary:** This post discusses maximizing performance in mixed GPU systems using llama.cpp/llama-server.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   The poster has 2xRTX5090 and 2 RTX3090 in eGPUs via thunderbolt.
    *   Looking for the absolute lowest anyone's seen an RTX 6000 Pro being picked up for.

**[how to feed my local AI tech documentation? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1orxfu1/how_to_feed_my_local_ai_tech_documentation/)**
*   **Summary:** This thread discusses methods for feeding local AI systems with technical documentation.
*   **Emotion:** Neutral
*   **Top 3 Points of View:**
    *   The document is added to the context window, kept there, and all future questions are going against the context.
    *   The easiest RAG to start with is [AnythingLLM].

**[What if AI didn’t live in the cloud anymore? (Score: 0)](https://i.redd.it/2uxr1fmbq20g1.jpeg)**
*   **Summary:** This thread explores the idea of AI existing outside the cloud.
*   **Emotion:** Contains both Negative and Neutral sentiments.
*   **Top 3 Points of View:**
    * The industry is slowly moving in this direction.
    * How would our overlords get all our personal data then?
    * That's what this whole sub is about. What do you think we're doing here?

**[Kimi K2 Thinking benchmark (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1orxyln/kimi_k2_thinking_benchmark/)**
*   **Summary:** This thread discusses the Kimi K2 Thinking benchmark.
*   **Emotion:** A mix of Negative and Neutral.
*   **Top 3 Points of View:**
    *   It's very good, but not as exceptional as the overly hyped posts online suggest.
    *   Thoses are *** results

**[Zero-Knowledge AI inference (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1orye15/zeroknowledge_ai_inference/)**
*   **Summary:** This thread focuses on the possibility of Zero-Knowledge AI inference.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   If there actually was a cryptographically sound way to do this, I think it would be a good solution for many users.
    *   The technology might exist, but transparency doesn't.

**[I want to learn how to build and use AI agents to make money — total beginner here (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1orwokq/i_want_to_learn_how_to_build_and_use_ai_agents_to/)**
*   **Summary:** This thread explores building and using AI agents to make money.
*   **Emotion:** Neutral
*   **Top 3 Points of View:**
    *   I'm not making any money off this hobby.

**[hello community please help! seems like our model outperformed Open AI realtime, google live and sesame (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1orzdbt/hello_community_please_help_seems_like_our_model/)**
*   **Summary:** The thread discusses a model that claims to have outperformed Open AI realtime, google live and sesame.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   Put the demo in the wild for people to try it and create buzz.
    *   here is the open source framework we have made to validate s2s performance
