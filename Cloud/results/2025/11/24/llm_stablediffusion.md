---
title: "Stable Diffusion Subreddit"
date: "2025-11-24"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [[D] NOCTURNE - [WAN 2.2]](https://v.redd.it/u72lwv2gz73g1) (Score: 105)
    *   The post showcases "NOCTURNE" generated using WAN 2.2. Users discussed whether it was T2V or I2V and the level of detail in the skin.
2.  [Hunyuan 1.5 step distilled loras are out.](https://www.reddit.com/r/StableDiffusion/comments/1p5ou1s/hunyuan_15_step_distilled_loras_are_out/) (Score: 48)
    *   The post announces the release of Hunyuan 1.5 step distilled LoRAs and compares its speed with WAN 2.2 14B T2V.
3.  [600k 1mp+ dataset](https://www.reddit.com/r/StableDiffusion/comments/1p5kjuk/600k_1mp_dataset/) (Score: 25)
    *   The post discusses a large image dataset. Users discuss creating similar datasets and question why movie screenshots are not used.
4.  [Best prompting resources?](https://www.reddit.com/r/StableDiffusion/comments/1p5hfbw/best_prompting_resources/) (Score: 8)
    *   Users share resources and methods for effective prompting in stable diffusion, including using multimodal LLMs and image analysis tools.
5.  [How to recognize other parts of the body and use something like FaceDetailer on them](https://www.reddit.com/r/StableDiffusion/comments/1p5kazw/how_to_recognize_other_parts_of_the_body_and_use/) (Score: 8)
    *   The post discusses how to use tools like FaceDetailer on other parts of the body. Users suggest using specific models for detection.
6.  [Switching to Nvidia for SD](https://www.reddit.com/r/StableDiffusion/comments/1p5pvu1/switching_to_nvidia_for_sd/) (Score: 6)
    *   The post discusses the best Nvidia GPUs for Stable Diffusion, with recommendations focusing on VRAM and compatibility with newer libraries.
7.  [Video from a sequence of images, using first last frame (or first-middle-last)?](https://www.reddit.com/r/StableDiffusion/comments/1p5so4h/video_from_a_sequence_of_images_using_first_last/) (Score: 1)
    *   The post seeks a tool for creating videos from image sequences. Users recommend WAN 2.2 I2V.
8.  [Run](https://i.redd.it/4ulo6uk5t83g1.png) (Score: 0)
    *   Discussion about the post.
9.  [Understanding Wan 2.2 I2V seed vs resolution affect on motion and output change](https://www.reddit.com/r/StableDiffusion/comments/1p5m2qk/understanding_wan_22_i2v_seed_vs_resolution/) (Score: 0)
    *   The post explores how seed and resolution impact motion and output in Wan 2.2 I2V. Users discuss the effect of resolution on latent space and motion.
10. [stable diffusion model for images with no background](https://www.reddit.com/r/StableDiffusion/comments/1p5jz48/stable_diffusion_model_for_images_with_no/) (Score: 0)
    *   The post requests a stable diffusion model for generating images without a background. Users suggest using rembg nodes, specific prompts, and models like Wan Alpha and Flux.
11. [Wan2.2 camera control](https://www.reddit.com/r/StableDiffusion/comments/1p5pjdk/wan22_camera_control/) (Score: 0)
    *   The post discusses camera control in Wan2.2, suggesting that custom movements may require training a LoRA.
12. [Where can I find a comprehensive list of danbooru-style prompts for pony/illustrious?](https://www.reddit.com/r/StableDiffusion/comments/1p5jpiz/where_can_i_find_a_comprehensive_list_of/) (Score: 0)
    *   The post seeks a comprehensive list of Danbooru-style prompts for Pony/Illustrious. Users suggest using Danbooru itself and Gemini with a tag list.
13. [Full Music Video generated with AI - Wan2.1 Infinitetalk + 2.2 Animate](https://www.reddit.com/r/StableDiffusion/comments/1p5jmp5/full_music_video_generated_with_ai_wan21/) (Score: 0)
    *   The post refers to a full music video generated with AI using Wan2.1 Infinitetalk + 2.2 Animate.
14. [How do I solve this issue? Manager isn't helping. Can I use a replacement for this?](https://www.reddit.com/r/StableDiffusion/comments/1p5j4dy/how_do_i_solve_this_issue_manager_isnt_helping/) (Score: 0)
    *   The post seeks help with an issue, mentioning that the manager is not helping. Users suggest checking the console for import errors and installing the LTXVideo custom nodes.
15. [Alternative to modal.com with free trial](https://www.reddit.com/r/StableDiffusion/comments/1p5hpeb/alternative_to_modalcom_with_free_trial/) (Score: 0)
    *   The post seeks an alternative to modal.com with a free trial. Users point out that Modal offers free credits every month.
16. ["Prison City" Short AI Film (Wan22 I2V ComfyUI)](https://youtu.be/DqpOvi1ZOyk) (Score: 0)
    *   The post showcases a short AI film made with Wan22 I2V ComfyUI. Users suggest using time-remapping techniques to smooth transitions.

# Detailed Analysis by Thread
**[[D] NOCTURNE - [WAN 2.2] (Score: 105)](https://v.redd.it/u72lwv2gz73g1)**
*   **Summary:**  A user shared a video generated using WAN 2.2, titled "NOCTURNE." Other users inquired whether the video was created using text-to-video (T2V) or image-to-video (I2V) techniques. One user expressed appreciation for the detailed skin rendering in the video's close-up shots, also questioning the use of infinitetalk.
*   **Emotion:** The overall emotional tone is mixed, with a blend of positive (appreciation for detail), neutral (inquiries about the method), and negative (downvote due to missing workflow).
*   **Top 3 Points of View:**
    *   Inquiry about the generation method (T2V vs I2V).
    *   Appreciation for the detailed skin rendering.
    *   Criticism due to the absence of a workflow.

**[Hunyuan 1.5 step distilled loras are out. (Score: 48)](https://www.reddit.com/r/StableDiffusion/comments/1p5ou1s/hunyuan_15_step_distilled_loras_are_out/)**
*   **Summary:**  This thread is about the release of Hunyuan 1.5 step distilled LoRAs. Users are discussing its speed compared to WAN 2.2, whether it's worth using, and how to use it properly, including the correct CFG settings. The model works with both I2V and T2V.
*   **Emotion:** The overall emotional tone is neutral, with a hint of positivity due to performance improvements.
*   **Top 3 Points of View:**
    *   Hunyuan 1.5 is faster than WAN 2.2, according to a benchmark.
    *   It's important to use the correct LoRA and CFG settings for optimal results.
    *   The model works with both I2V and T2V.

**[600k 1mp+ dataset (Score: 25)](https://www.reddit.com/r/StableDiffusion/comments/1p5kjuk/600k_1mp_dataset/)**
*   **Summary:** The thread discusses a large (600k+) image dataset being released. People are discussing creating their own datasets and question the use of movie screenshots for building datasets.
*   **Emotion:** The overall emotional tone is neutral, with users sharing information and asking questions.
*   **Top 3 Points of View:**
    *   Users are creating their own smaller datasets for specific purposes (e.g., anime).
    *   A question is raised about why movie screenshots aren't used for datasets.
    *   Another user is planning to release a similar dataset soon.

**[Best prompting resources? (Score: 8)](https://www.reddit.com/r/StableDiffusion/comments/1p5hfbw/best_prompting_resources/)**
*   **Summary:**  Users are asking for the best resources for prompting in stable diffusion. Responses include links to tools such as ComfyUI-Autocomplete-Plus and guides, as well as advice on using LLMs and image analysis nodes to generate prompts.
*   **Emotion:** The overall emotional tone is neutral and helpful, with users sharing resources and advice.
*   **Top 3 Points of View:**
    *   Use multimodal LLMs (like Gemma 3) to create and expand prompts.
    *   Employ controlnets, reference images, and style transfers in addition to prompts.
    *   Utilize image analysis tools like WD-14, Qwen Image analyst, and Florence2.

**[How to recognize other parts of the body and use something like FaceDetailer on them (Score: 8)](https://www.reddit.com/r/StableDiffusion/comments/1p5kazw/how_to_recognize_other_parts_of_the_body_and_use/)**
*   **Summary:**  This thread focuses on how to detect and enhance different body parts using tools like FaceDetailer in Stable Diffusion. Users suggest using specific models designed for body part detection.
*   **Emotion:** The overall tone is neutral and informative, with users providing technical suggestions and resources.
*   **Top 3 Points of View:**
    *   Yolo8v models don't cover all body parts.
    *   N\*deNet can be used for detecting specific parts.
    *   Civitai has detection models.

**[Switching to Nvidia for SD (Score: 6)](https://www.reddit.com/r/StableDiffusion/comments/1p5pvu1/switching_to_nvidia_for_sd/)**
*   **Summary:**  A user is considering switching to Nvidia for Stable Diffusion and is seeking advice. Discussions revolve around the ideal amount of VRAM, specific card recommendations (5060 Ti 16GB, 3060 Ti 12GB), and the benefits of using Nvidia GPUs over AMD.
*   **Emotion:** The overall tone is neutral, providing advice and recommendations.
*   **Top 3 Points of View:**
    *   A minimum of 16GB VRAM is recommended for newer models.
    *   The RTX 5060 Ti 16GB is a good budget option.
    *   Using ROCm on Linux with the existing AMD card might be more performant.

**[Video from a sequence of images, using first last frame (or first-middle-last)? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1p5so4h/video_from_a_sequence_of_images_using_first_last/)**
*   **Summary:** The user asked about using image to video models. A user recommended WAN 2.2 I2V.
*   **Emotion:** The overall emotion is neutral.
*   **Top 3 Points of View:**
    *   User recommended WAN 2.2 I2V.

**[Run (Score: 0)](https://i.redd.it/4ulo6uk5t83g1.png)**
*   **Summary:** The users discuss politics and rules.
*   **Emotion:** The overall emotion is neutral.
*   **Top 3 Points of View:**
    *   The post is not political.
    *   Read the rules.
    *   It would be more believable if they were hanging by rope.

**[Understanding Wan 2.2 I2V seed vs resolution affect on motion and output change (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1p5m2qk/understanding_wan_22_i2v_seed_vs_resolution/)**
*   **Summary:** The thread explores the impact of seed and resolution on motion and output changes in Wan 2.2 I2V. Participants discuss how resolution affects the latent space and suggest upscaling as a way to maintain consistency.
*   **Emotion:** The overall tone is neutral, focusing on technical explanations.
*   **Top 3 Points of View:**
    *   Different resolutions lead to different latent shapes and activation patterns.
    *   Higher resolutions reduce motion by focusing on details.
    *   Upscaling can maintain consistency when increasing resolution.

**[stable diffusion model for images with no background (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1p5jz48/stable_diffusion_model_for_images_with_no/)**
*   **Summary:** The discussion is about generating images with no background, specifically focusing on different models and techniques. The user asked about using stable diffusion models to generate images with no background. The solution is to use layerdiffuse.
*   **Emotion:** The overall emotion is neutral.
*   **Top 3 Points of View:**
    *   Use a specific background color and then map that color to an alpha channel in GIMP.
    *   Wan Alpha can generate against a transparent background.
    *   Use layerdiffuse for creating images with no background.

**[Wan2.2 camera control (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1p5pjdk/wan22_camera_control/)**
*   **Summary:** The discussion is about camera control in Wan2.2.
*   **Emotion:** The overall emotion is neutral.
*   **Top 3 Points of View:**
    *   Training a LoRA for custom movements of this complexity.

**[Where can I find a comprehensive list of danbooru-style prompts for pony/illustrious? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1p5jpiz/where_can_i_find_a_comprehensive_list_of/)**
*   **Summary:** This thread is about a list of danbooru prompts for pony/illustrious.
*   **Emotion:** The overall emotion is neutral.
*   **Top 3 Points of View:**
    *   Use danbooru.

**[Full Music Video generated with AI - Wan2.1 Infinitetalk + 2.2 Animate (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1p5jmp5/full_music_video_generated_with_ai_wan21/)**
*   **Summary:** This post is about full music video generated with AI.
*   **Emotion:** The overall emotion is neutral.
*   **Top 3 Points of View:**
    *   Refer to the previous post.

**[How do I solve this issue? Manager isn't helping. Can I use a replacement for this? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1p5j4dy/how_do_i_solve_this_issue_manager_isnt_helping/)**
*   **Summary:** This post is about technical question and how to solve it.
*   **Emotion:** The overall emotion is neutral.
*   **Top 3 Points of View:**
    *   Check your console.
    *   Install the LTXVideo custom nodes.

**[Alternative to modal.com with free trial (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1p5hpeb/alternative_to_modalcom_with_free_trial/)**
*   **Summary:** This post is about modal.com with free trial.
*   **Emotion:** The overall emotion is neutral.
*   **Top 3 Points of View:**
    *   Modal gives you $30 in free credits every month.
    *   Your link is broken.

**["Prison City" Short AI Film (Wan22 I2V ComfyUI) (Score: 0)](https://youtu.be/DqpOvi1ZOyk)**
*   **Summary:** This post is about "Prison City" Short AI Film
*   **Emotion:** The overall emotion is neutral.
*   **Top 3 Points of View:**
    *   You should use a time-remapping / speed-ramp technique for your transitions to smooth them out.
