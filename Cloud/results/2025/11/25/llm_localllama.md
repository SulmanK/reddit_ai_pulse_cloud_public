---
title: "LocalLLaMA Subreddit"
date: "2025-11-25"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["AI", "Local LLMs", "Machine Learning"]
---

# Overall Ranking and Top Discussions
1.  [[D] Flux 2 can be run on 24gb vram!!!](https://i.redd.it/m9ud0rs8pf3g1.png) (Score: 171)
    *   This thread discusses the new Flux 2 model that can be run on 24GB VRAM, with users sharing links and discussing performance aspects.
2.  [You can now do FP8 reinforcement learning locally! (<5GB VRAM)](https://i.redd.it/t5wv1iax1g3g1.png) (Score: 122)
    *   The discussion revolves around the ability to perform FP8 reinforcement learning locally with low VRAM requirements.
3.  [I built an AI research platform and just open sourced it.](https://www.reddit.com/r/LocalLLaMA/comments/1p6ksc6/i_built_an_ai_research_platform_and_just_open/) (Score: 14)
    *   Users are giving feedback on an open-sourced AI research platform.
4.  [SearXNG-LDR-Academic: I made a "safe for work" fork of SearXNG optimized for use with LearningCircuit's Local Deep Research Tool.](https://www.reddit.com/r/LocalLLaMA/comments/1p6l1lz/searxngldracademic_i_made_a_safe_for_work_fork_of/) (Score: 5)
    *   This thread is about a user creating a "safe for work" fork of SearXNG optimized for local deep research.
5.  [How I replaced Gemini CLI & Copilot with a local stack using Ollama, Continue.dev and MCP servers](https://www.reddit.com/r/LocalLLaMA/comments/1p6nf1r/how_i_replaced_gemini_cli_copilot_with_a_local/) (Score: 4)
    *   The thread is about replacing Gemini CLI & Copilot with a local stack. A user is having issues getting agent mode working properly.
6.  [Cheapest $/vRAM GPU right now? Is it a good time?](https://www.reddit.com/r/LocalLLaMA/comments/1p6o5hf/cheapest_vram_gpu_right_now_is_it_a_good_time/) (Score: 3)
    *   The discussion centers around finding the cheapest GPUs for VRAM and whether it's a good time to buy.
7.  [Trying to build a "Jarvis" that never phones home - on-device AI with full access to your digital life (free beta, roast us)](https://i.redd.it/loj0n38hkg3g1.jpeg) (Score: 3)
    *   This thread is about building a local "Jarvis" AI assistant.
8.  [Does gpt-oss:20b’s thinking output cause more confusion than help in multi-step tasks?](https://v.redd.it/oz01ix8qjf3g1) (Score: 2)
    *   The thread discusses whether the thinking output of gpt-oss:20b helps or hinders multi-step tasks.
9.  [New to local LLMs. Can I give hands on control my of system?](https://www.reddit.com/r/LocalLLaMA/comments/1p6l6ay/new_to_local_llms_can_i_give_hands_on_control_my/) (Score: 1)
    *   A user is asking if they can give hands-on control of their system to local LLMs.
10. [I built a fully local, offline J.A.R.V.I.S. using Python and Ollama (Uncensored and Private)](https://v.redd.it/qs1rong18g3g1) (Score: 0)
    *   The discussion is on a user building a fully local, offline J.A.R.V.I.S. using Python and Ollama
11. [I stucked DeepSeek in a loop using Ollama](https://v.redd.it/ucxbcqm5cg3g1) (Score: 0)
    *   A user stuck DeepSeek in a loop using Ollama.
12. [Local Whisper model for speech-to-text](https://www.reddit.com/r/LocalLLaMA/comments/1p6iytz/local_whisper_model_for_speechtotext/) (Score: 0)
    *   The thread discusses the use of a local Whisper model for speech-to-text.
13. [What is currently the best model balancing speed and accuracy on a 16gb MBA?](https://www.reddit.com/r/LocalLLaMA/comments/1p6kdj8/what_is_currently_the_best_model_balancing_speed/) (Score: 0)
    *   The thread discusses what the best model is that balances speed and accuracy on a 16gb MBA.
14. [Local LLaMA helped me deal with a family tech crisis](https://www.reddit.com/r/LocalLLaMA/comments/1p6mnjs/local_llama_helped_me_deal_with_a_family_tech/) (Score: 0)
    *   A user shares how Local LLaMA helped them with a family tech crisis.

# Detailed Analysis by Thread
**[[D] Flux 2 can be run on 24gb vram!!! (Score: 171)](https://i.redd.it/m9ud0rs8pf3g1.png)**
*   **Summary:** The thread discusses the new Flux 2 model that can be run on 24GB VRAM. Users share links to the model and discuss its performance, focusing on generation time, comparison to other models like Qwen, and the impact of text encoder quantization on result quality.
*   **Emotion:** The overall emotional tone is neutral. While there's some excitement, the discussion is primarily technical and informative. One comment expresses positivity, calling the model "a gift from Allah to humanity!".
*   **Top 3 Points of View:**
    *   Flux 2 can be run on 24GB VRAM.
    *   Text encoder quants and number of steps impact the quality of results.
    *   ComfyUI offers different implementations and offloading techniques.

**[You can now do FP8 reinforcement learning locally! (<5GB VRAM) (Score: 122)](https://i.redd.it/t5wv1iax1g3g1.png)**
*   **Summary:** The thread is about the ability to perform FP8 reinforcement learning locally with less than 5GB of VRAM. Users express interest and ask questions about compatibility with different hardware (like MLX and ROCM) and software frameworks (like torch tune).
*   **Emotion:** The emotional tone is largely positive, with users expressing excitement and interest in the new development.
*   **Top 3 Points of View:**
    *   FP8 reinforcement learning is now possible locally with low VRAM requirements.
    *   There's interest in compatibility with various hardware and software platforms.
    *   Users are curious about the advantages of Unsloth compared to other libraries like torch tune.

**[I built an AI research platform and just open sourced it. (Score: 14)](https://www.reddit.com/r/LocalLLaMA/comments/1p6ksc6/i_built_an_ai_research_platform_and_just_open/)**
*   **Summary:** The thread is about a user open-sourcing an AI research platform they built. The discussion focuses on the platform's features, particularly the ingestion and chunking process, and suggestions for improvement, such as including images or a live demo.
*   **Emotion:** The overall tone is positive, with users praising the project and offering helpful suggestions.
*   **Top 3 Points of View:**
    *   The open-sourced AI research platform is a cool project.
    *   Ingestion and chunking are critical bottlenecks for quality results.
    *   Including images or a live demo would be beneficial.

**[SearXNG-LDR-Academic: I made a "safe for work" fork of SearXNG optimized for use with LearningCircuit's Local Deep Research Tool. (Score: 5)](https://www.reddit.com/r/LocalLLaMA/comments/1p6l1lz/searxngldracademic_i_made_a_safe_for_work_fork_of/)**
*   **Summary:** This thread is about a user creating a "safe for work" fork of SearXNG optimized for local deep research. The primary point of discussion is concern about upcoming deep-learning-based scraping detection.
*   **Emotion:** The emotional tone is negative due to worry about scraping detection.
*   **Top 1 Point of View:**
    *   There is concern about SearXNG being impacted by upcoming deep-learning-based scraping detection.

**[How I replaced Gemini CLI & Copilot with a local stack using Ollama, Continue.dev and MCP servers (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1p6nf1r/how_i_replaced_gemini_cli_copilot_with_a_local/)**
*   **Summary:** The thread is about replacing Gemini CLI & Copilot with a local stack. A user is having issues getting agent mode working properly with Continue.dev and Ollama, trying different models and hardware configurations.
*   **Emotion:** The emotional tone is neutral, reflecting a problem-solving and question-asking environment.
*   **Top 1 Point of View:**
    *   A user is experiencing issues with agent mode in Continue.dev and Ollama and is seeking suggestions.

**[Cheapest $/vRAM GPU right now? Is it a good time? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1p6o5hf/cheapest_vram_gpu_right_now_is_it_a_good_time/)**
*   **Summary:** The discussion centers around finding the cheapest GPUs for VRAM.  Users suggest various options, including the RTX 3090, 32GB V100 "OEM" on Alibaba, and 2x 4060 Ti 16Gb. They also discuss market trends and whether it's a good time to buy.
*   **Emotion:** The emotional tone is neutral and informative, driven by practical advice.
*   **Top 3 Points of View:**
    *   The RTX 3090 remains a good option due to its VRAM and bandwidth.
    *   The market is unlikely to get better soon, making now a reasonable time to buy.
    *   $/GB isn't the only factor to consider, memory speed is also important.

**[Trying to build a "Jarvis" that never phones home - on-device AI with full access to your digital life (free beta, roast us) (Score: 3)](https://i.redd.it/loj0n38hkg3g1.jpeg)**
*   **Summary:** This thread introduces a project aiming to build a "Jarvis"-like on-device AI assistant. Critics question the practicality of running such a system locally, express skepticism about the use of "Jarvis" in the name, and raise concerns about handling sensitive data like health information with a local LLM.
*   **Emotion:** The emotional tone is primarily neutral.
*   **Top 3 Points of View:**
    *   It is questionable whether a mobile phone can afford the computing power in the long run, especially using long-term memory.
    *   The project is just a vibe coded wrapper around a local LLM.
    *   The concept of an AI coded wrapper for AI is not necessarily novel or impressive.

**[Does gpt-oss:20b’s thinking output cause more confusion than help in multi-step tasks? (Score: 2)](https://v.redd.it/oz01ix8qjf3g1)**
*   **Summary:** The thread discusses whether the "thinking output" of gpt-oss:20b helps or hinders multi-step tasks. Suggestions include simplifying workflows, reducing information overload for the model, and using well-defined tools.
*   **Emotion:** The overall tone is neutral.
*   **Top 3 Points of View:**
    *   Smaller models may get confused by their own reasoning.
    *   Simplifying the workflow and offloading difficulty to tools can improve performance.
    *   The 120b version performs well with agent tasks.

**[New to local LLMs. Can I give hands on control my of system? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1p6l6ay/new_to_local_llms_can_i_give_hands_on_control_my/)**
*   **Summary:** A user is asking if they can give hands-on control of their system to local LLMs.
*   **Emotion:** The emotional tone is neutral.
*   **Top 1 Point of View:**
    *   Fine-tuning and reinforcement learning can enable local LLMs to control a system.

**[I built a fully local, offline J.A.R.V.I.S. using Python and Ollama (Uncensored and Private) (Score: 0)](https://v.redd.it/qs1rong18g3g1)**
*   **Summary:** The discussion is on a user building a fully local, offline J.A.R.V.I.S. using Python and Ollama
*   **Emotion:** The emotional tone is neutral.
*   **Top 2 Points of View:**
    *   Other users are curious about how to get the project.
    *   Other users are curious about how to try it out.

**[I stucked DeepSeek in a loop using Ollama (Score: 0)](https://v.redd.it/ucxbcqm5cg3g1)**
*   **Summary:** A user stuck DeepSeek in a loop using Ollama.
*   **Emotion:** The emotional tone is neutral.
*   **Top 1 Point of View:**
    *   DeepSeek sometimes likes to think way too much.

**[Local Whisper model for speech-to-text (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1p6iytz/local_whisper_model_for_speechtotext/)**
*   **Summary:** The thread discusses the use of a local Whisper model for speech-to-text.
*   **Emotion:** The emotional tone is negative.
*   **Top 1 Point of View:**
    *   The user suggest that faster whisper should be used instead.

**[What is currently the best model balancing speed and accuracy on a 16gb MBA? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1p6kdj8/what_is_currently_the_best_model_balancing_speed/)**
*   **Summary:** The thread discusses what the best model is that balances speed and accuracy on a 16gb MBA.
*   **Emotion:** The emotional tone is neutral.
*   **Top 1 Point of View:**
    *   A bigger Qwen3 model can be run on the MBA.

**[Local LLaMA helped me deal with a family tech crisis (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1p6mnjs/local_llama_helped_me_deal_with_a_family_tech/)**
*   **Summary:** A user shares how Local LLaMA helped them with a family tech crisis.
*   **Emotion:** The emotional tone is negative.
*   **Top 1 Point of View:**
    *   The user ignores messages or emails that seem to be llm written.
