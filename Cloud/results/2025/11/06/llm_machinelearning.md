---
title: "Machine Learning Subreddit"
date: "2025-11-06"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "deeplearning"]
---

# Overall Ranking and Top Discussions
1.  [[R] Reasoning models don't degrade gracefully - they hit a complexity cliff and collapse entirely [Research Analysis]](https://www.reddit.com/r/MachineLearning/comments/1ophthe/reasoning_models_dont_degrade_gracefully_they_hit/) (Score: 161)
    * The thread discusses the issue of reasoning models experiencing a sudden and complete failure when faced with increasing complexity.
2.  [[R][N] TabPFN-2.5 is now available: Tabular foundation model for datasets up to 50k samples](https://www.reddit.com/r/MachineLearning/comments/1oq1gq1/rn_tabpfn25_is_now_available_tabular_foundation/) (Score: 26)
    * The thread is about the availability of TabPFN-2.5, a tabular foundation model for datasets up to 50k samples.
3.  [[D] Favorite Deep Learning Textbook for teaching undergrads?](https://www.reddit.com/r/MachineLearning/comments/1oprm3b/d_favorite_deep_learning_textbook_for_teaching/) (Score: 14)
    * The discussion is about recommending favorite deep learning textbooks for teaching undergraduate students.
4.  [[P] Generating Knowledge Graphs From Unstructured Text Data](https://www.reddit.com/r/MachineLearning/comments/1opohcg/p_generating_knowledge_graphs_from_unstructured/) (Score: 6)
    * This post discusses methods and tools for generating knowledge graphs from unstructured text data.
5.  [[D] Kosmos achieves 79.4% accuracy in 12-hour autonomous research sessions, but
  verification remains the bottleneck](https://www.reddit.com/r/MachineLearning/comments/1opy7b9/d_kosmos_achieves_794_accuracy_in_12hour/) (Score: 3)
    * The thread discusses Kosmos, a system that achieves 79.4% accuracy in 12-hour autonomous research sessions, but verification remains a challenge.
6.  [[D] Returning large number of exact passages with LLM document retrieval?](https://www.reddit.com/r/MachineLearning/comments/1oq9sqj/d_returning_large_number_of_exact_passages_with/) (Score: 2)
    * The thread discusses the problem of LLM document retrieval returning a large number of exact passages.
7.  [[D] Is ST-MOE model Decoder only or Encoder-Decoder architecture?](https://www.reddit.com/r/MachineLearning/comments/1oproth/d_is_stmoe_model_decoder_only_or_encoderdecoder/) (Score: 2)
    * A question asking whether the ST-MOE model uses a decoder-only or encoder-decoder architecture.
8.  [[R]You’re not fixing the math problem in AI. You’re ignoring the behavior problem.](https://www.reddit.com/r/MachineLearning/comments/1oq3u89/ryoure_not_fixing_the_math_problem_in_ai_youre/) (Score: 0)
    * The thread questions whether the focus in AI is too much on mathematical problems and not enough on behavioral problems.

# Detailed Analysis by Thread
**[[R] Reasoning models don't degrade gracefully - they hit a complexity cliff and collapse entirely [Research Analysis] (Score: 161)](https://www.reddit.com/r/MachineLearning/comments/1ophthe/reasoning_models_dont_degrade_gracefully_they_hit/)**
*  **Summary:** The thread discusses research suggesting that reasoning models don't degrade gracefully but rather experience a sudden and complete failure when complexity increases. Users discuss potential causes, compare this behavior to human reasoning, and suggest possible solutions.
*  **Emotion:** The overall emotional tone is neutral, with some negative sentiment expressed in disagreement with the initial assertion.
*  **Top 3 Points of View:**
    *   Reasoning models experience a complexity cliff.
    *   This behavior is similar to humans dealing with string theory.
    *   The sudden drop is related to training length.

**[[R][N] TabPFN-2.5 is now available: Tabular foundation model for datasets up to 50k samples (Score: 26)](https://www.reddit.com/r/MachineLearning/comments/1oq1gq1/rn_tabpfn25_is_now_available_tabular_foundation/)**
*  **Summary:** The thread discusses the release of TabPFN-2.5, a tabular foundation model. Users inquire about architectural changes and request more technical information.
*  **Emotion:** The overall emotional tone is neutral with positive sentiment.
*  **Top 3 Points of View:**
    *   The user suggests TabPFN requires a feature-to-label relationship.
    *   Some users are seeking technical reports for the model.
    *   Users are interested in the changes since the last version.

**[[D] Favorite Deep Learning Textbook for teaching undergrads? (Score: 14)](https://www.reddit.com/r/MachineLearning/comments/1oprm3b/d_favorite_deep_learning_textbook_for_teaching/)**
*  **Summary:** The thread discusses and recommends different deep learning textbooks suitable for undergraduate students.
*  **Emotion:** The overall emotional tone is positive.
*  **Top 3 Points of View:**
    *   *Understanding Deep Learning* by Simon Prince is a good choice.
    *   Goodfellow book and Bishop book are great starting points.
    *   Tuning Playbook is recommended for practice.

**[[P] Generating Knowledge Graphs From Unstructured Text Data (Score: 6)](https://www.reddit.com/r/MachineLearning/comments/1opohcg/p_generating_knowledge_graphs_from_unstructured/)**
*  **Summary:** The thread is centered around generating knowledge graphs from unstructured text data. Users share tools, techniques, and resources for this task.
*  **Emotion:** The overall emotional tone is neutral with some positive sentiment.
*  **Top 3 Points of View:**
    *   REBEL can be used to build a canonical map.
    *   ragGraph from Microsoft Research can be used.
    *   LLMs work well for this task, given a few shot examples.

**[[D] Kosmos achieves 79.4% accuracy in 12-hour autonomous research sessions, but
  verification remains the bottleneck (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1opy7b9/d_kosmos_achieves_794_accuracy_in_12hour/)**
*  **Summary:** The discussion revolves around the performance and cost-effectiveness of Kosmos, a system for autonomous research.
*  **Emotion:** The overall emotional tone is mixed, with negative sentiment regarding the cost.
*  **Top 3 Points of View:**
    *   Kosmos is too expensive because the runs might fail.
    *   Verification remains a bottleneck.
    *   Kosmos consists of explicit data models and update attribution rules with a knowledge graph.

**[[D] Returning large number of exact passages with LLM document retrieval? (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1oq9sqj/d_returning_large_number_of_exact_passages_with/)**
*  **Summary:** The thread discusses methods to improve LLM document retrieval and reduce the retrieval of numerous exact passages.
*  **Emotion:** The overall emotional tone is neutral with some negative sentiment.
*  **Top 3 Points of View:**
    *   Smart chunking for search is important.
    *   Consider rewriting text with special tokens.
    *   Fine-tuned embeddings are better than general embeddings.

**[[D] Is ST-MOE model Decoder only or Encoder-Decoder architecture? (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1oproth/d_is_stmoe_model_decoder_only_or_encoderdecoder/)**
*  **Summary:** A user asks whether the ST-MOE model is decoder-only or encoder-decoder architecture.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   The ST-MOE model is an encoder-decoder architecture.

**[[R]You’re not fixing the math problem in AI. You’re ignoring the behavior problem. (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1oq3u89/ryoure_not_fixing_the_math_problem_in_ai_youre/)**
*  **Summary:** The thread discusses the importance of behavioral issues in AI research, suggesting they are being overshadowed by mathematical problems.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   The post is a machine learning post written by AI.
    *   The post explains the challenges with autoregressive models.
    *   A user questions the relevance of the post to machine learning.
