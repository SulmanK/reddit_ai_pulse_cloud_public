---
title: "Machine Learning Subreddit"
date: "2025-11-13"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "reddit", "analysis"]
---

# Overall Ranking and Top Discussions
1.  [[R] LeJEPA: New Yann Lecun paper](https://www.reddit.com/r/MachineLearning/comments/1ovm4fd/r_lejepa_new_yann_lecun_paper/) (Score: 189)
    *   Discusses a new paper by Yann LeCun, with users sharing their initial thoughts and links to the paper.
2.  [[D] CVPR submission number almost at 30k](https://www.reddit.com/r/MachineLearning/comments/1ovqvdr/d_cvpr_submission_number_almost_at_30k/) (Score: 45)
    *   Users are discussing the high number of submissions to CVPR and the potential impact on the review process.
3.  [[D] How to sound more like a Researcher](https://www.reddit.com/r/MachineLearning/comments/1ovtrn4/d_how_to_sound_more_like_a_researcher/) (Score: 26)
    *   Discussion on how to improve communication skills to be perceived as a researcher, especially when transitioning from applied ML.
4.  [[D] how to calculate aic/bic for Huber loss?](https://www.reddit.com/gallery/1ovtq2a) (Score: 2)
    *   A question about how to calculate AIC/BIC for Huber loss.
5.  [[P] What does AGPL 3.0 actually include?](https://www.reddit.com/r/MachineLearning/comments/1ovu5g9/p_what_does_agpl_30_actually_include/) (Score: 2)
    *   Inquiring about the extent of AGPL 3.0's coverage, especially regarding model weights vs. code.
6.  [[D] Question about self-referential novelty gating](https://www.reddit.com/r/MachineLearning/comments/1ow8587/d_question_about_selfreferential_novelty_gating/) (Score: 0)
    *   A question about self-referential novelty gating.
7.  [[P] Looking for Insight: How to Identify Clients With No Intention to Pay (Fraudulent Credit Behavior)](https://www.reddit.com/r/MachineLearning/comments/1ow9qxj/p_looking_for_insight_how_to_identify_clients/) (Score: 0)
    *   Seeking insights on how to identify clients with fraudulent credit behavior.

# Detailed Analysis by Thread
**[[R] LeJEPA: New Yann Lecun paper (Score: 189)](https://www.reddit.com/r/MachineLearning/comments/1ovm4fd/r_lejepa_new_yann_lecun_paper/)**
*  **Summary:**  Discussion about Yann Lecun's new paper, LeJEPA, covering various aspects like its theoretical implications, comparison to other models (like VICReg), and its potential impact on building brain-like systems. Some users express respect for LeCun's approach and highlight the practical aspects of the research.
*  **Emotion:** The overall emotional tone is positive, with users expressing respect and interest in the paper. There are also neutral sentiments as users share links and make factual observations.
*  **Top 3 Points of View:**
    *   LeJEPA is seen as a potentially important step towards building more brain-like systems.
    *   Some users are unsure about the efficiency of the "views" concept used in the paper.
    *   There is respect for Lecun's theoretical approach compared to purely empirical methods.

**[[D] CVPR submission number almost at 30k (Score: 45)](https://www.reddit.com/r/MachineLearning/comments/1ovqvdr/d_cvpr_submission_number_almost_at_30k/)**
*  **Summary:**  This thread discusses the high volume of submissions to CVPR and the challenges it poses to maintaining review quality and fairness. Users express concerns about a potentially chaotic review process and the need for better reviewer accountability. A PhD student asks questions regarding the anonymization requirements for submission.
*  **Emotion:** The thread has a neutral emotional tone, mostly due to factual observations. Some comments express concern about the review process.
*  **Top 3 Points of View:**
    *   The high number of submissions will negatively impact the review quality.
    *   There is a need for a better mechanism to ensure reviewer accountability, possibly by partially de-anonymizing reviewers.
    *   The increasing number of submissions creates a chaotic cycle as rejected papers are resubmitted elsewhere.

**[[D] How to sound more like a Researcher (Score: 26)](https://www.reddit.com/r/MachineLearning/comments/1ovtrn4/d_how_to_sound_more_like_a_researcher/)**
*  **Summary:**  This thread discusses the communication skills needed to be perceived as a researcher, particularly when transitioning from applied ML. Users emphasize the importance of a deep knowledge of the field, strong publication record, and the ability to articulate the 'why' behind decisions.
*  **Emotion:** The thread maintains a neutral tone, with advice and suggestions being given in an objective manner.
*  **Top 3 Points of View:**
    *   Deep knowledge of the field and a strong publication record are crucial for being seen as a researcher.
    *   Articulating the reasoning and hypothesis behind decisions is more important than simply describing engineering solutions.
    *   Exposure through journal clubs, conferences, and technical blogs can help improve communication skills.

**[[D] how to calculate aic/bic for Huber loss? (Score: 2)](https://www.reddit.com/gallery/1ovtq2a)**
*  **Summary:**  A user seeks guidance on calculating AIC/BIC (Akaike Information Criterion/Bayesian Information Criterion) for Huber loss, a loss function used in statistical modeling.
*  **Emotion:** The thread has a neutral emotional tone.
*  **Top 3 Points of View:**
    *   AIC/BIC are based on posterior model density comparison.
    *   Replacing the Gaussian distribution with Hubert loss might lead to different AIC like criterionâ€™s if you can compute them analytically.

**[[P] What does AGPL 3.0 actually include? (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1ovu5g9/p_what_does_agpl_30_actually_include/)**
*  **Summary:**  This post asks about the scope of the AGPL 3.0 license, specifically whether it applies to model weights in addition to code.
*  **Emotion:** The thread has a neutral emotional tone.
*  **Top 3 Points of View:**
    *   It is unclear whether AGPL 3.0 covers model weights as mathematical formulas.
    *   If the code is used, AGPL 3.0 definitely applies.

**[[D] Question about self-referential novelty gating (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ow8587/d_question_about_selfreferential_novelty_gating/)**
*  **Summary:**  This post poses a question about self-referential novelty gating mechanisms.
*  **Emotion:** The thread has a neutral emotional tone.
*  **Top 3 Points of View:**
    *   The only point of view is someone asking a question about "what long term memory?" in the original post.

**[[P] Looking for Insight: How to Identify Clients With No Intention to Pay (Fraudulent Credit Behavior) (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ow9qxj/p_looking_for_insight_how_to_identify_clients/)**
*  **Summary:**  A user seeks advice on identifying fraudulent clients and asks for input on modeling strategies, feature selection, evaluation metrics, and dataset requirements for fraud detection.
*  **Emotion:** The thread has a neutral emotional tone.
*  **Top 3 Points of View:**
    *   Fraud detection is best treated as an imbalanced classification task.
    *   Update the post with what you have tried so far so that people will know how to guide you.
    *   AVG Precision is most important.
