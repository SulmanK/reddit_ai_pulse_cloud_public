---
title: "Stable Diffusion Subreddit"
date: "2025-11-21"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [I love Qwen](https://www.reddit.com/gallery/1p37d2h) (Score: 115)
    * The thread is centered around the user's positive experience with the Qwen model for image generation, showcasing results and prompting discussion about workflows and comparisons with other models.
2.  [Some HunyuanVideo 1.5 T2V examples](https://v.redd.it/r65sjh6m7n2g1) (Score: 58)
    * This post showcases examples of text-to-video generation using HunyuanVideo 1.5, sparking a conversation about its quality compared to other models like WAN, its potential for animation, and its licensing restrictions.
3.  [found these old AI images](https://www.reddit.com/gallery/1p34zgb) (Score: 37)
    * A user shares a collection of older AI-generated images, prompting nostalgic comments about past AI art styles and discussions about the models used.
4.  [Unreal Engine 5 Style Qwen LoRA](https://www.reddit.com/gallery/1p38huf) (Score: 11)
    * The thread features an Unreal Engine 5 style LoRA for the Qwen model, with users expressing their admiration for the results but lamenting the lack of Qwen support in Forge.
5.  [Most realistic way to composite a product into a new scene without changing any text on the object?﻿](https://www.reddit.com/r/StableDiffusion/comments/1p36ydd/most_realistic_way_to_composite_a_product_into_a/) (Score: 5)
    * This post seeks advice on compositing a product into a new scene without altering the text on the object, leading to suggestions of using Qwen Image Edit or Flux Kontext, particularly with specific LoRAs.
6.  [What happened to the Tencent's HunyuanImage-3.0 model? seems like Nano banana pro.](https://www.reddit.com/r/StableDiffusion/comments/1p384ir/what_happened_to_the_tencents_hunyuanimage30/) (Score: 2)
    * A user inquires about the Tencent's HunyuanImage-3.0 model, comparing it to Nano banana pro, and the discussion revolves around its memory requirements, potential for local running, and the need for specific nodes or GGUFs.
7.  [SDXL LoRA workflow with refiner for comfyUI?](https://www.reddit.com/r/StableDiffusion/comments/1p3194q/sdxl_lora_workflow_with_refiner_for_comfyui/) (Score: 2)
    * The thread asks about SDXL LoRA workflow with refiner for ComfyUI, with users sharing their experiences and advice on checkpoints, upscaling, and alternative models like Flux or Latent Interposer.
8.  [One or two threads full of basic knowledge ?](https://www.reddit.com/r/StableDiffusion/comments/1p314iz/one_or_two_threads_full_of_basic_knowledge/) (Score: 2)
    * This post suggests creating a thread with basic knowledge, sparking a discussion about the feasibility of maintaining a comprehensive wiki, the limitations of Reddit's search, and the usefulness of LLMs and YouTube tutorials.
9.  [Double GPU Bandwidth Question](https://www.reddit.com/r/StableDiffusion/comments/1p38pn3/double_gpu_bandwidth_question/) (Score: 1)
    * This thread poses a question about double GPU bandwidth, with the comments discussing using a PCIe splitter or different PCIe slots for optimal performance.
10. [help retaining composition with SDXL artist studies](https://www.reddit.com/r/StableDiffusion/comments/1p38m48/help_retaining_composition_with_sdxl_artist/) (Score: 1)
    * A user seeks help retaining composition with SDXL artist studies. Discussion focuses on using img2img with lower denoise levels or utilizing ControlNet for better composition control.
11. [Any recommended models (and lora) for text to pencil sketch for illustrating children's books?](https://www.reddit.com/r/StableDiffusion/comments/1p32po6/any_recommended_models_and_lora_for_text_to/) (Score: 1)
    * User asks for model and LoRA recommendations for creating text-to-pencil sketches suitable for children's books. The response suggests a specific LoRA and advises on GPU handling for certain models.
12. [AI Video Generation Comparison - Paid/Free and Local](https://www.reddit.com/r/StableDiffusion/comments/1p38742/ai_video_generation_comparison_paidfree_and_local/) (Score: 0)
    * The discussion centers around comparing AI video generation options, including paid, free, and local solutions. One comment suggests upgrading to a newer RTX card for better local performance.
13. [Starting out with AI content creation… what tools should I actually use?](https://www.reddit.com/r/StableDiffusion/comments/1p36izj/starting_out_with_ai_content_creation_what_tools/) (Score: 0)
    * User is seeking recommendations for AI content creation tools. The comments point out the sub's focus on local/open-source generation and suggest exploring local models to avoid incurring additional charges.
14. [Upscaling video using high resolution "assets" to help with missing details.](https://www.reddit.com/r/StableDiffusion/comments/1p38hqo/upscaling_video_using_high_resolution_assets_to/) (Score: 0)
    * The post is about upscaling video. A commenter shares their experience using Topaz with the Proteus Model to upscale old music videos.
15. [Best Ai Short Video Maker App](https://www.reddit.com/r/StableDiffusion/comments/1p33js8/best_ai_short_video_maker_app/) (Score: 0)
    * The thread asks for the best AI short video maker app, and the suggestions include wan.video and Google video.
16. [What are the best diffusion models nowadays for image and video?](https://www.reddit.com/r/StableDiffusion/comments/1p31li2/what_are_the_best_diffusion_models_nowadays_for/) (Score: 0)
    *  This post asks for recommendations on the best diffusion models for image and video generation. Commenters suggest Qwen image, Qwen edit, Wan 2.2, and mention the importance of considering each model's particular 'vibe'.
17. [Changing AMD_SERIALIZE_KERNEL](https://www.reddit.com/r/StableDiffusion/comments/1p32amb/changing_amd_serialize_kernel/) (Score: 0)
    * The thread discusses changing AMD_SERIALIZE_KERNEL. Solutions provided involve setting HIP_VISIBLE_DEVICES or CUDA_VISIBLE_DEVICES to specify the GPU to use.

# Detailed Analysis by Thread
**[I love Qwen (Score: 115)](https://www.reddit.com/gallery/1p37d2h)**
*   **Summary:** The user expresses strong affinity for the Qwen model, showcasing generated images. Discussion includes requests for workflow details, comparisons to other models like Gemini Pro and Grok, and debate about local vs online model capabilities. Some users also provide advice or disagree with the user's assessment, citing prompting skill as a factor.
*   **Emotion:** Predominantly Positive, with a mix of Neutral sentiments. The overall tone is enthusiastic, driven by the user's initial statement. Some comments express disagreement or offer constructive criticism, resulting in some Negative scores as well.
*   **Top 3 Points of View:**
    *   Qwen is an excellent local model for image generation.
    *   The user's results depend on prompting skill, not solely on the model's capabilities.
    *   Local models are superior to online models due to censorship limitations.

**[Some HunyuanVideo 1.5 T2V examples (Score: 58)](https://v.redd.it/r65sjh6m7n2g1)**
*   **Summary:** The thread showcases examples of text-to-video generation using HunyuanVideo 1.5. Comments compare its quality to Wan 2.2, discuss its potential for animation, and question its censorship levels. Licensing restrictions for specific regions are also brought up.
*   **Emotion:** Mixed, with Neutral and Positive sentiments dominating, although there is a strong Negative sentiment that the model is unimpressive and not better than Wan 2.2. The thread exhibits a mix of enthusiasm, skepticism, and practical concerns.
*   **Top 3 Points of View:**
    *   HunyuanVideo 1.5 has promise for animation due to its improved cartoon output.
    *   HunyuanVideo 1.5 is not as good as Wan 2.2, especially regarding ControlNets.
    *   Licensing restrictions in some regions make HunyuanVideo 1.5 unusable for some users.

**[found these old AI images (Score: 37)](https://www.reddit.com/gallery/1p34zgb)**
*   **Summary:** The user shares a collection of "old" AI-generated images, prompting nostalgia and discussion about the evolution of AI art. Some comments reminisce about past AI art styles, and identify which AI models might have been used.
*   **Emotion:** Predominantly Positive and Neutral. A sense of nostalgia and appreciation for the earlier days of AI art is apparent.
*   **Top 3 Points of View:**
    *   The images evoke a sense of nostalgia for earlier AI art styles.
    *   The images may have been generated using Dalle AI.
    *   The images were good times.

**[Unreal Engine 5 Style Qwen LoRA (Score: 11)](https://www.reddit.com/gallery/1p38huf)**
*   **Summary:** The post showcases an Unreal Engine 5 style LoRA for the Qwen model. The comments express admiration for the visual style of the LoRA and disappointment that Forge doesn't support Qwen.
*   **Emotion:** There is a mixed sentiment, with some positivity about the quality of the LoRA and some negativity about the lack of support of Qwen in Forge.
*   **Top 2 Points of View:**
    *   The LoRA produces very cool-looking images in the style of Unreal Engine 5.
    *   The lack of Qwen support in Forge is a drawback.

**[Most realistic way to composite a product into a new scene without changing any text on the object?﻿ (Score: 5)](https://www.reddit.com/r/StableDiffusion/comments/1p36ydd/most_realistic_way_to_composite_a_product_into_a/)**
*   **Summary:** A user is seeking advice on how to realistically composite a product into a new scene without altering text. Suggestions involve Qwen Image Edit, Flux Kontext and Gemini 3. The need to clarify if the text or the scene needs to be changed is also a topic.
*   **Emotion:** Negative sentiments are present, mixed with Neutral and Positive.
*   **Top 3 Points of View:**
    *   Qwen Image Edit or Flux Kontext are good solutions, especially with specific LoRAs.
    *   Google Gemini 3 is also a potential option.
    *   Clarification is needed on whether the object needs to be manipulated or if only the scene needs to be adapted.

**[What happened to the Tencent's HunyuanImage-3.0 model? seems like Nano banana pro. (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1p384ir/what_happened_to_the_tencents_hunyuanimage30/)**
*   **Summary:** A user asks about Tencent's HunyuanImage-3.0 model. Commenters discuss its architecture (MoE with 80B parameters), VRAM requirements, and challenges in running it locally. Some are waiting for nodes or GGUFs to make it easier to use.
*   **Emotion:** The sentiment is largely Neutral, with some positivity.
*   **Top 3 Points of View:**
    *   HunyuanImage-3.0 is an MoE model with high VRAM requirements.
    *   Running it locally is challenging due to the need for custom kernels and high memory demands.
    *   Some users are waiting for easier ways to run it, such as nodes or GGUFs.

**[SDXL LoRA workflow with refiner for comfyUI? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1p3194q/sdxl_lora_workflow_with_refiner_for_comfyui/)**
*   **Summary:** A user asks about an SDXL LoRA workflow with a refiner in ComfyUI. Commenters offer advice on checkpoints, upscaling, and alternative methods such as using Flux or latent interposers.
*   **Emotion:** The overall sentiment is Neutral, with a bit of positivity that the user is on the right track.
*   **Top 3 Points of View:**
    *   Refiner models are not always the best approach and should be replaced with flux or latent interposer.
    *   Good SDXL models are a great substitute for Refiner models.
    *   SDXL LoRA workflow is a common and worthy pursuit.

**[One or two threads full of basic knowledge ? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1p314iz/one_or_two_threads_full_of_basic_knowledge/)**
*   **Summary:** A user suggests creating threads with basic knowledge about stable diffusion. The discussion explores the feasibility of maintaining a well-maintained wiki, the limitations of Reddit's search, and the effectiveness of different learning resources such as YouTube and LLMs.
*   **Emotion:** Mixed, with a tendency towards Negative sentiment. The overall tone is skeptical, focusing on the challenges of creating and maintaining a comprehensive knowledge base.
*   **Top 3 Points of View:**
    *   Maintaining a comprehensive wiki is difficult due to the complexity of the subject and the lack of a large, dedicated community.
    *   Reddit's search functionality is inadequate for finding specific information.
    *   YouTube and LLMs are already effective resources for learning and troubleshooting.

**[Double GPU Bandwidth Question (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1p38pn3/double_gpu_bandwidth_question/)**
*   **Summary:** This is a question about doubling GPU bandwidth. The thread discusses the possibilities of PCIe slot configurations using a splitter or alternative slot locations.
*   **Emotion:** The overall sentiment is Neutral.
*   **Top 2 Points of View:**
    *   One option is to use a PCIe splitter from 16x to 2-8x.
    *   Alternative configurations may not significantly impact performance for specific GPUs.

**[help retaining composition with SDXL artist studies (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1p38m48/help_retaining_composition_with_sdxl_artist/)**
*   **Summary:** A user is asking how to best retain the composition when using SDXL for artist studies. Answers consist of lowering the denoise level or using ControlNet.
*   **Emotion:** The overall sentiment is Positive, although there is Neutral sentiment as well.
*   **Top 3 Points of View:**
    *   ControlNet is generally the best option for maintaining composition.
    *   img2img is an option, however, a lower denoise level should be used.

**[Any recommended models (and lora) for text to pencil sketch for illustrating children's books? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1p32po6/any_recommended_models_and_lora_for_text_to/)**
*   **Summary:** A user seeks models and LoRAs for creating text-to-pencil sketch images for children's books. A commenter offers a specific LoRA recommendation and advice on using Flux with different quantization methods.
*   **Emotion:** The sentiment is largely Neutral, though a bit of positive as the comment offers a solution.
*   **Top 2 Points of View:**
    *   The recommended LoRA is well suited for creating children's book illustrations.
    *   Flux can be used if proper quantization methods are applied.

**[AI Video Generation Comparison - Paid/Free and Local (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1p38742/ai_video_generation_comparison_paidfree_and_local/)**
*   **Summary:** The content involves a comparison of AI video generation options, including paid, free, and local solutions. A commenter notes that local workflow speed may be related to the RTX card being used.
*   **Emotion:** Largely Neutral.
*   **Top 1 Point of View:**
    *   Newer RTX cards are recommended for better performance in local AI video generation workflows.

**[Starting out with AI content creation… what tools should I actually use? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1p36izj/starting_out_with_ai_content_creation_what_tools/)**
*   **Summary:** A user is seeking recommendations for starting with AI content creation. Community members suggest using local open-source tools.
*   **Emotion:** Largely Neutral.
*   **Top 2 Points of View:**
    *   The sub focuses on local and open-source tools.
    *   Using local models is a good way to learn without incurring extra charges.

**[Upscaling video using high resolution "assets" to help with missing details. (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1p38hqo/upscaling_video_using_high_resolution_assets_to/)**
*   **Summary:** The discussion revolves around upscaling video. A commenter shares a method of using Topaz with the Proteus Model to upscale music videos.
*   **Emotion:** Positive
*   **Top 1 Point of View:**
    *   Topaz with the Proteus Model is a good tool for upscaling videos with positive results.

**[Best Ai Short Video Maker App (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1p33js8/best_ai_short_video_maker_app/)**
*   **Summary:** Seeking the best AI short video maker app. Suggesting wan.video and Google video
*   **Emotion:** The overall sentiment is Neutral.
*   **Top 2 Points of View:**
    *   Wan.video is a good option.
    *   Google video can also be used for short videos, but is limited to 8-10 seconds.

**[What are the best diffusion models nowadays for image and video? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1p31li2/what_are_the_best_diffusion_models_nowadays_for/)**
*   **Summary:** The thread is centered on finding the best diffusion models for image and video creation. Some suggestions include using different diffusion models.
*   **Emotion:** Mixed, a touch of Positive, although, the overall sentiment is Neutral.
*   **Top 3 Points of View:**
    *   Qwen image for image generation, Qwen edit for editing, and wan 2.2 for video.
    *   Consider the unique "vibe" of each model.
    *   There are slight, incremental improvements to models.

**[Changing AMD_SERIALIZE_KERNEL (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1p32amb/changing_amd_serialize_kernel/)**
*   **Summary:** Discussion on how to change AMD_SERIALIZE_KERNEL, providing solutions for ComfyUI. The content discusses options for solving the issue.
*   **Emotion:** The overall sentiment is Neutral.
*   **Top 2 Points of View:**
    *   Set `HIP_VISIBLE_DEVICES=1` in the batch file used to launch ComfyUI.
    *   Use `set CUDA_VISIBLE_DEVICES=1` to specify GPU ID.
