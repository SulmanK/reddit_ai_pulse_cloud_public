---
title: "Stable Diffusion Subreddit"
date: "2025-11-10"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["AI", "stablediffusion", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [[D] how was this made?](https://v.redd.it/52xht1osxf0g1) (Score: 186)
    *   Users discuss the methods used to create a particular video, debating whether it was made with AI or other software like Unreal Engine or Unity.

2.  [A video taken with a Seestar, mistaken for AI, hated for being AI when it's not.](https://v.redd.it/kizzz9kt2g0g1) (Score: 103)
    *   A video created with a Seestar telescope is mistaken for AI-generated content, sparking discussion about the challenges artists face in proving their work is authentic.

3.  [Ovi 1.1 is now 10 seconds](https://www.reddit.com/r/StableDiffusion/comments/1otllcy/ovi_11_is_now_10_seconds/) (Score: 68)
    *   Users discuss the features and potential of the new Ovi 1.1 model, including generating sound effects and the possibility of Tupac rapping about something.

4.  [Wan 2.2's still got it! Used it + Qwen Image Edit 2509 exclusively to locally gen on my 4090 all my shots for some client work.](https://v.redd.it/i5gvtyflhh0g1) (Score: 42)
    *   A user shares their experience using Wan 2.2 and Qwen Image Edit 2509 for client work, providing workflow details and expressing satisfaction with the results.

5.  [Is there a way to edit photos inside ComfyUI? like a photoshop node or something](https://i.redd.it/i2hvv9ijnf0g1.png) (Score: 18)
    *   Users seek ways to edit photos within ComfyUI, discussing the use of Photopea, Krita, and various nodes like LayerForge and StarNodes.

6.  [UniLumos: Fast and Unified Image and Video Relighting](https://www.reddit.com/r/StableDiffusion/comments/1ota9tc/unilumos_fast_and_unified_image_and_video/) (Score: 15)
    *   Users discuss UniLumos, a tool for image and video relighting, and when it will be available in ComfyUI.

7.  [The simplest workflow for Qwen-Image-Edit-2509 that simply works](https://www.reddit.com/r/StableDiffusion/comments/1otityx/the_simplest_workflow_for_qwenimageedit2509_that/) (Score: 6)
    *   A discussion about the simplest workflow for Qwen-Image-Edit-2509 and some questions about whether the workflow fixes the pixel offset issue.

8.  [Wan 2.1 Action Motion LoRA Training on 4090.](https://www.reddit.com/r/StableDiffusion/comments/1oti5hn/wan_21_action_motion_lora_training_on_4090/) (Score: 3)
    *   A user is planning to train Wan 2.1 motion lora on a 4090 local machine using musubi tuner

9.  [[Help] Can't succeed to install ReActor requirements.txt for ComfyUI portable (Python 3.13.6) - Error with mesonpy / meson-python](https://www.reddit.com/r/StableDiffusion/comments/1oti8i5/help_cant_succeed_to_install_reactor/) (Score: 2)
    *   A user asks for help with installing ReActor and then provides their own solution.

10. [A question about using AI Toolkit for Training Wan 2.2 LoRas](https://www.reddit.com/r/StableDiffusion/comments/1otlqp4/a_question_about_using_ai_toolkit_for_training/) (Score: 1)
    *   A question about using AI Toolkit for Training Wan 2.2 LoRas.

11. [Save IMG with LORA and the Model name automatically?](https://www.reddit.com/r/StableDiffusion/comments/1otnytl/save_img_with_lora_and_the_model_name/) (Score: 1)
    *   A question about how to save image files with the LORA and the Model name automatically.

12. [how to generate images like this?](https://www.reddit.com/gallery/1otmm2u) (Score: 0)
    *   User is asking how to generate a set of images.

13. [Was this done with Stable Diffusion? If so, which model? And if not, could Stable Diffusion do something like this with SDXL, FLUX, QWEN, etc?](https://www.youtube.com/watch?v=wWZYP5jn5w4) (Score: 0)
    *   User wants to know if a video was made with Stable Diffusion and, if so, which model.

14. [How do you use LLMs to write good prompts for realistic Stable Diffusion images?](https://www.reddit.com/r/StableDiffusion/comments/1otdlzg/how_do_you_use_llms_to_write_good_prompts_for/) (Score: 0)
    *   User is asking how to use LLMs to write good prompts for realistic Stable Diffusion images.

15. [FaceFusion only shows “CPU” under Execution Providers — how to enable GPU (RTX 4070, Windows 11)?](https://www.reddit.com/r/StableDiffusion/comments/1otcegd/facefusion_only_shows_cpu_under_execution/) (Score: 0)
    *   A user wants to know how to enable GPU for FaceFusion.

16. [Reverse Aging](https://www.reddit.com/r/StableDiffusion/comments/1otl0u2/reverse_aging/) (Score: 0)
    *   User wants to know how to reverse aging in images.

17. [Help with image](https://www.reddit.com/gallery/1ote1hq) (Score: 0)
    *   User wants help with their images.

# Detailed Analysis by Thread
**[[D] how was this made? (Score: 186)](https://v.redd.it/52xht1osxf0g1)**
*   **Summary:** Users debate the creation method of a video, suggesting VR, Wan2.2 img-video with specific LoRAs, Unreal Engine/Unity, or a combination of game screenshots and AI video generation. Some users find the video unimpressive.
*   **Emotion:** Predominantly Neutral, with some Positive and Negative sentiments expressed. The overall emotional tone is inquisitive and analytical.
*   **Top 3 Points of View:**
    *   The video was created using VR technology.
    *   The video was created using Wan2.2 img-video with specific camera movement and quality LoRAs and editing software.
    *   The video was created using Unreal Engine or Unity, not AI.

**[A video taken with a Seestar, mistaken for AI, hated for being AI when it's not. (Score: 103)](https://v.redd.it/kizzz9kt2g0g1)**
*   **Summary:** A video taken with a Seestar telescope is mistaken for AI, leading to a discussion about the prevalence of AI skepticism, the challenges artists face in proving their work, and the tendency for people to claim expertise without experience.
*   **Emotion:** Mostly Neutral, some comments express Positive sentiment regarding the video and frustration with AI skepticism.
*   **Top 3 Points of View:**
    *   It's frustrating when genuine work is mistaken for AI due to widespread skepticism.
    *   AI Reddit bots are mistakenly downvoting real content.
    *   It's easy to fake the video with basic compositing software.

**[Ovi 1.1 is now 10 seconds (Score: 68)](https://www.reddit.com/r/StableDiffusion/comments/1otllcy/ovi_11_is_now_10_seconds/)**
*   **Summary:**  Users discuss the Ovi 1.1 model, its new 10-second capability, and features like sound effect generation. Some express impatience for the release of model weights and desire specific formats like GGUF and FP8.
*   **Emotion:** Predominantly Neutral, mixed with some Positive sentiment regarding the new features, and Negative sentiment due to hype and pre-release.
*   **Top 3 Points of View:**
    *   Excitement about the new 10-second video generation capability.
    *   Anticipation for specific model formats (GGUF, FP8).
    *   Frustration with pre-release hype and delayed model weight releases.

**[Wan 2.2's still got it! Used it + Qwen Image Edit 2509 exclusively to locally gen on my 4090 all my shots for some client work. (Score: 42)](https://v.redd.it/i5gvtyflhh0g1)**
*   **Summary:** A user showcases their client work generated using Wan 2.2 and Qwen Image Edit 2509, providing workflow details and links to resources. Other users respond with praise and questions about the process.
*   **Emotion:** Mostly Positive, with users praising the work and expressing interest in the workflow. Some Neutral sentiment related to technical questions.
*   **Top 3 Points of View:**
    *   Wan 2.2 and Qwen Image Edit are effective tools for generating high-quality content.
    *   The user's workflow and resource sharing are helpful for others.
    *   Question about how it is possible to generate in such a high resolution.

**[Is there a way to edit photos inside ComfyUI? like a photoshop node or something (Score: 18)](https://i.redd.it/i2hvv9ijnf0g1.png)**
*   **Summary:**  Users are looking for ways to edit photos directly within ComfyUI. The discussion includes suggestions for using Photopea, Krita with AI diffusion, and various custom nodes.
*   **Emotion:** Mostly Neutral, with users offering suggestions and seeking information. Some Positive sentiment about the usefulness of certain tools.
*   **Top 3 Points of View:**
    *   Photopea can be launched from within ComfyUI.
    *   Wrapping Comfy in Photoshop or using Krita with AI diffusion might be a better approach.
    *   Use LayerForge node in Starnodes for photoshop layer style composition.

**[UniLumos: Fast and Unified Image and Video Relighting (Score: 15)](https://www.reddit.com/r/StableDiffusion/comments/1ota9tc/unilumos_fast_and_unified_image_and_video/)**
*   **Summary:** A discussion about UniLumos: Fast and Unified Image and Video Relighting.
*   **Emotion:** Mostly Positive, with users thanking the poster for the explanation. One user is Negative, calling themself lazy.
*   **Top 3 Points of View:**
    *   UniLumos sounds useful.
    *   User is lazy and doesn't want to keep up with all the AI news.
    *   Question when UniLumos will be available in ComfyUI.

**[The simplest workflow for Qwen-Image-Edit-2509 that simply works (Score: 6)](https://www.reddit.com/r/StableDiffusion/comments/1otityx/the_simplest_workflow_for_qwenimageedit2509_that/)**
*   **Summary:** A discussion about the simplest workflow for Qwen-Image-Edit-2509 and some questions about whether the workflow fixes the pixel offset issue.
*   **Emotion:** Mostly Neutral with one comment expressing Negative sentiment due to prompt comprehension.
*   **Top 3 Points of View:**
    *   Prompt comprehension and following is abysmal
    *   An image resize node can be used to get a 2-megapixel result and compare it with the resized original in the image comparer node.
    *   Removed the resize node to get crazy results.

**[Wan 2.1 Action Motion LoRA Training on 4090. (Score: 3)](https://www.reddit.com/r/StableDiffusion/comments/1oti5hn/wan_21_action_motion_lora_training_on_4090/)**
*   **Summary:** A question is asked related to Wan 2.1 Action Motion LoRA Training on 4090.
*   **Emotion:** Mostly Neutral.
*   **Top 3 Points of View:**
    *   User is planning to train Wan 2.1 motion lora on a 4090 local machine using musubi tuner.

**[[Help] Can't succeed to install ReActor requirements.txt for ComfyUI portable (Python 3.13.6) - Error with mesonpy / meson-python (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1oti8i5/help_cant_succeed_to_install_reactor/)**
*   **Summary:** User had a problem installing ReActor, but found a tutorial that helped him.
*   **Emotion:** Mostly Positive.
*   **Top 3 Points of View:**
    *   User had a problem installing ReActor, but found a tutorial that helped him.

**[A question about using AI Toolkit for Training Wan 2.2 LoRas (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1otlqp4/a_question_about_using_ai_toolkit_for_training/)**
*   **Summary:** A question about using AI Toolkit for Training Wan 2.2 LoRas.
*   **Emotion:** Mostly Positive and Neutral
*   **Top 3 Points of View:**
    *   User has created two wan 2.2 Loras from just static images and it's worked very well so far.
    *   For camera movements, use videos and for characters you can use both.

**[Save IMG with LORA and the Model name automatically? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1otnytl/save_img_with_lora_and_the_model_name/)**
*   **Summary:** A question about how to save image files with the LORA and the Model name automatically.
*   **Emotion:** Mostly Neutral
*   **Top 3 Points of View:**
    *   This is easiest done outside of generation, IMHO. Just have some AI write you a script that extracts metadata and uses it to rename.
    *   You can connect other nodes to filename prefix in Save Image node.
**[how to generate images like this? (Score: 0)](https://www.reddit.com/gallery/1otmm2u)**
*   **Summary:** User is asking how to generate a set of images.
*   **Emotion:** Mostly Neutral, with some Negative and Positive.
*   **Top 3 Points of View:**
    *   Using open source models like Qwen or Wan
    *   You'd have to run your own ComfyAI or Llama for copyrighted material.
    *   Start with a prompt describing what you want to see. You can feed the image into chatgpt/gemini and ask the model to describe it in detail.

**[Was this done with Stable Diffusion? If so, which model? And if not, could Stable Diffusion do something like this with SDXL, FLUX, QWEN, etc? (Score: 0)](https://www.youtube.com/watch?v=wWZYP5jn5w4)**
*   **Summary:** User wants to know if a video was made with Stable Diffusion and, if so, which model.
*   **Emotion:** Mostly Neutral, with some Positive.
*   **Top 3 Points of View:**
    *   Stable Diffusion makes images, not videos.
    *   All in ComfyUI. Qwen Image and Qwen image edit. Wan 2.2 for Image-to-video generations. Suno for music. Premier and/or After Effects for cuts, edits and syncs.
    *   After Effects was used to sync things with the music in this Grok Imagine video.

**[How do you use LLMs to write good prompts for realistic Stable Diffusion images? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1otdlzg/how_do_you_use_llms_to_write_good_prompts_for/)**
*   **Summary:** User is asking how to use LLMs to write good prompts for realistic Stable Diffusion images.
*   **Emotion:** Mostly Neutral, with some Positive.
*   **Top 3 Points of View:**
    *   Don't. Go look at images you like, and see what terms/tags/phrases they are using. Experiment by taking their prompts and tweaking it.
    *   Gemma 3 27B IQ3_XXS model is used to recreate the image of a hooded figure standing before a gnarled, ancient tree with glowing red eyes, a silver sword with intricate hilt, a white wolf perched on a branch of the tree
    *   Accurate system prompt with RAG for reference

**[FaceFusion only shows “CPU” under Execution Providers — how to enable GPU (RTX 4070, Windows 11)? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1otcegd/facefusion_only_shows_cpu_under_execution/)**
*   **Summary:** A user wants to know how to enable GPU for FaceFusion.
*   **Emotion:** Mostly Neutral.
*   **Top 3 Points of View:**
    *   I got around it by just using the tensorrtexecutionprovider instead.
    *   For Cuda 12.x:cd <your_cloned_facefusion> -rf venv, python3.12 -m venv venv and pip install "numpy <2.3" and run facefusion.py.
    *   Have you checked this command inside of your venv :nvcc --version

**[Reverse Aging (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1otl0u2/reverse_aging/)**
*   **Summary:** User wants to know how to reverse aging in images.
*   **Emotion:** Mostly Neutral and Positive
*   **Top 3 Points of View:**
    *   For the first part of the task, you could use any web service that supports Kontext or Qwen Edit.
    *   There are a lot of ways to make a time lapse video of aging.
    *   Consider hiring a professional.

**[Help with image (Score: 0)](https://www.reddit.com/gallery/1ote1hq)**
*   **Summary:** User wants help with their images.
*   **Emotion:** Mostly Neutral and Positive
*   **Top 3 Points of View:**
    *   There are many common problems with Lora training. Bad prompts, bad checkpoint and incompatible Lora can cause problems.
    *   Qwen Image Edit took "Italian" too literally. He used parts of your post as a prompt.
    *   Try various token weights on the orc features until you get what you want? like (token:0.5)
