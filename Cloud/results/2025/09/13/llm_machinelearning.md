---
title: "Machine Learning Subreddit"
date: "2025-09-13"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "papers"]
---

# Overall Ranking and Top Discussions
1.  [[R] New "Illusion" Paper Just Dropped For Long Horizon Agents](https://www.reddit.com/r/MachineLearning/comments/1nfrpvz/r_new_illusion_paper_just_dropped_for_long/) (Score: 29)
    *   The thread discusses a new paper about AI progress and long horizon agents, specifically addressing the illusion that AI progress is slowing down.
2.  [[R] Debunking the Claims of K2-Think](https://www.reddit.com/r/MachineLearning/comments/1nfestz/r_debunking_the_claims_of_k2think/) (Score: 25)
    *   The thread critiques the claims of K2-Think, focusing on its low performance score on the HLE benchmark.
3.  [[D] which papers HAVEN'T stood the test of time?](https://www.reddit.com/r/MachineLearning/comments/1ng6dsf/d_which_papers_havent_stood_the_test_of_time/) (Score: 7)
    *   The thread discusses machine learning papers that haven't aged well, with one user suggesting the "Emergent Abilities of Large Language Models" paper as a candidate.
4.  [[D] AAAI 26 Main Track](https://www.reddit.com/r/MachineLearning/comments/1ng51zx/d_aaai_26_main_track/) (Score: 1)
    *   The thread discusses the availability of the AAAI 26 Main Track papers and the large number of submissions received this year.
5.  [[D] OOM When Using Gradient Accumulation](https://www.reddit.com/r/MachineLearning/comments/1nfgc8h/d_oom_when_using_gradient_accumulation/) (Score: 0)
    *   The thread discusses a memory issue (OOM) encountered when using gradient accumulation in machine learning, with suggestions to reduce precision or offload to RAM.
6.  [[R] A Framework for Entropic Generative Systems: Mapping Cosmic Principles to Novel Creation in AI](https://www.reddit.com/r/MachineLearning/comments/1nfpgoh/r_a_framework_for_entropic_generative_systems/) (Score: 0)
    *   The thread discusses a framework for entropic generative systems, linking cosmic principles to AI creation and requesting a Github link for the autonomous engine.
7.  [[P] Training an ML model to detect fake product reviews](https://www.reddit.com/r/MachineLearning/comments/1nfpusc/p_training_an_ml_model_to_detect_fake_product/) (Score: 0)
    *   The thread discusses training a machine learning model to detect fake product reviews.
8.  [[D] Why does nobody talk about the “energy per token” cost of AI?](https://www.reddit.com/r/MachineLearning/comments/1ng6jb9/d_why_does_nobody_talk_about_the_energy_per_token/) (Score: 0)
    *   The thread discusses the energy consumption of AI per token.

# Detailed Analysis by Thread
**[[R] New "Illusion" Paper Just Dropped For Long Horizon Agents (Score: 29)](https://www.reddit.com/r/MachineLearning/comments/1nfrpvz/r_new_illusion_paper_just_dropped_for_long/)**
*  **Summary:**  The thread discusses a new paper about AI progress and long horizon agents, specifically addressing the illusion that AI progress is slowing down.
*  **Emotion:** The overall emotional tone is neutral, with users primarily sharing information and observations.
*  **Top 3 Points of View:**
    *   AI progress is not slowing down, despite mainstream media claims.
    *   Recent advancements, particularly in "thinking" models, have enabled longer context processing and improved task completion.
    *   Some users are interested in seeing a wider variety of tasks applied to these models beyond dictionary retrieval and counting.

**[[R] Debunking the Claims of K2-Think (Score: 25)](https://www.reddit.com/r/MachineLearning/comments/1nfestz/r_debunking_the_claims_of_k2think/)**
*  **Summary:**  The thread critiques the claims of K2-Think, focusing on its low performance score on the HLE benchmark.
*  **Emotion:** The thread has a neutral emotional tone.
*  **Top 3 Points of View:**
    *   K2-Think's HLE score of 9.95% indicates very low performance.
    *   The HLE benchmark includes multiple-choice questions, meaning random guessing would yield a higher score.
    *   The thread references an external table of model performances as evidence.

**[[D] which papers HAVEN'T stood the test of time? (Score: 7)](https://www.reddit.com/r/MachineLearning/comments/1ng6dsf/d_which_papers_havent_stood_the_test_of_time/)**
*  **Summary:**  The thread discusses machine learning papers that haven't aged well, with one user suggesting the "Emergent Abilities of Large Language Models" paper as a candidate.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   The paper "Emergent Abilities of Large Language Models" is suggested as a paper that hasn't stood the test of time.
    *   A subsequent paper disputed the findings of the first paper, winning an award.
    *   HRM is also suggested as a paper that didn't stand the test of time.

**[[D] AAAI 26 Main Track (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1ng51zx/d_aaai_26_main_track/)**
*  **Summary:**  The thread discusses the availability of the AAAI 26 Main Track papers and the large number of submissions received this year.
*  **Emotion:** The overall sentiment is neutral.
*  **Top 3 Points of View:**
    *   AAAI 26 Main Track papers are now available.
    *   The submission numbers this year were exceptionally high (30k).
    *   The AAAI 26 Main Track information can be found on the website.

**[[D] OOM When Using Gradient Accumulation (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1nfgc8h/d_oom_when_using_gradient_accumulation/)**
*  **Summary:**  The thread discusses a memory issue (OOM) encountered when using gradient accumulation in machine learning, with suggestions to reduce precision or offload to RAM.
*  **Emotion:** The thread has a neutral tone.
*  **Top 3 Points of View:**
    *   The user is running out of memory (OOM) when using gradient accumulation.
    *   Suggestions include reducing precision or offloading to RAM.
    *   Without code and hardware details, further speculation is difficult.

**[[R] A Framework for Entropic Generative Systems: Mapping Cosmic Principles to Novel Creation in AI (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1nfpgoh/r_a_framework_for_entropic_generative_systems/)**
*  **Summary:**  The thread discusses a framework for entropic generative systems, linking cosmic principles to AI creation and requesting a Github link for the autonomous engine.
*  **Emotion:** The sentiment is generally neutral.
*  **Top 3 Points of View:**
    *   Suggests posting to r/llmphysics
    *   Asks for the Github link for the autonomous engine

**[[P] Training an ML model to detect fake product reviews (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1nfpusc/p_training_an_ml_model_to_detect_fake_product/)**
*  **Summary:**  The thread discusses training a machine learning model to detect fake product reviews.
*  **Emotion:** The emotional tone of the thread is slightly positive.
*  **Top 3 Points of View:**
    *   A user suggests using an LLM for the task, arguing it would be more effective with minimal effort.

**[[D] Why does nobody talk about the “energy per token” cost of AI? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ng6jb9/d_why_does_nobody_talk_about_the_energy_per_token/)**
*  **Summary:**  The thread discusses the energy consumption of AI per token.
*  **Emotion:** The overall sentiment is neutral.
*  **Top 3 Points of View:**
    *   The "energy per token" cost is not a simple metric to analyze due to various factors such as hardware, batch size, and latency tradeoffs.
    *   Some argue that the energy cost is too small to be worth discussing, or that it is already priced in.
    *   Others say that they discuss the topic.
