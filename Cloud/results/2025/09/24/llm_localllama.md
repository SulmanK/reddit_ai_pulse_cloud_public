---
title: "LocalLLaMA Subreddit"
date: "2025-09-24"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "Local", "AI"]
---

# Overall Ranking and Top Discussions
1.  [New Agent benchmark from Meta Super Intelligence Lab and Hugging Face](https://i.redd.it/fjardl7x45rf1.png) (Score: 77)
    *   This thread discusses a new agent benchmark from Meta Super Intelligence Lab and Hugging Face, with users questioning the absence of certain models like Deepseek and GLM in the evaluation.

2.  [Chinese modified 3080 20GB performance..](https://www.reddit.com/gallery/1npfnvw) (Score: 73)
    *   This thread presents the performance of a Chinese modified 3080 20GB, with discussions about its benchmark scores and comparisons to other GPUs.

3.  [China's latest GPU arrives with claims of CUDA compatibility and RT support — Fenghua No.3 also boasts 112GB+ of HBM memory for AI](https://www.tomshardware.com/pc-components/gpus/chinas-latest-gpu-arrives-with-claims-of-cuda-compatibility-and-rt-support-fenghua-no-3-also-boasts-112gb-of-hbm-memory-for-ai) (Score: 68)
    *   This thread focuses on China's new Fenghua No.3 GPU, which claims CUDA compatibility and Ray Tracing support, with users questioning the validity of these claims and discussing potential US bans.

4.  [Qwen3-30B-A3B for role-playing](https://www.reddit.com/r/LocalLLaMA/comments/1nphn86/qwen330ba3b_for_roleplaying/) (Score: 11)
    *   This thread touches on role-playing with Qwen3-30B-A3B.

5.  [How do you know which contributors’ quantisation to trust on huggingface?](https://www.reddit.com/r/LocalLLaMA/comments/1npdnok/how_do_you_know_which_contributors_quantisation/) (Score: 7)
    *   This thread discusses how to determine which contributors' quantizations on Hugging Face are trustworthy, considering both quality and security aspects.

6.  [Which quantizations are you using?](https://www.reddit.com/r/LocalLLaMA/comments/1npcj8a/which_quantizations_are_you_using/) (Score: 6)
    *   This thread revolves around different quantization techniques used in LLMs and their performance, with users sharing their experiences and preferences.

7.  [oom using ik_llama with iq_k quants](https://www.reddit.com/r/LocalLLaMA/comments/1nphifl/oom_using_ik_llama_with_iq_k_quants/) (Score: 4)
    *   This thread discusses an out-of-memory error while using ik_llama with iq_k quants.

8.  [Any good resources to learn llama.cpp tool and its parameters and settings?](https://www.reddit.com/r/LocalLLaMA/comments/1npi8lt/any_good_resources_to_learn_llamacpp_tool_and_its/) (Score: 4)
    *   This thread seeks resources for learning about the llama.cpp tool, its parameters, and settings, with users suggesting documentation and tools.

9.  [Anyone tried Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010 on HF ?](https://www.reddit.com/r/LocalLLaMA/comments/1npky3c/anyone_tried/) (Score: 4)
    *   This thread asks if anyone has tried Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010 on Hugging Face, with users sharing their experiences and comparing it to other models.

10. [Does anybody know how to configure maximum context length or input tokens in litellm?](https://www.reddit.com/r/LocalLLaMA/comments/1npe2uq/does_anybody_know_how_to_configure_maximum/) (Score: 3)
    *   This thread discusses how to configure maximum context length or input tokens in litellm.

11. [suggestions for AI workstation](https://www.reddit.com/r/LocalLLaMA/comments/1npixnz/suggestions_for_ai_workstation/) (Score: 2)
    *   This thread revolves around suggestions for building an AI workstation, discussing CPU and GPU options, and DIY vs. vendor-built machines.

12. [Why is my DeepSeek like this?](https://i.redd.it/y2gay7ea36rf1.png) (Score: 0)
    *   The user's question is deemed to be for the wrong subreddit.

13. [Mix of feelings](https://www.reddit.com/r/LocalLLaMA/comments/1nphvx3/mix_of_feelings/) (Score: 0)
    *   This thread is about the user's experience of feelings in regards to using GLM 4.5.

14. [Talk me out of it.. provide me better choices.](https://www.reddit.com/r/LocalLLaMA/comments/1npj8ap/talk_me_out_of_it_provide_me_better_choices/) (Score: 0)
    *   This thread is about the user trying to talk themselves out of buying an AI max and asking for better choices.

15. [Is there a way to upload LLMs to cloud servers with better GPUs and run them locally?](https://www.reddit.com/r/LocalLLaMA/comments/1npk517/is_there_a_way_to_upload_llms_to_cloud_servers/) (Score: 0)
    *   This thread is about the user attempting to upload LLMs to cloud servers with better GPUs and run them locally.

16. [Prototype Cognitive Engine Using E8 Lattice, Wave Dynamics, and Bandit-VAE Compression](https://www.reddit.com/r/LocalLLaMA/comments/1npk7hj/prototype_cognitive_engine_using_e8_lattice_wave/) (Score: 0)
    *   This thread is about a Cognitive Engine using E8 Lattice.

17. [Detecting hallucination from the hidden space of an LLM](https://www.reddit.com/r/LocalLLaMA/comments/1npkf9d/detecting_hallucination_from_the_hidden_space_of/) (Score: 0)
    *   This thread is about detecting hallucination from the hidden space of an LLM.

# Detailed Analysis by Thread

**[[New Agent benchmark from Meta Super Intelligence Lab and Hugging Face](https://i.redd.it/fjardl7x45rf1.png) (Score: 77)](https://i.redd.it/fjardl7x45rf1.png)**
*  **Summary:**  The thread discusses a new agent benchmark from Meta Super Intelligence Lab and Hugging Face. Users express curiosity and skepticism about the absence of certain models like Deepseek and GLM in the evaluation. Some users also found the benchmark useful and interesting.
*  **Emotion:** The overall emotional tone is neutral, with curiosity and mild skepticism dominating the discussion.
*  **Top 3 Points of View:**
    *   Questioning the absence of Deepseek and GLM models in the benchmark.
    *   Finding the benchmark useful for assessing LLMs.
    *   Highlighting LLMs' struggles with understanding their relation to time and the interesting agent2agent metric.

**[[Chinese modified 3080 20GB performance..](https://www.reddit.com/gallery/1npfnvw) (Score: 73)](https://www.reddit.com/gallery/1npfnvw)**
*  **Summary:**  The thread showcases the performance of a Chinese modified 3080 20GB GPU. Users discuss the benchmark results, compare it to the 3080Ti, and express interest in AI benchmarks for the card.
*  **Emotion:** The overall emotional tone is mixed, with neutral observations and a hint of positive sentiment regarding AI benchmarks.
*  **Top 3 Points of View:**
    *   Comparing the modified 3080's performance to the 3080Ti.
    *   Expressing interest in AI benchmarks for the card.
    *   Questioning the noise level of the modified card.

**[China's latest GPU arrives with claims of CUDA compatibility and RT support — Fenghua No.3 also boasts 112GB+ of HBM memory for AI (Score: 68)](https://www.tomshardware.com/pc-components/gpus/chinas-latest-gpu-arrives-with-claims-of-cuda-compatibility-and-rt-support-fenghua-no-3-also-boasts-112gb-of-hbm-memory-for-ai)**
*  **Summary:**  The thread discusses China's new Fenghua No.3 GPU, which claims CUDA compatibility and Ray Tracing support. Users are skeptical about these claims and speculate about potential US bans.
*  **Emotion:** The overall emotional tone is neutral, with a hint of skepticism and anticipation regarding potential regulations.
*  **Top 3 Points of View:**
    *   Questioning the validity of CUDA compatibility and Ray Tracing support claims.
    *   Speculating about potential US bans on the GPU.
    *   Expressing interest in the price, warranty, and availability of the GPU.

**[[Qwen3-30B-A3B for role-playing](https://www.reddit.com/r/LocalLLaMA/comments/1nphn86/qwen330ba3b_for_roleplaying/) (Score: 11)](https://www.reddit.com/r/LocalLLaMA/comments/1nphn86/qwen330ba3b_for_roleplaying/)**
*  **Summary:**  The thread is focused on Qwen3-30B-A3B for role-playing and that A3B is not tight
*  **Emotion:** Negative
*  **Top 3 Points of View:**
    *   The user is discussing if the A3B model is "tight"

**[How do you know which contributors’ quantisation to trust on huggingface? (Score: 7)](https://www.reddit.com/r/LocalLLaMA/comments/1npdnok/how_do_you_know_which_contributors_quantisation/)**
*  **Summary:**  The thread discusses methods for identifying trustworthy contributors of quantized models on Hugging Face, emphasizing factors like reputation, security risks, and potential malicious behavior.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Trusting quants from known contributors like unsloth and bartowski.
    *   Checking contributors' GitHub repos for detailed info on their work and expertise.
    *   Being aware of potential security risks, such as bad actors fine-tuning models for malicious behavior.

**[Which quantizations are you using? (Score: 6)](https://www.reddit.com/r/LocalLLaMA/comments/1npcj8a/which_quantizations_are_you_using/)**
*  **Summary:**  Users discuss their preferred quantization techniques for LLMs, sharing experiences with different formats like MLX BF16, Q8, Q6, AWQ, and GGUF, and detailing their specific use cases and hardware configurations.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Using MLX BF16 for Qwen3 30B A3B variations on M4 Max 128GB MBP16.
    *   Hesitancy to go below 8bit quantization, considering it the "sweet spot."
    *   Preference for AWQ with a MARLIN kernel for its speed and performance.

**[oom using ik_llama with iq_k quants (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1nphifl/oom_using_ik_llama_with_iq_k_quants/)**
*  **Summary:**  The discussion revolves around an out-of-memory (OOM) error encountered while using ik_llama with iq_k quants, with potential causes being the model loading process or VRAM overhead from flash attention.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Identifying whether the OOM occurs before or after the model is loaded.
    *   Considering flash attention's VRAM overhead as a potential cause.
    *   Suggesting building flash attention with `-DGGML_SCHED_MAX_COPIES=1` to reduce VRAM usage.

**[Any good resources to learn llama.cpp tool and its parameters and settings? (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1npi8lt/any_good_resources_to_learn_llamacpp_tool_and_its/)**
*  **Summary:**  Users are asking about the different tools available for llama.cpp and what the best ones are to use.
*  **Emotion:** Positive
*  **Top 3 Points of View:**
    *   Using the actual repo for the llama.cpp
    *   Looking at this one persons app called Llama-OS
    *   Waiting on someones custom tool to be released

**[Anyone tried Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010 on HF ? (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1npky3c/anyone_tried/)**
*  **Summary:**  Users are asking if anyone has used the  Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010 on HF.
*  **Emotion:** Positive
*  **Top 3 Points of View:**
    *   They did not like the Huihui Abliterated
    *   They enjoyed the Huihui Abliterated and it was fun
    *   The user likes Gemma 27B

**[Does anybody know how to configure maximum context length or input tokens in litellm? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1npe2uq/does_anybody_know_how_to_configure_maximum/)**
*  **Summary:**  litellm is a client library, while maximum context length is enforced by the server (e.g. in llama.cpp you set \`./llama-server -c 32768\`)
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   The way to configure is through the server

**[suggestions for AI workstation (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1npixnz/suggestions_for_ai_workstation/)**
*  **Summary:**  The user needs suggestions for an AI workstation.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Copy that guys build for 4k then get a 6000 Blackwell for 9k, then sell your current rig for ~3k
    *   The price premium for vendor + new parts is probably close to 100% compared to diy/used parts.

**[Why is my DeepSeek like this? (Score: 0)](https://i.redd.it/y2gay7ea36rf1.png)**
*  **Summary:**  Wrong sub
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Wrong sub

**[Mix of feelings (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1nphvx3/mix_of_feelings/)**
*  **Summary:**  The user is discussing feelings in regards to using GLM 4.5.
*  **Emotion:** Positive
*  **Top 3 Points of View:**
    *   GLM 4.5 and GLM 4.5 Air on Mac Studio with 256GB ram and pretty happy with the setup
    *   Suggesting getting 768 GB, the you will be able to run K2 (IQ4 quant has size of 555 GB
    *   Realistically it's less a question of what you can run and more a matter of how fast you can run it and at what quant.

**[Talk me out of it.. provide me better choices. (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1npj8ap/talk_me_out_of_it_provide_me_better_choices/)**
*  **Summary:**  The user is trying to talk themselves out of buying an AI max and asking for better choices.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Three paths. You buy a big GPU and fit everything in VRAM. Really fast work. Or, you go for CPU inference and shitload of ram for giant models (120b and above).
    *   The Ai max has nearly 1.1tbps? What?
    *   What is "this"?  If you mean the AI Max, it's not even close... it has about 1/4 the speed of the 4090's memory

**[Is there a way to upload LLMs to cloud servers with better GPUs and run them locally? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1npk517/is_there_a_way_to_upload_llms_to_cloud_servers/)**
*  **Summary:**  The user is asking about uploading LLMs to cloud servers with better GPUs and run them locally.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   If you are running it on a cloud server, then you are not running it locally.
    *   Locally = your own hardware, on your own premises. Cloud server = not your own hardware, not on your own premises, not local.
    *   You should look at a service like Runpod.

**[Prototype Cognitive Engine Using E8 Lattice, Wave Dynamics, and Bandit-VAE Compression (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1npk7hj/prototype_cognitive_engine_using_e8_lattice_wave/)**
*  **Summary:**  The thread is about a Cognitive Engine using E8 Lattice.
*  **Emotion:** Negative
*  **Top 3 Points of View:**
    *   I hate these AI psychosis posts so much
    *   When I see someone use the word "semantic" I know the SNR is going to be close to 0
    *   Wow, mass conservation is guaranteed for a piece of software! Needs more technobabble.

**[Detecting hallucination from the hidden space of an LLM (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1npkf9d/detecting_hallucination_from_the_hidden_space_of/)**
*  **Summary:**  The thread is about detecting hallucination from the hidden space of an LLM.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Your cosine score is not a proxy for factuality. It's a familiarity meter, not a hallucination detector.
