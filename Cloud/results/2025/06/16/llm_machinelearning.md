---
title: "Machine Learning Subreddit"
date: "2025-06-16"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "NLP"]
---

# Overall Ranking and Top Discussions
1.  [[D] ML Research: Industry vs Academia](https://www.reddit.com/r/MachineLearning/comments/1lcfd1d/ml_research_industry_vs_academia_d/) (Score: 85)
    *  This thread discusses the differences between conducting machine learning research in industry versus in academia.
2.  [I'm not obsolete, am I? [P]](https://www.reddit.com/r/MachineLearning/comments/1lcrsly/im_not_obsolete_am_i_p/) (Score: 74)
    *  The thread is about whether old school computer vision approaches are becoming obsolete with the rise of newer machine learning models.
3.  [[R] Vision Transformers Don't Need Trained Registers](https://www.reddit.com/r/MachineLearning/comments/1lcja93/r_vision_transformers_dont_need_trained_registers/) (Score: 47)
    *  This thread discusses a research paper about vision transformers not needing trained registers for certain tasks.
4.  [[P] Research Scientists + Engineers for Generative AI at NVIDIA](https://www.reddit.com/r/MachineLearning/comments/1lcmxeb/p_research_scientists_engineers_for_generative_ai/) (Score: 36)
    *  This thread is a job posting for research scientists and engineers at NVIDIA, specifically for generative AI roles.
5.  [[Q], [D]: What tools do you use to create informative, visually appealing and above all clear figures for your papers?](https://www.reddit.com/r/MachineLearning/comments/1lcuoah/q_d_what_tools_do_you_use_to_create_informative/) (Score: 23)
    *  The thread asks about the tools people use to create informative and visually appealing figures for their research papers.
6.  [[D] HighNoon LLM: Exploring Hierarchical Memory for Efficient NLP](https://www.reddit.com/r/MachineLearning/comments/1lcjjd2/d_highnoon_llm_exploring_hierarchical_memory_for/) (Score: 15)
    *  This thread discusses a new LLM architecture called HighNoon LLM, which explores hierarchical memory for more efficient natural language processing.
7.  [[R] Which of A star AI ML conferences allow virtual presentation upon acceptance?](https://www.reddit.com/r/MachineLearning/comments/1lcu047/r_which_of_a_star_ai_ml_conferences_allow_virtual/) (Score: 6)
    *  This thread asks which top-tier AI/ML conferences allow virtual presentations after a paper has been accepted.
8.  [[R] Struggling to Define Novelty in My AI Master’s Thesis](https://www.reddit.com/r/MachineLearning/comments/1lcwnf4/r_struggling_to_define_novelty_in_my_ai_masters/) (Score: 5)
    *  The thread is about the difficulty of defining novelty in an AI master's thesis.
9.  [[D] Time series Transformers- Autogressive or all at once?](https://www.reddit.com/r/MachineLearning/comments/1lcqcd6/d_time_series_transformers_autogressive_or_all_at/) (Score: 2)
    *  This thread seems to be discussing the use of transformers for time series forecasting, specifically comparing autoregressive and all-at-once approaches.
10. [[P] Solving SlimeVolley with NEAT](https://www.reddit.com/r/MachineLearning/comments/1lcldz9/p_solving_slimevolley_with_neat/) (Score: 1)
    *  The thread discusses the challenges of using NEAT (NeuroEvolution of Augmenting Topologies) to solve the SlimeVolley environment.
11. [[P] spy search a llm search engine](https://i.redd.it/vwc5bcmsp77f1.jpeg) (Score: 0)
    *  The thread presents a new LLM search engine called "spy search".
12. [[R] The Illusion of "The Illusion of Thinking"](https://www.reddit.com/r/MachineLearning/comments/1ld0evr/r_the_illusion_of_the_illusion_of_thinking/) (Score: 0)
    *  This thread discusses and critiques a paper titled "The Illusion of Thinking".

# Detailed Analysis by Thread
**[[D] ML Research: Industry vs Academia (Score: 85)](https://www.reddit.com/r/MachineLearning/comments/1lcfd1d/ml_research_industry_vs_academia_d/)**
*  **Summary:** This thread explores the differences between conducting machine learning research in industry and academia, with discussions on theoretical vs. applied research, resource availability, and career expectations.
*  **Emotion:** The overall emotional tone of the thread is neutral, with a mix of objective observations and personal experiences. A few comments express a positive sentiment about the benefits of industry research.
*  **Top 3 Points of View:**
    *   Industry research is often more product-focused and less theoretical than academic research.
    *   Academia requires researchers to be leaders and bring in funding, while industry offers more staff positions.
    *   Some believe that academic research is often corrupt and does not hold up to real-world testing, while others think that large companies engage in "product engineering style alchemy" disguised as scientific research.

**[I'm not obsolete, am I? [P] (Score: 74)](https://www.reddit.com/r/MachineLearning/comments/1lcrsly/im_not_obsolete_am_i_p/)**
*  **Summary:** The thread discusses whether older computer vision techniques, specifically CNNs, are becoming obsolete with the advent of more modern machine learning models, particularly in the context of a chicken breed classification bot.
*  **Emotion:** The overall emotional tone of the thread is positive and supportive, with people encouraging the original poster and highlighting the continued relevance of older methods.
*  **Top 3 Points of View:**
    *   If it works well and efficiently, older methods are still valuable.
    *   Modern methods like multimodal LLMs might be overkill for certain tasks.
    *   The value of a model depends on its performance and the resources it requires.

**[[R] Vision Transformers Don't Need Trained Registers (Score: 47)](https://www.reddit.com/r/MachineLearning/comments/1lcja93/r_vision_transformers_dont_need_trained_registers/)**
*  **Summary:** The thread discusses a research paper about vision transformers that demonstrates that trained registers are not needed. The emergent segmentation properties is similar to “white box transformers”.
*  **Emotion:** The overall emotional tone of the thread is neutral and curious, with users asking questions and sharing their related experiences.
*  **Top 3 Points of View:**
    *   The paper is interesting and presents a neat trick for improving attention in vision transformers.
    *   The new approach may improve segmentation but not classification.
    *   The concept of registers in ViTs might exist in other transformer-based models.

**[[P] Research Scientists + Engineers for Generative AI at NVIDIA (Score: 36)](https://www.reddit.com/r/MachineLearning/comments/1lcmxeb/p_research_scientists_engineers_for_generative_ai/)**
*  **Summary:** This thread is a job posting for research scientists and engineers at NVIDIA, specifically for generative AI roles. Most comments revolve around asking if there are openings for more junior roles such as internships.
*  **Emotion:** The overall emotional tone of the thread is neutral, with users inquiring about job opportunities.
*  **Top 3 Points of View:**
    *   There is interest in junior roles, such as internships.
    *   Some users are wondering if positions are eligible for MS or PhD graduates.
    *   Some think that the positions aren't truly research scientist jobs or even MLE jobs.

**[[Q], [D]: What tools do you use to create informative, visually appealing and above all clear figures for your papers? (Score: 23)](https://www.reddit.com/r/MachineLearning/comments/1lcuoah/q_d_what_tools_do_you_use_to_create_informative/)**
*  **Summary:** The thread is a question asking for recommendations on tools to create clear and informative figures for research papers.
*  **Emotion:** The overall emotional tone of the thread is neutral, simply asking a question and getting different responses.
*  **Top 3 Points of View:**
    *   Vector graphic editors like Illustrator or Inkscape.
    *   Using a tikz environment.
    *   PlotNeuralNet tool to create neural network diagrams.

**[[D] HighNoon LLM: Exploring Hierarchical Memory for Efficient NLP (Score: 15)](https://www.reddit.com/r/MachineLearning/comments/1lcjjd2/d_highnoon_llm_exploring_hierarchical_memory_for/)**
*  **Summary:** This thread discusses a new LLM architecture called HighNoon LLM, which explores hierarchical memory for more efficient natural language processing.
*  **Emotion:** The overall emotional tone of the thread is positive and curious, with users expressing interest in the architecture and asking questions about its implementation and performance.
*  **Top 3 Points of View:**
    *   The hierarchical memory structure is an elegant approach to modeling language.
    *   There are some concerns regarding the fixed size chunks for nested language structures.
    *   Local inference is a hugely underrated benefit to this architecture.

**[[R] Which of A star AI ML conferences allow virtual presentation upon acceptance? (Score: 6)](https://www.reddit.com/r/MachineLearning/comments/1lcu047/r_which_of_a_star_ai_ml_conferences_allow_virtual/)**
*  **Summary:** This thread asks which top-tier AI/ML conferences allow virtual presentations after a paper has been accepted.
*  **Emotion:** The overall emotional tone of the thread is neutral, with users providing information and expressing opinions.
*  **Top 3 Points of View:**
    *   Virtual presentations are generally allowed if physical attendance is not possible due to visa issues.
    *   Most conferences no longer accept remote presentations if the presenter simply doesn't want to pay for travel.
    *   If you don't want to present in person, submit to a journal.

**[[R] Struggling to Define Novelty in My AI Master’s Thesis (Score: 5)](https://www.reddit.com/r/MachineLearning/comments/1lcwnf4/r_struggling_to_define_novelty_in_my_ai_masters/)**
*  **Summary:** The thread is about the difficulty of defining novelty in an AI master's thesis.
*  **Emotion:** The overall emotional tone of the thread is neutral, with users offering advice and sharing experiences.
*  **Top 3 Points of View:**
    *   A master's thesis may require more 'significant engineering effort' than actual novelty.
    *   Submit the thesis for review and let the reviewers provide a novelty check.
    *   Applying existing approaches in a novel and unique way is sufficient.

**[[D] Time series Transformers- Autogressive or all at once? (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1lcqcd6/d_time_series_transformers_autogressive_or_all_at/)**
*  **Summary:** This thread seems to be discussing the use of transformers for time series forecasting, specifically comparing autoregressive and all-at-once approaches. One user is happy using amazon/chronos.
*  **Emotion:** The overall emotional tone of the thread is positive.
*  **Top 1 Point of View:**
    *   One user is happy with amazon/chronos.

**[[P] Solving SlimeVolley with NEAT (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1lcldz9/p_solving_slimevolley_with_neat/)**
*  **Summary:** The thread discusses the challenges of using NEAT (NeuroEvolution of Augmenting Topologies) to solve the SlimeVolley environment.
*  **Emotion:** The overall emotional tone of the thread is neutral and helpful, with suggestions for improving the NEAT implementation.
*  **Top 3 Points of View:**
    *   SlimeVolley's sparse rewards are brutal for NEAT, especially feedforward NEAT.
    *   Novelty search could help NEAT explore behaviors that eventually lead to scoring.
    *   Environmental shaping and easier starting setups might improve training.

**[[P] spy search a llm search engine (Score: 0)](https://i.redd.it/vwc5bcmsp77f1.jpeg)**
*   **Summary:** The thread presents a new LLM search engine called "spy search". It's an API wrapper with RAG.
*   **Emotion:** The overall emotional tone of the thread is positive.
*   **Top 1 Point of View:**
    *   It's another API wrapper with RAG, but a nice project.

**[[R] The Illusion of "The Illusion of Thinking" (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ld0evr/r_the_illusion_of_the_illusion_of_thinking/)**
*   **Summary:** This thread discusses and critiques a paper titled "The Illusion of Thinking".
*   **Emotion:** The overall emotional tone of the thread is mixed, with some users expressing skepticism and criticism of the paper being discussed, while others express a positive sentiment towards the discussion itself.
*   **Top 3 Points of View:**
    *   The paper being discussed is a lazy "paper".
    *   The original paper has definitely made a lot of noise but not that interesting.
    *   Centuries of philosophy haven't brought us to a point where we can satisfactorily distinguish thinking from typing.
