---
title: "LocalLLaMA Subreddit"
date: "2025-06-30"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["localllama", "AI", "Models"]
---

# Overall Ranking and Top Discussions
1.  [[D] Open Source AI Editor: First Milestone](https://code.visualstudio.com/blogs/2025/06/30/openSourceAIEditorFirstMilestone) (Score: 62)
    *   Users are discussing an open-source AI editor, with questions about self-hosting, its suitability for fiction writing, the openness of prompts, and security concerns.
2.  [ERNIE 4.5 Collection from Baidu](https://ernie.baidu.com/blog/posts/ernie4.5/) (Score: 56)
    *   The thread discusses Baidu's ERNIE 4.5 model, including testing it with cloud APIs, concerns about lossless quantization, the publication of base models, and comparisons to Mistral Nemo.
3.  [What is the current best local coding model with <= 4B parameters?](https://www.reddit.com/r/LocalLLaMA/comments/1lo5vnf/what_is_the_current_best_local_coding_model_with/) (Score: 29)
    *   Users are seeking recommendations for the best local coding model with 4B parameters or less, with suggestions including Qwen 2.5 coder 3b, Jetbrains' Mellum, Gemma 3n, and Polaris 4B.
4.  [[Day 6/50] Building a Small Language Model from Scratch - What Is Positional Embedding and Why Does It Matter?](https://www.reddit.com/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/) (Score: 20)
    *   The thread discusses a blog series on building a small language model from scratch, focusing on positional embeddings.
5.  [[2506.21734] Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734) (Score: 14)
    *   The discussion revolves around a paper on a Hierarchical Reasoning Model (HRM), with skepticism about the model's limitations and scalability due to its small size and limited training data.
6.  [Has anyone tried running 2 AMD Ryzen™ AI Max+ 395 in parallel?](https://www.reddit.com/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/) (Score: 13)
    *   The thread explores the possibility of running two AMD Ryzen AI Max+ 395 processors in parallel for local LLMs, with some users reporting a speed penalty and others suggesting using vllm or sglang for multiple nodes.
7.  [Upcoming Coding Models?](https://www.reddit.com/r/LocalLLaMA/comments/1lobyx5/upcoming_coding_models/) (Score: 13)
    *   Users are discussing and inquiring about upcoming coding models.
8.  [n8n ,proxmox ,docker and Google API.](https://i.redd.it/02pteeydc3af1.png) (Score: 8)
    *   The thread discusses issues related to connecting to Google APIs using n8n, Proxmox, and Docker, with suggestions for using Cloudflare tunnels and Tailscale magicDNS.
9.  [Been experimenting with “agent graphs” for local LLMs — basically turning thoughts into modular code](https://www.reddit.com/r/LocalLLaMA/comments/1lo5xyx/been_experimenting_with_agent_graphs_for_local/) (Score: 4)
    *   The thread discusses the use of "agent graphs" for local LLMs, turning thoughts into modular code.
10. [Ollama and llama3.2-vision broken?](https://www.reddit.com/r/LocalLLaMA/comments/1lo6gc0/ollama_and_llama32vision_broken/) (Score: 2)
    *   The thread discusses problems with running the Llama 3.2 vision model in Ollama and suggests trying alternative models.
11. [Run any LLM locally on your Mac in less than 2 mins](https://www.dsdev.in/run-any-llm-locally-on-your-mac-in-less-than-2-mins) (Score: 0)
    *   This thread questions why articles recommend Ollama over GUIs.
12. [Which would be the best uncensored model to run on 4gb Vram laptop using LMStudio?](https://www.reddit.com/r/LocalLLaMA/comments/1lo42x8/which_would_be_the_best_uncensored_model_to_run/) (Score: 0)
    *   The thread asks about the best uncensored models to run in LMStudio with 4GB of VRAM.
13. [Ollama or VLLM?](https://www.reddit.com/r/LocalLLaMA/comments/1lo4qxf/ollama_or_vllm/) (Score: 0)
    *   The discussion compares Ollama and VLLM for local LLM deployment, with users advocating for VLLM for production environments and Ollama for development/prototyping.
14. [Got all the hardware, Got my dataset, why does it take soo long to learn how to fine-tune?](https://www.reddit.com/r/LocalLLaMA/comments/1lo61eb/got_all_the_hardware_got_my_dataset_why_does_it/) (Score: 0)
    *   The discussion revolves around the difficulties of learning how to fine-tune LLMs, with suggestions to learn PyTorch and avoid relying too much on GUIs.
15. [Query](https://www.reddit.com/r/LocalLLaMA/comments/1lo9mcm/query/) (Score: 0)
    *   The thread is a general query about the hardware needed for LLMs, with emphasis that prioritizing RAM, and accessing to remote servers are more important.
16. [MCP tool development -- repeated calls with no further processing](https://www.reddit.com/r/LocalLLaMA/comments/1lobqvc/mcp_tool_development_repeated_calls_with_no/) (Score: 0)
    *   The thread addresses a problem with MCP tool development in LMStudio where the model repeatedly calls a tool without further processing.
17. [Language Model Maker](https://www.reddit.com/r/LocalLLaMA/comments/1lobx8p/language_model_maker/) (Score: 0)
    *   The thread discusses the possibility of creating a "Language Model Maker" and suggests that creating models from scratch is unrealistic.
18. [Need open source Vlm for Trading chart analysis](https://www.reddit.com/r/LocalLLaMA/comments/1lofsxc/need_open_source_vlm_for_trading_chart_analysis/) (Score: 0)
    *   The thread asks for an open-source vision language model (VLM) to analyze trading charts.

# Detailed Analysis by Thread
**[[D] Open Source AI Editor: First Milestone (Score: 62)](https://code.visualstudio.com/blogs/2025/06/30/openSourceAIEditorFirstMilestone)**
*   **Summary:**  Users are discussing an open-source AI editor, with questions about self-hosting, its suitability for fiction writing, the openness of prompts, and security concerns.
*   **Emotion:** The emotional tone is mostly Neutral, with a hint of Positive sentiment.
*   **Top 3 Points of View:**
    *   Users are interested in self-hosting the AI editor.
    *   The editor might be useful for fiction writing.
    *   There are security concerns about the sandboxing of the editor.

**[ERNIE 4.5 Collection from Baidu (Score: 56)](https://ernie.baidu.com/blog/posts/ernie4.5/)**
*   **Summary:** The thread discusses Baidu's ERNIE 4.5 model, including testing it with cloud APIs, concerns about lossless quantization, the publication of base models, and comparisons to Mistral Nemo.
*   **Emotion:** The emotional tone is primarily Neutral, with some Positive sentiment regarding the release of base models.
*   **Top 3 Points of View:**
    *   Some users want to test the model using a cloud API.
    *   Some are skeptical about "lossless" 2-bit quantization.
    *   Some users are happy that someone is publishing base models.

**[What is the current best local coding model with <= 4B parameters? (Score: 29)](https://www.reddit.com/r/LocalLLaMA/comments/1lo5vnf/what_is_the_current_best_local_coding_model_with/)**
*   **Summary:**  Users are seeking recommendations for the best local coding model with 4B parameters or less, with suggestions including Qwen 2.5 coder 3b, Jetbrains' Mellum, Gemma 3n, and Polaris 4B.
*   **Emotion:** The emotional tone is largely Neutral, with a mix of positive and negative opinions.
*   **Top 3 Points of View:**
    *   Some suggest Qwen 2.5 coder 3b for autocomplete tasks.
    *   Others recommend Jetbrains' Mellum.
    *   Some believe there are no good coding models at this size.

**[[Day 6/50] Building a Small Language Model from Scratch - What Is Positional Embedding and Why Does It Matter? (Score: 20)](https://www.reddit.com/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/)**
*   **Summary:**  The thread discusses a blog series on building a small language model from scratch, focusing on positional embeddings.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   One user is interested in subscribing to the blog series.
    *   Another shares the link to the blog post.

**[[2506.21734] Hierarchical Reasoning Model (Score: 14)](https://arxiv.org/abs/2506.21734)**
*   **Summary:**  The discussion revolves around a paper on a Hierarchical Reasoning Model (HRM), with skepticism about the model's limitations and scalability due to its small size and limited training data.
*   **Emotion:** The emotional tone is mixed, with a slightly Negative leaning due to skepticism.
*   **Top 3 Points of View:**
    *   One user thinks the HRM architecture is a milestone in AI development despite limitations.
    *   Another user is skeptical about the model's scalability due to its small parameter size and limited training data.

**[Has anyone tried running 2 AMD Ryzen™ AI Max+ 395 in parallel? (Score: 13)](https://www.reddit.com/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/)**
*   **Summary:**  The thread explores the possibility of running two AMD Ryzen AI Max+ 395 processors in parallel for local LLMs, with some users reporting a speed penalty and others suggesting using vllm or sglang for multiple nodes.
*   **Emotion:** The emotional tone is mixed, with a blend of Positive and Negative sentiment.
*   **Top 3 Points of View:**
    *   One user wants to know about the potential speed of using two different machines for inference.
    *   Another user reports a significant speed penalty due to asynchronous communication.
    *   One user believes local LLMs on consumer hardware is not worth the effort.

**[Upcoming Coding Models? (Score: 13)](https://www.reddit.com/r/LocalLLaMA/comments/1lobyx5/upcoming_coding_models/)**
*   **Summary:**  Users are discussing and inquiring about upcoming coding models.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   A user mentions that Baidu just released something.
    *   A user suggests following huggingface and the ceo on LinkedIn to keep track of all the big news.
    *   A user mentions Jetbrains released their llm Mellum, onto HF.

**[n8n ,proxmox ,docker and Google API. (Score: 8)](https://i.redd.it/02pteeydc3af1.png)**
*   **Summary:**  The thread discusses issues related to connecting to Google APIs using n8n, Proxmox, and Docker, with suggestions for using Cloudflare tunnels and Tailscale magicDNS.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   One user suggests binding to 0.0.0.0 then use local ip to connect.
    *   One user suggests Cloudflare tunnels to expose the IP.
    *   One user suggests Tailscale magicDNS

**[Been experimenting with “agent graphs” for local LLMs — basically turning thoughts into modular code (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1lo5xyx/been_experimenting_with_agent_graphs_for_local/)**
*   **Summary:**  The thread discusses the use of "agent graphs" for local LLMs, turning thoughts into modular code.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   One user is working on an ML project that uses metafunctions.
    *   The author posts a link to the github repo
    *   A user made a comment that is related to dead internet theory

**[Ollama and llama3.2-vision broken? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1lo6gc0/ollama_and_llama32vision_broken/)**
*   **Summary:**  The thread discusses problems with running the Llama 3.2 vision model in Ollama and suggests trying alternative models.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   The solution points to qwen2.5 vl or gemma3.

**[Run any LLM locally on your Mac in less than 2 mins (Score: 0)](https://www.dsdev.in/run-any-llm-locally-on-your-mac-in-less-than-2-mins)**
*   **Summary:**  This thread questions why articles recommend Ollama over GUIs.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   A user is curious why articles recommend Ollama over GUIs.

**[Which would be the best uncensored model to run on 4gb Vram laptop using LMStudio? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1lo42x8/which_would_be_the_best_uncensored_model_to_run/)**
*   **Summary:**  The thread asks about the best uncensored models to run in LMStudio with 4GB of VRAM.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   The solution points to "gemma-3-4b-it-abliterated-GGUF" and "gemma-3-4b-it-qat-abliterated-i1-GGUF".

**[Ollama or VLLM? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1lo4qxf/ollama_or_vllm/)**
*   **Summary:**  The discussion compares Ollama and VLLM for local LLM deployment, with users advocating for VLLM for production environments and Ollama for development/prototyping.
*   **Emotion:** The emotional tone is Positive.
*   **Top 3 Points of View:**
    *   Some suggest that VLLM is the way to go for concurrent users.
    *   Some believe Ollama is great for model switching but with OpenWebUI.
    *   Some suggests renting bare metal and hosting off site.

**[Got all the hardware, Got my dataset, why does it take soo long to learn how to fine-tune? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1lo61eb/got_all_the_hardware_got_my_dataset_why_does_it/)**
*   **Summary:**  The discussion revolves around the difficulties of learning how to fine-tune LLMs, with suggestions to learn PyTorch and avoid relying too much on GUIs.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   Transformer is still in bleeding edge side of ML and most of the resource is still not being "wiki/book"-ified.
    *   One suggests that OP needs to take time and learn the information first, instead of using AI as a heavy crutch
    *   One suggest that OP will continue to struggle as long as OP is hung up on needing a GUI.

**[Query (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1lo9mcm/query/)**
*   **Summary:** The thread is a general query about the hardware needed for LLMs, with emphasis that prioritizing RAM, and accessing to remote servers are more important.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   OP should not buy base.
    *   Prioritize RAM.

**[MCP tool development -- repeated calls with no further processing (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1lobqvc/mcp_tool_development_repeated_calls_with_no/)**
*   **Summary:**  The thread addresses a problem with MCP tool development in LMStudio where the model repeatedly calls a tool without further processing.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   One user says that he is not familiar with llmstudio though.
    *   OP needs to return a string that tells the model that it worked, along with the results.
    *   LMStudio's new built-in MCP has a problem with calling a tool repeatedly.

**[Language Model Maker (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1lobx8p/language_model_maker/)**
*   **Summary:**  The thread discusses the possibility of creating a "Language Model Maker" and suggests that creating models from scratch is unrealistic.
*   **Emotion:** The emotional tone is Neutral, with a negative sentiment on the possibility of creating a model maker.
*   **Top 3 Points of View:**
    *   The only realistic way of doing this would be to provide a service where you can prompt a hypermodel to create a lora for a list of supported models.
    *   Yesterday's solutions may become outdated as of today because of the new solution:  "LLMs Can Now Be Pre-Trained Using Pure Reinforcement Learning".
    *   The OP is greatly underestimating the amount of time and compute resources that are required to create even a simple LLM.

**[Need open source Vlm for Trading chart analysis (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1lofsxc/need_open_source_vlm_for_trading_chart_analysis/)**
*   **Summary:**  The thread asks for an open-source vision language model (VLM) to analyze trading charts.
*   **Emotion:** The emotional tone is Neutral.
*   **Top 3 Points of View:**
    *   A user asks the question: how will the data be fed in?

