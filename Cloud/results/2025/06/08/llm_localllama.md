---
title: "LocalLLaMA Subreddit"
date: "2025-06-08"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "AI", "Local"]
---

# Overall Ranking and Top Discussions

1.  [When you figure out it’s all just math:](https://i.redd.it/t7ko9eywrq5f1.jpeg) (Score: 535)
    *   This thread discusses a paper on the limitations of reasoning models, specifically how they function as token prediction machines rather than true thinkers. Users debate whether transformers can achieve AGI and the importance of internal chaos and contradictions in thinking.
2.  [I Built 50 AI Personalities - Here's What Actually Made Them Feel Human](https://www.reddit.com/r/LocalLLaMA/comments/1l69w7i/i_built_50_ai_personalities_heres_what_actually/) (Score: 419)
    *   This thread discusses the experience of building 50 AI personalities and what factors contribute to making them feel human. Users share their own experiences and insights on creating character cards and interacting with AI.
3.  [Confirmation that Qwen3-coder is in works](https://www.reddit.com/r/LocalLLaMA/comments/1l68m1m/confirmation_that_qwen3coder_is_in_works/) (Score: 226)
    *   This thread discusses the confirmation of Qwen3-coder being in development. Users express their excitement and expectations for the new coder model, particularly its potential for local use and autonomous workflows.
4.  [[In Development] Serene Pub, a simpler SillyTavern like roleplay client](https://www.reddit.com/r/LocalLLaMA/comments/1l67i14/in_development_serene_pub_a_simpler_sillytavern/) (Score: 22)
    *   This thread introduces Serene Pub, a new roleplay client aiming for simplicity compared to SillyTavern. Users discuss the complexities of SillyTavern and express their preferences for more intuitive and open-ended adventure experiences.
5.  [What is your sampler order (not sampler settings) for llama.cpp?](https://www.reddit.com/r/LocalLLaMA/comments/1l68hjc/what_is_your_sampler_order_not_sampler_settings/) (Score: 19)
    *   This thread asks users about their preferred sampler order in llama.cpp, emphasizing that the discussion is about the order of samplers, not the specific settings of each sampler.
6.  [Gigabyte AI-TOP-500-TRX50](https://www.gigabyte.com/us/Gaming-PC/AI-TOP-500-TRX50) (Score: 18)
    *   This thread discusses the Gigabyte AI-TOP-500-TRX50, a pre-built system marketed for AI development. Users debate its value, specifications, and suitability compared to alternative setups like EPYC-based systems or custom builds.
7.  [Ruminate: From All-or-Nothing to Just-Right Reasoning in LLMs](https://www.reddit.com/r/LocalLLaMA/comments/1l6gc5o/ruminate_from_allornothing_to_justright_reasoning/) (Score: 18)
    *   This thread discusses a paper about "Ruminate: From All-or-Nothing to Just-Right Reasoning in LLMs".
8.  [Thinking about buying a 3090. Good for local llm?](https://www.reddit.com/r/LocalLLaMA/comments/1l6hzl2/thinking_about_buying_a_3090_good_for_local_llm/) (Score: 4)
    *   This thread discusses the suitability of a 3090 GPU for running local LLMs. Users share their experiences and recommendations, comparing it to other GPUs and discussing the benefits of larger VRAM.
9.  [Tech Stack for Minion Voice..](https://www.reddit.com/r/LocalLLaMA/comments/1l68tgx/tech_stack_for_minion_voice/) (Score: 4)
    *   This thread discusses the tech stack for a "Minion Voice".
10. [M.2 to external gpu](http://joshvoigts.com/articles/m2-to-external-gpu/) (Score: 3)
    *   This thread discusses using M.2 slots for external GPUs. Users share their experiences and discuss the importance of considering motherboard and CPU specifications for optimal performance.
11. [Locally ran coding assistant on Apple M2?](https://www.reddit.com/r/LocalLLaMA/comments/1l69vze/locally_ran_coding_assistant_on_apple_m2/) (Score: 3)
    *   This thread explores the possibility of running a local coding assistant on an Apple M2 chip. Users provide advice on suitable models, quantization methods, and tools for setting up a local development environment.
12. [Create 2 and 3-bit GPTQ quantization for Qwen3-235B-A22B?](https://www.reddit.com/r/LocalLLaMA/comments/1l67vkt/create_2_and_3bit_gptq_quantization_for/) (Score: 3)
    *   This thread discusses the creation of 2 and 3-bit GPTQ quantization for Qwen3-235B-A22B.
13. [4x RTX Pro 6000 fail to boot, 3x is OK](https://www.reddit.com/r/LocalLLaMA/comments/1l6hnfg/4x_rtx_pro_6000_fail_to_boot_3x_is_ok/) (Score: 2)
    *   This thread discusses an issue where 4 RTX Pro 6000 GPUs fail to boot, while 3 GPUs work fine. Users suggest potential causes such as PCIe lane limitations, power supply issues, and BIOS settings, and recommend using a workstation or server-grade motherboard.
14. [Good current Linux OSS LLM inference SW/backend/config for AMD Ryzen 7 PRO 8840HS + Radeon 780M IGPU, 4-32B MoE / dense / Q8-Q4ish?](https://www.reddit.com/r/LocalLLaMA/comments/1l6ik8z/good_current_linux_oss_llm_inference/) (Score: 1)
    *   This thread seeks recommendations for Linux OSS LLM inference software, backends, and configurations for an AMD Ryzen 7 PRO 8840HS + Radeon 780M IGPU, suitable for 4-32B MoE/dense models in Q8-Q4 quantization.
15. [AI Studio ‘App’ on iOS](https://www.icloud.com/shortcuts/9cd63478017648cba611378ba372b19d) (Score: 0)
    *   This thread is about an AI Studio ‘App’ on iOS.
16. [Need a tutorial on GPUs](https://www.reddit.com/r/LocalLLaMA/comments/1l67obk/need_a_tutorial_on_gpus/) (Score: 0)
    *   This thread requests a tutorial on GPUs.
17. [How do I finetune Devstral with vision support?](https://www.reddit.com/r/LocalLLaMA/comments/1l6bn1t/how_do_i_finetune_devstral_with_vision_support/) (Score: 0)
    *   This thread asks how to finetune Devstral with vision support.
18. [Can we all admit that getting into local AI requires an unimaginable amount of knowledge in 2025?](https://www.reddit.com/r/LocalLLaMA/comments/1l6f4ei/can_we_all_admit_that_getting_into_local_ai/) (Score: 0)
    *   This thread discusses the knowledge required to get into local AI in 2025.
19. [Is it possible to run 32B model on 100 requests at a time at 200 Tok/s per second?](https://www.reddit.com/r/LocalLLaMA/comments/1l6iz1t/is_it_possible_to_run_32b_model_on_100_requests/) (Score: 0)
    *   This thread asks if it is possible to run a 32B model on 100 requests at a time at 200 Tok/s per second.
20. ["Given infinite time, would a language model ever respond to 'how is the weather' with the entire U.S. Declaration of Independence?"](https://www.reddit.com/r/LocalLLaMA/comments/1l6kvk5/given_infinite_time_would_a_language_model_ever/) (Score: 0)
    *   This thread is about "Given infinite time, would a language model ever respond to 'how is the weather' with the entire U.S. Declaration of Independence?"

# Detailed Analysis by Thread

**[[D] When you figure out it’s all just math: (Score: 535)](https://i.redd.it/t7ko9eywrq5f1.jpeg)**
*  **Summary:**  This thread discusses a paper on the limitations of reasoning models, specifically how they function as token prediction machines rather than true thinkers. Users debate whether transformers can achieve AGI and the importance of internal chaos and contradictions in thinking.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    * Transformers are token prediction machines and can't achieve AGI.
    * Reasoning models are just output with a different label. Populating the context window with relevant stuff can increase the fitness of the model in a lot of tasks.
    * Current methods are too inefficient for reasoning.

**[I Built 50 AI Personalities - Here's What Actually Made Them Feel Human (Score: 419)](https://www.reddit.com/r/LocalLLaMA/comments/1l69w7i/i_built_50_ai_personalities_heres_what_actually/)**
*  **Summary:**  This thread discusses the experience of building 50 AI personalities and what factors contribute to making them feel human. Users share their own experiences and insights on creating character cards and interacting with AI.
*  **Emotion:** The overall emotional tone is Neutral, with some positive sentiments.
*  **Top 3 Points of View:**
    * Nuanced prompt comprehension at 12-16k token context length is non-negotiable.
    * The LLM clearly tends to reproduce too many stereotypes.
    * This is really great. You're talking about some very subtle factors that are pretty "fuzzy".

**[Confirmation that Qwen3-coder is in works (Score: 226)](https://www.reddit.com/r/LocalLLaMA/comments/1l68m1m/confirmation_that_qwen3coder_is_in_works/)**
*  **Summary:**  This thread discusses the confirmation of Qwen3-coder being in development. Users express their excitement and expectations for the new coder model, particularly its potential for local use and autonomous workflows.
*  **Emotion:** The overall emotional tone is Neutral, with some positive sentiments.
*  **Top 3 Points of View:**
    * A 30B-3A Coder with coding and tool use finetuning would be a beast.
    * A proper multimodal Qwen3 is needed.
    * It might not be one for frontier performance or benchmark chasers, but should be exciting from local perspective.

**[[In Development] Serene Pub, a simpler SillyTavern like roleplay client (Score: 22)](https://www.reddit.com/r/LocalLLaMA/comments/1l67i14/in_development_serene_pub_a_simpler_sillytavern/)**
*  **Summary:**  This thread introduces Serene Pub, a new roleplay client aiming for simplicity compared to SillyTavern. Users discuss the complexities of SillyTavern and express their preferences for more intuitive and open-ended adventure experiences.
*  **Emotion:** The overall emotional tone is Neutral, with some positive sentiments.
*  **Top 3 Points of View:**
    * Silly Tavern's disadvantages actually seem like great advantages to me.
    * My biggest gripe with ST is the focus on individual character chats over open-ended adventures with the AI taking the role of a narrator.
    * A year ago I made a SillyTavern alternative due to its poor mobile UX. He advises not to focus too much on Silly Tavern's data formats.

**[What is your sampler order (not sampler settings) for llama.cpp? (Score: 19)](https://www.reddit.com/r/LocalLLaMA/comments/1l68hjc/what_is_your_sampler_order_not_sampler_settings/)**
*  **Summary:**  This thread asks users about their preferred sampler order in llama.cpp, emphasizing that the discussion is about the order of samplers, not the specific settings of each sampler.
*  **Emotion:** The overall emotional tone is Positive.
*  **Top 3 Points of View:**
    * Depends heavily on the context/problem. Best approach I've found is to create a tiny test/benchmark harness, then produce a cartesian product and test out all the orders, then use whatever ends up best for that particular problem.
    * The link you shared has been inactive for some time. The article you shared is very insightful.

**[Gigabyte AI-TOP-500-TRX50 (Score: 18)](https://www.gigabyte.com/us/Gaming-PC/AI-TOP-500-TRX50)**
*  **Summary:**  This thread discusses the Gigabyte AI-TOP-500-TRX50, a pre-built system marketed for AI development. Users debate its value, specifications, and suitability compared to alternative setups like EPYC-based systems or custom builds.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    * The setup costs 8K-13K EUR. Switching out the 5090 with a Pro 6000 would increase the price by 10K EUR or so.
    * There are no AI specific features on this setup. 405B model is 4 CCD and not the faster one among TR or Epyc.
    * 8 channels ddr5 ,  4 pcie 5.0 x 16 slots , the TRX50 motherboard is suitable for AI.

**[Ruminate: From All-or-Nothing to Just-Right Reasoning in LLMs (Score: 18)](https://www.reddit.com/r/LocalLLaMA/comments/1l6gc5o/ruminate_from_allornothing_to_justright_reasoning/)**
*  **Summary:**  This thread discusses a paper about "Ruminate: From All-or-Nothing to Just-Right Reasoning in LLMs".
*  **Emotion:** The overall emotional tone is Positive.
*  **Top 3 Points of View:**
    * Cool, very interesting! Thanks for sharing.

**[Thinking about buying a 3090. Good for local llm? (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1l6hzl2/thinking_about_buying_a_3090_good_for_local_llm/)**
*  **Summary:**  This thread discusses the suitability of a 3090 GPU for running local LLMs. Users share their experiences and recommendations, comparing it to other GPUs and discussing the benefits of larger VRAM.
*  **Emotion:** The overall emotional tone is Neutral, with some positive sentiments.
*  **Top 3 Points of View:**
    * 3090 is best bang for the buck.
    * Your 3070 can be used to achieve your stated goals.
    * If you're just starting, you don't need to buy anything.

**[Tech Stack for Minion Voice.. (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1l68tgx/tech_stack_for_minion_voice/)**
*  **Summary:**  This thread discusses the tech stack for a "Minion Voice".
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    * In english?

**[M.2 to external gpu (Score: 3)](http://joshvoigts.com/articles/m2-to-external-gpu/)**
*  **Summary:**  This thread discusses using M.2 slots for external GPUs. Users share their experiences and discuss the importance of considering motherboard and CPU specifications for optimal performance.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    * The guide is missing some vital details about the motherboard and the CPU. The amount of lanes you have available will be too little regardless of what connector you use.
    * I have 3090x2+4090x3+5090 x2 on a consumer X670E motherboard (Carbon) for CPU directly from PCIe slots.

**[Locally ran coding assistant on Apple M2? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1l69vze/locally_ran_coding_assistant_on_apple_m2/)**
*  **Summary:**  This thread explores the possibility of running a local coding assistant on an Apple M2 chip. Users provide advice on suitable models, quantization methods, and tools for setting up a local development environment.
*  **Emotion:** The overall emotional tone is Neutral, with some negative sentiments.
*  **Top 3 Points of View:**
    * MLX quants around 4B-14B are recommended for coding models.
    * Ollama, LM Studio, VSCode extensions Cline, RooCode, or VSCodes Copilot are good choices for a coder model.
    * Gemma 3 and Qwen 3 are good models for coding on an M3 8gb system.

**[Create 2 and 3-bit GPTQ quantization for Qwen3-235B-A22B? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1l67vkt/create_2_and_3bit_gptq_quantization_for/)**
*  **Summary:**  This thread discusses the creation of 2 and 3-bit GPTQ quantization for Qwen3-235B-A22B.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    * I'm not sure GPTQ < 4 bit has been implemented in vLLM.
    * Performance of GPTQ not so hot under 4bpw, you're far better off with the unsloth dynamic GGUFs..
    * There is already EXL3 that will fit in that memory.

**[4x RTX Pro 6000 fail to boot, 3x is OK (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1l6hnfg/4x_rtx_pro_6000_fail_to_boot_3x_is_ok/)**
*  **Summary:**  This thread discusses an issue where 4 RTX Pro 6000 GPUs fail to boot, while 3 GPUs work fine. Users suggest potential causes such as PCIe lane limitations, power supply issues, and BIOS settings, and recommend using a workstation or server-grade motherboard.
*  **Emotion:** The overall emotional tone is Negative, with some Neutral sentiments.
*  **Top 3 Points of View:**
    * My guy get a workstation or server, do not put these on a consumer board, will severely bottleneck the PCIe bandwidth
    * Could it be a power issue?
    * It might be a BAR issue.

**[Good current Linux OSS LLM inference SW/backend/config for AMD Ryzen 7 PRO 8840HS + Radeon 780M IGPU, 4-32B MoE / dense / Q8-Q4ish? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1l6ik8z/good_current_linux_oss_llm_inference/)**
*  **Summary:**  This thread seeks recommendations for Linux OSS LLM inference software, backends, and configurations for an AMD Ryzen 7 PRO 8840HS + Radeon 780M IGPU, suitable for 4-32B MoE/dense models in Q8-Q4 quantization.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    * There is no "best" answer. What is great for one person might not be good for your use case.
    * I know llama.cpp + Vulkan back-end will support inferring on both of your GPU and CPU splitting along layers, but it's hard to say whether it's best suited to your use-cases without knowing more.

**[AI Studio ‘App’ on iOS (Score: 0)](https://www.icloud.com/shortcuts/9cd63478017648cba611378ba372b19d)**
*  **Summary:**  This thread is about an AI Studio ‘App’ on iOS.
*  **Emotion:** The overall emotional tone is Neutral.

**[Need a tutorial on GPUs (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1l67obk/need_a_tutorial_on_gpus/)**
*  **Summary:**  This thread requests a tutorial on GPUs.
*  **Emotion:** The overall emotional tone is Neutral.

**[How do I finetune Devstral with vision support? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1l6bn1t/how_do_i_finetune_devstral_with_vision_support/)**
*  **Summary:**  This thread asks how to finetune Devstral with vision support.
*  **Emotion:** The overall emotional tone is Neutral.

**[Can we all admit that getting into local AI requires an unimaginable amount of knowledge in 2025? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1l6f4ei/can_we_all_admit_that_getting_into_local_ai/)**
*  **Summary:**  This thread discusses the knowledge required to get into local AI in 2025.
*  **Emotion:** The overall emotional tone is Neutral, with some negative and positive sentiments.

**[Is it possible to run 32B model on 100 requests at a time at 200 Tok/s per second? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1l6iz1t/is_it_possible_to_run_32b_model_on_100_requests/)**
*  **Summary:**  This thread asks if it is possible to run a 32B model on 100 requests at a time at 200 Tok/s per second.
*  **Emotion:** The overall emotional tone is Neutral.

**["Given infinite time, would a language model ever respond to 'how is the weather' with the entire U.S. Declaration of Independence?" (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1l6kvk5/given_infinite_time_would_a_language_model_ever/)**
*  **Summary:**  This thread is about "Given infinite time, would a language model ever respond to 'how is the weather' with the entire U.S. Declaration of Independence?"
*  **Emotion:** The overall emotional tone is Neutral.
