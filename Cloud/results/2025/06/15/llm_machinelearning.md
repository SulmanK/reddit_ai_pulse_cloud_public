---
title: "Machine Learning Subreddit"
date: "2025-06-15"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "deeplearning"]
---

# Overall Ranking and Top Discussions
1.  [[D] Q-learning is not yet scalable](https://seohong.me/blog/q-learning-is-not-yet-scalable/) (Score: 35)
    *   The author claims Q-learning is not scalable.
2.  [[D] Best websites for Scientific Researching](https://www.reddit.com/r/MachineLearning/comments/1lbivpx/d_best_websites_for_scientific_researching/) (Score: 21)
    *   Users are sharing their favorite websites for scientific research.
3.  [[D] What is XAI missing?](https://www.reddit.com/r/MachineLearning/comments/1lc0y8f/d_what_is_xai_missing/) (Score: 21)
    *   People are discussing the limitations and challenges of Explainable AI (XAI).
4.  [[D] What are some low hanging fruits in ML/DL research that can still be done using small compute (say a couple of GPUs)?](https://www.reddit.com/r/MachineLearning/comments/1lbtgeg/d_what_are_some_low_hanging_fruits_in_mldl/) (Score: 17)
    *   The thread discusses ML/DL research areas that can be explored with limited computational resources.
5.  [[N] "Foundations of Computer Vision" book from MIT](https://visionbook.mit.edu/) (Score: 9)
    *   A user shared a link to the "Foundations of Computer Vision" book from MIT.
6.  [[D] Hardware focused/Embedded engineer seeking advices for moving to Edge AI ML](https://www.reddit.com/r/MachineLearning/comments/1lbinek/d_hardware_focusedembedded_engineer_seeking/) (Score: 5)
    *   A hardware engineer is asking for advice on transitioning to Edge AI/ML.
7.  [[D] Pytorch-forecasting TFT vs Neuralforecast (Nixtla) TFT](https://www.reddit.com/r/MachineLearning/comments/1lbl5vg/d_pytorchforecasting_tft_vs_neuralforecast_nixtla/) (Score: 3)
    *   The thread discusses comparing pytorch-forecasting TFT vs Neuralforecast.
8.  [[P] I built a symbolic operating system for LLMs with deterministic memory, trace logging, and red-teamable audit layers — all in plain text](https://www.reddit.com/r/MachineLearning/comments/1lbo799/p_i_built_a_symbolic_operating_system_for_llms/) (Score: 0)
    *   A user presented a symbolic operating system for LLMs with deterministic memory, trace logging, and red-teamable audit layers.
9.  [[D] How do you buid your inference pipeline after training?](https://www.reddit.com/r/MachineLearning/comments/1lc46k0/d_how_do_you_buid_your_inference_pipeline_after/) (Score: 0)
    *   A discussion on building inference pipelines after model training.
10. [[D]stationary gan training machine](https://www.reddit.com/r/MachineLearning/comments/1lc4bku/dstationary_gan_training_machine/) (Score: 0)
    *   A discussion on GAN training hardware.
11. [[P] How do I profitably use 2x 12x RTX 4090 servers?](https://www.reddit.com/r/MachineLearning/comments/1lc9dek/p_how_do_i_profitably_use_2x_12x_rtx_4090_servers/) (Score: 0)
    *   A user asking how to use 2x 12x RTX 4090 servers profitably.

# Detailed Analysis by Thread
**[[D] Q-learning is not yet scalable (Score: 35)](https://seohong.me/blog/q-learning-is-not-yet-scalable/)**
*  **Summary:** The author claims Q-learning is not scalable, prompting a response asking for clarification and mentioning prior experience with massively parallel Q-learning.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Q-learning is not scalable.
    *   Massively parallel Q-learning is possible.

**[[D] Best websites for Scientific Researching (Score: 21)](https://www.reddit.com/r/MachineLearning/comments/1lbivpx/d_best_websites_for_scientific_researching/)**
*  **Summary:** Users are sharing their favorite websites and resources for scientific research, including Google Scholar, Arxiv, Paperswithcode, and MIT OpenCourseware. Some users provide advice on the necessary mathematical foundations for understanding research papers.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Google Scholar, arxiv, and scholar inbox are useful resources.
    *   Paperswithcode is a good resource for research.
    *   MIT OpenCourseware is a good starting point for learning the basics.

**[[D] What is XAI missing? (Score: 21)](https://www.reddit.com/r/MachineLearning/comments/1lc0y8f/d_what_is_xai_missing/)**
*  **Summary:** This thread discusses the current state and challenges of Explainable AI (XAI). The limitations of explainable models, the focus on data scientists instead of end-users, and the lack of a strong business case are highlighted as key issues. Some suggest focusing on evaluating the quality of explanations and understanding how neural networks process individual tokens.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Truly explainable models are limited, and performance often outweighs explainability in real-world use cases.
    *   XAI needs to focus on explaining models to end-users, not just data scientists.
    *   The quality of explanations needs to be evaluated to ensure they accurately reflect the model's decision-making process.

**[[D] What are some low hanging fruits in ML/DL research that can still be done using small compute (say a couple of GPUs)? (Score: 17)](https://www.reddit.com/r/MachineLearning/comments/1lbtgeg/d_what_are_some_low_hanging_fruits_in_mldl/)**
*  **Summary:** The thread explores research areas in machine learning and deep learning that can be pursued with limited computational resources, such as a couple of GPUs. Some suggestions include fine-grained image recognition, software acceleration techniques, reinforcement learning, theoretical research, NLP for rarer languages, and work on data filtering and small models.
*  **Emotion:** Positive
*  **Top 3 Points of View:**
    *   Fine-grained image recognition and software acceleration are viable areas.
    *   Reinforcement learning can be fruitful with limited compute.
    *   Theoretical research and working with smaller models are good options.

**[[N] "Foundations of Computer Vision" book from MIT (Score: 9)](https://visionbook.mit.edu/)**
*  **Summary:** A user shared a link to the "Foundations of Computer Vision" book from MIT, and another user commented that it "Looks very nice".
*  **Emotion:** Positive
*  **Top 3 Points of View:**
    *   The book looks very nice.

**[[D] Hardware focused/Embedded engineer seeking advices for moving to Edge AI ML (Score: 5)](https://www.reddit.com/r/MachineLearning/comments/1lbinek/d_hardware_focusedembedded_engineer_seeking/)**
*  **Summary:** A hardware/embedded engineer is seeking advice on transitioning to Edge AI/ML. Recommendations include learning about FPGAs, Nvidia's Jetson series, TensorRT, ONNX, and Apple's ANE. The focus should be on older established models, model optimization, and deployment toolkits.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   FPGAs are a key area for Edge AI.
    *   Nvidia's Jetson series, TensorRT optimization engine, and ONNX model format are useful to know.
    *   Focus on older established models, model optimization, and deployment toolkits.

**[[D] Pytorch-forecasting TFT vs Neuralforecast (Nixtla) TFT (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1lbl5vg/d_pytorchforecasting_tft_vs_neuralforecast_nixtla/)**
*  **Summary:** The discussion involves a user asking to try exporting to ONNX and running it in another environment for better performance.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Exporting to ONNX and running in another environment might improve performance.

**[[P] I built a symbolic operating system for LLMs with deterministic memory, trace logging, and red-teamable audit layers — all in plain text (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1lbo799/p_i_built_a_symbolic_operating_system_for_llms/)**
*  **Summary:** A user presents a symbolic operating system for LLMs. The comments are critical, calling it "AI slop" and questioning the deterministic nature of the system.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   The project is considered "AI slop".
    *   The creator did not correct the LLM's broken ascii output in that diagram.
    *   It is questioned whether GPT4o can run such a system deterministically and reliably, given the fuzziness of context.

**[[D] How do you buid your inference pipeline after training? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1lc46k0/d_how_do_you_buid_your_inference_pipeline_after/)**
*  **Summary:** A user has a question of building inference pipeline after model training.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Feature selection should be done before training the final model.

**[[D]stationary gan training machine (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1lc4bku/dstationary_gan_training_machine/)**
*  **Summary:** A discussion on hardware setup for training GANs. One user recommends using an RTX PRO 6000 over multiple 4090s and suggesting renting a cloud instance before investing. Another user shares their experience with GANs and how Stable Diffusion has overshadowed their work.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   An RTX PRO 6000 is better than 4x 4090s for most use cases.
    *   Hyperparameter tuning for GANs can be challenging.
    *   Stable Diffusion has surpassed much of the previous work done with GANs.

**[[P] How do I profitably use 2x 12x RTX 4090 servers? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1lc9dek/p_how_do_i_profitably_use_2x_12x_rtx_4090_servers/)**
*  **Summary:** A user is asking how to profitably use 2x 12x RTX 4090 servers and another users suggests to Sell them.
*  **Emotion:** Neutral
*  **Top 3 Points of View:**
    *   Sell them.
