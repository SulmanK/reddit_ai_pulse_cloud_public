---
title: "Stable Diffusion Subreddit"
date: "2025-06-19"
description: "Analysis of top discussions and trends in the stablediffusion subreddit"
tags: ["stablediffusion", "AI", "image generation"]
---

# Overall Ranking and Top Discussions
1.  [Dark Fantasy test with chroma-unlocked-v38-detail-calibrated](https://www.reddit.com/gallery/1lfb3q4) (Score: 93)
    *   The thread discusses a dark fantasy-themed image generation test using a specific model and settings.
2.  [8 Depth Estimation Models Tested with the Highest Settings on ComfyUI](https://i.redd.it/noifyjoitw7f1.jpeg) (Score: 62)
    *   The thread compares different depth estimation models within the ComfyUI environment.
3.  [Does anyone know anything about context windows on longer (20-30 second) Wan videos?](https://i.redd.it/keexq04rms7f1.gif) (Score: 12)
    *   The thread asks about techniques used for maintaining context in long AI-generated videos.
4.  [What this setting does in the Chroma workflow?](https://i.redd.it/dtqwpm4vww7f1.png) (Score: 9)
    *   The thread seeks to understand the function of a specific setting in the Chroma workflow for stable diffusion.
5.  [Wan2GP - Fusion X 14b (Motion Transfer Compilation) 1280x720, NVIDIA 4090, 81 Frames, 10 Steps, Aprox. 400s](https://v.redd.it/2q110z8vgx7f1) (Score: 4)
    *   This thread shares results of a motion transfer compilation using Fusion X 14b.
6.  [Any Tips for Character Consistency with SDXL?](https://www.reddit.com/r/StableDiffusion/comments/1lfcjr4/any_tips_for_character_consistency_with_sdxl/) (Score: 2)
    *   The thread asks for advice on achieving consistent character generation with SDXL.
7.  [Anyone know how to inpaint in FastSD CPU? Is it even possible?](https://www.reddit.com/r/StableDiffusion/comments/1lfd3f7/anyone_know_how_to_inpaint_in_fastsd_cpu_is_it/) (Score: 2)
    *   The thread seeks information on inpainting capabilities within the FastSD CPU version of Stable Diffusion.
8.  [Invoke with docker?](https://www.reddit.com/r/StableDiffusion/comments/1lfiljo/invoke_with_docker/) (Score: 2)
    *   The thread inquires about using InvokeAI with Docker, troubleshooting potential issues.
9.  [Is it possible to use Krita with Stable Diffusion for img2img?](https://www.reddit.com/r/StableDiffusion/comments/1lffc0m/is_it_possible_to_use_krita_with_stable_diffusion/) (Score: 1)
    *   The thread asks about the possibility of integrating Krita with Stable Diffusion for image-to-image transformations.
10. [Is there a way to add realistic people in this image ?](https://www.reddit.com/r/StableDiffusion/comments/1lfhom5/is_there_a_way_to_add_realistic_people_in_this/) (Score: 1)
    *   The thread inquires about methods for incorporating realistic people into an existing image using stable diffusion tools.
11. [Tensorart](https://i.redd.it/is4ceq7mhx7f1.jpeg) (Score: 0)
    *   The thread shows an image from Tensorart and asks about improving it.
12. [Fusionx results](https://v.redd.it/totjr0s9nw7f1) (Score: 0)
    *   This thread shares the results of Fusionx and asks for the workflow used.
13. [Max CPU does not always mean max heat](https://www.reddit.com/r/StableDiffusion/comments/1lfbr5x/max_cpu_does_not_always_mean_max_heat/) (Score: 0)
    *   The thread discusses CPU and GPU usage in Stable Diffusion, noting potential discrepancies in heat generation.
14. [Do oyu know any RunningHub alternatives but without 18+ filter](https://www.reddit.com/r/StableDiffusion/comments/1lfifmv/do_oyu_know_any_runninghub_alternatives_but/) (Score: 0)
    *   This thread asks if there are any RunningHub alternatives, without the 18+ filter.
15. [How do you inpaint using SDXL?](https://www.reddit.com/r/StableDiffusion/comments/1lfk4rs/how_do_you_inpaint_using_sdxl/) (Score: 0)
    *   The thread asks about the techniques for inpainting using SDXL.

# Detailed Analysis by Thread
**[Dark Fantasy test with chroma-unlocked-v38-detail-calibrated (Score: 93)](https://www.reddit.com/gallery/1lfb3q4)**
*  **Summary:** The thread features a user sharing a "Dark Fantasy" image generation test using the "chroma-unlocked-v38-detail-calibrated" model. Users comment on the generated images, providing feedback and asking questions about the workflow used.
*  **Emotion:** The overall emotional tone is mostly positive. Users express admiration for the images, although some provide constructive criticism.
*  **Top 3 Points of View:**
    *   The fourth picture is super cool.
    *   Something in the workflow is grainier than usual.
    *   The user provides details on the workflow used, including animation and prompts.

**[8 Depth Estimation Models Tested with the Highest Settings on ComfyUI (Score: 62)](https://i.redd.it/noifyjoitw7f1.jpeg)**
*  **Summary:** The thread presents a comparison of 8 different depth estimation models tested within the ComfyUI environment. Users discuss the strengths and weaknesses of each model based on the visual results.
*  **Emotion:** The overall emotional tone is positive and informative, with users expressing appreciation for the comparison and sharing their own observations.
*  **Top 3 Points of View:**
    *   Lotus seems to capture a lot of details.
    *   Depth Anything V2, DepthFM and Lotus-G provide good contrast despite small differences in depth.
    *   The comparison is useful, and Marigold was expected to be the best, but DepthFM looks really good too.

**[Does anyone know anything about context windows on longer (20-30 second) Wan videos? (Score: 12)](https://i.redd.it/keexq04rms7f1.gif)**
*  **Summary:** The thread asks about techniques for maintaining context in longer (20-30 second) AI-generated videos using Wan. The user wants to know about context windows.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   The video is likely doing I2V from the last frame of the previous clip and then stitching them together.

**[What this setting does in the Chroma workflow? (Score: 9)](https://i.redd.it/dtqwpm4vww7f1.png)**
*  **Summary:** The thread is about a user seeking clarification on the function of a specific setting within the Chroma workflow.
*  **Emotion:** The emotional tone is neutral and inquisitive.
*  **Top 3 Points of View:**
    *   Chroma was trained with an extra padding token but I think the outputs look better without it.
    *   It pads the prompt to a certain length.
    *   It has something to do with this: [https://huggingface.co/lodestones/Chroma#mmdit-masking](https://huggingface.co/lodestones/Chroma#mmdit-masking)

**[Wan2GP - Fusion X 14b (Motion Transfer Compilation) 1280x720, NVIDIA 4090, 81 Frames, 10 Steps, Aprox. 400s (Score: 4)](https://v.redd.it/2q110z8vgx7f1)**
*  **Summary:** The thread showcases a motion transfer compilation using Fusion X 14b.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   It's so uncanny

**[Any Tips for Character Consistency with SDXL? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1lfcjr4/any_tips_for_character_consistency_with_sdxl/)**
*  **Summary:** The thread seeks advice on maintaining character consistency when using SDXL for image generation.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Use Loras.
    *   You will need to create LoRA for all your character, training a LoRA reference: [https://youtu.be/-L9tP7\_9ejI](https://youtu.be/-L9tP7_9ejI)
    *   Train a lora + instant ID, works with standard SDXLs and Lustify both.

**[Anyone know how to inpaint in FastSD CPU? Is it even possible? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1lfd3f7/anyone_know_how_to_inpaint_in_fastsd_cpu_is_it/)**
*  **Summary:** The thread asks about the possibility of inpainting with FastSD CPU.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   ControlNet support in FastSD CPU is very basic, just meant to be used with canny and sketch controlnets.

**[Invoke with docker? (Score: 2)](https://www.reddit.com/r/StableDiffusion/comments/1lfiljo/invoke_with_docker/)**
*  **Summary:** The thread discusses the process of using InvokeAI with Docker, and troubleshooting potential issues.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Official documentation:
https://invoke-ai.github.io/InvokeAI/installation/docker/?

But I'd suggest asking on their Discord if you want help.
    *   Have you tried python virtual environments? It allows you to set your target python version.

**[Is it possible to use Krita with Stable Diffusion for img2img? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1lffc0m/is_it_possible_to_use_krita_with_stable_diffusion/)**
*  **Summary:** The thread discusses the possibility of using Krita with Stable Diffusion for image-to-image transformations.
*  **Emotion:** The emotional tone is positive and neutral.
*  **Top 3 Points of View:**
    *   There's a plugin you can get for krita: https://github.com/Acly/krita-ai-diffusion
    *   With the help of plugin "Krita AI diffusion", you can add those images and use the tools on it, somewhat like this: [https://www.youtube.com/watch?v=PPQnL0IRv8g](https://www.youtube.com/watch?v=PPQnL0IRv8g)

**[Is there a way to add realistic people in this image ? (Score: 1)](https://www.reddit.com/r/StableDiffusion/comments/1lfhom5/is_there_a_way_to_add_realistic_people_in_this/)**
*  **Summary:** The thread is about finding ways to add realistic people to an image.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Maybe try to use Daz studio?

**[Tensorart (Score: 0)](https://i.redd.it/is4ceq7mhx7f1.jpeg)**
*  **Summary:** The thread features an image generated with Tensorart and seeks suggestions for improvement.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Try cfg 3.5
    *   Your CFG is way too high for Flux.
Try with Euler beta or sgm uniform

**[Fusionx results (Score: 0)](https://v.redd.it/totjr0s9nw7f1)**
*  **Summary:** The thread shares the results of Fusionx.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Workflow? If not, just spam

**[Max CPU does not always mean max heat (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1lfbr5x/max_cpu_does_not_always_mean_max_heat/)**
*  **Summary:** The thread discusses CPU and GPU usage in Stable Diffusion, noting potential discrepancies in heat generation.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   It could be that the duty cycle on the GPU is rapidly spiking near 100%, but the way it is being measured only sees the spikes.

**[Do oyu know any RunningHub alternatives but without 18+ filter (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1lfifmv/do_oyu_know_any_runninghub_alternatives_but/)**
*  **Summary:** The thread asks if there are any RunningHub alternatives without the 18+ filter.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   If you find remember to tel me.

**[How do you inpaint using SDXL? (Score: 0)](https://www.reddit.com/r/StableDiffusion/comments/1lfk4rs/how_do_you_inpaint_using_sdxl/)**
*  **Summary:** The thread asks about the techniques for inpainting using SDXL.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   [Fooocus](https://github.com/lllyasviel/Fooocus) has great inpainting with any SDXL model
    *   I just use the same model I used to generate original image...
    *   Use brushnet
