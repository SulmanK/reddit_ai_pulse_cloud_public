---
title: "LocalLLaMA Subreddit"
date: "2025-06-05"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "Local Models", "AI"]
---

# Overall Ranking and Top Discussions
1.  [[D] Sparse Transformers: Run 2x faster LLM with 30% lesser memory](https://github.com/NimbleEdge/sparse_transformers) (Score: 152)
    * This thread discusses sparse transformers and their potential to speed up LLMs while reducing memory usage.
2.  [BAIDU joined huggingface](https://huggingface.co/baidu) (Score: 132)
    *  This thread discusses BAIDU joining huggingface.
3.  [DeepSeek’s new R1-0528-Qwen3-8B is the most intelligent 8B parameter model yet, but not by much: Alibaba’s own Qwen3 8B is just one point behind](https://www.reddit.com/r/LocalLLaMA/comments/1l41p1x/deepseeks_new_r10528qwen38b_is_the_most/) (Score: 52)
    *  The thread discusses the performance of DeepSeek's new model compared to Alibaba's Qwen3 8B.
4.  [What's the cheapest setup for running full Deepseek R1](https://www.reddit.com/r/LocalLLaMA/comments/1l40ip8/whats_the_cheapest_setup_for_running_full/) (Score: 46)
    *   This thread explores the most cost-effective hardware configurations for running the full Deepseek R1 model.
5.  [Best world knowledge model that can run on your phone](https://www.reddit.com/r/LocalLLaMA/comments/1l3z2m3/best_world_knowledge_model_that_can_run_on_your/) (Score: 28)
    *   This thread discusses the best models for world knowledge that can be run on a phone.
6.  [Qwen3-32b /nothink or qwen3-14b /think?](https://www.reddit.com/r/LocalLLaMA/comments/1l3yjeb/qwen332b_nothink_or_qwen314b_think/) (Score: 15)
    *   This thread compares the performance of Qwen3-32b /nothink and Qwen3-14b /think models.
7.  [I wrote a little script to automate commit messages](https://i.redd.it/shflqezx845f1.png) (Score: 13)
    *   This thread discusses a script to automate commit messages.
8.  [Non-reasoning Qwen3-235B worse than maverick? Is this experience real with you guys?](https://www.reddit.com/r/LocalLLaMA/comments/1l3yamg/nonreasoning_qwen3235b_worse_than_maverick_is/) (Score: 7)
    *   This thread debates whether Non-reasoning Qwen3-235B worse than maverick.
9.  [Hybrid setup for reasoning](https://www.reddit.com/r/LocalLLaMA/comments/1l40gij/hybrid_setup_for_reasoning/) (Score: 7)
    *   This thread talks about hybrid setup for reasoning.
10. [How can I connect to a local LLM from my iPhone?](https://www.reddit.com/r/LocalLLaMA/comments/1l4450t/how_can_i_connect_to_a_local_llm_from_my_iphone/) (Score: 6)
    *   This thread discusses how to connect to a local LLM from an iPhone.
11. [Is it dumb to build a server with 7x 5060 Ti?](https://www.reddit.com/r/LocalLLaMA/comments/1l48cnk/is_it_dumb_to_build_a_server_with_7x_5060_ti/) (Score: 6)
    *   The thread discusses the practicality of building a server with 7x 5060 Ti.
12. [With 8gb vram: qwen3 8b q6 or 32b iq1?](https://www.reddit.com/r/LocalLLaMA/comments/1l47fv0/with_8gb_vram_qwen3_8b_q6_or_32b_iq1/) (Score: 4)
    *   This thread compares the performance of Qwen3 8b q6 and 32b iq1 with 8gb vram.
13. [Looking for Advice: Best LLM/Embedding Models for Precise Document Retrieval (Product Standards)](https://www.reddit.com/r/LocalLLaMA/comments/1l3xxpw/looking_for_advice_best_llmembedding_models_for/) (Score: 3)
    *   The thread is looking for advice on the best LLM/Embedding Models for Precise Document Retrieval (Product Standards).
14. [4090 boards with 48gb Ram - will there ever be an upgrade service?](https://www.reddit.com/r/LocalLLaMA/comments/1l3ykjn/4090_boards_with_48gb_ram_will_there_ever_be_an/) (Score: 3)
    *   The thread is about if there will ever be an upgrade service for 4090 boards with 48gb Ram.
15. [Looking for UI that can store and reference characters easily](https://www.reddit.com/r/LocalLLaMA/comments/1l42woy/looking_for_ui_that_can_store_and_reference/) (Score: 1)
    *   The thread is looking for UI that can store and reference characters easily.
16. [What's the best model for playing a role right now , that will fit on 8gbvram?](https://www.reddit.com/r/LocalLLaMA/comments/1l45p2d/whats_the_best_model_for_playing_a_role_right_now/) (Score: 1)
    *   The thread is asking about the best model for playing a role right now that will fit on 8gbvram.
17. [Is Qwen the new face of local LLMs?](https://www.reddit.com/r/LocalLLaMA/comments/1l47dav/is_qwen_the_new_face_of_local_llms/) (Score: 1)
    *   The thread is about if Qwen is the new face of local LLMs.
18. [smollm is crazy](https://v.redd.it/l1u09vctg55f1) (Score: 0)
    *   The thread is about smollm.

# Detailed Analysis by Thread
**[[D] Sparse Transformers: Run 2x faster LLM with 30% lesser memory (Score: 152)](https://github.com/NimbleEdge/sparse_transformers)**
*  **Summary:** The discussion centers around "Sparse Transformers," a method to accelerate LLMs (Large Language Models) while reducing memory footprint. Users inquire about quality degradation, compatibility with torch.compile, and potential benefits for llama.cpp and GGUF models. Concerns about a broken link in the README were raised, as well as the potential for further compression with quantization.
*  **Emotion:** The overall emotional tone is positive due to the excitement and interest in the new technology. However, there are variations with neutral comments asking technical questions.
*  **Top 3 Points of View:**
    *   **Optimistic about Performance:**  Users express excitement about potential speed gains for real-time applications.
    *   **Inquisitive about Implementation:**  Users have questions about compatibility with existing frameworks and hardware.
    *   **Concerned about Quality:**  Users want to know about potential quality degradation resulting from the sparse transformation.

**[BAIDU joined huggingface (Score: 132)](https://huggingface.co/baidu)**
*   **Summary:** This thread discusses BAIDU joining huggingface. Some express hope for open-source Ernie models, while others are wary given Baidu's reputation.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   Hope that BAIDU will release open source Ernie models.
    *   Baidu is notorious even in China.
    *   Hope someone releases a competing BERT model.

**[DeepSeek’s new R1-0528-Qwen3-8B is the most intelligent 8B parameter model yet, but not by much: Alibaba’s own Qwen3 8B is just one point behind (Score: 52)](https://www.reddit.com/r/LocalLLaMA/comments/1l41p1x/deepseeks_new_r10528qwen38b_is_the_most/)**
*   **Summary:** The discussion revolves around the intelligence of DeepSeek’s new R1-0528-Qwen3-8B model compared to Alibaba’s Qwen3 8B. Users debate the reliability of benchmarks, the definition of "intelligence" in models, and the subjective performance of different models in specific tasks like coding and writing.
*   **Emotion:** The thread has a mixed emotional tone. There is excitement about new models, but also skepticism about benchmarks and their real-world implications. Some express negative sentiments about benchmark reliability and reaching intelligence limits for small-sized models.
*   **Top 3 Points of View:**
    *   **Skepticism about Benchmarks:**  Some users believe that benchmarks are unreliable and don't reflect real-world performance.
    *   **Subjective Performance Differences:**  Users note that different models perform better in different tasks, like coding vs. writing.
    *   **Definition of Intelligence:**  Some users question what "intelligence" means in the context of these models.

**[What's the cheapest setup for running full Deepseek R1 (Score: 46)](https://www.reddit.com/r/LocalLLaMA/comments/1l40ip8/whats_the_cheapest_setup_for_running_full/)**
*   **Summary:** The thread is about finding the most affordable hardware to run the Deepseek R1 model. Various suggestions are offered, including dual-socket setups, DDR5 Epyc, Mac Studio Ultra, and combinations of GPUs and RAM.
*   **Emotion:** The emotional tone is primarily neutral, with some negative sentiments regarding the feasibility of achieving high throughput with CPU-only setups and the high cost of RAM. Some positive sentiment is expressed towards the hustle for efficient solutions.
*   **Top 3 Points of View:**
    *   **DDR4 Epyc is a bad plan:** Dual DDR4 Epyc is a bad plan. A second CPU gives marginal gains if any. Go for a single (single) DDR5 Epyc with 12 memory channels.
    *   **Mac Studio Ultra:** The cheapest way would be to get a mac studio ultra with 512GB of unified ram and run it at 4-bit.
    *   **CPU and GPU mix:** You will Have to have a combo of as much GPU you can fit and then cover the rest with RAM.

**[Best world knowledge model that can run on your phone (Score: 28)](https://www.reddit.com/r/LocalLLaMA/comments/1l3z2m3/best_world_knowledge_model_that_can_run_on_your/)**
*   **Summary:** The thread discusses which LLMs are best for running on a phone and having access to world knowledge. Some users recommend specific models (e.g., Qwen 2.5/2 7B VL, Gemma 3n 4b) and applications (e.g., MNN-Chat). Others suggest using offline resources like Kiwix.
*   **Emotion:** The emotional tone is mixed, with a general positive sentiment towards the idea of having world knowledge on a phone, but also realism about the limitations of current LLMs.
*   **Top 3 Points of View:**
    *   **Using LLMs on phones has limitations:** Llms on phones are barely worth having at all.
    *   **Use external sources of knowledge:** Download a library from kiwix, download pdfs/ebooks, and save a repo to a folder on your phone.
    *   **Try a specific model:** Try Qwen 2.5/2 7B VL, it runs in MNN-Chat app.

**[Qwen3-32b /nothink or qwen3-14b /think? (Score: 15)](https://www.reddit.com/r/LocalLLaMA/comments/1l3yjeb/qwen332b_nothink_or_qwen314b_think/)**
*   **Summary:**  The thread asks for opinions on whether to use Qwen3-32b /nothink or Qwen3-14b /think. Most user suggest to use a different model.
*   **Emotion:**  The overall emotional tone is positive with excitement around the new technology.
*   **Top 3 Points of View:**
    *   Qwen3 14b is shockingly good.
    *   32B /nothink for code, 30B-A3B in rambling mode for almost everything else.
    *   If you have the VRAM, 30B-AB3 Think is the best of both worlds.

**[I wrote a little script to automate commit messages (Score: 13)](https://i.redd.it/shflqezx845f1.png)**
*   **Summary:** The thread discusses a script the poster wrote to automate commit messages.
*   **Emotion:**  The overall emotional tone is positive with excitement around the new technology.
*   **Top 3 Points of View:**
    *   Automating commit messages is a really nice idea.
    *   You might get even better results by adding examples in your prompt.
    *   This would slot nicely into a pre-commit hook.

**[Non-reasoning Qwen3-235B worse than maverick? Is this experience real with you guys? (Score: 7)](https://www.reddit.com/r/LocalLLaMA/comments/1l3yamg/nonreasoning_qwen3235b_worse_than_maverick_is/)**
*   **Summary:**  This thread explores the performance of the Non-reasoning Qwen3-235B model compared to the Maverick model.
*   **Emotion:**  The overall emotional tone is negative.
*   **Top 3 Points of View:**
    *   Qwen lacks knowledge but it works for most things I've thrown at it.
    *   Depends on use case. For general purpose, I found non-thinking 235B to be about on par with Maverick. But Maverick is a worse coder even when thinking is disabled on 235B.
    *   That's highly dependent on use cases in my experience.

**[Hybrid setup for reasoning (Score: 7)](https://www.reddit.com/r/LocalLLaMA/comments/1l40gij/hybrid_setup_for_reasoning/)**
*   **Summary:** The thread is about a hybrid setup for reasoning, using a smaller model for thinking and a larger model for making the output pretty.
*   **Emotion:** The emotional tone is mixed, with positive sentiment towards the concept and negative sentiment due to it being slower.
*   **Top 3 Points of View:**
    *   Hybrid setup is not a bad idea.
    *   "I don't like to wait for it to load" - Your proposed solution will be slower than just Qwen3-30B-A3B for the whole thing.
    *   Have you checked out speculative decoding?

**[How can I connect to a local LLM from my iPhone? (Score: 6)](https://www.reddit.com/r/LocalLLaMA/comments/1l4450t/how_can_i_connect_to_a_local_llm_from_my_iphone/)**
*   **Summary:** The thread is about the ways to connect to a local LLM from an iPhone. Some users suggest using apps that support custom endpoints, while others recommend using tools like Ngrok or Cloudflare tunnel to expose the localhost address securely.
*   **Emotion:** The emotional tone is mainly neutral, with some positive sentiment towards the solutions provided.
*   **Top 3 Points of View:**
    *   [Pal Chat](https://apps.apple.com/us/app/pal-chat-ai-chat-client/id6447545085) seems to support your use case
    *   You can expose a localhost address to the internet using something like Ngrok or Cloudflare tunnel.
    *   A new app will be released in the next few days.

**[Is it dumb to build a server with 7x 5060 Ti? (Score: 6)](https://www.reddit.com/r/LocalLLaMA/comments/1l48cnk/is_it_dumb_to_build_a_server_with_7x_5060_ti/)**
*   **Summary:** The thread discusses whether it's a good idea to build a server with 7x 5060 Ti GPUs for local LLM hosting, with users offering alternative suggestions and considerations.
*   **Emotion:** The overall emotional tone is neutral, with a hint of skepticism about the practicality of the proposed build.
*   **Top 3 Points of View:**
    *   Suggests considering a Mac M3 Ultra.
    *   Questions the usefulness of having more VRAM than available models.
    *   Recommends using Ubuntu instead of Windows.

**[With 8gb vram: qwen3 8b q6 or 32b iq1? (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1l47fv0/with_8gb_vram_qwen3_8b_q6_or_32b_iq1/)**
*   **Summary:** The thread compares the performance of Qwen3 8b q6 and 32b iq1 with 8gb vram.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 3 Points of View:**
    *   8b q6 (or maybe 14b q4)
        Iq1 are barely usable

**[Looking for Advice: Best LLM/Embedding Models for Precise Document Retrieval (Product Standards) (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1l3xxpw/looking_for_advice_best_llmembedding_models_for/)**
*   **Summary:** The thread seeks advice on the best LLM/Embedding Models for Precise Document Retrieval (Product Standards).
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 3 Points of View:**
    *   try finetuning the llm on your knowledge base if you are using a os one.
    *   use visual document retrieval model if feasible (this improved my accuracy a lot)
    *   try hybrid rag [https://github.com/SciPhi-AI/R2R](https://github.com/SciPhi-AI/R2R)

**[4090 boards with 48gb Ram - will there ever be an upgrade service? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1l3ykjn/4090_boards_with_48gb_ram_will_there_ever_be_an/)**
*   **Summary:** The thread is about if there will ever be an upgrade service for 4090 boards with 48gb Ram.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 3 Points of View:**
    *   Since they transplant the cores to a different PCB afaik, probably not.
    *   Sent them my 4090s, they stress tested them as a quality check, paid an upgrade fee, and they sent me 48GB 4090s.
    *   Not from Nvidia or any of their official partners

**[Looking for UI that can store and reference characters easily (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1l42woy/looking_for_ui_that_can_store_and_reference/)**
*   **Summary:** The thread is looking for UI that can store and reference characters easily.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 3 Points of View:**
    *   Not really, sillytavern mostly

**[What's the best model for playing a role right now , that will fit on 8gbvram? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1l45p2d/whats_the_best_model_for_playing_a_role_right_now/)**
*   **Summary:** The thread is asking about the best model for playing a role right now that will fit on 8gbvram.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 3 Points of View:**
    *   Pick a base model itself. Assume every fine-tune mega merge is done by a clueless teenager.

**[Is Qwen the new face of local LLMs? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1l47dav/is_qwen_the_new_face_of_local_llms/)**
*   **Summary:** The thread is about if Qwen is the new face of local LLMs.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 3 Points of View:**
    *   It has been since Qwen2.5 imo
    *   While I don’t evaluate myself my manager just tasked me to use the new qwen 3 model quantized version
    *   Small models have different strengths. For fiction Gemma3, GLM-4 or mistral Nemo all are better than qwen.

**[smollm is crazy (Score: 0)](https://v.redd.it/l1u09vctg55f1)**
*   **Summary:** The thread is about smollm.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 3 Points of View:**
    *   Whoa.  (And not in a good way either)
    *   It's a coherent model, which is by impressive by itself, but not really useful without any finetuning, it's not even able to tell what is the number after 1
    *   Instructions unclear, -rm -rf /
