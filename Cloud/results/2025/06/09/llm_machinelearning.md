---
title: "Machine Learning Subreddit"
date: "2025-06-09"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "ML"]
---

# Overall Ranking and Top Discussions
1.  [[P][R] Sparse Transformers: Run 2x faster LLM with 30% lesser memory](https://www.reddit.com/r/MachineLearning/comments/1l74fv7/pr_sparse_transformers_run_2x_faster_llm_with_30/) (Score: 36)
    *   This thread discusses a new sparse transformer architecture that allows for faster LLM inference with less memory usage.
2.  [[D] What underrated ML techniques are better than the defaults](https://www.reddit.com/r/MachineLearning/comments/1l7bo8h/d_what_underrated_ml_techniques_are_better_than/) (Score: 33)
    *   This thread explores underrated machine learning techniques that are potentially more effective than the standard or default approaches.
3.  [[D] ML Engineer Routine: What Am I Missing?](https://www.reddit.com/r/MachineLearning/comments/1l75f3o/d_ml_engineer_routine_what_am_i_missing/) (Score: 30)
    *   This thread discusses the daily routines and responsibilities of a machine learning engineer, with users sharing their experiences and advice.
4.  [[R][D] Let’s Fork Deep Learning: The Hidden Symmetry Bias No One Talks About](https://www.reddit.com/r/MachineLearning/comments/1l726bp/rd_lets_fork_deep_learning_the_hidden_symmetry/) (Score: 20)
    *   This thread explores the concept of symmetry bias in deep learning models and proposes a new approach to address it.
5.  [[D] JMLR Publishing procedure](https://www.reddit.com/r/MachineLearning/comments/1l70jrl/d_jmlr_publishing_procedure/) (Score: 7)
    *   This thread discusses the publishing process for the Journal of Machine Learning Research (JMLR), with users sharing their experiences regarding the speed and rigor of the review process.
6.  [[D] BMVC 2025 Reviews Discussion](https://www.reddit.com/r/MachineLearning/comments/1l71jng/d_bmvc_2025_reviews_discussion/) (Score: 4)
    *   This thread discusses the review process and timeline for the British Machine Vision Conference (BMVC) 2025.
7.  [A good reminder for reductionists to not get too ambitious with their dismissive concrete claims. We are still actively exploring the true nature of how these models function day-to-day](https://www.anthropic.com/research/tracing-thoughts-language-model) (Score: 0)
    *   This thread discusses the functionality of machine learning models.
8.  [[P] Why does my AI finally stop making things up? (Open Source COMPASS approach inside)](https://www.reddit.com/r/MachineLearning/comments/1l6n3oe/p_why_does_my_ai_finally_stop_making_things_up/) (Score: 0)
    *   This thread discusses the COMPASS approach to preventing AI from hallucinating.
9.  [[D] 100% proof AI cant and wont ever create anything new](https://www.reddit.com/r/MachineLearning/comments/1l6zgii/d_100_proof_ai_cant_and_wont_ever_create_anything/) (Score: 0)
    *   This thread debates whether AI can create anything new.
10. [[D] Is Google colab pro+ sufficient for my project?](https://www.reddit.com/r/MachineLearning/comments/1l7a5xf/d_is_google_colab_pro_sufficient_for_my_project/) (Score: 0)
    *   This thread is about whether Google Colab Pro+ is sufficient for the user's project.
11. [[P] [D] Thesis: Offer generator](https://www.reddit.com/r/MachineLearning/comments/1l7b8co/p_d_thesis_offer_generator/) (Score: 0)
    *   This thread is about generating thesis offers.
12. [Discussion] My Thesis: The United States can be modeled as a neural network, and it's suffering from catastrophic overfitting and data poisoning.](https://www.reddit.com/r/MachineLearning/comments/1l7d3gx/discussion_my_thesis_the_united_states_can_be/) (Score: 0)
    *   This thread compares The United States to a neural network, and that it is suffering from catastrophic overfitting and data poisoning.

# Detailed Analysis by Thread

**[[P][R] Sparse Transformers: Run 2x faster LLM with 30% lesser memory](https://www.reddit.com/r/MachineLearning/comments/1l74fv7/pr_sparse_transformers_run_2x_faster_llm_with_30/) (Score: 36)**
*   **Summary:** This thread discusses a new sparse transformer architecture that allows for faster LLM inference with less memory usage. The discussion includes a link to the Github project. People are curious about accuracy degradation and interpretability.
*   **Emotion:** The overall emotional tone of the thread is neutral, with users expressing curiosity and offering congratulations on the release.
*   **Top 3 Points of View:**
    *   The sparse transformers run 2x faster with 30% less memory.
    *   Increased model sparsity should make it easier to disentangle features.
    *   There's curiosity on how much accuracy degradation is found when applying DejaVu to SwiGLU-based LLMs.

**[[D] What underrated ML techniques are better than the defaults](https://www.reddit.com/r/MachineLearning/comments/1l7bo8h/d_what_underrated_ml_techniques_are_better_than/) (Score: 33)**
*   **Summary:** This thread explores underrated machine learning techniques that are potentially more effective than the standard or default approaches. Discussion points include feature engineering, ensemble methods, hyperparameter tuning, and training the final model on validation and test data before deployment.
*   **Emotion:** The overall emotional tone of the thread is neutral, with users offering advice and sharing their experiences.
*   **Top 3 Points of View:**
    *   Feature engineering yields the best performance.
    *   Ensemble methods give better results than individual models.
    *   Training the final model on validation and test data before deployment.

**[[D] ML Engineer Routine: What Am I Missing?](https://www.reddit.com/r/MachineLearning/comments/1l75f3o/d_ml_engineer_routine_what_am_i_missing/) (Score: 30)**
*   **Summary:** This thread discusses the daily routines and responsibilities of a machine learning engineer, with users sharing their experiences and advice. Topics include data acquisition and cleaning, model training and evaluation, deployment, and monitoring. The discussion highlights the difference between traditional software engineering and ML engineering, as well as the importance of team maturity in software development.
*   **Emotion:** The overall emotional tone of the thread is neutral, with some users expressing concern about the ambiguity and challenges of ML engineering.
*   **Top 3 Points of View:**
    *   Training production-ready models is harder than people think.
    *   The culture in software development has matured, but ML is still catching up.
    *   Ambiguity is a big difference between traditional software engineering and ML engineering.

**[[R][D] Let’s Fork Deep Learning: The Hidden Symmetry Bias No One Talks About](https://www.reddit.com/r/MachineLearning/comments/1l726bp/rd_lets_fork_deep_learning_the_hidden_symmetry/) (Score: 20)**
*   **Summary:** This thread explores the concept of symmetry bias in deep learning models and proposes a new approach to address it. Users discuss the theoretical foundations of the idea, potential applications, and the need for experimental validation. There is also some skepticism about the clarity and rigor of the presented work.
*   **Emotion:** The overall emotional tone of the thread is neutral, with a mix of curiosity, skepticism, and interest in further exploration.
*   **Top 3 Points of View:**
    *   Symmetry is one of the reasons why NN models are readily trainable.
    *   Next steps would be to prove the isotropic approach improves the model quality.
    *   The paper contains far more speculation than concrete arguments or empirical results.

**[[D] JMLR Publishing procedure](https://www.reddit.com/r/MachineLearning/comments/1l70jrl/d_jmlr_publishing_procedure/) (Score: 7)**
*   **Summary:** This thread discusses the publishing process for the Journal of Machine Learning Research (JMLR), with users sharing their experiences regarding the speed and rigor of the review process. The consensus is that JMLR is very slow.
*   **Emotion:** The overall emotional tone of the thread is negative, with users expressing frustration about the slow turnaround time for reviews.
*   **Top 3 Points of View:**
    *   JMLR is painfully slow.
    *   Expect that it will take a year to get the first response.
    *   Once submitted to JMLR, forget about it.

**[[D] BMVC 2025 Reviews Discussion](https://www.reddit.com/r/MachineLearning/comments/1l71jng/d_bmvc_2025_reviews_discussion/) (Score: 4)**
*   **Summary:** This thread discusses the review process and timeline for the British Machine Vision Conference (BMVC) 2025.
*   **Emotion:** The overall emotional tone of the thread is neutral.
*   **Top 1 Points of View:**
    *   BMVC 2025 does not have an author-reviewer discussion period.

**[A good reminder for reductionists to not get too ambitious with their dismissive concrete claims. We are still actively exploring the true nature of how these models function day-to-day](https://www.anthropic.com/research/tracing-thoughts-language-model) (Score: 0)**
*   **Summary:** This thread discusses the functionality of machine learning models.
*   **Emotion:** The overall emotional tone of the thread is neutral.
*   **Top 1 Points of View:**
    *   The poster wants to know what people want to say about the post.

**[[P] Why does my AI finally stop making things up? (Open Source COMPASS approach inside)](https://www.reddit.com/r/MachineLearning/comments/1l6n3oe/p_why_does_my_ai_finally_stop_making_things_up/) (Score: 0)**
*   **Summary:** This thread discusses the COMPASS approach to preventing AI from hallucinating.
*   **Emotion:** The overall emotional tone of the thread is neutral.
*   **Top 2 Points of View:**
    *   Compass is a well-structured and prompt orchestration framework.
    *   The paper lacks any data, metrics, baselines, or statistical analysis.

**[[D] 100% proof AI cant and wont ever create anything new](https://www.reddit.com/r/MachineLearning/comments/1l6zgii/d_100_proof_ai_cant_and_wont_ever_create_anything/) (Score: 0)**
*   **Summary:** This thread debates whether AI can create anything new.
*   **Emotion:** The overall emotional tone of the thread is mixed.
*   **Top 3 Points of View:**
    *   AI cannot create anything new.
    *   AI can generate cartoon images that are not in any data set.
    *   Replace "AI" with "current LLMs."

**[[D] Is Google colab pro+ sufficient for my project?](https://www.reddit.com/r/MachineLearning/comments/1l7a5xf/d_is_google_colab_pro_sufficient_for_my_project/) (Score: 0)**
*   **Summary:** This thread is about whether Google Colab Pro+ is sufficient for the user's project.
*   **Emotion:** The overall emotional tone of the thread is negative.
*   **Top 2 Points of View:**
    *   Google Colab Pro+ is not sufficient for the project.
    *   The modern VLMs are well trained and advanced.

**[[P] [D] Thesis: Offer generator](https://www.reddit.com/r/MachineLearning/comments/1l7b8co/p_d_thesis_offer_generator/) (Score: 0)**
*   **Summary:** This thread is about generating thesis offers.
*   **Emotion:** The overall emotional tone of the thread is neutral.
*   **Top 3 Points of View:**
    *   The poster will help the user on this.
    *   What does success look like?
    *   A language model can produce the offer.

**[Discussion] My Thesis: The United States can be modeled as a neural network, and it's suffering from catastrophic overfitting and data poisoning.](https://www.reddit.com/r/MachineLearning/comments/1l7d3gx/discussion_my_thesis_the_united_states_can_be/) (Score: 0)**
*   **Summary:** This thread compares The United States to a neural network, and that it is suffering from catastrophic overfitting and data poisoning.
*   **Emotion:** The overall emotional tone of the thread is neutral.
*   **Top 3 Points of View:**
    *   The crackpots are out in full force.
    *   According to Damasio, people make decisions based on emotions and try to confirm it later.
    *   The ML model is a useful diagnostic framework.
