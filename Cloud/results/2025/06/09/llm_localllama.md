---
title: "LocalLLaMA Subreddit"
date: "2025-06-09"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "LocalAI", "GPU"]
---

# Overall Ranking and Top Discussions
1.  [[D] KVzip: Query-agnostic KV Cache Eviction — 3~4× memory reduction and 2× lower decoding latency](https://i.redd.it/bpxlu6tfnw5f1.png) (Score: 234)
    *   Discussion about KVzip, a query-agnostic KV cache eviction method that reduces memory usage and decoding latency in LLMs.
2.  [DeepSeek R1 0528 Hits 71% (+14.5 pts from R1) on Aider Polyglot Coding Leaderboard](https://www.reddit.com/r/LocalLLaMA/comments/1l76ab7/deepseek_r1_0528_hits_71_145_pts_from_r1_on_aider/) (Score: 194)
    *   A post highlighting the improved performance of DeepSeek R1 0528 on the Aider Polyglot Coding Leaderboard, with users discussing its cost-effectiveness and comparing it to other models.
3.  [I built a Code Agent that writes code and live-debugs itself by reading and walking the call stack.](https://v.redd.it/b1pnpj9lsw5f1) (Score: 45)
    *   A user showcases a code agent capable of writing and debugging code, leading to discussions about its implementation, comparisons to existing tools, and potential for future automation.
4.  [China starts mass producing a Ternary AI Chip.](https://www.reddit.com/r/LocalLLaMA/comments/1l7dj3z/china_starts_mass_producing_a_ternary_ai_chip/) (Score: 30)
    *   Discussion around China's mass production of a ternary AI chip, with skepticism about its performance and software compatibility.
5.  [Why isn't it common for companies to compare the evaluation of the different quantizations of their model?](https://www.reddit.com/r/LocalLLaMA/comments/1l748qc/why_isnt_it_common_for_companies_to_compare_the/) (Score: 18)
    *   Users discuss why companies don't typically compare different quantization levels of their models, citing factors like marketing incentives, lack of thoroughness, and assumptions about users' GPU capabilities.
6.  [7900 XTX what are your go-to models for 24GB VRAM?](https://www.reddit.com/r/LocalLLaMA/comments/1l76cg2/7900_xtx_what_are_your_goto_models_for_24gb_vram/) (Score: 8)
    *   Users share their preferred LLMs for the 7900 XTX GPU with 24GB of VRAM, including Gemma3-27b and various Qwen models.
7.  [Apple Intelligence on device model available to developers](https://www.apple.com/newsroom/2025/06/apple-intelligence-gets-even-more-powerful-with-new-capabilities-across-apple-devices/) (Score: 7)
    *   Users are discussing text extraction of iphone
8.  [Lightweight writing model as of June 2025](https://www.reddit.com/r/LocalLLaMA/comments/1l7ab18/lightweight_writing_model_as_of_june_2025/) (Score: 3)
    *   Users share their opinions on lightweight writing models.
9.  [Dual RTX8000 48GB vs. Dual RTX3090 24GB](https://www.reddit.com/r/LocalLLaMA/comments/1l7asxt/dual_rtx8000_48gb_vs_dual_rtx3090_24gb/) (Score: 3)
    *   Discussion on whether to choose dual RTX8000 (48GB) or dual RTX3090 (24GB) GPUs, focusing on VRAM capacity and performance considerations.
10. [RAG - Usable for my application?](https://www.reddit.com/r/LocalLLaMA/comments/1l7d9gf/rag_usable_for_my_application/) (Score: 3)
    *   This thread discusses the use of Retrieval-Augmented Generation (RAG) for legal applications, with advice on embedding models, search techniques, and reranking.
11. [Dolphin appreciation post.](https://i.redd.it/9w0uktkaxw5f1.jpeg) (Score: 1)
    *   Users express their appreciation for the Dolphin model, with some newer users unfamiliar with it.
12. [Translation models that support streaming](https://www.reddit.com/r/LocalLLaMA/comments/1l78eb8/translation_models_that_support_streaming/) (Score: 1)
    *   Users discuss the ability of translation models to support streaming and backends.
13. [Winter has arrived](https://www.reddit.com/r/LocalLLaMA/comments/1l76sg1/winter_has_arrived/) (Score: 0)
    *   Users are discussing the limit of transformer architecture
14. [Build a full on-device rag app using qwen3 embedding and qwen3 llm](https://www.reddit.com/r/LocalLLaMA/comments/1l76tvu/build_a_full_ondevice_rag_app_using_qwen3/) (Score: 0)
    *   Users are commenting on an app using qwen3
15. [Models and where to find them?](https://www.reddit.com/r/LocalLLaMA/comments/1l77g2z/models_and_where_to_find_them/) (Score: 0)
    *   Users are suggesting where to find models
16. [Fully Offline AI Computer (works standalone or online)](https://www.reddit.com/r/LocalLLaMA/comments/1l78v5g/fully_offline_ai_computer_works_standalone_or/) (Score: 0)
    *   Users are suspicious of specs and metrics
17. [Is there a DeepSeek-R1-0528 14B or just DeepSeek-R1 14B that I can download and run via vLLM?](https://www.reddit.com/r/LocalLLaMA/comments/1l79frx/is_there_a_deepseekr10528_14b_or_just_deepseekr1/) (Score: 0)
    *   Users are sharing that there is only one original DeepSeek-R1 with 685B parameters
18. [Good pc build specs for 5090](https://www.reddit.com/r/LocalLLaMA/comments/1l79ksy/good_pc_build_specs_for_5090/) (Score: 0)
    *   Users are sharing their opinions on good PC build specs for 5090

# Detailed Analysis by Thread
**[KVzip: Query-agnostic KV Cache Eviction — 3~4× memory reduction and 2× lower decoding latency (Score: 234)](https://i.redd.it/bpxlu6tfnw5f1.png)**
*  **Summary:** This thread discusses KVzip, a new method for KV cache eviction that promises significant memory reduction and faster decoding. Users are debating its testing methodology, potential applications, and integration with existing tools.
*  **Emotion:** The overall emotional tone is Neutral, with some Positive excitement about the potential of the technology. There's also some Negative concern about the testing methodology and implementation challenges.
*  **Top 3 Points of View:**
    *   KVzip is a revolutionary technology that could significantly improve LLM performance.
    *   The current testing methodology is flawed and doesn't accurately reflect real-world performance.
    *   Integration with existing tools like llama.cpp and vLLM is crucial for adoption.

**[DeepSeek R1 0528 Hits 71% (+14.5 pts from R1) on Aider Polyglot Coding Leaderboard (Score: 194)](https://www.reddit.com/r/LocalLLaMA/comments/1l76ab7/deepseek_r1_0528_hits_71_145_pts_from_r1_on_aider/)**
*  **Summary:**  This thread discusses the impressive performance of DeepSeek R1 0528 on the Aider Polyglot Coding Leaderboard. Users highlight its cost-effectiveness compared to other LLMs and speculate on its potential impact.
*  **Emotion:** The overall emotional tone is Positive, with users expressing excitement and admiration for the model's performance and value. There's also some Neutral discussion about its limitations and comparisons to other models.
*  **Top 3 Points of View:**
    *   DeepSeek R1 0528 is a significant upgrade and a cost-effective alternative to other LLMs.
    *   The Aider Polyglot Coding Leaderboard may not be a perfect benchmark.
    *   The speed of the model needs to be considered along with its accuracy.

**[I built a Code Agent that writes code and live-debugs itself by reading and walking the call stack. (Score: 45)](https://v.redd.it/b1pnpj9lsw5f1)**
*  **Summary:** This thread showcases a code agent built by the poster that can write code and debug itself by reading the call stack. The discussion revolves around its implementation, potential, and comparisons to other tools.
*  **Emotion:** The overall emotional tone is Neutral, with some Positive interest in the project and questions about its functionality. There's also some Negative concern about the approach and potential issues.
*  **Top 3 Points of View:**
    *   The code agent is an interesting and innovative approach to automated coding and debugging.
    *   It's important to understand the implementation details and compare it to existing tools like Kilocode.
    *   The agent's ability to handle real-world messy setups needs to be demonstrated.

**[China starts mass producing a Ternary AI Chip. (Score: 30)](https://www.reddit.com/r/LocalLLaMA/comments/1l7dj3z/china_starts_mass_producing_a_ternary_ai_chip/)**
*  **Summary:**  This thread discusses China's mass production of a ternary AI chip. Users are skeptical about the chip's performance and the challenges of building software for it.
*  **Emotion:** The overall emotional tone is Neutral, with a hint of Negative skepticism.
*  **Top 3 Points of View:**
    *   The ternary AI chip could potentially outperform Nvidia GPUs.
    *   Building software for a ternary chip would be a major challenge.
    *   The news should be viewed with skepticism until independent reviews are available.

**[Why isn't it common for companies to compare the evaluation of the different quantizations of their model? (Score: 18)](https://www.reddit.com/r/LocalLLaMA/comments/1l748qc/why_isnt_it_common_for_companies_to_compare_the/)**
*  **Summary:** This thread explores the reasons why companies don't commonly compare the evaluation of different quantizations of their models. Users discuss factors such as marketing incentives, the cost of benchmarking, and the limited popularity of quantization in industrial applications.
*  **Emotion:** The overall emotional tone is Neutral.
*  **Top 3 Points of View:**
    *   Companies prioritize showcasing the highest possible score for marketing purposes.
    *   Quantization is primarily used for optimization in specific tasks.
    *   The cost of running all benchmarks for different quantizations is significant.

**[7900 XTX what are your go-to models for 24GB VRAM? (Score: 8)](https://www.reddit.com/r/LocalLLaMA/comments/1l76cg2/7900_xtx_what_are_your_goto_models_for_24gb_vram/)**
*  **Summary:** This thread is a request for model recommendations for 24GB of VRAM, specifically the 7900 XTX.
*  **Emotion:** The overall emotional tone is Neutral
*  **Top 3 Points of View:**
    *  Gemma3-27b is recommended
    *  Qwen3:32b from unsloth is a good option
    *  Qwen32 with k/v cache set to q8 is another option

**[Apple Intelligence on device model available to developers (Score: 7)](https://www.apple.com/newsroom/2025/06/apple-intelligence-gets-even-more-powerful-with-new-capabilities-across-apple-devices/)**
*  **Summary:** Local text extraction has already been so good for so long on the iPhone.
*  **Emotion:** The overall emotional tone is Positive
*  **Top 3 Points of View:**
    * Local text extraction has already been so good for so long on the iPhone

**[Lightweight writing model as of June 2025 (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1l7ab18/lightweight_writing_model_as_of_june_2025/)**
*  **Summary:** A request for suggestions on lightweight writing models.
*  **Emotion:** The overall emotional tone is Neutral
*  **Top 3 Points of View:**
    *  Mistral Nemo is suggested.
    *  Mistral Small 22b (not 24b) is suggested.
    *  GLM-4 32b is suggested

**[Dual RTX8000 48GB vs. Dual RTX3090 24GB (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1l7asxt/dual_rtx8000_48gb_vs_dual_rtx3090_24gb/)**
*  **Summary:** This thread discusses the pros and cons of using dual RTX8000 48GB cards versus dual RTX3090 24GB cards for local LLM development, weighing VRAM capacity against compute performance and power consumption.
*  **Emotion:** The overall emotional tone is Neutral
*  **Top 3 Points of View:**
    *  Prioritize VRAM for larger models and longer contexts.
    *  RTX 8000s consume less power.
    *  Consider cost effectiveness and availability of the cards.

**[RAG - Usable for my application? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1l7d9gf/rag_usable_for_my_application/)**
*  **Summary:** The discussion is centered around whether RAG can be used to answer legal questions.
*  **Emotion:** The overall emotional tone is Neutral
*  **Top 3 Points of View:**
    *  RAG is a good tool for answering legal questions.
    *  Try different embedding models, rather aim for something bigger or fine-tuned especially for law-domain.
    *  Hybrid search may be a really good improvement - try combination like dense model + bm25 or Splade.

**[Dolphin appreciation post. (Score: 1)](https://i.redd.it/9w0uktkaxw5f1.jpeg)**
*  **Summary:** The post is an appreciation post for Dolphin
*  **Emotion:** The overall emotional tone is Positive
*  **Top 3 Points of View:**
    *  Users are appreciating Dolphin

**[Translation models that support streaming (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1l78eb8/translation_models_that_support_streaming/)**
*  **Summary:** Users discuss the ability of translation models to support streaming and backends.
*  **Emotion:** The overall emotional tone is Neutral
*  **Top 3 Points of View:**
    *  Every model is able to do streaming.
    *  Streaming come from your backend.

**[Winter has arrived (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1l76sg1/winter_has_arrived/)**
*  **Summary:** Users are discussing the limit of transformer architecture
*  **Emotion:** The overall emotional tone is Neutral
*  **Top 3 Points of View:**
    *  Someone should design LLMs for conversations from the ground-up.
    *  Transformer architecture is reaching its limits
    *  There is a new gemini every month and we just got a new r1

**[Build a full on-device rag app using qwen3 embedding and qwen3 llm (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1l76tvu/build_a_full_ondevice_rag_app_using_qwen3/)**
*  **Summary:** Users are commenting on an app using qwen3
*  **Emotion:** The overall emotional tone is Positive
*  **Top 3 Points of View:**
    *  Qwen3 0.6B is a surprisingly valuable little workhorse.

**[Models and where to find them? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1l77g2z/models_and_where_to_find_them/)**
*  **Summary:** Users are suggesting where to find models
*  **Emotion:** The overall emotional tone is Neutral
*  **Top 3 Points of View:**
    *  Huggingface and ollama are mostly used
    *  Hugging face unsloth is
    *  Follow ubergarm, unsloth, bartowski, mradermacher, and MaziyarPanahi as well as lmstudio or ollama etc.

**[Fully Offline AI Computer (works standalone or online) (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1l78v5g/fully_offline_ai_computer_works_standalone_or/)**
*  **Summary:** Users are suspicious of specs and metrics
*  **Emotion:** The overall emotional tone is Neutral
*  **Top 3 Points of View:**
    *  Need to show the details of the config, data and benchmarks and maybe your post will be of some interest
    *  GMtek mini PC's are coming at around 2K USD and the upcoming Nvidia GB10 blackwell based systems are coming around 3K USD
    *  Hey guys do you want to buy a $1k+  computer with NO SPECS from a redditor??

**[Is there a DeepSeek-R1-0528 14B or just DeepSeek-R1 14B that I can download and run via vLLM? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1l79frx/is_there_a_deepseekr10528_14b_or_just_deepseekr1/)**
*  **Summary:** Users are sharing that there is only one original DeepSeek-R1 with 685B parameters
*  **Emotion:** The overall emotional tone is Neutral
*  **Top 3 Points of View:**
    *  You have been lied by Ollama. They are famous from changing model names and messing everything up.
    *  They only trained one distilled version atop Qwen3-8B this time.
    * unofficial R1-0528 distills with varying levels of quality, gemma 3 based.

**[Good pc build specs for 5090 (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1l79ksy/good_pc_build_specs_for_5090/)**
*  **Summary:** Users are sharing their opinions on good PC build specs for 5090
*  **Emotion:** The overall emotional tone is Neutral
*  **Top 3 Points of View:**
    *  If you're serious about AI and LLMs you should get a threadripper pro 7000 series setup built around nice workstation board with DDR5 RAM
    *  Basically everything other than the card is irrelevant when it comes to LLMs
    *  With a 5090 I'd recommend at least 96 GB of RAM

