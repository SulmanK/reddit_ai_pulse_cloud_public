---
title: "LocalLLaMA Subreddit"
date: "2025-06-18"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["localllama", "AI", "LLM"]
---

# Overall Ranking and Top Discussions
1. [[D] new 72B and 70B models from Arcee](https://www.reddit.com/r/LocalLLaMA/comments/1lenf36/new_72b_and_70b_models_from_arcee/) (Score: 33)
    *   Discussion about the release of new 72B and 70B models from Arcee, including their architecture, licensing, and potential performance.
2.  [OpenAI found features in AI models that correspond to different ‘personas’](https://www.reddit.com/r/LocalLLaMA/comments/1leod7d/openai_found_features_in_ai_models_that/) (Score: 20)
    *   Discussion about OpenAI's finding of distinct "personas" within AI models, drawing parallels to previous observations about GPT-4 extracting millions of concepts.
3.  [We took Qwen3 235B A22B from 34 tokens/sec to 54 tokens/sec by switching from llama.cpp with Unsloth dynamic Q4_K_M GGUF to vLLM with INT4 w4a16](https://www.reddit.com/r/LocalLLaMA/comments/1lemmsq/we_took_qwen3_235b_a22b_from_34_tokenssec_to_54/) (Score: 18)
    *   Discussion about increasing token generation speed in Qwen3 235B A22B by switching from llama.cpp with Unsloth dynamic Q4_K_M GGUF to vLLM with INT4 w4a16, and the implications of different quantization methods and algorithms.
4.  [gemini-2.5-flash-lite-preview-06-17 performance on IDP Leaderboard](https://www.reddit.com/r/LocalLLaMA/comments/1lekndj/gemini25flashlitepreview0617_performance_on_idp/) (Score: 9)
    *   Discussion about the performance of Gemini 2.5 Flash Lite on the IDP Leaderboard, with questions about the non-lite version and reasoning capabilities.
5.  [Best non-Chinese open models?](https://www.reddit.com/r/LocalLLaMA/comments/1lel886/best_nonchinese_open_models/) (Score: 8)
    *   Discussion about the best open-source language models that are not Chinese, with recommendations including Mistral, Llama 3, and others.
6.  [Joycap-beta with llama.cpp](https://www.reddit.com/r/LocalLLaMA/comments/1lencvg/joycapbeta_with_llamacpp/) (Score: 3)
    *   Discussion about Joycap-beta and how it works with llama.cpp
7.  [Mobile Phones are becoming better at running AI locally on the device.](https://www.reddit.com/r/LocalLLaMA/comments/1lepjc5/mobile_phones_are_becoming_better_at_running_ai/) (Score: 3)
    *   Discussion about how mobile phones are becoming better at running AI locally on the device
8.  [How much does it cost ai companies to train xbillion amount of parameters?](https://www.reddit.com/r/LocalLLaMA/comments/1leoej7/how_much_does_it_cost_ai_companies_to_train/) (Score: 2)
    *   Discussion about the cost of training large language models, with considerations for GPU memory, modeling techniques, and optimization.
9.  [Development environment setup](https://www.reddit.com/r/LocalLLaMA/comments/1leml1x/development_environment_setup/) (Score: 1)
    *   Discussion about setting up a development environment for local LLMs.
10. [Daily Paper Discussions on the Yannic Kilcher Discord -> V-JEPA 2](https://www.reddit.com/r/LocalLLaMA/comments/1leoy4x/daily_paper_discussions_on_the_yannic_kilcher/) (Score: 1)
    *   This post provides a link to a daily paper discussion on the Yannic Kilcher Discord server, focusing on V-JEPA 2.
11. [Lorras for LLMs](https://www.reddit.com/r/LocalLLaMA/comments/1leoyg3/lorras_for_llms/) (Score: 1)
    *   This post discusses the use of LoRAs (Low-Rank Adaptation) for LLMs and provides a link to Unsloth for an easy starting point.
12. [self host minimax?](https://www.reddit.com/r/LocalLLaMA/comments/1leoyu2/self_host_minimax/) (Score: 1)
    *   This post asks about self-hosting Minimax.
13. [Which local API is the best to work with when developing local LLM apps for yourself?](https://www.reddit.com/r/LocalLLaMA/comments/1leqcc5/which_local_api_is_the_best_to_work_with_when/) (Score: 1)
    *   This post asks about which local API is best to work with when developing local LLM apps for yourself.
14. [MiCA – A new parameter-efficient fine-tuning method with higher knowledge uptake and less forgetting (beats LoRA in my tests)](https://www.reddit.com/r/LocalLLaMA/comments/1lek9yr/mica_a_new_parameterefficient_finetuning_method/) (Score: 0)
    *   Discussion about a new parameter-efficient fine-tuning method called MiCA, which claims to outperform LoRA in certain tests.
15. [lmarena not telling us chatbot names after battle](https://www.reddit.com/r/LocalLLaMA/comments/1lepgii/lmarena_not_telling_us_chatbot_names_after_battle/) (Score: 0)
    *   This post is about lmarena not telling chatbot names after battle.

# Detailed Analysis by Thread
**[[D] new 72B and 70B models from Arcee (Score: 33)](https://www.reddit.com/r/LocalLLaMA/comments/1lenf36/new_72b_and_70b_models_from_arcee/)**
*   **Summary:** Discussion about the release of new 72B and 70B models from Arcee, including their architecture, licensing, and potential performance.
*   **Emotion:** Predominantly Neutral, with some Positive sentiment expressing excitement about the release.
*   **Top 3 Points of View:**
    *   The models are releases of previously private proprietary models and are not necessarily SOTA.
    *   SuperNova-v1 is based on Llama 3.1 but is listed under the Apache 2.0 license.
    *   There is a lack of benchmarks demonstrating the effect of finetuning on the models.

**[OpenAI found features in AI models that correspond to different ‘personas’ (Score: 20)](https://www.reddit.com/r/LocalLLaMA/comments/1leod7d/openai_found_features_in_ai_models_that/)**
*   **Summary:** Discussion about OpenAI's finding of distinct "personas" within AI models, drawing parallels to previous observations about GPT-4 extracting millions of concepts.
*   **Emotion:** Predominantly Neutral.
*   **Top 3 Points of View:**
    *   OpenAI has found that GPT-4 extracted millions of concepts from the model.
    *   LLMs can't actually learn anything
    *   Ilya believes that this could be due to learning.

**[We took Qwen3 235B A22B from 34 tokens/sec to 54 tokens/sec by switching from llama.cpp with Unsloth dynamic Q4_K_M GGUF to vLLM with INT4 w4a16 (Score: 18)](https://www.reddit.com/r/LocalLLaMA/comments/1lemmsq/we_took_qwen3_235b_a22b_from_34_tokenssec_to_54/)**
*   **Summary:** Discussion about increasing token generation speed in Qwen3 235B A22B by switching from llama.cpp with Unsloth dynamic Q4_K_M GGUF to vLLM with INT4 w4a16, and the implications of different quantization methods and algorithms.
*   **Emotion:** Predominantly Neutral, with a mix of positive and negative sentiments regarding performance comparisons.
*   **Top 3 Points of View:**
    *   There's a difference between the quantization format and quantization algorithm.
    *   GPTQ quants can vary greatly depending on your calibration set and group size or act order.
    *   With multiple GPUs you shouldn't use llama.cpp anyway, but it does well when you want to do partial offload.

**[gemini-2.5-flash-lite-preview-06-17 performance on IDP Leaderboard (Score: 9)](https://www.reddit.com/r/LocalLLaMA/comments/1lekndj/gemini25flashlitepreview0617_performance_on_idp/)**
*   **Summary:** Discussion about the performance of Gemini 2.5 Flash Lite on the IDP Leaderboard, with questions about the non-lite version and reasoning capabilities.
*   **Emotion:** Predominantly Neutral.
*   **Top 3 Points of View:**
    *   Why is there no gemini flash 2.5 non lite?
    *   Is it with enabled reasoning?
    *   LITE 2.5 > LITE 2.0 ? Not hard to believe.

**[Best non-Chinese open models? (Score: 8)](https://www.reddit.com/r/LocalLLaMA/comments/1lel886/best_nonchinese_open_models/)**
*   **Summary:** Discussion about the best open-source language models that are not Chinese, with recommendations including Mistral, Llama 3, and others.
*   **Emotion:** Predominantly Neutral, with some positive sentiment towards specific models.
*   **Top 3 Points of View:**
    *   Mistral and family are probably your best bet.
    *   The best non-chinese open models are imo Llama 3.1 405B, Llama 3.3 70B, Llama 3.3 Nemotron Super 49B.
    *   Microsoft's Phi4 14B is still one of my favorite models.

**[Joycap-beta with llama.cpp (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1lencvg/joycapbeta_with_llamacpp/)**
*   **Summary:** Discussion about Joycap-beta and how it works with llama.cpp
*   **Emotion:** Predominantly Neutral.
*   **Top 3 Points of View:**
    *   Joycap-beta works just fine with llama.cpp.

**[Mobile Phones are becoming better at running AI locally on the device. (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1lepjc5/mobile_phones_are_becoming_better_at_running_ai/)**
*   **Summary:** Discussion about how mobile phones are becoming better at running AI locally on the device
*   **Emotion:** Predominantly Neutral.
*   **Top 3 Points of View:**
    *   Phones get very hot when running AI locally.

**[How much does it cost ai companies to train xbillion amount of parameters? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1leoej7/how_much_does_it_cost_ai_companies_to_train/)**
*   **Summary:** Discussion about the cost of training large language models, with considerations for GPU memory, modeling techniques, and optimization.
*   **Emotion:** Predominantly Neutral.
*   **Top 3 Points of View:**
    *   The cost to train isn't straightforward and doesn't scale linearly with the number of parameters.
    *   Major Tech Companies are spending billions of dollars on GPUs
    *   Techniques like expert parallelism can help to train it further for the same cost.

**[Development environment setup (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1leml1x/development_environment_setup/)**
*   **Summary:** Discussion about setting up a development environment for local LLMs.
*   **Emotion:** Predominantly Neutral.
*   **Top 3 Points of View:**
    *   There are a lot of dockers for different use cases.
    *   WSL is another option, or a VM if you want a more GUI approach to get familairity if you ever decide to go dual boot.
    *   Dual boot offers faster speed and full customization compared to WSL and Docker.

**[Daily Paper Discussions on the Yannic Kilcher Discord -> V-JEPA 2 (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1leoy4x/daily_paper_discussions_on_the_yannic_kilcher/)**
*   **Summary:** This post provides a link to a daily paper discussion on the Yannic Kilcher Discord server, focusing on V-JEPA 2.
*   **Emotion:** Predominantly Neutral.
*   **Top 3 Points of View:**
    *   Information about the timing of the daily paper discussion.

**[Lorras for LLMs (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1leoyg3/lorras_for_llms/)**
*   **Summary:** This post discusses the use of LoRAs (Low-Rank Adaptation) for LLMs and provides a link to Unsloth for an easy starting point.
*   **Emotion:** Predominantly Neutral.
*   **Top 3 Points of View:**
    *   A LoRA is model-specific, so you have to train them individually to any given general model you use.

**[self host minimax? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1leoyu2/self_host_minimax/)**
*   **Summary:** This post asks about self-hosting Minimax.
*   **Emotion:** Predominantly Neutral.
*   **Top 3 Points of View:**
    *   Ktransformers will most likely support this model.

**[Which local API is the best to work with when developing local LLM apps for yourself? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1leqcc5/which_local_api_is_the_best_to_work_with_when/)**
*   **Summary:** This post asks about which local API is best to work with when developing local LLM apps for yourself.
*   **Emotion:** Predominantly Neutral.
*   **Top 3 Points of View:**
    *   Use openai’s api.

**[MiCA – A new parameter-efficient fine-tuning method with higher knowledge uptake and less forgetting (beats LoRA in my tests) (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1lek9yr/mica_a_new_parameterefficient_finetuning_method/)**
*   **Summary:** Discussion about a new parameter-efficient fine-tuning method called MiCA, which claims to outperform LoRA in certain tests.
*   **Emotion:** Predominantly Neutral.
*   **Top 3 Points of View:**
    *   PCE PEFT is a new approach to Principle Component Analysis PEFT.
    *   Doubts raised because the method isn't clearly explained, and results lack sufficient detail.
    *   Comparisons requested between MiCA and other LoRA alternatives (DoRA, ABBA).

**[lmarena not telling us chatbot names after battle (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1lepgii/lmarena_not_telling_us_chatbot_names_after_battle/)**
*   **Summary:** This post is about lmarena not telling chatbot names after battle.
*   **Emotion:** Predominantly Neutral.
*   **Top 3 Points of View:**
    *   lmarena does show the chatbot names.
