---
title: "Machine Learning Subreddit"
date: "2025-06-22"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "ML"]
---

# Overall Ranking and Top Discussions
1.  [[P] This has been done like a thousand time before, but here I am presenting my very own image denoising model](https://www.reddit.com/gallery/1lhny9b) (Score: 219)
    *   The user presents their image denoising model and asks for feedback.
2.  [D] RL/GRPO for lossless compression of text passages into 'least token representation', then using this emergent 'language' as the basis for reasoning instead of english](https://www.reddit.com/gallery/1lh741j) (Score: 41)
    *   The user introduces an idea of lossless compression of text passages and using this compressed data as the basis for reasoning.
3.  [[P] I made a website to visualize machine learning algorithms + derive math from scratch](https://i.redd.it/jb3bjn90li8f1.gif) (Score: 18)
    *   The user created a website to visualize machine learning algorithms and derive math from scratch.
4.  [[P] Open source astronomy project: need best-fit circle advice](https://i.redd.it/lrckhubldg8f1.png) (Score: 18)
    *   The user is requesting help with finding the best-fit circle in an open-source astronomy project.
5.  [[R] Mech Interp: How are researchers working with model's internals?](https://www.reddit.com/r/MachineLearning/comments/1lhbnpf/r_mech_interp_how_are_researchers_working_with/) (Score: 15)
    *   The thread discusses how researchers are working with model's internals.
6.  [[D] How do you keep up with the flood of new ML papers and avoid getting scooped?](https://www.reddit.com/r/MachineLearning/comments/1lhv42l/d_how_do_you_keep_up_with_the_flood_of_new_ml/) (Score: 14)
    *   The thread discusses how to keep up with the flood of new ML papers and avoid getting scooped.
7.  [[R] [MICCAI 2025] U-Net Transplant: The Role of Pre-training for Model Merging in 3D Medical Segmentation](https://i.redd.it/mnh90irz5h8f1.png) (Score: 5)
    *   The thread discusses the U-Net transplant and the role of pre-training for model merging in 3D medical segmentation.
8.  [[P] Writing a CNN from scratch in C++ (no ML/math libs) - a detailed guide](https://deadbeef.io/cnn_from_scratch) (Score: 3)
    *   The user wrote a detailed guide on writing a CNN from scratch in C++ without ML/math libraries.
9.  [[P] XGboost Binary Classication](https://www.reddit.com/r/MachineLearning/comments/1lhb52p/p_xgboost_binary_classication/) (Score: 3)
    *   The user is showcasing a project on XGBoost binary classification and asking for feedback.
10. [Spam/Fraud Call Detection Using ML [P]](https://www.reddit.com/r/MachineLearning/comments/1lhuztd/spamfraud_call_detection_using_ml_p/) (Score: 2)
    *   The user presents project on spam/fraud call detection using ML.
11. [[D]Best metrics for ordinal regression?](https://www.reddit.com/r/MachineLearning/comments/1lhlsds/dbest_metrics_for_ordinal_regression/) (Score: 1)
    *   The thread discusses the best metrics for ordinal regression.
12. [[P] Built a Customer Churn Prediction System using XGBoost + SMOTE + Streamlit Project](https://www.reddit.com/r/MachineLearning/comments/1lho4ay/p_built_a_customer_churn_prediction_system_using/) (Score: 1)
    *   The user presents a customer churn prediction system using XGBoost, SMOTE, and Streamlit.
13. [[P] Are my IoT botnet detection results too good to be true?](https://i.redd.it/9m6oaor3rh8f1.jpeg) (Score: 0)
    *   The user is asking if their IoT botnet detection results are too good to be true.
14. [[P] I built a platform where LLMs debate each otherâ€”randomly assigned to the pro and con sides](https://www.reddit.com/r/MachineLearning/comments/1lh6wpz/p_i_built_a_platform_where_llms_debate_each/) (Score: 0)
    *   The user is showcasing a platform where LLMs debate each other.
15. [[D] How structured prediction differs from classification and regression?](https://www.reddit.com/r/MachineLearning/comments/1lhrwqf/d_how_structured_prediction_differs_from/) (Score: 0)
    *   The thread discusses how structured prediction differs from classification and regression.


# Detailed Analysis by Thread
**[[P] This has been done like a thousand time before, but here I am presenting my very own image denoising model (Score: 219)](https://www.reddit.com/gallery/1lhny9b)**
*  **Summary:** The user presents their image denoising model, acknowledging it's a common task, and seeks feedback. They provide a GitHub link for the project. Users ask about the type of noise, the model architecture (CNN), and training setup. One user noted its similarity to upsampling tasks.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Inquiry about the type of noise used in training.
    *   Discussion on the similarity between denoising and upsampling.
    *   Requests for specifics about the model architecture and training setup.

**[[D] RL/GRPO for lossless compression of text passages into 'least token representation', then using this emergent 'language' as the basis for reasoning instead of english (Score: 41)](https://www.reddit.com/gallery/1lh741j)**
*  **Summary:** The discussion revolves around using RL/GRPO for lossless compression of text into a minimal token representation, aiming to create a basis for reasoning. People question the usability, potential for meaning loss, and comparison to existing methods like VAEs and continuous chain of thought.  There's also a comment about the efficiency of using RL for this task.
*  **Emotion:** The overall emotional tone is mixed, ranging from neutral to negative due to concerns about practicality and meaning loss.
*  **Top 3 Points of View:**
    *   Concerns about critical meaning loss during compression and decompression.
    *   Questioning the practical usability without an explicit metric for interpolation.
    *   Suggesting the idea is superseded by continuous chain of thought methods.

**[[P] I made a website to visualize machine learning algorithms + derive math from scratch (Score: 18)](https://i.redd.it/jb3bjn90li8f1.gif)**
*  **Summary:** The user shares a website that visualizes ML algorithms and derives the math behind them.  The single comment expresses appreciation for the impressive work and the inclusion of code.
*  **Emotion:** The overall emotional tone is overwhelmingly positive.
*  **Top 3 Points of View:**
    *   Positive feedback on the website's impressive nature and inclusion of code.

**[[P] Open source astronomy project: need best-fit circle advice (Score: 18)](https://i.redd.it/lrckhubldg8f1.png)**
*  **Summary:** The user requests advice on finding the best-fit circle in images for an astronomy project. People suggest preprocessing steps to account for the stripy nature of the image, using a Laplacian of Gaussian filter to clean up noise, and identifying long, continuous stripes to separate the circle from the background.  There are also suggestions related to using Gabor transforms and corner detectors.
*  **Emotion:** The overall emotional tone is neutral, focused on problem-solving and offering technical advice.
*  **Top 3 Points of View:**
    *   Preprocessing to handle the stripy nature of the image using existing computer vision techniques.
    *   Employing a Laplacian of Gaussian filter to address the high noise levels.
    *   Using an algorithm to identify long, continuous stripes to distinguish the circle from the background.

**[[R] Mech Interp: How are researchers working with model's internals? (Score: 15)](https://www.reddit.com/r/MachineLearning/comments/1lhbnpf/r_mech_interp_how_are_researchers_working_with/)**
*  **Summary:** The discussion revolves around Mechanical Interpretability (Mech Interp) and its applications. Some express skepticism, while others provide examples like pivotal token search and activation vector steering.
*  **Emotion:** The emotional tone is mixed, ranging from positive about the potential to bearish about its current productivity.
*  **Top 3 Points of View:**
    *   Skepticism about the productivity and value of Mech Interp based on past observations.
    *   Highlighting the application of pivotal token search for identifying critical decision points in generations.
    *   Mentioning the use of activation vectors for steering models.

**[[D] How do you keep up with the flood of new ML papers and avoid getting scooped? (Score: 14)](https://www.reddit.com/r/MachineLearning/comments/1lhv42l/d_how_do_you_keep_up_with_the_flood_of_new_ml/)**
*  **Summary:** The discussion is about strategies for staying up-to-date with the overwhelming amount of ML papers and preventing being "scooped."  Suggestions include prioritizing papers based on titles and abstracts, reproducing implementations, using preprints to establish priority, and leveraging social media for updates.
*  **Emotion:** The overall emotional tone is neutral and helpful, focused on providing practical advice.
*  **Top 3 Points of View:**
    *   Prioritizing reading based on titles and abstracts, then diving deeper only when relevant.
    *   Using preprints on arXiv to establish priority and protect ideas.
    *   Leveraging social media to follow researchers and research groups for updates.

**[[R] [MICCAI 2025] U-Net Transplant: The Role of Pre-training for Model Merging in 3D Medical Segmentation (Score: 5)](https://i.redd.it/mnh90irz5h8f1.png)**
*  **Summary:** The discussion is about a research paper on U-Net Transplant and pre-training's role in 3D medical segmentation. One comment calls the post a "Chatgpt post," while the other says they will check it out.
*  **Emotion:** The emotional tone is mostly neutral.
*  **Top 3 Points of View:**
    *   Calling the post a "Chatgpt post," implying potentially low value or automated content.
    *   Expressing interest in checking out the content of the post.

**[[P] Writing a CNN from scratch in C++ (no ML/math libs) - a detailed guide (Score: 3)](https://deadbeef.io/cnn_from_scratch)**
*  **Summary:** The user presents a detailed guide on writing a CNN from scratch in C++ without using ML or math libraries. The comment praises the user's impressive implementation and expresses admiration for C++.
*  **Emotion:** The overall emotional tone is very positive.
*  **Top 3 Points of View:**
    *   Admiration for the CNN implementation in C++.

**[[P] XGboost Binary Classication (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1lhb52p/p_xgboost_binary_classication/)**
*  **Summary:** The user is presenting a project using XGBoost for binary classification. Users are asking clarifying questions about the data being used and the problem being solved. Suggestions were given for imbalanced datasets.
*  **Emotion:** The overall emotional tone is neutral and inquisitive.
*  **Top 3 Points of View:**
    *   Inquiring about the nature of the data and the prediction task.
    *   Questioning the balance of classes in the training dataset.
    *   Recommending plotting predicted probabilities for imbalanced datasets.

**[Spam/Fraud Call Detection Using ML [P] (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1lhuztd/spamfraud_call_detection_using_ml_p/)**
*  **Summary:** The user is presenting a project on spam/fraud call detection using ML.  The comment provides potential signals for identifying spam calls and raises questions about how the model handles calls of different durations.
*  **Emotion:** The overall emotional tone is neutral and inquisitive.
*  **Top 3 Points of View:**
    *   Suggesting potential signals for identifying spam calls, such as frequent calls or unusual number formats.
    *   Questioning how the model handles call duration as a factor in spam detection.

**[[D]Best metrics for ordinal regression? (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1lhlsds/dbest_metrics_for_ordinal_regression/)**
*  **Summary:** The discussion is centered around the best metrics for ordinal regression.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Kendall Tau is a ranking correlation metric
    *   Regular metrics e.g. accuracy or AUROC can also be used if the ordinal aspect is ignored.

**[[P] Built a Customer Churn Prediction System using XGBoost + SMOTE + Streamlit Project (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1lho4ay/p_built_a_customer_churn_prediction_system_using/)**
*  **Summary:** The user is sharing a customer churn prediction system using XGBoost, SMOTE, and Streamlit.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Expressing surprise that SMOTE performed better, as that's rare.

**[[P] Are my IoT botnet detection results too good to be true? (Score: 0)](https://i.redd.it/9m6oaor3rh8f1.jpeg)**
*  **Summary:** The user asks if their IoT botnet detection results are too good to be true, expressing concerns about the validity of their evaluation metrics. People suggest potential problems, such as leaking data, a biased dataset, and the impact of highly imbalanced classes on evaluation metrics. They recommend techniques like cross-validation, anomaly detection, and careful attention to creating a representative test set.
*  **Emotion:** The overall emotional tone is neutral and helpful, focused on identifying potential issues and offering solutions.
*  **Top 3 Points of View:**
    *   Suggesting the possibility of data leakage between the training and test sets.
    *   Highlighting the impact of the imbalanced dataset on evaluation metrics, particularly AUC.
    *   Recommending anomaly detection approaches that focus on training only on normal flows.

**[[P] I built a platform where LLMs debate each otherâ€”randomly assigned to the pro and con sides (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1lh6wpz/p_i_built_a_platform_where_llms_debate_each/)**
*  **Summary:** The user presents a platform where LLMs debate each other.
*  **Emotion:** The emotional tone is mixed, with one negative comment about the text input and another positive comment about the name.
*  **Top 3 Points of View:**
    *   Complaining about broken text box input on mobile.
    *   Praising the name "Bot Bicker" as genius.

**[[D] How structured prediction differs from classification and regression? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1lhrwqf/d_how_structured_prediction_differs_from/)**
*  **Summary:** The discussion clarifies how structured prediction differs from classification and regression, emphasizing complex relationships between outputs beyond simple softmax.
*  **Emotion:** The overall emotional tone is neutral.
*  **Top 3 Points of View:**
    *   YOLO, where bounding box regression and classification is mixed, is a good example of structured output.
