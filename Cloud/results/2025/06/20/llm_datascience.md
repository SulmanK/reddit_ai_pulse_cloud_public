---
title: "Data Science Subreddit"
date: "2025-06-20"
description: "Analysis of top discussions and trends in the datascience subreddit"
tags: ["data science", "AI", "career"]
---

# Overall Ranking and Top Discussions
1.  [[D] Ridiculous offer, how to proceed?](https://www.reddit.com/r/datascience/comments/1lg5mrg/ridiculous_offer_how_to_proceed/) (Score: 102)
    *   The discussion revolves around a job offer that the original poster (OP) considers to be a lowball offer. People are giving advice on whether to accept, reject, or counter the offer.
2.  [How are you making AI applications in settings where no external APIs are allowed?](https://www.reddit.com/r/datascience/comments/1lg4t92/how_are_you_making_ai_applications_in_settings/) (Score: 19)
    *   The discussion centers on building AI applications in environments with restrictions on using external APIs. People share strategies and tools for running models locally or in private cloud environments.
3.  [Confidence interval width vs training MAPE](https://www.reddit.com/r/datascience/comments/1lfp3ge/confidence_interval_width_vs_training_mape/) (Score: 9)
    *   This thread discusses the relationship between the width of confidence intervals and the training MAPE (Mean Absolute Percentage Error) in a model, particularly when dealing with varying levels of data granularity.
4.  [Problem identification & specification in Data Science (a metacognitive deep dive)](https://www.reddit.com/r/datascience/comments/1lg5043/problem_identification_specification_in_data/) (Score: 3)
    *   The discussion emphasizes the importance of correctly defining the problem in data science projects. It highlights that solving the wrong problem renders even the most sophisticated approaches useless.
5.  [How to build a usability metric that is "normalized" across flows?](https://www.reddit.com/r/datascience/comments/1lg60ju/how_to_build_a_usability_metric_that_is/) (Score: 1)
    *   The discussion focuses on creating a usability metric that can be standardized across different user flows.
6.  [Has anyone seen research or articles proving that code quality matters in data science projects?](https://www.reddit.com/r/datascience/comments/1lgdg9j/has_anyone_seen_research_or_articles_proving_that/) (Score: 1)
    *   The discussion centers around whether code quality matters in data science projects.

# Detailed Analysis by Thread
**[ [D] Ridiculous offer, how to proceed? (Score: 102)](https://www.reddit.com/r/datascience/comments/1lg5mrg/ridiculous_offer_how_to_proceed/)**
*   **Summary:** The thread discusses whether to accept, reject, or counter a job offer perceived as a lowball.
*   **Emotion:** Predominantly Neutral, with some Negative sentiment. The emotional tone is a mix of frustration and pragmatic advice.
*   **Top 3 Points of View:**
    *   Reject the offer and move on, as it signals potential future issues with compensation.
    *   Counter with a desired salary, and be prepared to walk away if the counter is rejected.
    *   Consider accepting the offer to gain experience, especially if currently unemployed, while continuing the job search.

**[How are you making AI applications in settings where no external APIs are allowed? (Score: 19)](https://www.reddit.com/r/datascience/comments/1lg4t92/how_are_you_making_ai_applications_in_settings/)**
*   **Summary:** The thread explores approaches for building AI applications within environments that restrict external API access.
*   **Emotion:** Neutral to Positive. The thread is generally informative and solution-oriented.
*   **Top 3 Points of View:**
    *   Utilize cloud-based solutions with zero-trust architecture and private networking.
    *   Run models locally on machines with sufficient memory.
    *   Develop applications generically using mocked data and deploy using tools like vLLM or llama.cpp behind a corporate firewall.

**[Confidence interval width vs training MAPE (Score: 9)](https://www.reddit.com/r/datascience/comments/1lfp3ge/confidence_interval_width_vs_training_mape/)**
*   **Summary:** The thread discusses the relationship between confidence interval width and training MAPE, highlighting that they measure different aspects of model performance.
*   **Emotion:** Neutral. The discussion is technical and analytical.
*   **Top 3 Points of View:**
    *   Confidence intervals indicate the variance of the model parameters, while MAPE quantifies prediction error.
    *   The standard error is increasing in the residual variance, not the variance in price behavior.
    *   The results obtained are logical and are demonstrating the bias-variance tradeoff.

**[Problem identification & specification in Data Science (a metacognitive deep dive) (Score: 3)](https://www.reddit.com/r/datascience/comments/1lg5043/problem_identification_specification_in_data/)**
*   **Summary:** This thread stresses the importance of accurately defining the problem in data science projects, asserting that solving the wrong problem negates the value of any work done.
*   **Emotion:** Neutral. The tone is informative and reflective.
*   **Top 3 Points of View:**
    *   Meetings are essential for insightful discussions to define the right questions and the problem clearly.
    *   Once the problem is well-defined, the rest of the data science process is generally straightforward in business use cases.
    *   Solving the wrong problem renders the work done valueless, regardless of the sophistication of the approach.

**[How to build a usability metric that is "normalized" across flows? (Score: 1)](https://www.reddit.com/r/datascience/comments/1lg60ju/how_to_build_a_usability_metric_that_is/)**
*   **Summary:** The discussion addresses the challenge of creating a standardized usability metric applicable across different user flows.
*   **Emotion:** Neutral to Negative. There's a sense of difficulty in finding a straightforward solution.
*   **Top 3 Points of View:**
    *   Collect data under current workflows and compare those samples to ones where changes are introduced, instead of trying to bring them together.
    *   Use PCA to collapse information into a single score.
    *   Plot each flow as a bell curve normalized to the average completion rate.

**[Has anyone seen research or articles proving that code quality matters in data science projects? (Score: 1)](https://www.reddit.com/r/datascience/comments/1lgdg9j/has_anyone_seen_research_or_articles_proving_that/)**
*   **Summary:** The thread explores the relevance of code quality in data science projects.
*   **Emotion:** Neutral to Negative. The thread seems to be searching for concrete evidence but also expressing frustration with poor code quality.
*   **Top 3 Points of View:**
    *   Code quality matters as soon as someone else needs to read the code.
