---
title: "LocalLLaMA Subreddit"
date: "2025-06-20"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "Local Models", "AI"]
---

# Overall Ranking and Top Discussions
1.  [mistralai/Mistral-Small-3.2-24B-Instruct-2506 · Hugging Face](https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506) (Score: 220)
    *   Discussion about the new Mistral Small 3.2 model and its improvements over previous versions.
2.  [New Mistral Small 3.2](https://www.reddit.com/r/LocalLLaMA/comments/1lg80cq/new_mistral_small_32/) (Score: 95)
    *   This thread is a discussion about the new Mistral Small 3.2 model.
3.  [OpenBuddy R1 0528 Distil into Qwen 32B](https://i.redd.it/lpxeubca848f1.gif) (Score: 42)
    *   Discussion about the OpenBuddy R1 0528 model distilled into Qwen 32B.
4.  [Performance comparison on gemma-3-27b-it-Q4_K_M, on 5090 vs 4090 vs 3090 vs A6000, tuned for performance. Both compute and bandwidth bound.](https://www.reddit.com/r/LocalLLaMA/comments/1lgcbyh/performance_comparison_on_gemma327bitq4_k_m_on/) (Score: 23)
    *   Performance comparison of the Gemma model on various GPUs.
5.  [Running two models using NPU and CPU](https://v.redd.it/c3489gtgb48f1) (Score: 4)
    *   Discussion about running two models simultaneously using NPU and CPU.
6.  [Trouble setting up 7x3090](https://www.reddit.com/r/LocalLLaMA/comments/1lgcxez/trouble_setting_up_7x3090/) (Score: 3)
    *   A user is having trouble setting up a system with 7x3090 GPUs and is seeking advice.
7.  [Why haven't I tried llama.cpp yet?](https://www.reddit.com/r/LocalLLaMA/comments/1lgd4tq/why_havent_i_tried_llamacpp_yet/) (Score: 3)
    *   Discussion about the llama.cpp inference engine and its alternatives.
8.  [Help me decide on hardware for LLMs](https://www.reddit.com/r/LocalLLaMA/comments/1lg7zmb/help_me_decide_on_hardware_for_llms/) (Score: 2)
    *   A user is seeking advice on hardware for running LLMs.
9.  [Any free APIs that can scrape the web, or ways to have local LLM scrape the web autonomously?](https://www.reddit.com/r/LocalLLaMA/comments/1lg76cn/any_free_apis_that_can_scrape_the_web_or_ways_to/) (Score: 1)
    *   A user is asking about free APIs for web scraping to use with local LLMs.
10. [How to be sure how much data we need for LoRA trainings](https://www.reddit.com/r/LocalLLaMA/comments/1lg7ymc/how_to_be_sure_how_much_data_we_need_for_lora/) (Score: 1)
    *   Discussion on how much data is needed for LoRA training.
11. [Retrain/Connect Models with Existing database](https://www.reddit.com/r/LocalLLaMA/comments/1lg94vr/retrainconnect_models_with_existing_database/) (Score: 1)
    *   Discussion about ways to add data to an LLM.
12. [A Systematic Methodology for AI Consciousness Development: "Consciousness Engineering"](https://www.reddit.com/r/LocalLLaMA/comments/1lg75zi/a_systematic_methodology_for_ai_consciousness/) (Score: 0)
    *   Discussion about AI consciousness development.
13. [Anyone tried this...](https://www.reddit.com/r/LocalLLaMA/comments/1lgb22n/anyone_tried_this/) (Score: 0)
    *   Discussion about an experiment where different models were prompted with a question that resulted in the same answer.

# Detailed Analysis by Thread
**[mistralai/Mistral-Small-3.2-24B-Instruct-2506 · Hugging Face (Score: 220)](https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506)**
*   **Summary:**  Discussion about the new Mistral Small 3.2 model and its improvements over previous versions, with users expressing excitement and comparing it to other models like Qwen3. The updates include better instruction following, less repetition errors, and a more robust function calling template.
*   **Emotion:** Predominantly Positive, with expressions of excitement and appreciation for the improvements in the new model.
*   **Top 3 Points of View:**
    *   The new model shows significant improvements over previous versions.
    *   It is comparable to other models like Qwen3 in terms of performance.
    *   Users are particularly interested in the improvements in instruction following, repetition errors, and function calling.

**[New Mistral Small 3.2 (Score: 95)](https://www.reddit.com/r/LocalLLaMA/comments/1lg80cq/new_mistral_small_32/)**
*   **Summary:** This thread is a discussion about the new Mistral Small 3.2 model, with users comparing it to other models, anticipating quantized versions, and speculating about future "large" models from Mistral.
*   **Emotion:** Predominantly Positive, with users expressing anticipation and interest in the new model and future developments.
*   **Top 3 Points of View:**
    *   Users are curious about how Mistral Small 3.2 compares to other models.
    *   There is excitement about the possibility of quantized versions of the model.
    *   Users are anticipating the release of a "large" model from Mistral.

**[OpenBuddy R1 0528 Distil into Qwen 32B (Score: 42)](https://i.redd.it/lpxeubca848f1.gif)**
*   **Summary:** The thread discusses the OpenBuddy R1 0528 model distilled into Qwen 32B, with the expectation that the distilled version would benefit from the improvements seen in the original.
*   **Emotion:** Neutral, with a hint of optimism about the potential benefits of the distillation.
*   **Top 3 Points of View:**
    *   The OpenBuddy R1 0528 model was a significant improvement.
    *   The distilled version is expected to benefit similarly.

**[Performance comparison on gemma-3-27b-it-Q4_K_M, on 5090 vs 4090 vs 3090 vs A6000, tuned for performance. Both compute and bandwidth bound. (Score: 23)](https://www.reddit.com/r/LocalLLaMA/comments/1lgcbyh/performance_comparison_on_gemma327bitq4_k_m_on/)**
*   **Summary:** Performance comparison of the Gemma model on various GPUs (5090, 4090, 3090, A6000), focusing on compute and bandwidth limitations, with users discussing the results and potential for other quantization formats.
*   **Emotion:** Mostly Positive, with expressions of appreciation for the benchmarks and curiosity about further testing.
*   **Top 3 Points of View:**
    *   The 3090 still offers excellent price-performance.
    *   Users are interested in seeing results with other quants and formats like AWQ via vLLM.
    *   Some users had to tinker with the RAM voltages and resistances on the BIOS.

**[Running two models using NPU and CPU (Score: 4)](https://v.redd.it/c3489gtgb48f1)**
*   **Summary:** Discussion about running two models simultaneously using NPU and CPU, with users expressing interest and asking about the sustainability of NPU speed. One user proposes using NPU for whisper and CPU for Qwen 30B MoE to create a low-latency voice assistant.
*   **Emotion:** Positive, with expressions of excitement and interest in the capabilities of running models on NPU and CPU.
*   **Top 3 Points of View:**
    *   Running models on NPU and CPU simultaneously is a significant development.
    *   Users are interested in the real-world performance and sustainability of NPU.
    *   NPU can be leveraged for tasks like voice recognition (whisper) to create low-latency applications.

**[Trouble setting up 7x3090 (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1lgcxez/trouble_setting_up_7x3090/)**
*   **Summary:** A user is having trouble setting up a system with 7x3090 GPUs and is seeking advice. The discussion revolves around potential issues such as insufficient power, boot problems, and alternatives to Proxmox.
*   **Emotion:** Mixed, with some frustration from the user seeking help and helpful suggestions from others.
*   **Top 3 Points of View:**
    *   Insufficient power could be the cause of the boot problems.
    *   Updating the BIOS to the most recent version might resolve boot issues.
    *   Proxmox might not be worth the headache for some use cases.

**[Why haven't I tried llama.cpp yet? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1lgd4tq/why_havent_i_tried_llamacpp_yet/)**
*   **Summary:** Discussion about the llama.cpp inference engine and its alternatives, with users comparing its performance to ik\_llama.cpp and asking about its compatibility with different systems and tools.
*   **Emotion:** Neutral, with users sharing their experiences and asking questions about llama.cpp and its alternatives.
*   **Top 3 Points of View:**
    *   ik\_llama.cpp is faster than llama.cpp, especially when using both GPU+CPU and heavy MoE models.
    *   Users are wondering if it can run on Mac
    *   Users are wondering if it's possible to run llama.cpp server together with Open Hands?

**[Help me decide on hardware for LLMs (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1lg7zmb/help_me_decide_on_hardware_for_llms/)**
*   **Summary:** A user is seeking advice on hardware for running LLMs. The discussion covers various options, including cloud-based solutions like Colab Pro, high DRAM counts on Intel Xeon or AMD Epyc, Apple Macs, and multiple GPUs.
*   **Emotion:** Mixed, with some users expressing concerns about budget and performance, while others recommend specific hardware configurations.
*   **Top 3 Points of View:**
    *   At least 24GB VRAM is needed for anything remotely useful.
    *   Consider using Colab Pro as a cost-effective alternative to building a local machine.
    *   Integrated architectures like Apple Silicon are promising for LLMs.

**[Any free APIs that can scrape the web, or ways to have local LLM scrape the web autonomously? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1lg76cn/any_free_apis_that_can_scrape_the_web_or_ways_to/)**
*   **Summary:** A user is asking about free APIs for web scraping to use with local LLMs. The discussion provides several options, including DuckDuckGo's official package, Trafilatura, and links to example code.
*   **Emotion:** Neutral, with helpful suggestions and resources provided by other users.
*   **Top 3 Points of View:**
    *   DuckDuckGo's official package can be used for web/news search.
    *   Trafilatura can extract content from webpages.
    *   Model Context Protocol lets you wire things into it.

**[How to be sure how much data we need for LoRA trainings (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1lg7ymc/how_to_be_sure_how_much_data_we_need_for_lora/)**
*   **Summary:** Discussion on how much data is needed for LoRA training.
*   **Emotion:** Neutral
*   **Top 3 Points of View:**
    *   Just put training data in your prompt and say "write like these".
    *   Sometimes as low as 10 responses can be enough for classification.

**[Retrain/Connect Models with Existing database (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1lg94vr/retrainconnect_models_with_existing_database/)**
*   **Summary:** Discussion about ways to add data to an LLM.
*   **Emotion:** Neutral
*   **Top 3 Points of View:**
    *   Context stuff
    *   RAG
    *   Finetune further

**[A Systematic Methodology for AI Consciousness Development: "Consciousness Engineering" (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1lg75zi/a_systematic_methodology_for_ai_consciousness/)**
*   **Summary:** Discussion about AI consciousness development.
*   **Emotion:** Neutral
*   **Top 3 Points of View:**
    *   Users are pondering if it's possible to engineer such a thing.
    *   Consciousness is just an emergent property.

**[Anyone tried this... (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1lgb22n/anyone_tried_this/)**
*   **Summary:** Discussion about an experiment where different models were prompted with a question that resulted in the same answer.
*   **Emotion:** Neutral
*   **Top 3 Points of View:**
    *   Qwen3 4b had a meltdown in the thinking phase of Q8's game and burned through 2000 tokens.
    *   All of the models prompted gave the same answer in every single run.
