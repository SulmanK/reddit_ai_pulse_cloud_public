---
title: "Machine Learning Subreddit"
date: "2025-06-06"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "LLMs", "AI"]
---

# Overall Ranking and Top Discussions
1.  [[R] LLMs are Locally Linear Mappings: Qwen 3, Gemma 3 and Llama 3 can be converted to exactly equivalent locally linear systems for interpretability](https://www.reddit.com/r/MachineLearning/comments/1l4rpe2/r_llms_are_locally_linear_mappings_qwen_3_gemma_3/) (Score: 120)
    * The thread discusses a research paper that finds that LLMs can be converted to locally linear systems for interpretability.
2.  [[R] What do you all think of the latest Apple paper on current LLM capabilities?](https://www.reddit.com/r/MachineLearning/comments/1l4nk5s/r_what_do_you_all_think_of_the_latest_apple_paper/) (Score: 46)
    * The thread discusses a paper by Apple on the current capabilities of LLMs and their limitations.
3.  [[R] 100M Open source notebooklm speech model](https://www.reddit.com/r/MachineLearning/comments/1l4dovp/r_100m_open_source_notebooklm_speech_model/) (Score: 11)
    * The thread asks if there is a plan to open source the training script as well
4.  [[R] Better quantization: Yet Another Quantization Algorithm](https://www.reddit.com/r/MachineLearning/comments/1l4we1t/r_better_quantization_yet_another_quantization/) (Score: 11)
    * The thread discusses a novel quantization algorithm.
5.  [[D] Is there an video or article or book where a lot of real world datasets are used to train industry level LLM with all the code?](https://www.reddit.com/r/MachineLearning/comments/1l4lw22/d_is_there_an_video_or_article_or_book_where_a/) (Score: 3)
    * The thread asks if there's material available showcasing the training of industry-level LLMs using real-world datasets with accompanying code.
6.  [[D] Stacking Ensemble Model - Model Selection](https://www.reddit.com/r/MachineLearning/comments/1l4fm0j/d_stacking_ensemble_model_model_selection/) (Score: 2)
    * The thread discusses model selection for stacking ensemble models.
7.  [[D] Gemini Diffusion Early Access invitation not working?](https://www.reddit.com/r/MachineLearning/comments/1l4wj5n/d_gemini_diffusion_early_access_invitation_not/) (Score: 2)
    * The thread reports the Gemini Diffusion Early Access invitation not working
8.  [[D] How fast can you process images on 4 A100 40 gig gpus?](https://www.reddit.com/r/MachineLearning/comments/1l4gv65/d_how_fast_can_you_process_images_on_4_a100_40/) (Score: 0)
    * The thread discusses optimizing image processing speed on A100 GPUs.

# Detailed Analysis by Thread
**[[R] LLMs are Locally Linear Mappings: Qwen 3, Gemma 3 and Llama 3 can be converted to exactly equivalent locally linear systems for interpretability (Score: 120)](https://www.reddit.com/r/MachineLearning/comments/1l4rpe2/r_llms_are_locally_linear_mappings_qwen_3_gemma_3/)**
*   **Summary:** The thread revolves around a research paper suggesting that LLMs like Qwen 3, Gemma 3, and Llama 3 can be transformed into locally linear systems for better interpretability. People are discussing the implications of this finding, its novelty, and its potential connection to existing concepts like LIME and Taylor expansion.
*   **Emotion:** The overall emotional tone is positive and neutral, with a mix of excitement and skepticism. Many comments express interest and acknowledge the potential of the research, while others question its novelty and the strength of the claims.
*   **Top 3 Points of View:**
    *   The research is awesome and has implications for training methodology and efficient inference.
    *   The claim of "exactly equivalent" needs mathematical proof since it is empirical work.
    *   This research is similar to other known concepts like LIME or Taylor expansion.

**[[R] What do you all think of the latest Apple paper on current LLM capabilities? (Score: 46)](https://www.reddit.com/r/MachineLearning/comments/1l4nk5s/r_what_do_you_all_think_of_the_latest_apple_paper/)**
*   **Summary:**  This thread is a discussion about Apple's recent paper on the capabilities of LLMs. The discussion centers around the limitations of LLMs, particularly their "shallow reasoning" and inability to infer mental models or demonstrate a true "theory of mind." Some users feel the paper confirms what has been apparent for a while, while others share their own experiences with LLMs struggling with creative writing and understanding character motivations.
*   **Emotion:** The emotional tone is generally neutral to positive, with a focus on critical analysis and sharing personal experiences. There's a sense of agreement about the limitations of current LLMs.
*   **Top 3 Points of View:**
    *   LLMs lack true reasoning capabilities and instead rely on shallow reasoning that collapses after a certain number of variables.
    *   LLMs are unable to infer the mental model of a character and are limited in creative writing tasks.
    *   The limitations are not surprising because the training objective is still next token prediction.

**[[R] 100M Open source notebooklm speech model (Score: 11)](https://www.reddit.com/r/MachineLearning/comments/1l4dovp/r_100m_open_source_notebooklm_speech_model/)**
*   **Summary:** This thread is about an open-source speech model and the discussion revolves around whether the training script will also be open-sourced.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   Request for open-sourcing the training script.

**[[R] Better quantization: Yet Another Quantization Algorithm (Score: 11)](https://www.reddit.com/r/MachineLearning/comments/1l4we1t/r_better_quantization_yet_another_quantization/)**
*   **Summary:** The thread discusses a quantization algorithm, specifically praising the objective of minimizing KL divergence despite quantization.
*   **Emotion:** Positive.
*   **Top 3 Points of View:**
    *   Minimizing KL divergence despite quantization is an excellent objective.

**[[D] Is there an video or article or book where a lot of real world datasets are used to train industry level LLM with all the code? (Score: 3)](https://www.reddit.com/r/MachineLearning/comments/1l4lw22/d_is_there_an_video_or_article_or_book_where_a/)**
*   **Summary:**  The thread is a question about the availability of resources (video, article, or book) that demonstrate the training of industry-level LLMs using real-world datasets, complete with code. The replies suggest that such resources are unlikely to be available due to proprietary information and copyright concerns.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   The specific sources used to train LLMs are not mentioned because it's against copyright and the terms of service.
    *   Major companies would not reveal their trade secrets because training these models is insanely expensive.

**[[D] Stacking Ensemble Model - Model Selection (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1l4fm0j/d_stacking_ensemble_model_model_selection/)**
*   **Summary:** This thread is about model selection for stacking ensemble models.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   Ensemble models are not used in production but for Kaggle competitions.
    *   Try tabpfmv2 for tabular data.

**[[D] Gemini Diffusion Early Access invitation not working? (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1l4wj5n/d_gemini_diffusion_early_access_invitation_not/)**
*   **Summary:** The thread reports that the Gemini Diffusion Early Access invitation is not working
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   The other user also experienced the same problem

**[[D] How fast can you process images on 4 A100 40 gig gpus? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1l4gv65/d_how_fast_can_you_process_images_on_4_a100_40/)**
*   **Summary:** The thread discusses optimizing image processing speed on 4 A100 GPUs.  Suggestions include checking for data loading bottlenecks, using quantized versions of models, and profiling the code to identify performance issues.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   Most performance issues have to do with data loading and memory bound.
    *   Consider using a quantized version of the model or Llama4 on TensorRT.
    *   Profile the code to identify the bottleneck.
