---
title: "LocalLLaMA Subreddit"
date: "2025-06-28"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LLM", "Local Models", "AI Agents"]
---

# Overall Ranking and Top Discussions
1.  [What framework are you using to build AI Agents?](https://www.reddit.com/r/LocalLLaMA/comments/1lmni3q/what_framework_are_you_using_to_build_ai_agents/) (Score: 50)
    *   The thread discusses various frameworks for building AI agents, including LangGraph, CrewAI, PydanticAI, Google ADK, and HF smolagents. Some users prefer creating their own frameworks using in-memory state managers and the Unix philosophy.
2.  [support for the upcoming ERNIE 4.5 0.3B model has been merged into llama.cpp](https://github.com/ggml-org/llama.cpp/pull/14408) (Score: 35)
    *   The thread announces the integration of the ERNIE 4.5 0.3B model into llama.cpp, with discussions focusing on its potential performance, origin (ARM employee contribution), and accessibility outside of China.
3.  [Consumer hardware landscape for local LLMs June 2025](https://www.reddit.com/r/LocalLLaMA/comments/1lmmh3l/consumer_hardware_landscape_for_local_llms_june/) (Score: 25)
    *   The thread discusses the best consumer hardware for running local LLMs, comparing different GPUs (3090, 4090, 4070ti super, Intel B60, Radeon W9700, Arc Pro B60, 5060ti), EPYC systems, memory bandwidth considerations, and APUs (Strix Halo, DGX Spark, Mac).
4.  [deepseek-r1-0528 ranked #2 on lmarena, matching best from chatgpt](https://www.reddit.com/r/LocalLLaMA/comments/1lmqsru/deepseekr10528_ranked_2_on_lmarena_matching_best/) (Score: 19)
    *   The thread discusses the performance of the Deepseek-r1-0528 model, which ranked highly on the lmarena benchmark. Users share their experiences using it with Fireworks.AI and compare it to other models like Claude Sonnet and GPT.
5.  [Many small evals are better than one big eval [techniques]](https://www.reddit.com/r/LocalLLaMA/comments/1lmmvmj/many_small_evals_are_better_than_one_big_eval/) (Score: 14)
    *   The thread discusses the advantages of using smaller, more frequent evaluations during the development process. It also touches upon whether this principle applies to the development of AI agents.
6.  [Gemma3n:2B and Gemma3n:4B models are ~40% slower than equivalent models in size running on Llama.cpp](https://www.reddit.com/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/) (Score: 6)
    *   The thread discusses the performance of Gemma3n models, noting that they are slower than other models of similar size when running on Llama.cpp. Users share their experiences with the models on different hardware.
7.  [What are Coqui-TTS alternatives?](https://www.reddit.com/r/LocalLLaMA/comments/1lmn5k2/what_are_coquitts_alternatives/) (Score: 4)
    *   The thread seeks alternatives to Coqui-TTS, with users suggesting ChatterBox TTS, kokoro, and Orpheus.
8.  [i5-8500 (6 cores), 24GB DDR4 2666 dual channel, realistic expectations for 3b/4b models?](https://www.reddit.com/r/LocalLLaMA/comments/1lmt3kt/i58500_6_cores_24gb_ddr4_2666_dual_channel/) (Score: 4)
    *   The thread discusses the performance expectations for 3b/4b LLMs on a system with an i5-8500 CPU, 24GB DDR4 RAM, and dual-channel memory. Users discuss the limitations of memory bandwidth and suggest potential optimizations like upgrading RAM or using a GPU.
9.  [Looking for Android chat ui](https://www.reddit.com/r/LocalLLaMA/comments/1lmrd6x/looking_for_android_chat_ui/) (Score: 3)
    *   The thread is looking for an Android chat UI, with users suggesting Firefox + PageAssist and MyDeviceAI.
10. [Link between LM Studio and tools/functions?](https://www.reddit.com/r/LocalLLaMA/comments/1lmoqsl/link_between_lm_studio_and_toolsfunctions/) (Score: 2)
    *   The thread asks about the link between LM Studio and tools/functions, with one user recommending the Python SDK.
11. [EPYC cpu build. Which cpu? (9354, 9534, 9654)](https://www.reddit.com/r/LocalLLaMA/comments/1lmr1qh/epyc_cpu_build_which_cpu_9354_9534_9654/) (Score: 2)
    *   The thread discusses the best EPYC CPU for a build, suggesting to prioritize higher clock speeds, more CCDs for memory bandwidth, and higher single-core performance (16 cores +).
12. [Best model tuned specifically for Programming?](https://www.reddit.com/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/) (Score: 1)
    *   The thread asks for recommendations on models tuned for programming. Suggestions include GLM 4-0414, Qwen, Codestral, QWQ, Qwen2.5 32B and Hunyuan-A13B. Recommends investing in a used 3090, 4090, or 5090 for the vram.
13. [Can Copilot be trusted with private source code more than competition?](https://www.reddit.com/r/LocalLLaMA/comments/1lmsme1/can_copilot_be_trusted_with_private_source_code/) (Score: 1)
    *   The thread questions the trustworthiness of Copilot with private source code compared to its competitors. The consensus is no, with users suggesting alternatives like OpenAI, Jan, or LMStudio and recommending renting GPUs on vast.ai.
14. [Best GGUF Base Models Under 3B for Unfiltered NSFW Roleplay?](https://www.reddit.com/r/LocalLLaMA/comments/1lmvosa/best_gguf_base_models_under_3b_for_unfiltered/) (Score: 1)
    *   The thread is looking for GGUF base models under 3B for unfiltered NSFW roleplay, with one user suggesting installing sillytavern on a normal computer and using tailscale to access it from a phone.
15. [Multimodal Multistage Reasoning](https://i.redd.it/a5k3d6h18p9f1.jpeg) (Score: 0)
    *   The thread shows an image of a system prompt and includes a link to a comment.
16. [Play Infinite Tic Tac Toe against LLM Models](https://v.redd.it/v346kcuiio9f1) (Score: 0)
    *   The thread features a game where a Local LLM uses an MCP with an inventory of strategies.
17. [Which are the best realistic video generation tools](https://www.reddit.com/r/LocalLLaMA/comments/1lmmxh1/which_are_the_best_realistic_video_generation/) (Score: 0)
    *   The thread lists video generation tools, separating them into closed source (Veo, Sora, Kling) and open source (Wan, Ltx).
18. [Como mejorar un sistema RAG?](https://www.reddit.com/r/LocalLLaMA/comments/1lmqtby/como_mejorar_un_sistema_rag/) (Score: 0)
    *   The thread is in Spanish asking about how to improve a RAG system. A user recommends using an existing library or framework instead of starting from scratch.
19. [Mercury Diffusion - 700t/s !!](https://www.reddit.com/r/LocalLLaMA/comments/1lmrump/mercury_diffusion_700ts/) (Score: 0)
    *   The thread discusses Mercury Diffusion. Users point out that it's not local, weak at coding, and not new.
20. [The ollama models are excellent models that can be installed locally as a starting point but.....](https://www.reddit.com/r/LocalLLaMA/comments/1lmtlgp/the_ollama_models_are_excellent_models_that_can/) (Score: 0)
    *   The thread discusses Ollama models. A user mentions checking devstral and using it with openhands/roo code. They clarify that Ollama is a backend, not a model, and suggest that the user should learn more about how LLMs work and what finetuning is.

# Detailed Analysis by Thread
**[What framework are you using to build AI Agents? (Score: 50)](https://www.reddit.com/r/LocalLLaMA/comments/1lmni3q/what_framework_are_you_using_to_build_ai_agents/)**
*   **Summary:** The thread explores different frameworks and approaches for building AI agents, with users sharing their experiences and preferences.
*   **Emotion:** The overall emotional tone of the thread is neutral, with some comments expressing positive or negative sentiment towards specific frameworks.
*   **Top 3 Points of View:**
    *   Some prefer using established frameworks like LangGraph, CrewAI, and PydanticAI.
    *   Others advocate for building custom solutions using in-memory state managers and the Unix philosophy.
    *   Some users are experimenting with new frameworks like HF smolagents and are looking for recommendations regarding GPU compute and VRAM requirements.

**[support for the upcoming ERNIE 4.5 0.3B model has been merged into llama.cpp (Score: 35)](https://github.com/ggml-org/llama.cpp/pull/14408)**
*   **Summary:** The thread announces the merging of support for the ERNIE 4.5 0.3B model into llama.cpp and discusses its potential.
*   **Emotion:** The overall emotional tone of the thread is positive, with excitement about the new model and its potential performance.
*   **Top 3 Points of View:**
    *   The release is greatly appreciated and the day 0 support is welcome.
    *   Some are curious about the model's architecture, capabilities, and origin.
    *   Some users are skeptical of claims that it outperforms OpenAI's models.

**[Consumer hardware landscape for local LLMs June 2025 (Score: 25)](https://www.reddit.com/r/LocalLLaMA/comments/1lmmh3l/consumer_hardware_landscape_for_local_llms_june/)**
*   **Summary:** This thread dives into the current consumer hardware options for running local LLMs, with comparisons between different GPUs, APUs and CPU systems.
*   **Emotion:** The overall emotional tone is neutral, focused on providing information and comparing hardware options. There's some disappointment expressed towards the APU space.
*   **Top 3 Points of View:**
    *   3090 remains the best bang for your buck, but a 4090 has additional compute that makes it useful for batch processing,image/video gen, or training.
    *   For inference computers, it's recommended to spend $2-5K on a system with 400GB/s+ of theoretical system MBW and 768GB-1TB of RAM, and a beefy GPU or two for prefill and shared experts.
    *   Intel B60 is an interesting option if it launches at $500.

**[deepseek-r1-0528 ranked #2 on lmarena, matching best from chatgpt (Score: 19)](https://www.reddit.com/r/LocalLLaMA/comments/1lmqsru/deepseekr10528_ranked_2_on_lmarena_matching_best/)**
*   **Summary:** The thread discusses the deepseek-r1-0528 model's ranking on lmarena and its practical performance in systems engineering.
*   **Emotion:** The overall emotional tone is positive, with users sharing positive experiences with the model.
*   **Top 3 Points of View:**
    *   Deepseek-r1-0528 is considered hard to beat in practical day-to-day Systems Engineering usage.
    *   Deepseek feels better than Claude Sonnet or GPT.
    *   Claude Opus 4 is on par with Mistral Medium 3.

**[Many small evals are better than one big eval [techniques] (Score: 14)](https://www.reddit.com/r/LocalLLaMA/comments/1lmmvmj/many_small_evals_are_better_than_one_big_eval/)**
*   **Summary:** This thread advocates for small, iterative evaluations during development.
*   **Emotion:** The overall emotional tone is positive, with agreement on the benefits of small evals.
*   **Top 3 Points of View:**
    *   It's better to start small, even while prototyping.
    *   The industry is pushing towards agents, and the same principle may apply.

**[Gemma3n:2B and Gemma3n:4B models are ~40% slower than equivalent models in size running on Llama.cpp (Score: 6)](https://www.reddit.com/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/)**
*   **Summary:** The thread discusses the performance of Gemma3n models on Llama.cpp.
*   **Emotion:** The overall emotional tone is neutral, with users sharing technical observations about the model's performance.
*   **Top 3 Points of View:**
    *   Gemma3n models are slower than other models of similar size when running on Llama.cpp.
    *   Gemma3n E4B UD-Q6_K_XL is only slightly faster than Gemma 3 27B UD-Q4_K_XL on a 4090.
    *   CPU usage is heavier with E4B.

**[What are Coqui-TTS alternatives? (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1lmn5k2/what_are_coquitts_alternatives/)**
*   **Summary:** This thread is looking for alternatives to Coqui-TTS.
*   **Emotion:** The overall emotional tone is neutral, with some expressions of positivity regarding suggested alternatives.
*   **Top 3 Points of View:**
    *   Checkout ChatterBox TTS by Resemble AI.
    *   kokoro is really good
    *   Take a look at Orpheus.

**[i5-8500 (6 cores), 24GB DDR4 2666 dual channel, realistic expectations for 3b/4b models? (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1lmt3kt/i58500_6_cores_24gb_ddr4_2666_dual_channel/)**
*   **Summary:** The thread discusses the performance expectations for 3b/4b LLMs on an i5-8500 system with 24GB DDR4 RAM.
*   **Emotion:** The overall emotional tone is neutral, focused on technical limitations and potential solutions.
*   **Top 3 Points of View:**
    *   Memory bandwidth is the primary bottleneck when running LLMs on the CPU.
    *   Upgrading DDR4 RAM to 3600+ MHz or adding a GPU can improve performance.
    *   Qwen-30B-A3B could be a good option as a sparse model.

**[Looking for Android chat ui (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1lmrd6x/looking_for_android_chat_ui/)**
*   **Summary:** This thread is a request for recommendations on Android chat UIs.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 2 Points of View:**
    *   Installation of Firefox, adding PageAssist extension and setting it up with your API is recommended.
    *   Have you jiba jabbered with Gemini?

**[Link between LM Studio and tools/functions? (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1lmoqsl/link_between_lm_studio_and_toolsfunctions/)**
*   **Summary:** The thread asks about the link between LM Studio and tools/functions.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top Points of View:**
    *   I've been writing tools with the python SDK

**[EPYC cpu build. Which cpu? (9354, 9534, 9654) (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1lmr1qh/epyc_cpu_build_which_cpu_9354_9534_9654/)**
*   **Summary:** The thread is about choosing the right EPYC CPU for a build.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 3 Points of View:**
    *   Pick the one with highest clock-speed.
    *   More CCDs -> more memory bandwidth.
    *   Higher single core performance, 16 core +

**[Best model tuned specifically for Programming? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/)**
*   **Summary:** The thread asks for recommendations on models tuned for programming.
*   **Emotion:** The overall emotional tone is neutral, with some expressing positivity regarding recommended models.
*   **Top 3 Points of View:**
    *   I've been satisfied with GLM 4-0414.
    *   I’d recommend a used 3090, 4090, or 5090.
    *   The best model is the next model: Qwen 3 coder is on the way.

**[Can Copilot be trusted with private source code more than competition? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1lmsme1/can_copilot_be_trusted_with_private_source_code/)**
*   **Summary:** This thread discusses the trustworthiness of Copilot with private source code.
*   **Emotion:** The overall emotional tone is negative, with widespread distrust.
*   **Top 3 Points of View:**
    *   Snowden says no.
    *   well do you trust microsoft enough to put your code on github?
    *   They all capture our data!

**[Best GGUF Base Models Under 3B for Unfiltered NSFW Roleplay? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1lmvosa/best_gguf_base_models_under_3b_for_unfiltered/)**
*   **Summary:** This thread is looking for GGUF models for NSFW roleplay.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top Points of View:**
    *   install sillytavern on your normal computer and use tailscale to access it from your phone

**[Multimodal Multistage Reasoning (Score: 0)](https://i.redd.it/a5k3d6h18p9f1.jpeg)**
*   **Summary:** The thread shows an image of a system prompt and includes a link to a comment.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top Points of View:**
    *   nice system prompt btw
    *   coincidence?

**[Play Infinite Tic Tac Toe against LLM Models (Score: 0)](https://v.redd.it/v346kcuiio9f1)**
*   **Summary:** The thread features a game where a Local LLM uses an MCP with an inventory of strategies.
*   **Emotion:** The overall emotional tone is negative.
*   **Top 3 Points of View:**
    *   Gives me an idea for a game where a Local LLM uses an MCP with an inventory of strategies against the player and the model decides which move to make based on the available options.
    *   Sounds like a total waste of resources.
    *   At that point why even use an LLM in the first place?

**[Which are the best realistic video generation tools (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1lmmxh1/which_are_the_best_realistic_video_generation/)**
*   **Summary:** The thread lists video generation tools.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top Points of View:**
    *   Veo, Sora, Kling : Closed Source Pay to Use
    *   Wan, Ltx : Open Source + Pay to Use

**[Como mejorar un sistema RAG? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1lmqtby/como_mejorar_un_sistema_rag/)**
*   **Summary:** The thread is in Spanish asking about how to improve a RAG system.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top Points of View:**
    *   I don't think there is an "only English" rule but it would help if you wrote it in English if you want help.
    *   Has probado a usars alguna librería o framework ya existente de rag?

**[Mercury Diffusion - 700t/s !! (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1lmrump/mercury_diffusion_700ts/)**
*   **Summary:** The thread discusses Mercury Diffusion.
*   **Emotion:** The overall emotional tone is negative.
*   **Top 3 Points of View:**
    *   It’s not local, it is bad at coding, and it is nothing new/special.
    *   What’s it like working at Inception Labs?
    *   This is at least a third time I've seen a post about this model. It's not new and it's pretty weak at coding which is exactly the field it's supposed to be good at.

**[The ollama models are excellent models that can be installed locally as a starting point but..... (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1lmtlgp/the_ollama_models_are_excellent_models_that_can/)**
*   **Summary:** The thread discusses Ollama models.
*   **Emotion:** The overall emotional tone is neutral.
*   **Top 2 Points of View:**
    *   Check devstral and use it with openhands/roo code
    *   olama is just a backend, not a model

