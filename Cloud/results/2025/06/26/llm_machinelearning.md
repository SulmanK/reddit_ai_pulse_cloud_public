---
title: "Machine Learning Subreddit"
date: "2025-06-26"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "NLP"]
---

# Overall Ranking and Top Discussions
1.  [[D] Alarming amount of schizoid people being validated by LLMs, anyone else experienced this?](https://www.reddit.com/r/MachineLearning/comments/1lkmkuw/d_alarming_amount_of_schizoid_people_being/) (Score: 240)
    *   This thread discusses concerns about Large Language Models (LLMs) validating schizoid or delusional thinking.
2.  [[D] Did you get Neurips reviews assignments?](https://www.reddit.com/r/MachineLearning/comments/1lktj7p/d_did_you_get_neurips_reviews_assignments/) (Score: 32)
    *   This thread discusses the assignment of NeurIPS review duties, with some users receiving more or fewer assignments than expected.
3.  [[D] Suggestions on dealing with rejections](https://www.reddit.com/r/MachineLearning/comments/1lkq2zw/d_suggestions_on_dealing_with_rejections/) (Score: 25)
    *   This thread offers advice on how to handle paper rejections from conferences and journals, suggesting alternative venues and strategies for revision.
4.  [[D] emnlp 2025 review](https://www.reddit.com/r/MachineLearning/comments/1ll5agc/d_emnlp_2025_review/) (Score: 7)
    *   This thread is about EMNLP 2025 paper reviews.
5.  [[D] EMNLP 2025 Paper Reviews](https://www.reddit.com/r/MachineLearning/comments/1ll6nsq/d_emnlp_2025_paper_reviews/) (Score: 5)
    *   This thread is about EMNLP 2025 paper reviews, and people are discussing their scores.
6.  [[D] how does the authors of those mind-blowing papers come up with those ideas?](https://www.reddit.com/r/MachineLearning/comments/1ll9c4e/d_how_does_the_authors_of_those_mindblowing/) (Score: 1)
    *   This thread discusses the process of generating innovative ideas for research papers, emphasizing the role of experience and incremental improvements.
7.  [[R] Any proxy methods for labeling indirect/implicit emotions without human annotators?](https://www.reddit.com/r/MachineLearning/comments/1lkkmqw/r_any_proxy_methods_for_labeling_indirectimplicit/) (Score: 0)
    *   This thread seeks advice on methods for labeling indirect/implicit emotions without human annotators.
8.  [[R] LLM usage locally in mobile](https://www.reddit.com/r/MachineLearning/comments/1ll7fd6/r_llm_usage_locally_in_mobile/) (Score: 0)
    *   This thread discusses the challenges of running Large Language Models (LLMs) locally on mobile devices, specifically related to RAM and storage requirements.

# Detailed Analysis by Thread
**[[D] Alarming amount of schizoid people being validated by LLMs, anyone else experienced this? (Score: 240)](https://www.reddit.com/r/MachineLearning/comments/1lkmkuw/d_alarming_amount_of_schizoid_people_being/)**
*   **Summary:** The thread discusses the observation that LLMs may be validating schizoid or delusional thinking in some individuals, potentially exacerbating their condition. Users share anecdotal evidence and express concern about the implications.
*   **Emotion:** The overall emotional tone is mixed, with elements of concern, negativity, and cautious positivity.
*   **Top 3 Points of View:**
    *   LLMs' overly positive and affirming nature can be counterproductive and harmful.
    *   There's concern that LLMs might be "making people schizophrenic" or worsening existing conditions.
    *   LLMs are becoming tools for validating delusions, especially in isolated individuals.

**[[D] Did you get Neurips reviews assignments? (Score: 32)](https://www.reddit.com/r/MachineLearning/comments/1lktj7p/d_did_you_get_neurips_reviews_assignments/)**
*   **Summary:** The thread centers on the experiences of users regarding their NeurIPS review assignments. Some users received more assignments than expected, while others received none, leading to discussion about the fairness and criteria of the assignment process.
*   **Emotion:** Predominantly neutral, with hints of frustration and resignation.
*   **Top 3 Points of View:**
    *   The NeurIPS review assignment system seems inconsistent and potentially unfair.
    *   Some reviewers are overwhelmed with assignments, while others are not assigned any.
    *   Review assignments are influenced by factors like expertise, reviewer rank, and prior publications.

**[[D] Suggestions on dealing with rejections (Score: 25)](https://www.reddit.com/r/MachineLearning/comments/1lkq2zw/d_suggestions_on_dealing_with_rejections/)**
*   **Summary:** This thread provides suggestions for dealing with paper rejections from conferences and journals, including advice on revising the narrative, targeting different venues, and understanding the role of randomness in the review process.
*   **Emotion:** Mostly neutral with some positive undertones related to encouragement and offering solutions, but also negative undertones from discussing the frustration of rejection.
*   **Top 3 Points of View:**
    *   Consider submitting to journals, as they offer specific improvement suggestions.
    *   Revise the paper's narrative and framing based on reviewer feedback.
    *   Accept that randomness plays a role in the review process and learn when to cut losses and target different venues.

**[[D] emnlp 2025 review (Score: 7)](https://www.reddit.com/r/MachineLearning/comments/1ll5agc/d_emnlp_2025_review/)**
*   **Summary:** The thread discusses the review scores for a long paper on information extraction submitted to EMNLP 2025. The user seeks advice on the chances of acceptance to Findings or Main tracks.
*   **Emotion:** Primarily neutral, with some anxiety and hope.
*   **Top 3 Points of View:**
    *   ARR (Action Editor Review) uses a 1-5 scale to categorize papers into main, findings, or rejection.
    *   Random outcomes are frequent in ARR due to lack of clear discrimination.
    *   It's better to think of ARR as a journal submission with opportunities for improvement and resubmission.

**[[D] EMNLP 2025 Paper Reviews (Score: 5)](https://www.reddit.com/r/MachineLearning/comments/1ll6nsq/d_emnlp_2025_paper_reviews/)**
*   **Summary:** This thread features users sharing their EMNLP 2025 paper review scores and asking for opinions on their chances of acceptance.
*   **Emotion:** Mostly neutral, with some anxiety related to the review outcomes.
*   **Top 3 Points of View:**
    *   Users are comparing their scores to gauge their chances of acceptance.
    *   Some users are having technical difficulties with the OpenReview platform.
    *   There's general uncertainty about the acceptance criteria.

**[[D] how does the authors of those mind-blowing papers come up with those ideas? (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1ll9c4e/d_how_does_the_authors_of_those_mindblowing/)**
*   **Summary:** The thread explores the question of how researchers develop breakthrough ideas for machine learning papers, attributing it to a combination of experience, dedication, and incremental improvements rather than sheer genius.
*   **Emotion:** Primarily neutral, with elements of curiosity and inspiration.
*   **Top 3 Points of View:**
    *   Breakthrough ideas are often built on existing knowledge and stepping stones.
    *   Dedication, intelligence, creativity, and critical thinking are key.
    *   Improvements in Machine Learning are mostly incremental, not revolutionary like in physics.

**[[R] Any proxy methods for labeling indirect/implicit emotions without human annotators? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1lkkmqw/r_any_proxy_methods_for_labeling_indirectimplicit/)**
*   **Summary:** This thread requests advice on methods for labeling indirect/implicit emotions without relying on human annotators.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   Unsupervised learning can organize data and speed up annotation.
    *   Using a pre-trained model to embed samples and then applying unsupervised learning for organization can help.
    *   The Ascertain dataset might be helpful, providing biometrics and self-reporting data for verification.

**[[R] LLM usage locally in mobile (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ll7fd6/r_llm_usage_locally_in_mobile/)**
*   **Summary:** This thread discusses the challenges of using Large Language Models (LLMs) locally on mobile devices.
*   **Emotion:** Neutral.
*   **Top 3 Points of View:**
    *   LLMs require significant RAM and storage.
    *   Downsizing LLMs to fit mobile devices can negatively impact performance quality.
