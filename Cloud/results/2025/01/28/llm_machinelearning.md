---
title: "Machine Learning Subreddit"
date: "2025-01-28"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machine learning", "AI", "research"]
---

# Overall Ranking and Top Discussions
1.  [[D] Ever feel like you're reinventing the wheel with every scikit-learn project? Let's talk about making ML recommended practices less painful. ðŸ¤”](https://www.reddit.com/r/MachineLearning/comments/1ic5e7f/d_ever_feel_like_youre_reinventing_the_wheel_with/) (Score: 21)
    *   This thread discussed the pain points in using scikit-learn for ML projects, specifically the lack of standard practices for model iteration and inference.
2.  [[D] Censorship differences in Deepseek R1 between distilled versions](https://www.reddit.com/r/MachineLearning/comments/1ibkckg/d_censorship_differences_in_deepseek_r1_between/) (Score: 11)
    *   This thread discussed the perceived censorship in the Deepseek R1 model and its distilled versions, where users speculated about the possible reasons behind it.
3.  [[R] Anyone tried writing a CVPR rebuttal without a reference section ?](https://www.reddit.com/r/MachineLearning/comments/1ibllko/r_anyone_tried_writing_a_cvpr_rebuttal_without_a/) (Score: 4)
    *   This thread discussed the formatting requirements for CVPR rebuttals, specifically whether it's possible to omit the reference section, with users referencing the CVPR guidelines and suggesting alternative reference formats.
4.  [[D] What's the difference between model-based and model-free reinforcement learning?](https://www.reddit.com/r/MachineLearning/comments/1ibyiv6/d_whats_the_difference_between_modelbased_and/) (Score: 2)
    *   This thread explored the differences between model-based and model-free reinforcement learning, explaining the core differences in how they learn and use their models.
5.  [[D] Speaker diarization models for 3 or more overlapping speakers?](https://www.reddit.com/r/MachineLearning/comments/1ibzhsc/d_speaker_diarization_models_for_3_or_more/) (Score: 2)
    *  This thread explored speaker diarization models for scenarios with three or more overlapping speakers, where users discussed the challenges, suggesting libraries, and recommending ways to enhance accuracy
6.  [[P] Best model for multi class text classification](https://www.reddit.com/r/MachineLearning/comments/1ic49q4/p_best_model_for_multi_class_text_classification/) (Score: 2)
    *  This thread simply asked for the best model for multi-class text classification, receiving recommendations for Distil bert, Roberta, and Deberta.
7.  [[D] - Challenges in querying data from a knowledge graph](https://www.reddit.com/r/MachineLearning/comments/1ic5q7s/d_challenges_in_querying_data_from_a_knowledge/) (Score: 1)
    *   This thread discussed the challenges of querying data from a knowledge graph, with a user asking whether to create a base graph for themselves.
8.  [[D] DeepSeek R1 says he is Chat GPT?](https://www.reddit.com/gallery/1ibnz9t) (Score: 0)
    *  This thread discusses the DeepSeek R1 model incorrectly identifying itself as ChatGPT. It led to users discussing possible reasons for this, such as it being trained on ChatGPT data.
9.  [[P]  Implement GPT1 on Numpy](https://www.reddit.com/r/MachineLearning/comments/1ibohv3/p_implement_gpt1_on_numpy/) (Score: 0)
    *   This thread is about implementing GPT-1 on NumPy, with users suggesting it might be more suitable for a different subreddit.
10. [Discussion] Open source projects or research papers](https://www.reddit.com/r/MachineLearning/comments/1ibwkn5/discussion_open_source_projects_or_research_papers/) (Score: 0)
    *   This thread discussed open-source alternatives to Opus Clip, focusing on video editing capabilities
11. [[D] DeepSeekâ€™s $5.6M Training Cost: A Misleading Benchmark for AI Development?](https://www.reddit.com/r/MachineLearning/comments/1ibzsxa/d_deepseeks_56m_training_cost_a_misleading/) (Score: 0)
    *   This thread discussed if the $5.6 million DeepSeek training cost is misleading or a fair metric and discussed various aspects related to the costs of training such models
12.  [[D] Deepseek R1 cheating benchmarks?](https://www.reddit.com/r/MachineLearning/comments/1ic5961/d_deepseek_r1_cheating_benchmarks/) (Score: 0)
    *   This thread discussed whether Deepseek R1 is cheating on benchmarks by using a process called distillation, with another user pointing out that it was not distilled.

# Detailed Analysis by Thread
**[ [D] Ever feel like you're reinventing the wheel with every scikit-learn project? Let's talk about making ML recommended practices less painful. ðŸ¤” (Score: 21)](https://www.reddit.com/r/MachineLearning/comments/1ic5e7f/d_ever_feel_like_youre_reinventing_the_wheel_with/)**
*  **Summary:**  The thread discusses the challenges and frustrations in applying consistent, best-practice methods when working with scikit-learn for machine learning projects. Users highlight the lack of standardized approaches for model iteration, leading to repetitive work and difficulties in managing hyperparameters.
*  **Emotion:** The emotional tone of the thread is predominantly neutral, with a hint of frustration about the challenges faced in the ML workflow.
*  **Top 3 Points of View:**
    *   Users express that they feel like they are reinventing the wheel with every project.
    *   The lack of proper serialization for models makes them hard to use for inference.
    *   Keeping track of hyperparameters is a major roadblock in iterative model development.

**[[D] Censorship differences in Deepseek R1 between distilled versions (Score: 11)](https://www.reddit.com/r/MachineLearning/comments/1ibkckg/d_censorship_differences_in_deepseek_r1_between/)**
*  **Summary:** The thread discusses perceived differences in censorship between the Deepseek R1 model and its distilled versions, with the user interpreting the difference as more of an issue of a model being "dumb" rather than intentional censorship.
*  **Emotion:** The emotional tone is neutral, with a hint of humor due to the comment that the original model was "dumb".
*  **Top 3 Points of View:**
    *   Some users believe that the perceived censorship is due to the models being "dumb" rather than intentional censorship.
    *   There's speculation about whether the censorship is because it is a Chinese base model
    *   The user is not surprised by the censorship.

**[[R] Anyone tried writing a CVPR rebuttal without a reference section ? (Score: 4)](https://www.reddit.com/r/MachineLearning/comments/1ibllko/r_anyone_tried_writing_a_cvpr_rebuttal_without_a/)**
*  **Summary:** The thread discusses the feasibility of writing a CVPR rebuttal without a reference section. Users refer to CVPR guidelines, and suggest alternate referencing formats.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   According to CVPR 2025 guidelines, it is not appropriate to omit the reference section.
    *   It's acceptable to use shorthand references like "(*** et al. CVPR08)".
    *   Users point to previous discussions about CVPR rebuttals.

**[[D] What's the difference between model-based and model-free reinforcement learning? (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1ibyiv6/d_whats_the_difference_between_modelbased_and/)**
*  **Summary:** This thread explains the distinction between model-based and model-free reinforcement learning. Model-free learns a policy/value function, while model-based learns a model of the environment dynamics.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Model-free RL learns a policy/value function without explicitly modeling the environment.
    *   Model-based RL learns a dynamics model for planning and policy improvement.
    *   If the model can predict the next state and reward, it's model-based.

**[[D] Speaker diarization models for 3 or more overlapping speakers? (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1ibzhsc/d_speaker_diarization_models_for_3_or_more/)**
*  **Summary:** The thread seeks advice on speaker diarization models for three or more overlapping speakers. Users offer suggestions on libraries and ways to improve accuracy.
*  **Emotion:**  The emotional tone is neutral, with a mix of helpfulness and some negative feedback on the accuracy of some models.
*  **Top 3 Points of View:**
    *   The pyannote/speaker-diarization-3.1 model can do a decent job, but often creates too many speakers.
    *   Using Huggingface.co/speechbrain/spkrec-ecapa-voxceleb and generating speaker embeddings helps clean up diarization accuracy.
    *  Users recommend trying Deepgram for its commercial side.

**[[P] Best model for multi class text classification (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1ic49q4/p_best_model_for_multi_class_text_classification/)**
*  **Summary:** This thread simply asks for the best model for multi-class text classification and gets specific recommendations.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Distil bert is a good model for this task.
    *   Roberta is another good option.
    *   Deberta can also be used effectively.

**[[D] - Challenges in querying data from a knowledge graph (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1ic5q7s/d_challenges_in_querying_data_from_a_knowledge/)**
*  **Summary:** This thread discusses the challenges of querying data from a knowledge graph, with one user questioning if a base graph should be created to avoid random placements.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    * One point of view is to create a base graph with well-defined nodes and entities for better data organization.
    * Another point of view involves adding entities to nodes and not placing everything randomly.
    * There aren't really 3 points of view, just the core question of the post.

**[[D] DeepSeek R1 says he is Chat GPT? (Score: 0)](https://www.reddit.com/gallery/1ibnz9t)**
*  **Summary:** This thread discusses how the DeepSeek R1 model incorrectly identifies itself as ChatGPT. Users discuss possible reasons, including training data and hacking.
*  **Emotion:** The emotional tone is neutral, with a touch of concern about potential security issues.
*  **Top 3 Points of View:**
    *   DeepSeek might be trained on ChatGPT outputs.
    *   It's unlikely the model knows what it is due to how it's trained.
    *   Some users suspect DeepSeek is using hacked ChatGPT accounts.

**[[P]  Implement GPT1 on Numpy (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ibohv3/p_implement_gpt1_on_numpy/)**
*  **Summary:**  This thread is about an implementation of GPT-1 using Numpy. Users suggest that this post might be more relevant to a different subreddit.
*  **Emotion:** The emotional tone is positive and neutral.
*  **Top 3 Points of View:**
    *   The work is well done.
    *   The post may fit better in r/learnmachinelearning.
    *   The original poster was trying to do some good work.

**[[Discussion] Open source projects or research papers (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ibwkn5/discussion_open_source_projects_or_research_papers/)**
*  **Summary:** This thread discusses open-source alternatives to Opus Clip, focusing on video editing capabilities.
*  **Emotion:** The emotional tone is neutral.
*  **Top 3 Points of View:**
    *   Opus Clip automates short-form video creation.
    *   Shotcut and Kdenlive are good open-source alternatives.
    *   These tools offer advanced editing without subscription limitations.

**[[D] DeepSeekâ€™s $5.6M Training Cost: A Misleading Benchmark for AI Development? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ibzsxa/d_deepseeks_56m_training_cost_a_misleading/)**
*  **Summary:** This thread questions whether DeepSeek's $5.6M training cost is misleading as a benchmark for AI development, with users providing various viewpoints on this metric.
*  **Emotion:** The emotional tone is mixed, with some positive feelings about the cost efficiency and a few neutral opinions.
*  **Top 3 Points of View:**
    *   The cost is a positive shift towards cheaper model development.
    *   The training cost is important on its own.
    *   Deepseek may be saving costs through the use of synthetic data and proprietary efficiencies.

**[[D] Deepseek R1 cheating benchmarks? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ic5961/d_deepseek_r1_cheating_benchmarks/)**
*  **Summary:**  This thread discusses the possibility of Deepseek R1 cheating on benchmarks, mainly on the topic of distillation, where users clarify that R1 is not a distilled model.
*  **Emotion:** The emotional tone is informative, but mixed with some frustration for confusion.
*  **Top 3 Points of View:**
    *   Distillation is a process of training a model on the probabilities of another model.
    *   Deepseek R1 is a fine-tuned model, not a distilled one.
    *   Neither R1 nor its distilled version saw the answers to the benchmarks.
