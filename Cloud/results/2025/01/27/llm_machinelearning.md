---
title: "Machine Learning Subreddit"
date: "2025-01-27"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "deeplearning"]
---

# Overall Ranking and Top Discussions
1.  [[D] Why did DeepSeek open-source their work?](https://www.reddit.com/r/MachineLearning/comments/1ib2vtx/d_why_did_deepseek_opensource_their_work/) (Score: 468)
    *   The discussion revolves around why DeepSeek decided to open-source their work, with various theories and perspectives being shared.
2.  [[D] Why higher even powers like 4, 6, etc are not used in loss functions such as linear regression loss function?](https://www.reddit.com/r/MachineLearning/comments/1ib3dhn/d_why_higher_even_powers_like_4_6_etc_are_not/) (Score: 50)
    *   This thread discusses the reasons why higher even powers are not commonly used in loss functions for linear regression, exploring both theoretical and practical aspects.
3.  [[R][Q] Sorry i was looking for that book with all the basics in ML in math; it was about 400 pages, i remember someone posted here but can't found it](https://www.reddit.com/r/MachineLearning/comments/1iaw81h/rq_sorry_i_was_looking_for_that_book_with_all_the/) (Score: 15)
    *   Users are discussing and recommending books that cover the mathematical foundations of machine learning.
4.  [[D] what would you suggest a person who is coming back to the industry after a few years?](https://www.reddit.com/r/MachineLearning/comments/1ib6c03/d_what_would_you_suggest_a_person_who_is_coming/) (Score: 7)
    *   This thread offers advice to individuals re-entering the machine learning industry after a break, focusing on areas of study and resources.
5.  [[D] What do people do for storing/streaming LLM embeddings?](https://www.reddit.com/r/MachineLearning/comments/1ibe0bm/d_what_do_people_do_for_storingstreaming_llm/) (Score: 4)
     *  This post is asking how people handle storing and streaming LLM embeddings.
6.  [https://www.deeplearning.ai/the-batch/how-metas-llama-nlp-model-leaked/](https://www.deeplearning.ai/the-batch/how-metas-llama-nlp-model-leaked/) (Score: 1)
     *  The post discusses where to find Meta's Llama model weights.
7.  [[D] Trying to implement CarLLAVA](https://www.reddit.com/r/MachineLearning/comments/1ibevf4/d_trying_to_implement_carllava/) (Score: 0)
    *   A user is seeking help implementing a model called CarLLAVA.
8.  [local 8b model thinks like crazyyy [D]](https://www.reddit.com/r/MachineLearning/comments/1ib9aev/local_8b_model_thinks_like_crazyyy_d/) (Score: 0)
     *  A user is looking to share a document related to their local 8b model.
9.  [[D] Randomly Generated Maps for FP/OTS games](https://www.reddit.com/r/MachineLearning/comments/1ib8e5k/d_randomly_generated_maps_for_fpots_games/) (Score: 0)
    *   This thread explores the possibilities of randomly generated maps for first person/over the shoulder games and the challenges involved in using AI to do it.
10.  [[Discussion] I may have created a formula that gives AI emotions.](https://www.reddit.com/r/MachineLearning/comments/1iauvm4/discussion_i_may_have_created_a_formula_that/) (Score: 0)
    *   This thread discusses the possibility of creating AI emotions through a formula and delves into whether they are behavioural biases.

# Detailed Analysis by Thread
**[ [D] Why did DeepSeek open-source their work? (Score: 468)](https://www.reddit.com/r/MachineLearning/comments/1ib2vtx/d_why_did_deepseek_opensource_their_work/)**
*  **Summary:** The discussion revolves around the motivations behind DeepSeek's decision to open-source their work. Several reasons are proposed, including strategic growth, financial incentives, and political statements.
*  **Emotion:** The overall emotional tone of the thread is neutral, with a mix of curiosity and analysis.
*  **Top 3 Points of View:**
    * Open-sourcing is a strategic move for growth and adoption. It generates hype, attracts fine-tuning, and allows companies to later monetize through features and support.
    *  DeepSeek may have open-sourced their work as a political statement, especially considering the current geopolitical situation with the USA.
    *  DeepSeek is a company with a focus on hiring geniuses who like freely writing about their ideas in technical reports and getting famous for their work.

**[ [D] Why higher even powers like 4, 6, etc are not used in loss functions such as linear regression loss function? (Score: 50)](https://www.reddit.com/r/MachineLearning/comments/1ib3dhn/d_why_higher_even_powers_like_4_6_etc_are_not/)**
*  **Summary:** This thread discusses the theoretical and practical reasons behind using the squared loss function instead of higher-order even powers (like 4 or 6) in linear regression. The discussion touches on the statistical interpretation of the loss function, its relation to likelihood, and practical optimization considerations.
*  **Emotion:** The thread is predominantly neutral and analytical, focusing on technical explanations. There is some variation with the users also adding their experiences and insights from experimentation.
*  **Top 3 Points of View:**
    * The squared loss (L2 loss) corresponds to the assumption of normally distributed errors, making it statistically grounded and interpretable as the variance of errors.
    *  Using higher powers can lead to excessive penalization of outliers, numerical instability and potentially result in the loss function diverging.
    *  MSE is the loss function that gives the fastest and most stable convergence for minimizing squared error.

**[ [R][Q] Sorry i was looking for that book with all the basics in ML in math; it was about 400 pages, i remember someone posted here but can't found it (Score: 15)](https://www.reddit.com/r/MachineLearning/comments/1iaw81h/rq_sorry_i_was_looking_for_that_book_with_all_the/)**
*  **Summary:** The thread is a request for recommendations for a specific book on the math behind ML that the user saw previously, and others suggest various options.
*  **Emotion:** The overall tone is positive and helpful, with users actively trying to assist the original poster.
*  **Top 3 Points of View:**
    *  The book "[Mathematics for Machine Learning](https://mml-book.github.io/)" was recommended.
    *  The book "[Deep Learning](https://www.amazon.co.uk/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/)" was recommended for its depth and mathematical rigor.
    *  The book "[Dive into Deep Learning](https://d2l.ai/)" was recommended as another resource covering the math of ML and IA.

**[ [D] what would you suggest a person who is coming back to the industry after a few years? (Score: 7)](https://www.reddit.com/r/MachineLearning/comments/1ib6c03/d_what_would_you_suggest_a_person_who_is_coming/)**
*  **Summary:** This thread asks for suggestions on what someone returning to the ML industry after a few years should focus on. Advice includes refreshing coding skills, reviewing fundamental concepts, and exploring practical resources. The discussion acknowledges how quickly the field is evolving.
*  **Emotion:** The emotional tone is mixed. While some responses are positive and helpful, there's also a sense of being overwhelmed by the rapid pace of change in the field.
*  **Top 3 Points of View:**
    * Refresh coding skills and review basics like optimization, data preparation, and implementing relevant architectures.
    *  Explore resources like Hugging Face for learning and updates.
    *  Utilize tools like Copilot, Cursor, or ChatGPT to build infrastructure and stay up to date with new developments.

**[ [D] What do people do for storing/streaming LLM embeddings? (Score: 4)](https://www.reddit.com/r/MachineLearning/comments/1ibe0bm/d_what_do_people_do_for_storingstreaming_llm/)**
*  **Summary:**  This thread discusses strategies for storing and streaming LLM embeddings, with a specific focus on practical implementations.
*  **Emotion:** The tone is neutral and informative.
*  **Top 3 Points of View:**
    * One user suggests using a quantized model for lower memory usage.
    * They also suggested storing the embeddings as comma-separated strings of floats in a SQLite database.
    * They convert the comma seperated string back to an array with np.from_string.

**[https://www.deeplearning.ai/the-batch/how-metas-llama-nlp-model-leaked/](https://www.deeplearning.ai/the-batch/how-metas-llama-nlp-model-leaked/) (Score: 1)**
*  **Summary:** The thread is about finding the torrent files for the Llama model weights and it provides a link to a GitHub repository.
*   **Emotion:** The tone is neutral and informative
*  **Top 3 Points of View:**
    * The user provides a link to a Github repository with the torrent files.

**[ [D] Trying to implement CarLLAVA (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ibevf4/d_trying_to_implement_carllava/)**
*  **Summary:** The thread is about trying to implement the CarLLAVA model.
*  **Emotion:** The tone is neutral.
*   **Top 3 Points of View:**
   *  One user suggests that they should have translated their text with ChatGPT before posting.

**[local 8b model thinks like crazyyy [D] (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ib9aev/local_8b_model_thinks_like_crazyyy_d/)**
*  **Summary:** This thread is about a local 8b model and sharing a link to a google doc related to the topic.
*   **Emotion:** The tone is neutral and informative.
*  **Top 3 Points of View:**
    * One user requested public access to the google docs link.

**[ [D] Randomly Generated Maps for FP/OTS games (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1ib8e5k/d_randomly_generated_maps_for_fpots_games/)**
*   **Summary:** The thread is about generating maps for games using AI.
*   **Emotion:** The tone is positive with a hint of curiosity.
*   **Top 3 Points of View:**
    *  Evaluating the generated maps is a challenge since it's hard to mimic the way humans see using an algorithm.
    *   A human tester could give feedback and this feedback would be used to improve the maps.

**[ [Discussion] I may have created a formula that gives AI emotions. (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1iauvm4/discussion_i_may_have_created_a_formula_that/)**
*  **Summary:**  The thread discusses a user's claim of creating a formula that gives AI emotions and how LLMs can be sycophantic.
*  **Emotion:**  The emotional tone is mixed with some skepticism, and negative feedback.
*  **Top 3 Points of View:**
    * Emotions can be seen as behavioral biases implemented at the precognitive level by models interacting with their own neural networks.
    * LLMs are sycophantic and will tell you that anything is a thought-provoking idea.
    *  Some responses suggest the user should avoid using LSD and Reddit at the same time.
