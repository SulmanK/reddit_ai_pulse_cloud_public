---
title: "LocalLLaMA Subreddit"
date: "2025-01-30"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["AI", "LLM", "Open Source", "Models"]
---

# Overall Ranking and Top Discussions
1.  [[Marc Andreessen on Anthropic CEO's Call for Export Controls on China](https://i.redd.it/wlsi25dcn6ge1.png)](https://i.redd.it/wlsi25dcn6ge1.png) (Score: 242)
    *  This thread discusses Marc Andreessen's opinion on export controls on China, with various commenters offering views on competition, open-source models, and the role of closed AI labs.
2.  [CLOSEDAI](https://i.redd.it/114wlf69q6ge1.jpeg) (Score: 129)
    *  This thread is about user experiences with a "ClosedAI" service, with many users reporting temporary restrictions. The discussions revolve around alternatives to ClosedAI and the need for open-source models.
3.  [Mistral Small 3 one-shotting Unsloth's Flappy Bird coding test in 1 min (vs 3hrs for DeepSeek R1 using NVME drive)](https://i.redd.it/gazbvr6gi6ge1.jpeg) (Score: 66)
     *  This thread compares the performance of Mistral Small 3 with DeepSeek R1 on a coding test, discussing the fairness of the comparison, and hardware setups for running models off NVME drives.
4.  [Welcome back, Le Mistral!](https://i.redd.it/4td7dsrjn6ge1.png) (Score: 46)
    *  This thread discusses the implications of a change, possibly a licensing change, for "Le Mistral", with one user complaining about the use of "Le line" instead of "La line" in French.
5.  [Re-Distilling DeepSeek R1](https://www.reddit.com/r/LocalLLaMA/comments/1idvuch/redistilling_deepseek_r1/) (Score: 18)
    * This thread discusses the costs of experimentation versus training for DeepSeek R1.
6.  [Kimi k1.5: Scaling Reinforcement Learning with LLMs --- an o1-level multi-modal model](https://github.com/MoonshotAI/Kimi-k1.5) (Score: 12)
    *  This thread discusses a new model called Kimi k1.5, and a user mentions that it is not open-source.
7.  [JSON output from Deepseek R1 and distills with llamacpp's server](https://toao.com/blog/json-output-from-deepseek-r1-and-distills-with-llamacpp) (Score: 3)
    *  A user is asking for feedback about JSON output from Deepseek R1 with llama.cpp.
8.  [How can I generate COT dataset? (fine-tune  deepseek distilled model)](https://www.reddit.com/r/LocalLLaMA/comments/1idwlka/how_can_i_generate_cot_dataset_finetune_deepseek/) (Score: 2)
     *  Users are discussing how to generate COT datasets for fine-tuning a deepseek distilled model.
9. [I got my 3090! Now I feel like replacing everything else](https://www.reddit.com/r/LocalLLaMA/comments/1idwnsn/i_got_my_3090_now_i_feel_like_replacing/) (Score: 2)
     *  A user is discussing upgrading their PC after getting a 3090.
10.  [What is a good LM for improving my writing?](https://www.reddit.com/r/LocalLLaMA/comments/1idu1r2/what_is_a_good_lm_for_improving_my_writing/) (Score: 1)
    *  This thread discusses good language models for improving writing, and how to use them for grammar correction, rewording, etc.
11. [New to LocalLLM - is it normal for 32b / 8b models to forget stuff so easily?](https://www.reddit.com/r/LocalLLaMA/comments/1iduy3d/new_to_localllm_is_it_normal_for_32b_8b_models_to/) (Score: 1)
    *   This thread discusses the context window issues on local LLMs and the reason why they forget information.
12. [Open-R1: a fully open reproduction of DeepSeek-R1 from huggingface](https://huggingface.co/blog/open-r1?utm_source=tldrai#what-is-deepseek-r1) (Score: 1)
    *  Users are discussing a blog post from huggingface about open sourcing the DeepSeek R1 model.
13. [How many 3080s to run 70B model?](https://www.reddit.com/r/LocalLLaMA/comments/1idxbi8/how_many_3080s_to_run_70b_model/) (Score: 0)
    * A user asks about how many 3080s are required to run a 70B model.

# Detailed Analysis by Thread
**[Marc Andreessen on Anthropic CEO's Call for Export Controls on China (Score: 242)](https://i.redd.it/wlsi25dcn6ge1.png)**
*  **Summary:** This thread centers around Marc Andreessen's stance on export controls regarding AI technology to China. Discussions cover various viewpoints regarding market competition, the potential for job displacement caused by AI models, and the motivations of AI labs.
*  **Emotion:** The emotional tone is mixed, ranging from negative to positive, with a large portion of neutral opinions. There's a significant undercurrent of concern about losing the AI competition and potential job losses, alongside some agreement with Andreessen's views.
*  **Top 3 Points of View:**
    *  Some users express concern about being replaced by cheap Chinese models and want to implement export controls.
    *  Others argue that market competition should be fair without regulatory interference and that export controls are ineffective.
    *  There is also a cynical view that closed AI labs support export controls to maintain their market dominance.

**[CLOSEDAI (Score: 129)](https://i.redd.it/114wlf69q6ge1.jpeg)**
*  **Summary:** This thread discusses user experiences with "ClosedAI," a service that has been temporarily restricting users. The conversation includes discussions about the value proposition of subscription services, and alternative models.
*  **Emotion:** The emotional tone is primarily negative and neutral. There's dissatisfaction with how ClosedAI handles user accounts, along with calls for better alternatives, namely open-source models.
*  **Top 3 Points of View:**
    *  Some users are frustrated by the arbitrary restrictions imposed by ClosedAI, which limits their usage of the service.
    *  Others express that open-source solutions are essential to avoid monopolization and are glad that open source alternatives are catching up.
    *  There's a general sentiment that ClosedAI's business practices are not sustainable or user-friendly.

**[Mistral Small 3 one-shotting Unsloth's Flappy Bird coding test in 1 min (vs 3hrs for DeepSeek R1 using NVME drive) (Score: 66)](https://i.redd.it/gazbvr6gi6ge1.jpeg)**
*  **Summary:**  This thread discusses a performance test comparing Mistral Small 3 and DeepSeek R1 in a coding task. Users debate the fairness of the comparison, mention hardware configurations, and suggest specific optimizations.
*  **Emotion:** The emotional tone is mixed. Users have a positive reaction to the results of Mistral Small 3, but some frustration when the comparison is called unfair.
*  **Top 3 Points of View:**
    *  Some users argue that the comparison was unfair because DeepSeek was run with a 1-bit quantization which affects speed, and point out that the point was to highlight the speed difference.
    *  Others mention that using NVME drives is a promising direction for running models.
    *  Some ask for more details about the test (prompts)

**[Welcome back, Le Mistral! (Score: 46)](https://i.redd.it/4td7dsrjn6ge1.png)**
*   **Summary:** This thread discusses a change regarding "Le Mistral," possibly a licensing update. A user also gives a linguistic correction about a common grammar mistake with the French language.
*   **Emotion:** The emotional tone is mostly neutral, mixed with a touch of negative sentiment from someone who is annoyed at a language error.
*   **Top 3 Points of View:**
    *  Some users are simply seeking clarification on the implications of the change, particularly in regard to licensing.
    *  One user is triggered by the use of "Le line" instead of "La line" in French.
    *  Some are simply happy to see Le Mistral return.

**[Re-Distilling DeepSeek R1 (Score: 18)](https://www.reddit.com/r/LocalLLaMA/comments/1idvuch/redistilling_deepseek_r1/)**
*   **Summary:** This thread discusses the costs of experimentation versus the final training cost of DeepSeek R1.
*   **Emotion:** The overall emotional tone is positive, as users appreciate the transparency of the notes regarding the costs.
*   **Top 3 Points of View:**
    *   Users appreciate the note regarding the experimentation costs.
    *   Another user jokingly points out that "double distillation" would have been a good way to describe the process.

**[Kimi k1.5: Scaling Reinforcement Learning with LLMs --- an o1-level multi-modal model (Score: 12)](https://github.com/MoonshotAI/Kimi-k1.5)**
*  **Summary:** This thread discusses Kimi k1.5, and how it is not open-source.
*  **Emotion:** The emotional tone is neutral with a hint of frustration regarding the fact that it is not open-source.
*  **Top 3 Points of View:**
    *   Users are expressing frustration for the lack of open-source alternatives and how they don't want us to have nice models locally on our machines.

**[JSON output from Deepseek R1 and distills with llamacpp's server (Score: 3)](https://toao.com/blog/json-output-from-deepseek-r1-and-distills-with-llamacpp)**
*   **Summary:**  The user is asking if someone has found a better way to work with DeepSeek R1 JSON output with llama.cpp's server.
*   **Emotion:** The emotional tone is neutral.
*    **Top 3 Points of View:**
    *   Users are just looking for feedback.

**[How can I generate COT dataset? (fine-tune  deepseek distilled model) (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1idwlka/how_can_i_generate_cot_dataset_finetune_deepseek/)**
*   **Summary:**  Users are discussing how to generate COT datasets for fine-tuning a deepseek distilled model.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   One user suggests checking out the code from Open Thoughts.
    *   Another user suggests a method of generating data through an LLM using two different prompts.

**[I got my 3090! Now I feel like replacing everything else (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1idwnsn/i_got_my_3090_now_i_feel_like_replacing/)**
*  **Summary:** The user expresses the urge to upgrade their whole PC after purchasing a new 3090 GPU. The conversation focuses on bottlenecks caused by offloading layers to CPU/RAM and the dangers of spiraling out of control when upgrading PCs.
*  **Emotion:** The tone is mostly negative, since the user is detailing the problems they are facing.
*  **Top 3 Points of View:**
    *   A user mentions that offloading layers to the CPU and RAM will have a huge performance impact.
    *   Another user details how they got carried away with upgrades after getting a 3090.

**[What is a good LM for improving my writing? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1idu1r2/what_is_a_good_lm_for_improving_my_writing/)**
*   **Summary:** This thread discusses suitable language models for enhancing writing skills, and how to use them for grammar correction, rewording, etc.
*   **Emotion:** The emotional tone is mostly neutral and helpful.
*   **Top 3 Points of View:**
    *   One user provides a detailed example of using a system prompt to improve wording and grammar with 3 different options.
    *  Another user asks about Grammarly, and if "smol bots" can be used for this task.
    *  One user mentions that roleplaying with an LLM is a good way to improve writing, by asking it to rewrite the message in proper English.

**[New to LocalLLM - is it normal for 32b / 8b models to forget stuff so easily? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1iduy3d/new_to_localllm_is_it_normal_for_32b_8b_models_to/)**
*   **Summary:** This thread discusses the issue of local LLMs forgetting information, and the reason why that happens.
*   **Emotion:** The emotional tone is neutral, as users try to understand the problem and find a solution.
*  **Top 3 Points of View:**
    *  Some users point out that the context window might be too low.
    *  Another user asks for guidance on how to change the context size in Ollama.
    *  Another user suggests trying the new qwen-2.5 variants with 1M of context size.

**[Open-R1: a fully open reproduction of DeepSeek-R1 from huggingface (Score: 1)](https://huggingface.co/blog/open-r1?utm_source=tldrai#what-is-deepseek-r1)**
*  **Summary:** The thread is a discussion regarding the DeepSeek-R1 blog post from huggingface.
*  **Emotion:** The tone is neutral, since users are giving feedback on the content of the post.
*  **Top 3 Points of View:**
    *   One user points out that the post was already submitted.
    *   Another user mentions that huggingface hasn't made this work and that is just their plan of direction.

**[How many 3080s to run 70B model? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1idxbi8/how_many_3080s_to_run_70b_model/)**
*   **Summary:** A user is asking how many 3080s are required to run a 70B model.
*   **Emotion:** The tone is positive since the user that responded to the question added emojis.
*   **Top 3 Points of View:**
    *   A user mentions that you need at least 4 or 5 times 10gb for a decent 70b q4 model.
