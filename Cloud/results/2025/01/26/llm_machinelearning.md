---
title: "Machine Learning Subreddit"
date: "2025-01-26"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machinelearning", "AI", "LLM"]
---

# Overall Ranking and Top Discussions
1.  [[R] Replicating DeepSeek-R3-Zero RL recipe on 3B LLM for <30$, the model develops self-verification and search abilities all on its own](https://www.reddit.com/r/MachineLearning/comments/1i9dmwc/r_replicating_deepseekr3zero_rl_recipe_on_3b_llm/) (Score: 232)
    *   The discussion revolves around replicating the DeepSeek-R3-Zero RL recipe on a 3B LLM, costing less than $30, leading to the model developing self-verification and search abilities.
2.  [[R] Learn How to Run DeepSeek-R1 Locally, a Free Alternative to OpenAI’s $200/Month o1 model](https://www.reddit.com/r/MachineLearning/comments/1i9xwbr/r_learn_how_to_run_deepseekr1_locally_a_free/) (Score: 87)
    *   Users discuss the possibility of running DeepSeek-R1 locally as a free alternative to OpenAI's model, but highlight concerns about its true performance and hardware requirements.
3.  [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948) (Score: 54)
    *   The conversation centers on DeepSeek-R1, an LLM trained using large-scale reinforcement learning (RL) that is capable of complex reasoning.
4.  [[P] I’m building a community-driven list of Awesome European Tech, can someone help me with adding AI section (issue opened)?](https://www.reddit.com/r/MachineLearning/comments/1i9lcgn/p_im_building_a_communitydriven_list_of_awesome/) (Score: 28)
    *   Users discuss and propose additions to a community-driven list of European tech companies with a focus on AI companies.
5.  [[D] Considering Buying an RTX 5090 for $2,600 vs. 2x RTX 4090 for $2,800 – Which is Better?](https://www.reddit.com/r/MachineLearning/comments/1i9jdfd/d_considering_buying_an_rtx_5090_for_2600_vs_2x/) (Score: 17)
    *   Users debate the merits of purchasing an RTX 5090 or two RTX 4090s for machine learning tasks, taking into account factors like cost, performance, memory and multi-GPU support.
6.  [[D] CVPR reviews aftermath](https://www.reddit.com/r/MachineLearning/comments/1ia4lm1/d_cvpr_reviews_aftermath/) (Score: 13)
    *   A user shares frustrations about the paper review process at conferences, despite having numerous publications in top-tier journals, and other users chime in with support and shared experiences.
7.  [[R] Evolution and The Knightian Blindspot of Machine Learning](https://arxiv.org/abs/2501.13075) (Score: 6)
     * A user discusses Reinforcement learning and how it is similar to teaching a child.
8.  [[R] Advice on an ICML submission](https://www.reddit.com/r/MachineLearning/comments/1i9a3pu/r_advice_on_an_icml_submission/) (Score: 5)
    * Users are giving advice on submitting to ICML conference.
9.  [[D] Best practices to finetune clip contrastively with new data](https://www.reddit.com/r/MachineLearning/comments/1i9tcxz/d_best_practices_to_finetune_clip_contrastively/) (Score: 5)
    *   Users discuss the best practices for fine-tuning CLIP contrastively with new data.
10. [[P] Steganographically encode messages with LLMs and Arithmetic Coding](https://github.com/shawnz/textcoder) (Score: 4)
    * The post introduces a new technique for hiding messages using LLMs and arithmetic coding.
11. [[D] Interpreting Random Forest](https://www.reddit.com/r/MachineLearning/comments/1i9n8cu/d_interpreting_random_forest/) (Score: 2)
    *   Users are exchanging tips on how to interpret random forest models.
12. [[D] Best TTS to onnx](https://www.reddit.com/r/MachineLearning/comments/1i9j191/d_best_tts_to_onnx/) (Score: 1)
    *   Users are asking for recommendations on the best Text-to-Speech models to use with ONNX.
13. [[P] Looking for models for LIMITED reverse image search...](https://www.reddit.com/r/MachineLearning/comments/1i9quki/p_looking_for_models_for_limited_reverse_image/) (Score: 0)
    * Users are discussing models for limited reverse image search capabilities.

# Detailed Analysis by Thread
**[[R] Replicating DeepSeek-R3-Zero RL recipe on 3B LLM for <30$, the model develops self-verification and search abilities all on its own (Score: 232)](https://www.reddit.com/r/MachineLearning/comments/1i9dmwc/r_replicating_deepseekr3zero_rl_recipe_on_3b_llm/)**
*   **Summary:** This thread is about the successful replication of the DeepSeek-R3-Zero RL recipe on a 3B LLM for under $30.  The surprising outcome is that the model independently develops self-verification and search capabilities.
*   **Emotion:** The overall tone is neutral, with some positive undertones due to the surprise and excitement around the model’s emergent abilities.
*   **Top 3 Points of View:**
    *   A user inquired whether the model was trained using the full DeepSeek-R1 dataset or a subset (countdown dataset).
    *  A user suggests testing the model with larger and more varied data to prevent data leakage.
    *  Some users expressed doubts about the iterative revision of the model and requested a direct link to a repository or blog.

**[[R] Learn How to Run DeepSeek-R1 Locally, a Free Alternative to OpenAI’s $200/Month o1 model (Score: 87)](https://www.reddit.com/r/MachineLearning/comments/1i9xwbr/r_learn_how_to_run_deepseekr1_locally_a_free/)**
*   **Summary:**  The post discusses running DeepSeek-R1 locally as a free alternative to OpenAI, but the comments highlight concerns about whether the local models are actually DeepSeek-R1 and whether its promise of quality might be oversold.
*   **Emotion:** The general emotional tone is neutral, with a hint of skepticism and concern about the veracity of claims and the origin of the technology.
*   **Top 3 Points of View:**
    *  Users pointed out that the models being offered are not true DeepSeek-R1, but fine-tunes of other models using DeepSeek data.
    *   Users expressed concerns about potential Chinese propaganda in these models.
    *   Some users shared their experience using a distilled version of DeepSeek-R1 on a Mac M1.

**[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning (Score: 54)](https://arxiv.org/abs/2501.12948)**
*   **Summary:**  The thread discusses the training approach of DeepSeek-R1, highlighting its use of reinforcement learning (RL) to enhance reasoning capabilities, and also how it starts with a "cold start" to fine-tune the base model.
*  **Emotion:** The general emotional tone is neutral.
*   **Top 3 Points of View:**
    *  A user summarizes how DeepSeek-R1 is trained using RL.
    * A user expresses that they didn't find DeepSeek-R1 useful.
    *  A user provides a link to a descriptive summary of the model.

**[[P] I’m building a community-driven list of Awesome European Tech, can someone help me with adding AI section (issue opened)? (Score: 28)](https://www.reddit.com/r/MachineLearning/comments/1i9lcgn/p_im_building_a_communitydriven_list_of_awesome/)**
*   **Summary:** This thread discusses building a community list of European tech companies, with a specific request for contributions to the AI section. Users suggest potential companies.
*   **Emotion:** The emotional tone is positive, with users engaging with the initiative and offering helpful suggestions.
*   **Top 3 Points of View:**
    *  A user questioned if tech startups bought by US companies should be counted.
    *   Several users shared names and details of European AI companies, such as Cradle.bio, Leya, and Datacrunch.
    * Some users offered help and suggested adding country of origin to each company.

**[[D] Considering Buying an RTX 5090 for $2,600 vs. 2x RTX 4090 for $2,800 – Which is Better? (Score: 17)](https://www.reddit.com/r/MachineLearning/comments/1i9jdfd/d_considering_buying_an_rtx_5090_for_2600_vs_2x/)**
*   **Summary:** Users are discussing whether to buy one RTX 5090 or two RTX 4090s for machine learning, focusing on performance, memory, and multi-GPU efficiency.
*   **Emotion:** The tone is generally neutral, with practical and analytical discussions predominating, reflecting the user's focus on technical details and cost-benefit analysis.
*   **Top 3 Points of View:**
    *   Some users suggest that the RTX 5090 is better in most cases, except for multi-GPU LLM inference or training small models that fit within the 24GB memory of the RTX 4090.
    *   Other users argued that dual 4090s are better for parallel processing.
    * Some users suggest using cloud computing for serious model training.

**[[D] CVPR reviews aftermath (Score: 13)](https://www.reddit.com/r/MachineLearning/comments/1ia4lm1/d_cvpr_reviews_aftermath/)**
*   **Summary:** A user shares their frustrating experience with paper reviews at conferences, despite having numerous publications in top-tier journals.
*   **Emotion:**  The overall tone is negative and sympathetic, due to the user expressing their frustrations with the review process.
*   **Top 3 Points of View:**
    * A user expresses frustration with the randomness and inconsistencies of the review process.
    * Another user shares their experience of having papers rejected from one conference and then accepted at another, highlighting the noise in the system.
    * The user expresses that even with lots of publications, acceptance rates don't improve.

**[[R] Evolution and The Knightian Blindspot of Machine Learning (Score: 6)](https://arxiv.org/abs/2501.13075)**
*   **Summary:** This thread discusses how reinforcement learning can be compared to teaching a child, and how models can more easily go out of distribution as they gain capabilities.
*   **Emotion:** The overall tone is neutral.
*   **Top 3 Points of View:**
    * A user draws an analogy between RL and teaching a child.
    *  A user points out that models will more easily go out of distribution as they gain capability.

**[[R] Advice on an ICML submission (Score: 5)](https://www.reddit.com/r/MachineLearning/comments/1i9a3pu/r_advice_on_an_icml_submission/)**
*   **Summary:** The discussion gives advice on what to include when submitting to the ICML conference.
*   **Emotion:** The overall tone is neutral.
*   **Top 3 Points of View:**
    * The conversation revolves around adding efficiency and making the paper HW heavy.
    * The user is also made aware of the limitations of ML conferences and quality of HW reviewers.
    *  The conversation also highlights other conferences that might be a better fit for HW heavy papers.

**[[D] Best practices to finetune clip contrastively with new data (Score: 5)](https://www.reddit.com/r/MachineLearning/comments/1i9tcxz/d_best_practices_to_finetune_clip_contrastively/)**
*   **Summary:** Users are discussing best practices for fine-tuning CLIP contrastively with new data.
*   **Emotion:** The overall tone is neutral, with practical advice being shared.
*   **Top 3 Points of View:**
    * A user suggests it is not helpful to include external data that is unrelated to the domain.
    * Another user recommends using CoOp as a good beginner method for contrastive learning.

**[[P] Steganographically encode messages with LLMs and Arithmetic Coding (Score: 4)](https://github.com/shawnz/textcoder)**
*   **Summary:** A user introduces a prototype of a steganographic encoder that could be used to hide secret messages on public channels by using LLMs and Arithmetic Coding.
*  **Emotion:** The overall tone is neutral.
*   **Top 3 Points of View:**
    *   A user introduces their project for steganographically encoding messages.
    *   The user explains the technique behind the steganographic encoder.
    *   The user also highlights the benefits of using such a technique.

**[[D] Interpreting Random Forest (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1i9n8cu/d_interpreting_random_forest/)**
*   **Summary:** This thread is about how to interpret random forest models.
*   **Emotion:** The emotional tone is neutral and informative.
*   **Top 3 Points of View:**
    * One user suggests printing the trees to see where and why the model made a decision.
     * Another user mentions SHAP values as a method for interpreting individual predictions.
    * A user describes how to visualize the SHAP values using force or waterfall plots.

**[[D] Best TTS to onnx (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1i9j191/d_best_tts_to_onnx/)**
*   **Summary:** This thread is about which Text-to-Speech models to use with ONNX.
*   **Emotion:** The tone is neutral and informational.
*   **Top 3 Points of View:**
    * One user recommends using their github repo and scripts to export huggingface models to ONNX.
    *  Another user recommends the Kokoro model, which is in ONNX format.

**[[P] Looking for models for LIMITED reverse image search... (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1i9quki/p_looking_for_models_for_limited_reverse_image/)**
*   **Summary:**  This thread is about finding models for limited reverse image search capabilities.
*   **Emotion:** The tone is neutral and helpful.
*   **Top 3 Points of View:**
    * One user suggested training a CNN architecture to classify the images.
    *   Another user recommends using perceptual hashes before attempting any kind of ML.
