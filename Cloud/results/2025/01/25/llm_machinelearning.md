---
title: "Machine Learning Subreddit"
date: "2025-01-25"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machine learning", "AI", "deep learning"]
---

# Overall Ranking and Top Discussions
1.  [[R] Replicating DeepSeek-R3-Zero RL recipe on 3B LLM for <30$, the model develops self-verification and search abilities all on its own](https://www.reddit.com/r/MachineLearning/comments/1i9dmwc/r_replicating_deepseekr3zero_rl_recipe_on_3b_llm/) (Score: 203)
    *   This thread discusses the replication of the DeepSeek-R3-Zero RL recipe on a 3B LLM, which led to the model developing self-verification and search abilities.
2.  [[R] DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948) (Score: 36)
    *   This thread discusses a new paper about training LLMs using reinforcement learning to enhance their reasoning abilities.
3.  [[P] I’m building a community-driven list of Awesome European Tech, can someone help me with adding AI section (issue opened)?](https://www.reddit.com/r/MachineLearning/comments/1i9lcgn/p_im_building_a_communitydriven_list_of_awesome/) (Score: 21)
    *  This thread is about building a community-driven list of European tech companies, with a focus on adding an AI section.
4.  [[D] Considering Buying an RTX 5090 for $2,600 vs. 2x RTX 4090 for $2,800 – Which is Better?](https://www.reddit.com/r/MachineLearning/comments/1i9jdfd/d_considering_buying_an_rtx_5090_for_2600_vs_2x/) (Score: 15)
    *   This thread discusses whether to buy an RTX 5090 or two RTX 4090s for machine learning tasks.
5.  [[R] Advice on an ICML submission](https://www.reddit.com/r/MachineLearning/comments/1i9a3pu/r_advice_on_an_icml_submission/) (Score: 5)
    *  This thread is seeking advice on submitting a paper to ICML, a machine learning conference.
6.  [[D] Best practices to finetune clip contrastively with new data](https://www.reddit.com/r/MachineLearning/comments/1i9tcxz/d_best_practices_to_finetune_clip_contrastively/) (Score: 4)
    *  This thread asks for best practices on how to fine-tune a CLIP model contrastively with new data.
7.  [[R] Evolution and The Knightian Blindspot of Machine Learning](https://arxiv.org/abs/2501.13075) (Score: 3)
    *  This thread discusses a paper on the evolution and limitations of machine learning.
8.  [[D] Interpreting Random Forest](https://www.reddit.com/r/MachineLearning/comments/1i9n8cu/d_interpreting_random_forest/) (Score: 2)
    *   This thread is about techniques for interpreting decisions made by a Random Forest model.
9.  [[P] Steganographically encode messages with LLMs and Arithmetic Coding](https://github.com/shawnz/textcoder) (Score: 0)
    *  This thread introduces a project that encodes messages using LLMs and arithmetic coding for steganography.
10. [[D] Best TTS to onnx](https://www.reddit.com/r/MachineLearning/comments/1i9j191/d_best_tts_to_onnx/) (Score: 0)
    *   This thread discusses the best Text-to-Speech (TTS) models that can be converted to ONNX format.
11. [[P] Looking for models for LIMITED reverse image search...](https://www.reddit.com/r/MachineLearning/comments/1i9quki/p_looking_for_models_for_limited_reverse_image/) (Score: 0)
     * This thread discusses the search for models that can be used for limited reverse image searches.
12. [[D] ROADMAP FOR MACHINE LEARNING WITH (free) resources ??](https://www.reddit.com/r/MachineLearning/comments/1i9vbsd/d_roadmap_for_machine_learning_with_free_resources/) (Score: 0)
    *  This thread asks for a roadmap for learning machine learning with free resources.

# Detailed Analysis by Thread
**[[R] Replicating DeepSeek-R3-Zero RL recipe on 3B LLM for <30$, the model develops self-verification and search abilities all on its own (Score: 203)](https://www.reddit.com/r/MachineLearning/comments/1i9dmwc/r_replicating_deepseekr3zero_rl_recipe_on_3b_llm/)**
*   **Summary:** The thread discusses replicating DeepSeek-R3-Zero RL recipe on a 3B LLM for less than $30. The model developed self-verification and search abilities on its own, which is an exciting development for the community.
*   **Emotion:** The overall emotional tone is positive with a mix of neutral curiosity. There are comments expressing excitement and interest in learning more about this technique.
*   **Top 3 Points of View:**
    *   The replication of the DeepSeek-R3-Zero RL recipe is a significant achievement, showcasing the potential of RL for LLMs.
    *   Some users are skeptical and inquire about potential data leakage, suggesting further verification.
    *  There is a desire for easily usable implementations of this technology.

**[[R] DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning (Score: 36)](https://arxiv.org/abs/2501.12948)**
*   **Summary:** The thread focuses on a paper that uses reinforcement learning to improve the reasoning abilities of LLMs. It uses a multi-stage training approach, combining supervised fine-tuning and RL techniques to improve reasoning performance and readability.
*   **Emotion:** The thread's emotional tone is mostly positive, with some neutral curiosity. People are excited about the new advancements in the field.
*   **Top 3 Points of View:**
    *  There is a lot of excitement and interest around the DeepSeek-R1 paper.
    *  The paper describes an iterative training process to improve both reasoning and human-friendliness in LLMs
    *  Some users are still catching up with the details of the paper due to being busy.

**[[P] I’m building a community-driven list of Awesome European Tech, can someone help me with adding AI section (issue opened)? (Score: 21)](https://www.reddit.com/r/MachineLearning/comments/1i9lcgn/p_im_building_a_communitydriven_list_of_awesome/)**
*   **Summary:** This thread is about a project to create a community-driven list of European tech companies, with a call for help in adding an AI section. Users are suggesting various companies and features for the list.
*   **Emotion:** The overall emotional tone is positive and collaborative, with a neutral undertone. People are excited to contribute to the list and make suggestions.
*   **Top 3 Points of View:**
    *   There's a lot of interest in contributing to the list, with people suggesting specific companies.
    *   Users are offering feedback, such as requesting that the country of each company be included.
    *   There's a discussion on the criteria for inclusion, especially regarding companies with American investment or those bought by American corps.

**[[D] Considering Buying an RTX 5090 for $2,600 vs. 2x RTX 4090 for $2,800 – Which is Better? (Score: 15)](https://www.reddit.com/r/MachineLearning/comments/1i9jdfd/d_considering_buying_an_rtx_5090_for_2600_vs_2x/)**
*   **Summary:** This thread discusses the pros and cons of purchasing an RTX 5090 vs. 2 RTX 4090s for machine learning tasks, comparing the performance for model training, inference, and considering factors like memory and architectural differences.
*   **Emotion:** The thread has a neutral tone, with discussions focused on technical specifications and performance, with some positive comments about the 5090 card.
*   **Top 3 Points of View:**
    *   The RTX 5090 is generally preferred, unless multi-GPU inference or DDP training of small models is needed.
    *   Two 4090s can provide more throughput in certain use cases and offer more memory when combined, but can introduce latency
    *   Alternatives like two RTX A5000s with nvlink are also suggested.

**[[R] Advice on an ICML submission (Score: 5)](https://www.reddit.com/r/MachineLearning/comments/1i9a3pu/r_advice_on_an_icml_submission/)**
*   **Summary:** The thread seeks advice on a paper submission to ICML, specifically regarding the reviewer's preference for efficiency and the potential need to look at hardware-focused conferences.
*   **Emotion:** The emotional tone is neutral.
*   **Top 3 Points of View:**
    *   Focus on efficiency.
    *   ML conferences might not have the best hardware reviewers.
     *  Consider submitting to hardware-focused conferences if the paper has a heavy HW component.

**[[D] Best practices to finetune clip contrastively with new data (Score: 4)](https://www.reddit.com/r/MachineLearning/comments/1i9tcxz/d_best_practices_to_finetune_clip_contrastively/)**
*   **Summary:** This thread asks for advice on best practices for fine-tuning a CLIP model contrastively using new data.
*   **Emotion:** The emotional tone is neutral, seeking information.
*   **Top 3 Points of View:**
    *  Use CoOp is suggested as a good beginner method.

**[[R] Evolution and The Knightian Blindspot of Machine Learning (Score: 3)](https://arxiv.org/abs/2501.13075)**
*   **Summary:** The thread discusses a paper about the evolution and limitations of machine learning, specifically how RL models gain capability and move further out of distribution.
*   **Emotion:** The emotional tone is neutral, with discussions on the nature of RL.
*  **Top 3 Points of View:**
     *  RL is compared to teaching a child.
     *  RL models will move further out of distribution as they gain capability.

**[[D] Interpreting Random Forest (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1i9n8cu/d_interpreting_random_forest/)**
*  **Summary:** This thread asks how to interpret the decision-making process of a Random Forest model.
*  **Emotion:** The emotional tone is neutral, focused on technical advice.
*   **Top 3 Points of View:**
    *   You can print the tree to understand model decisions using sklearn.
    *   SHAP can be used to interpret individual examples by aggregating across the trees.

**[[P] Steganographically encode messages with LLMs and Arithmetic Coding (Score: 0)](https://github.com/shawnz/textcoder)**
*   **Summary:** This thread introduces a project that uses LLMs and arithmetic coding for steganography. It explains how to hide messages in plain sight on public channels by encrypting the message and then disguising it through a bijective arithmetic coder.
*   **Emotion:** The emotional tone is neutral, with curiosity about the project.
*   **Top 3 Points of View:**
    *   The prototype uses LLMs and arithmetic coding to hide messages in plain sight.
    *   Authenticated encryption provides security.
    *   The project is in early stages and feedback is welcomed.

**[[D] Best TTS to onnx (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1i9j191/d_best_tts_to_onnx/)**
*   **Summary:** This thread is looking for recommendations for the best Text-to-Speech (TTS) models that can be converted to the ONNX format, and users are sharing their recommendations.
*   **Emotion:** The emotional tone is neutral, seeking technical recommendations.
*   **Top 3 Points of View:**
    *  OneMinTTS is recommended as simple and customizable.
    *  Kokoro is mentioned as a good model for ONNX.

**[[P] Looking for models for LIMITED reverse image search... (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1i9quki/p_looking_for_models_for_limited_reverse_image/)**
*   **Summary:** This thread is looking for models to use in a limited reverse image search application and users suggest training a classifier using techniques that overfit on the correct images or to check out perceptual hashes.
*   **Emotion:** The emotional tone is neutral, seeking advice on building a model.
*   **Top 3 Points of View:**
    *   Train a CNN classifier, overfitting on the correct images
    *   Consider using perceptual hashes for similarity matching.

**[[D] ROADMAP FOR MACHINE LEARNING WITH (free) resources ?? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1i9vbsd/d_roadmap_for_machine_learning_with_free_resources/)**
*   **Summary:** This thread asks for a roadmap for learning machine learning with free resources, and receives a sarcastic remark about lack of python knowledge.
*   **Emotion:** The emotional tone is negative due to sarcasm, though the topic itself is about learning resources.
*   **Top 3 Points of View:**
    *  User is seeking for free resources to learn ML.
    *  A commenter sarcastically questions lack of python proficiency.
