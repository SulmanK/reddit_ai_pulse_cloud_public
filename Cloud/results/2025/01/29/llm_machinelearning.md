---
title: "Machine Learning Subreddit"
date: "2025-01-29"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machine learning", "AI", "research"]
---

# Overall Ranking and Top Discussions
1.  [[D] Why is most mechanistic interpretability research only published as preprints or blog articles ?](https://www.reddit.com/r/MachineLearning/comments/1icw2pi/d_why_is_most_mechanistic_interpretability/) (Score: 34)
    *   This thread discusses why much of the research on mechanistic interpretability is published as preprints or blog posts rather than in peer-reviewed journals.
2.  The scale vs. intelligence trade-off in retrieval augmented generation [Discussion](https://www.reddit.com/r/MachineLearning/comments/1ick63j/the_scale_vs_intelligence_tradeoff_in_retrieval/) (Score: 31)
    *   This thread discusses the trade-offs between scale and intelligence in retrieval augmented generation, and whether knowledge graph based RAG has been explored.
3.  [[D] Publications vs PhD](https://www.reddit.com/r/MachineLearning/comments/1icpb9c/d_publications_vs_phd/) (Score: 28)
    *   This thread debates the value of publications versus a PhD in the field of machine learning, and how they compare in terms of career opportunities and recognition.
4.  [[D] Revise an Accepted ICLR Paper to Remove a Flawed Contribution?](https://www.reddit.com/r/MachineLearning/comments/1icw8yf/d_revise_an_accepted_iclr_paper_to_remove_a/) (Score: 21)
    *   This thread discusses whether it's acceptable to revise an accepted paper to remove a flawed part, or if the ethical course of action is retraction.
5.  [[R] Multimodal Models Interpretability](https://www.reddit.com/r/MachineLearning/comments/1icnzzs/r_multimodal_models_interpretability/) (Score: 6)
    *   This thread provides links to two recent reviews on multimodal explainable AI and XAI from inherent explainability to LLMs.
6.  [[D] How does the LLM community make sure that a newly released model has not been overfitted to Q&As in popular benchmarks?](https://www.reddit.com/r/MachineLearning/comments/1iclxeo/d_how_does_the_llm_community_make_sure_that_a/) (Score: 1)
    *   This thread discusses the concern about newly released models potentially being overfitted to popular benchmark Q&As, using DeepSeek's accuracy metrics as an example.
7.  [Research] Which is more effective for NER? Training a spaCy model or using Meta’s LLaMA?](https://www.reddit.com/r/MachineLearning/comments/1icoi78/research_which_is_more_effective_for_ner_training/) (Score: 1)
     *   This thread discusses and recommends using spaCy over Meta's LLaMA for NER tasks, also mentioning some fine-tuning techniques for specific data.
8.  [[D] fine tuning](https://www.reddit.com/r/MachineLearning/comments/1id2vyl/d_fine_tuning/) (Score: 1)
    *   This thread discusses common issues with fine tuning of models, and potential solutions, such as proper loading and saving of trained adapters.
9.  [[D] Help me think. Self-supervised learning](https://www.reddit.com/r/MachineLearning/comments/1icgbw6/d_help_me_think_selfsupervised_learning/) (Score: 0)
     *   This thread discusses the concept of self-supervised learning, and the idea of the platonic representation hypothesis, among others.
10. [[D] Thoughts on deepfake detection software](https://www.reddit.com/r/MachineLearning/comments/1icpwux/d_thoughts_on_deepfake_detection_software/) (Score: 0)
    *   This thread discusses the challenges of creating effective deepfake detection software, especially given the rapid pace of improvement in generating fake media.
11. Researchers and Professionals: How do you foresee the impact of GPT models being trained on AI generated data now plaguing the internet? [Discussion](https://www.reddit.com/r/MachineLearning/comments/1icv6ub/researchers_and_professionals_how_do_you_foresee/) (Score: 0)
     *   This thread discusses the impact of AI generated data being used to train GPT models, touching on model collapse, anonymity, and synthetic texts.
12. [[P] Reasons for validation cost not aligning with training and test cost?](https://www.reddit.com/r/MachineLearning/comments/1icwx1r/p_reasons_for_validation_cost_not_aligning_with/) (Score: 0)
     *   This thread discusses reasons why validation cost might not align with training and test cost.

# Detailed Analysis by Thread
**[[D] Why is most mechanistic interpretability research only published as preprints or blog articles ? (Score: 34)](https://www.reddit.com/r/MachineLearning/comments/1icw2pi/d_why_is_most_mechanistic_interpretability/)**
*  **Summary:** The discussion centers on why mechanistic interpretability research is often found in preprints and blogs rather than peer-reviewed journals. Participants cite the time-consuming peer-review process, the fast-paced nature of AI advancements, and a skepticism towards traditional academic structures in this specific community.  Others mention that metrics for this field are difficult to define, and that some researchers in this area prefer informal publication styles.
*  **Emotion:** The overall emotional tone of this thread is primarily neutral. While there's a mix of opinions, the discussion is generally analytical and informative, with some slight positive undertones regarding the excitement around this field. There is one negative comment stating that "Peer reviewed ML publication is trash anyway".
*  **Top 3 Points of View:**
    * Peer review is too slow and not always necessary for dissemination in this field, where replication is often possible on one's own computer.
    * The lack of established metrics in mechanistic interpretability makes traditional peer review difficult.
    * Many researchers within this subfield prefer the less formal approach of blog posts and preprints.

**[The scale vs. intelligence trade-off in retrieval augmented generation [Discussion] (Score: 31)](https://www.reddit.com/r/MachineLearning/comments/1ick63j/the_scale_vs_intelligence_tradeoff_in_retrieval/)**
*   **Summary:** The thread discusses the balance between scale and intelligence in retrieval-augmented generation and specifically whether knowledge graphs have been explored in this context. The user admits to having limited experience but has been actively researching papers and projects in the area.
*   **Emotion:** The emotional tone of the thread is neutral, reflecting a factual and exploratory discussion.
*   **Top 3 Points of View:**
    * The main discussion point was regarding knowledge graph based RAG approaches.
    * The user is actively researching and trying to get a survey of the area, specifically with the mentioned mindful-rag paper.
    * The thread is focused on exploration and the need for more practical experience in the field.

**[[D] Publications vs PhD (Score: 28)](https://www.reddit.com/r/MachineLearning/comments/1icpb9c/d_publications_vs_phd/)**
*  **Summary:** The thread explores the comparative value of publications versus a PhD in machine learning careers, focusing on career paths and perceived legitimacy.  It considers various career goals (academia, research in big tech, visas) and whether each is better attained with publications or a PhD. The discussion indicates a lot of uncertainty and that there are multiple factors affecting the value of each.
*  **Emotion:** The emotional tone of the thread is largely neutral, with a slight positive undertone in some comments acknowledging the benefits of a PhD, although it is mostly analytical. There is also some negativity concerning the time investment it would take to be equivalent to a PhD through publications, and some comments expressing that there is no equivalency.
*  **Top 3 Points of View:**
    * There is no universal equivalence between a certain number of publications and a PhD; it varies based on the individual's goals and resources.
    * A PhD is useful for those seeking to work abroad or in academia, while publications might be sufficient for some industry research positions.
    * The value of a PhD is in the structured research experience, access to resources, and the long-term professional credential, not just the publications.

**[[D] Revise an Accepted ICLR Paper to Remove a Flawed Contribution? (Score: 21)](https://www.reddit.com/r/MachineLearning/comments/1icw8yf/d_revise_an_accepted_iclr_paper_to_remove_a/)**
*  **Summary:** This thread discusses an author's dilemma of whether to revise an accepted paper to remove a flawed section. The discussion revolves around ethical considerations, the peer review process, and how to best handle identified errors in research.
*  **Emotion:** The emotional tone of the thread is a mix of neutral, positive and negative, as participants debate the ethical implications of proceeding with the flawed version and the risks in proposing a new version to the AC. There is some slight negativity towards just publishing the work.
*  **Top 3 Points of View:**
    * Some suggest that the author should retract the paper, revise it, and resubmit to the next relevant conference.
    * Others think the author should mention it in the camera-ready version and add a discussion about the flawed methodology, but not to tell the AC.
    * The ethical thing to do would be to retract, however, it's the author's decision.

**[[R] Multimodal Models Interpretability (Score: 6)](https://www.reddit.com/r/MachineLearning/comments/1icnzzs/r_multimodal_models_interpretability/)**
*  **Summary:** This thread is a simple resource-sharing post, providing links to two recent review papers on the topic of multimodal explainable AI.
*  **Emotion:** The emotional tone is neutral, as it's primarily informative.
*  **Top 3 Points of View:**
    * The thread provides links to two recent reviews on multimodal XAI.
    * It suggests further reading and research for the topic of multimodal model interpretability.
    * This thread is only a resource post, not an active debate.

**[[D] How does the LLM community make sure that a newly released model has not been overfitted to Q&As in popular benchmarks? (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1iclxeo/d_how_does_the_llm_community_make_sure_that_a/)**
*   **Summary:** The thread raises the question of how the LLM community ensures that new models are not overfitted to popular benchmarks, using DeepSeek's accuracy metrics as an example but not accusing the developers of anything.
*   **Emotion:** The emotional tone is neutral, with a hint of concern about the potential for overfitting.
*   **Top 3 Points of View:**
    * The main point is the concern that LLMs might be overfitted to benchmarks.
    * It uses DeepSeek's accuracy metrics as an example of the reason for this concern.
    * The thread is primarily about expressing a concern, without providing any answers.

**[Research] Which is more effective for NER? Training a spaCy model or using Meta’s LLaMA? (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1icoi78/research_which_is_more_effective_for_ner_training/)**
*   **Summary:** The thread discusses and recommends using spaCy over LLaMA-2 for NER tasks based on a user's experience, also mentioning some fine-tuning techniques with data augmentation from LLaMA.
*   **Emotion:** The overall tone is negative due to the commenter's previous experiences with LLaMA, but it's also informative, making it mostly neutral.
*  **Top 3 Points of View:**
    * LLaMA-2 for NER was not effective for the commenter's task.
    * spaCy is recommended over LLaMA-2 for NER.
    * They propose a fine-tuning method with data augmentation for a different LLM.

**[[D] fine tuning (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1id2vyl/d_fine_tuning/)**
*  **Summary:** This thread discusses the importance of loading the trained adapter with the model correctly during fine-tuning, emphasizing the need for thorough checks on the saving and loading process. It also suggests a way to iterate with training a LoRA.
*  **Emotion:** The emotional tone is neutral, providing helpful advice without expressing any particular sentiment.
*  **Top 3 Points of View:**
    *   It highlights the need to ensure that the trained adapter is loaded correctly with the model.
    *   It recommends checking the model saving and loading process.
    *   It proposes to iterate first training a LoRA.

**[[D] Help me think. Self-supervised learning (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1icgbw6/d_help_me_think_selfsupervised_learning/)**
*   **Summary:** This thread discusses the conceptual underpinnings of self-supervised learning and references the platonic representation hypothesis, the NP-Hard continual learning problem, and the concept of transfer learning.
*   **Emotion:** The overall emotional tone is neutral with some positive undertones of willingness to learn and discuss.
*  **Top 3 Points of View:**
    * The platonic representation hypothesis suggests representations of concepts in different models converge with larger datasets.
    * Continual learning is an NP-Hard problem.
    * The thread suggests that the user google “transfer learning”.

**[[D] Thoughts on deepfake detection software (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1icpwux/d_thoughts_on_deepfake_detection_software/)**
*  **Summary:** The thread discusses the challenges of developing deepfake detection software. Participants express skepticism, highlighting the rapid pace of improvement in generative AI and the potential for detection methods to be used in GAN arrangements.
*  **Emotion:** The emotional tone of the thread is neutral, marked by a mixture of interest and pessimism.
*  **Top 3 Points of View:**
    * It's a good idea, but catching up with the pace of deepfake improvements might be hard.
    *  GenAI detection is at best snake oil.
    * Detection software could be used to generate even better fakes.

**[Researchers and Professionals: How do you foresee the impact of GPT models being trained on AI generated data now plaguing the internet? [Discussion] (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1icv6ub/researchers_and_professionals_how_do_you_foresee/)**
*  **Summary:** This thread explores the potential impact of GPT models being trained on AI-generated data, which is becoming increasingly common on the internet. The discussion touches on the "model collapse" theory, erosion of anonymity online and use of synthetic text.
*  **Emotion:** The emotional tone of the thread is mixed, including positive undertones in new algorithms, some concerns of anonymity loss, and skepticism about "model collapse".
*   **Top 3 Points of View:**
     * There are concerns about model collapse due to AI generated data, but is dismissed by some.
    *  AI generated data may lead to more intrusive methods to authenticate if someone is real or not.
    *   Advances have been made by using deliberately generated synthetic texts.

**[[P] Reasons for validation cost not aligning with training and test cost? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1icwx1r/p_reasons_for_validation_cost_not_aligning_with/)**
*  **Summary:** This thread discusses why validation costs can differ from training and test costs, pointing out potential issues such as model overfitting and the validation set containing unseen data.
*   **Emotion:** The tone is neutral and informational.
*   **Top 3 Points of View:**
    *  The model might be overfitting the training data.
    *  The validation dataset may contain data that is not generalizable.
    *  The sampling of the datasets should be checked and regularization may be needed.
