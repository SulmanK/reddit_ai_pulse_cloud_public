---
title: "Machine Learning Subreddit"
date: "2025-01-24"
description: "Analysis of top discussions and trends in the machinelearning subreddit"
tags: ["machine learning", "AI", "research"]
---

# Overall Ranking and Top Discussions
1.  [[N] Anthropic CEO says at the beginning of 2024, models scored ~3% at SWE-bench. Ten months later, we were at 50%. He thinks in another year we’ll probably be at 90%](https://www.reddit.com/r/MachineLearning/comments/1i8wkth/anthropic_ceo_says_at_the_beginning_of_2024/) (Score: 137)
    *   The discussion revolves around the Anthropic CEO's prediction of rapid progress in model performance on the SWE-bench, with several users expressing skepticism about the possibility of linear improvement.
2.  [[R] ENERGY-BASED DIFFUSION LANGUAGE MODELS FOR TEXT GENERATION](https://www.reddit.com/r/MachineLearning/comments/1i87lgy/r_energybased_diffusion_language_models_for_text/) (Score: 48)
    *   This thread is about a research paper on energy-based diffusion language models. The main discussion points include the need for publicly available code and the resurgence of energy-based models.
3.  [[D] Any details on Nvidia's DLSS 4 ViT model architecture?](https://www.reddit.com/r/MachineLearning/comments/1i8o4vf/d_any_details_on_nvidias_dlss_4_vit_model/) (Score: 31)
    *   Users are speculating on the architecture of Nvidia's DLSS 4 model which uses ViT. Discussions include the potential use of encoder-decoder structures, with the acknowledgment that detailed information is a closely guarded trade secret.
4.  [[D] ACL ARR December 2024 Discussions](https://www.reddit.com/r/MachineLearning/comments/1i8x3q4/d_acl_arr_december_2024_discussions/) (Score: 12)
    *   The discussion centers around the ACL ARR December 2024 submissions. Users are sharing their review scores, asking about the acceptance criteria and how to proceed with the rebuttal process.
5.  [[R] Efficient Lossless Compression of Vector IDs and Links in ANN Search Indexes](https://www.reddit.com/r/MachineLearning/comments/1i89hn0/r_efficient_lossless_compression_of_vector_ids/) (Score: 6)
    *   Users are discussing a research paper about compression of vector IDs in ANN search indexes. The conversation hints at how this compression might be related to Microsoft's LazyRAG system.
6.  [[R] Confidential Comments to AC for CVPR 2025](https://www.reddit.com/r/MachineLearning/comments/1i8w6oi/r_confidential_comments_to_ac_for_cvpr_2025/) (Score: 2)
    *   The discussion revolves around how to handle review comments for CVPR submissions, specifically regarding confidential comments to the Area Chairs.
7.  [[D] - Topic Modeling for high volume chat data](https://www.reddit.com/r/MachineLearning/comments/1i8msyv/d_topic_modeling_for_high_volume_chat_data/) (Score: 1)
    *   Users are discussing topic modeling for large chat datasets. Advice includes using a GPU to accelerate the process, data cleaning and using wordclouds to visualize topics.
8.  [[D] How to train a model for computer use? how different is CUA model from 4o?](https://www.reddit.com/r/MachineLearning/comments/1i8pvnb/d_how_to_train_a_model_for_computer_use_how/) (Score: 1)
     * This post asks about training models for computer use, and a user suggests using UI-Tars.
9.  [[D] CVPR review system](https://www.reddit.com/r/MachineLearning/comments/1i88j22/d_cvpr_review_system/) (Score: 0)
    *   The discussion is about the CVPR review process. Users describe the acceptance criteria as being based on a paper's score and a 25% acceptance rate.
10. [[D] Does ICML De-Anonymize Withdrawn/Rejected Submissions like ICLR?](https://www.reddit.com/r/MachineLearning/comments/1i8n40e/d_does_icml_deanonymize_withdrawnrejected/) (Score: 0)
    *  This thread asks about the ICML anonymization policy, specifically if rejected or withdrawn papers are deanonymized like they are in ICLR.
11. [[R] arXiv endorsement request for AV research](https://www.reddit.com/r/MachineLearning/comments/1i8prw0/r_arxiv_endorsement_request_for_av_research/) (Score: 0)
     * This thread discusses ArXiv endorsement requests, and it appears that using a university email may not require an endorsement.
12.  [[D] LLM for categorization](https://www.reddit.com/r/MachineLearning/comments/1i8sawf/d_llm_for_categorization/) (Score: 0)
     *  This thread is about using LLMs for categorization. A user suggests using embedding models and clustering. Another user notes this is a beginner question, directing to other subreddits.
13. [[P] Questions on document handling and privacy in LLM implementation](https://www.reddit.com/r/MachineLearning/comments/1i8wiww/p_questions_on_document_handling_and_privacy_in/) (Score: 0)
    *   This post is looking for guidance on handling documents and privacy in LLM implementation, but is directed towards other subreddits focusing on LLM development and prompt engineering.

# Detailed Analysis by Thread
**[[N] Anthropic CEO says at the beginning of 2024, models scored ~3% at SWE-bench. Ten months later, we were at 50%. He thinks in another year we’ll probably be at 90% (Score: 137)](https://www.reddit.com/r/MachineLearning/comments/1i8wkth/anthropic_ceo_says_at_the_beginning_of_2024/)**
*  **Summary:** The thread discusses the Anthropic CEO's prediction about the rapid increase in model performance on SWE-bench. Users debated if the progress will continue at the predicted rate.
*   **Emotion:** The emotional tone is predominantly neutral, with a mix of skepticism and some negativity regarding the CEO's projections. There is also a hint of optimism expressed by a user who is interpreting the predictions to be positive.
*   **Top 3 Points of View:**
    *   Some users believe the CEO's statements are just marketing, downplaying the prediction.
    *   Some users are skeptical about extrapolating progress linearly, as improvements get harder.
    *  Other users highlight the problem with benchmarks losing meaning once they become training data.

**[[R] ENERGY-BASED DIFFUSION LANGUAGE MODELS FOR TEXT GENERATION (Score: 48)](https://www.reddit.com/r/MachineLearning/comments/1i87lgy/r_energybased_diffusion_language_models_for_text/)**
*  **Summary:**  This thread discusses a research paper on Energy-Based Diffusion Language Models. Main topics include the lack of publicly available code, with some questioning the resurgence of the use of Energy Based Models.
*   **Emotion:** The emotional tone is mostly neutral, with some positive sentiment from users interested in the research and some frustration with the lack of code.
*   **Top 3 Points of View:**
    *   Users are emphasizing the need to release the code to the public, to help further research and reduce errors in implementation.
    *   Users are curious about the reason for the return of EBM in the recent past.
    *   One user is confused by the use of all caps in the title, which is usually used to imply anger.

**[[D] Any details on Nvidia's DLSS 4 ViT model architecture? (Score: 31)](https://www.reddit.com/r/MachineLearning/comments/1i8o4vf/d_any_details_on_nvidias_dlss_4_vit_model/)**
*   **Summary:** The discussion revolves around the architecture of Nvidia's DLSS 4 model, which is believed to use Vision Transformers (ViT). Users discuss the potential model architecture and the difficulties in obtaining detailed information due to its proprietary nature.
*   **Emotion:** The overall emotional tone is neutral, characterized by speculation and curiosity.
*   **Top 3 Points of View:**
    *  The most popular theory is that the model is based on an encoder-decoder architecture with a ViT model in place of a CNN.
    *   Users agree that DLSS architecture is a well-kept secret.
    *    Some users ask if reverse engineering of the .dll files to find the weights and architecture might be a viable option.

**[[D] ACL ARR December 2024 Discussions (Score: 12)](https://www.reddit.com/r/MachineLearning/comments/1i8x3q4/d_acl_arr_december_2024_discussions/)**
*   **Summary:** This thread is about the discussion of ACL ARR December 2024 submissions. People are sharing their reviews and looking for advice and information on the chances of their paper being accepted.
*   **Emotion:** The dominant emotion is neutral, with a mix of anxiety and curiosity about the review process and outcome.
*   **Top 3 Points of View:**
    * Users are sharing the review scores and confidence levels of the reviewers.
    *  Users are asking what score would be considered good enough to make it to the findings or main section.
    * Users are unsure how to proceed with the rebuttal process.

**[[R] Efficient Lossless Compression of Vector IDs and Links in ANN Search Indexes (Score: 6)](https://www.reddit.com/r/MachineLearning/comments/1i89hn0/r_efficient_lossless_compression_of_vector_ids/)**
*   **Summary:** This thread is about a research paper regarding the compression of vector IDs and Links in ANN Search Indexes. Users relate it to the Microsoft LazyRAG system.
*   **Emotion:** The emotional tone is neutral, with the discussion mostly factual and inquisitive.
*   **Top 2 Points of View:**
    *   Users speculate that the compression techniques discussed in the paper might be used by Microsoft's LazyRAG system to improve speed.
    *  Users are interested in the potential of vector search optimization through the method described in the research paper.

**[[R] Confidential Comments to AC for CVPR 2025 (Score: 2)](https://www.reddit.com/r/MachineLearning/comments/1i8w6oi/r_confidential_comments_to_ac_for_cvpr_2025/)**
*   **Summary:** This thread discusses how to handle confidential comments to Area Chairs for CVPR submissions.
*   **Emotion:** The emotional tone is neutral.
*   **Top 2 Points of View:**
    *   One user suggests not addressing non-credible comments, or comments from the highest scorer reviewer, but they also suggest addressing all the comments if a paper is rejected.
    *   Another user suggests directly pointing out missed experiments in the rebuttal, implying the AC would not prioritize such arguments.

**[[D] - Topic Modeling for high volume chat data (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1i8msyv/d_topic_modeling_for_high_volume_chat_data/)**
*  **Summary:**  Users are discussing topic modeling for large chat datasets and various strategies and tools to handle large volume datasets.
*   **Emotion:** The emotional tone is informative and helpful.
*   **Top 3 Points of View:**
    *   Users suggest using BERTopic with GPU acceleration to handle large datasets efficiently.
    *   Users discuss the size limitations of Bertopic and that using count vectorizers makes 2M chats feasible.
    *  Users emphasize the importance of visualization and data cleaning for improved topic interpretability.

**[[D] How to train a model for computer use? how different is CUA model from 4o? (Score: 1)](https://www.reddit.com/r/MachineLearning/comments/1i8pvnb/d_how_to_train_a_model_for_computer_use_how/)**
*   **Summary:** This thread asks about how to train a model for computer use, and if such a model is different from 4o models.
*   **Emotion:** The emotional tone is informative.
*   **Top 1 Point of View:**
    *  A user suggests looking into UI-Tars as an open-source option that does the same thing, while providing the source paper as reference.

**[[D] CVPR review system (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1i88j22/d_cvpr_review_system/)**
*  **Summary:** The discussion is about the CVPR review process, specifically how papers are accepted or rejected.
*   **Emotion:** The emotional tone is neutral, with a factual explanation of the review system.
*  **Top 2 Points of View:**
    *   Users explain that papers are accepted based on their score with a discretion from meta-reviewers.
    *   Users state that CVPR's acceptance rate is 25%. One user suggests only a 5 score would ensure a direct accept.

**[[D] Does ICML De-Anonymize Withdrawn/Rejected Submissions like ICLR? (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1i8n40e/d_does_icml_deanonymize_withdrawnrejected/)**
*  **Summary:** This thread is asking whether ICML de-anonymizes withdrawn or rejected submissions, similar to ICLR's policy.
*   **Emotion:** The emotional tone is neutral and reassuring.
*   **Top 1 Point of View:**
    *   Users mention that in the past years, ICML did not deanonymize names, and predict that this will be the case again this year.

**[[R] arXiv endorsement request for AV research (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1i8prw0/r_arxiv_endorsement_request_for_av_research/)**
*   **Summary:** This thread is requesting an arXiv endorsement.
*   **Emotion:** The emotional tone is neutral.
*   **Top 2 Points of View:**
    *   Users suggest that if an educational email is used, no endorsement is needed.
    *  Another user offers to endorse the paper if the work is acceptable.

**[[D] LLM for categorization (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1i8sawf/d_llm_for_categorization/)**
*   **Summary:** The thread discusses using LLMs for categorization tasks.
*   **Emotion:** The emotional tone is neutral and instructional.
*  **Top 2 Points of View:**
    * One user suggests using an embedding model combined with clustering algorithms for sentimental analysis. Alternatively they offer the use of LLMs with structured outputs to make the process easier for the user.
    *   Another user directs beginners to other subreddits, suggesting this is not an appropriate question for this particular forum.

**[[P] Questions on document handling and privacy in LLM implementation (Score: 0)](https://www.reddit.com/r/MachineLearning/comments/1i8wiww/p_questions_on_document_handling_and_privacy_in/)**
*   **Summary:** This thread is looking for advice on document handling and privacy in LLM implementation.
*   **Emotion:** The emotional tone is neutral and dismissive.
*   **Top 1 Point of View:**
    *  A user explains that this subreddit is meant for those who train ML models, and the post should be directed towards r/LLMDevs or r/PromptEngineering instead.
