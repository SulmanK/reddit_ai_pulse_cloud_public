---
title: "LocalLLaMA Subreddit"
date: "2025-01-24"
description: "Analysis of top discussions and trends in the localllama subreddit"
tags: ["LocalLLaMA", "LLM", "Deepseek", "Ollama", "AI"]
---

# Overall Ranking and Top Discussions
1.  [Ollama is confusing people by pretending that the little distillation models are "R1"](https://www.reddit.com/r/LocalLLaMA/comments/1i8ifxd/ollama_is_confusing_people_by_pretending_that_the/) (Score: 96)
    *   This thread discusses concerns about how Ollama presents distilled models, particularly the DeepSeek R1, and whether it's misleading users.
2.  [SmolVLM 256M: The world's smallest multimodal model, running 100% locally in-browser on WebGPU.](https://v.redd.it/qikrzy8witee1) (Score: 35)
    *   This thread focuses on the announcement and questions regarding the functionality of a small multimodal model called SmolVLM.
3.  [Openai is ahead only till china reverse engineers...](https://i.redd.it/zy8ljay42uee1.png) (Score: 16)
    *   This thread discusses the idea of China potentially reverse-engineering OpenAI's technology and how that might affect the competitive landscape.
4.  [DeepSeek R1 (reasoner) can use internet there o1 still can't](https://www.reddit.com/gallery/1i8hqp0) (Score: 13)
    *   This thread highlights DeepSeek R1's ability to use the internet, a feature not yet available in the o1 model.
5. [I Built an Open-Source RAG API for Docs, GitHub Issues and READMEs](https://www.reddit.com/r/LocalLLaMA/comments/1i8fn1q/i_built_an_opensource_rag_api_for_docs_github/) (Score: 4)
    *   This thread is about a user-built open-source RAG API for documents, GitHub issues, and READMEs.
6.  [Are There Any Uncensored DeepSeek R1 Distilled Models Out There?](https://www.reddit.com/r/LocalLLaMA/comments/1i8eth7/are_there_any_uncensored_deepseek_r1_distilled/) (Score: 3)
    *   This thread is a question about uncensored versions of DeepSeek R1 distilled models and where to find them.
7.  [Thinking Models in PocketPal iOS app](https://www.reddit.com/r/LocalLLaMA/comments/1i8ew4u/thinking_models_in_pocketpal_ios_app/) (Score: 3)
    *   This thread is about using models within the PocketPal iOS app.
8.  [deepseek-r1-distill-qwen-32b benchmark results on LiveBench](https://www.reddit.com/r/LocalLLaMA/comments/1i8k3i3/deepseekr1distillqwen32b_benchmark_results_on/) (Score: 2)
    * This thread shares benchmark results for the deepseek-r1-distill-qwen-32b model and user experiences
9. [Got a Mac M4 Mini 24GB for ollama. How do I increase the unified memory limit for the GPU?](https://www.reddit.com/r/LocalLLaMA/comments/1i8deyc/got_a_mac_m4_mini_24gb_for_ollama_how_do_i/) (Score: 1)
    *  This thread is about how to increase unified memory limit for the GPU on an M4 Mac Mini
10. [when is a model running 'locally"?](https://www.reddit.com/r/LocalLLaMA/comments/1i8fihc/when_is_a_model_running_locally/) (Score: 1)
    *   This thread is a discussion about the technical definition of "running locally" when it comes to large language models.
11. [Prompts are fine through ollama cli but responses are garbage and forgetful in chat box](https://www.reddit.com/r/LocalLLaMA/comments/1i8gi8b/prompts_are_fine_through_ollama_cli_but_responses/) (Score: 1)
    * This thread highlights a user's issue with garbage and forgetful responses using ollama's chat feature.
12. [How to build your own OpenAI operator](https://v.redd.it/mfuvwluwvtee1) (Score: 1)
    * This thread discusses how to build your own OpenAI operator.
13. [Value GPU for Ollama in a home server?](https://www.reddit.com/r/LocalLLaMA/comments/1i8itsg/value_gpu_for_ollama_in_a_home_server/) (Score: 1)
    * This thread is about the value of different GPUs when using Ollama in a home server.
14.  [How can deepseek leap ahead of competition with their open weight models?](https://www.reddit.com/r/LocalLLaMA/comments/1i8e9od/how_can_deepseek_leap_ahead_of_competition_with/) (Score: 0)
    *   This thread is a discussion on how Deepseek might surpass competitors using their open-weight models.
15.  [Local AGI in sight?](https://www.reddit.com/r/LocalLLaMA/comments/1i8fcfu/local_agi_in_sight/) (Score: 0)
    *   This thread questions whether "local AGI" is possible.
16.  [How good is deepseek-r1:32b?](https://www.reddit.com/r/LocalLLaMA/comments/1i8fu56/how_good_is_deepseekr132b/) (Score: 0)
    *   This thread is asking the question of how good is the deepseek-r1:32b model.
17. [Deepseek is good but why it doesnt retain the past convos on a new chat? for example in chat 1 you talk about potatoes and if you starta  new chat they dont remember it.](https://www.reddit.com/r/LocalLLaMA/comments/1i8ivwu/deepseek_is_good_but_why_it_doesnt_retain_the/) (Score: 0)
    *  This thread is a question regarding Deepseek's inability to retain past conversations in new chats.


# Detailed Analysis by Thread
**[Ollama is confusing people by pretending that the little distillation models are "R1" (Score: 96)](https://www.reddit.com/r/LocalLLaMA/comments/1i8ifxd/ollama_is_confusing_people_by_pretending_that_the/)**
*   **Summary:**  The thread's main discussion point is about how Ollama is presenting distilled models, specifically the DeepSeek R1, and whether it's leading to confusion among users. Many feel that the naming conventions are misleading, especially when used by YouTubers and in the Ollama hub. Some feel like the confusion is coming from Deepseek and not Ollama.
*   **Emotion:** The overall tone of the thread is neutral with some negative undertones. While some comments are simply informative, others express frustration and confusion. There are variations in sentiment, ranging from positive statements about liking 1.5b models, negative comments about models not working, and neutral comments about reading model cards.
*   **Top 3 Points of View:**
    *   Ollama is misrepresenting distilled models as the real "R1", leading to confusion.
    *   The issue is with Deepseek itself for using "R1" in the names of the distilled models.
    *  Users need to read model cards more carefully to avoid confusion, with some saying that YouTubers are a different breed and should not be trusted.

**[SmolVLM 256M: The world's smallest multimodal model, running 100% locally in-browser on WebGPU. (Score: 35)](https://v.redd.it/qikrzy8witee1)**
*   **Summary:**  This thread is centered on the announcement of SmolVLM, a tiny multimodal model capable of running locally in a browser. Users are interested in links to the project, how to get started with it on Windows, and whether the model supports Safari.
*   **Emotion:** The thread's overall tone is neutral, with a slight positive inclination due to excitement about the model.
*   **Top 3 Points of View:**
    *   Interest in the model and how to use it.
    *  Curiosity about its compatibility with different operating systems.
    *   Inquiries about its browser support.

**[Openai is ahead only till china reverse engineers... (Score: 16)](https://i.redd.it/zy8ljay42uee1.png)**
*  **Summary:**  This thread revolves around the speculative idea of China reverse engineering OpenAI's technology and how this might change the current AI landscape. The discussion touches on the difficulty of such an endeavor, the potential for espionage, and the current technological divide between OpenAI and others.
*  **Emotion:** The overall emotional tone is neutral. There is a mix of skepticism, curiosity, and mild excitement. One commenter jokingly expresses a desire to use AI to put a million dollars in their bank account.
*  **Top 3 Points of View:**
    *  Skepticism about the feasibility of China quickly reverse-engineering OpenAI's technology.
    *  The possibility of espionage influencing the development of AI, and that China may be using this strategy
    *  Discussion of open-source alternatives and their potential to catch up, and one comment saying that OpenAI spends more time on hiding it's models from others rather than actually training them.

**[DeepSeek R1 (reasoner) can use internet there o1 still can't (Score: 13)](https://www.reddit.com/gallery/1i8hqp0)**
*   **Summary:** This thread highlights a key difference between DeepSeek R1 and o1, namely DeepSeek R1's capability to use the internet. A comment in the thread also mentions that the information provided on the used hardware is not correct.
*   **Emotion:** The thread's tone is neutral with a slight undertone of excitement about the capabilities of the R1 model.
*   **Top 2 Points of View:**
    *   DeepSeek R1 has a clear advantage by being able to utilize internet.
    *   There are inaccuracies in the hardware specs being presented,

**[I Built an Open-Source RAG API for Docs, GitHub Issues and READMEs (Score: 4)](https://www.reddit.com/r/LocalLLaMA/comments/1i8fn1q/i_built_an_opensource_rag_api_for_docs_github/)**
*   **Summary:** This is a post announcing the user has built an open-source RAG API. The only comment is someone saying they are looking for it.
*   **Emotion:**  The emotional tone of the thread is positive, because someone has made something, and others are looking for it.
*   **Top 1 Point of View:**
    * User is looking for the API the OP has created

**[Are There Any Uncensored DeepSeek R1 Distilled Models Out There? (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1i8eth7/are_there_any_uncensored_deepseek_r1_distilled/)**
*   **Summary:** The thread is about a user's search for uncensored DeepSeek R1 models and is providing a link to one.
*  **Emotion:** The overall tone is neutral. There is an undertone of helpfulness with the provided link.
*   **Top 2 Points of View:**
    *   There is a desire for uncensored versions of the DeepSeek R1.
    *  The uncensored models are available, and it's more a prompt issue than the model itself that causes censorship.

**[Thinking Models in PocketPal iOS app (Score: 3)](https://www.reddit.com/r/LocalLLaMA/comments/1i8ew4u/thinking_models_in_pocketpal_ios_app/)**
*   **Summary:**  This thread contains information on using models in the PocketPal iOS app. The discussion includes quantization, adjusting n-prediction settings, and suggestions for template settings.
*   **Emotion:** The thread's overall tone is neutral and informative.
*   **Top 1 Point of View:**
    *   The 7b Q3 quantization on an iPhone 16 seems to work fine, and some suggestions are made for template and settings.

**[deepseek-r1-distill-qwen-32b benchmark results on LiveBench (Score: 2)](https://www.reddit.com/r/LocalLLaMA/comments/1i8k3i3/deepseekr1distillqwen32b_benchmark_results_on/)**
*   **Summary:** This thread shares benchmark results for the deepseek-r1-distill-qwen-32b model on LiveBench. User experiences with code generation are also shared, saying that the benchmark is not accurate for what they have experienced.
*   **Emotion:** The overall tone is mixed. There is frustration and surprise in the fact that the results do not match their experience. There are comments that are positive and optimistic.
*   **Top 3 Points of View:**
    * The model performs worse than expected based on the benchmark results.
    * The model is good at debugging code, and that the models perform differently on different tasks,
    * The model excels at math.

**[Got a Mac M4 Mini 24GB for ollama. How do I increase the unified memory limit for the GPU? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1i8deyc/got_a_mac_m4_mini_24gb_for_ollama_how_do_i/)**
*   **Summary:** The OP is asking how to increase the unified memory limit on his new Mac M4. The user ended up figuring it out by using a `sudo` command.
*   **Emotion:** The tone of the thread is neutral, and informative.
*   **Top 1 Points of View:**
    *   How to increase unified memory limit for the GPU on an M4 Mac Mini.

**[when is a model running 'locally"? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1i8fihc/when_is_a_model_running_locally/)**
*   **Summary:**  The thread is discussing the definition of running a model locally, focusing on aspects like hardware usage, model size, quantization, and the difference between the distilled and original models.
*   **Emotion:**  The tone is neutral and informative.
*   **Top 3 Points of View:**
    *   Running locally means running on your hardware, as opposed to the cloud.
    *  The size of a model, it's quantized size and cache impacts local usage
    *  Distilled models are smaller versions that are trained on larger model's output.

**[Prompts are fine through ollama cli but responses are garbage and forgetful in chat box (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1i8gi8b/prompts_are_fine_through_ollama_cli_but_responses/)**
*   **Summary:** This is a user reporting that the prompts are fine, but the responses are garbage and forgetful. The responses are forgetful because the context window is too small.
*  **Emotion:** The thread is neutral and informative.
*  **Top 1 Points of View:**
    * The reason the model is forgetful is because the context length needs to be increased in the config files.

**[How to build your own OpenAI operator (Score: 1)](https://v.redd.it/mfuvwluwvtee1)**
*   **Summary:** This post is about building your own OpenAI operator, with users pointing out ways to install required packages and how to run the code.
*  **Emotion:** The thread is neutral and informative.
*   **Top 2 Points of View:**
    * This tool uses the ai-gradio package to control browsers.
    * You need python 3.11+ to use this tool.

**[Value GPU for Ollama in a home server? (Score: 1)](https://www.reddit.com/r/LocalLLaMA/comments/1i8itsg/value_gpu_for_ollama_in_a_home_server/)**
*   **Summary:**  The thread discusses the value of different GPUs when using Ollama. The user recommends the 3060 12G and states AMD cards are disappointing.
*   **Emotion:** The tone is mixed, with some positive comments on the recommended 3060, but there is some disappointment in AMD's performance.
*  **Top 1 Points of View:**
    *   The 3060 12G is a good option, while AMD is not as good as NVIDIA for this purpose.

**[How can deepseek leap ahead of competition with their open weight models? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1i8e9od/how_can_deepseek_leap_ahead_of_competition_with/)**
*   **Summary:**  This thread is a discussion on Deepseek's strategies for outpacing competition. It is speculated that they focused on shortcuts, and use synthetic data generated by AI with reinforcement learning.
*   **Emotion:** The tone of the thread is informative and neutral.
*   **Top 3 Points of View:**
    *  Deepseek focused on possible shortcuts.
    *  Deepseek used high quality synthetic data.
    *  Deepseek has a fast-paced development.

**[Local AGI in sight? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1i8fcfu/local_agi_in_sight/)**
*   **Summary:**  The thread is a discussion about whether "local AGI" is currently in sight. The responses range from stating its impossible, to being 20-30 years away, or that we are underestimating human intelligence.
*   **Emotion:**  The tone is skeptical, and neutral.
*   **Top 3 Points of View:**
    *   Local AGI is not even remotely close, because local models can't generalize.
    *   The human mind is essentially a stack of vertically integrated LLMs.
    *    We overestimate the value of human intelligence

**[How good is deepseek-r1:32b? (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1i8fu56/how_good_is_deepseekr132b/)**
*   **Summary:** This thread is about how good the deepseek-r1:32b is, and that it's not good at generating code, but is good at debugging.
*  **Emotion:** The tone is neutral and informative.
*  **Top 2 Points of View:**
    * It is better to use a dedicated code-generation model.
    * The Deepseek-r1 is good at debugging.

**[Deepseek is good but why it doesnt retain the past convos on a new chat? for example in chat 1 you talk about potatoes and if you starta  new chat they dont remember it. (Score: 0)](https://www.reddit.com/r/LocalLLaMA/comments/1i8ivwu/deepseek_is_good_but_why_it_doesnt_retain_the/)**
*   **Summary:**  The thread is about why Deepseek does not remember previous conversations in new chats. The user was answered stating that it is how LLMs work, and that it is not a Deepseek issue.
*   **Emotion:** The tone of the thread is informative and neutral.
*   **Top 1 Points of View:**
    *  The behavior is expected, and is not a problem with Deepseek, but LLMs in general.
